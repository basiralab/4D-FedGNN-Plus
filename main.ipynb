{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18418,"status":"ok","timestamp":1690732511926,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"Gf9dOOYE9-Vn","outputId":"15730d28-4a22-45b0-8f19-2b4ef5a9349a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25480,"status":"ok","timestamp":1690732537401,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"3CI_146YdCYE","outputId":"b72ffcec-5e19-49ee-ab97-6a0268b18ae1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n","Building wheels for collected packages: torch_geometric\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910460 sha256=34762d819e8fedd9a1be98a104d77a3b57ddc29e576aef96448e489fa8fa89a5\n","  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n","Successfully built torch_geometric\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.3.1\n"]}],"source":["!pip install torch_geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690732537402,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"rVif2_Ez33qd","outputId":"c22cfe67-05bd-4fbe-9b70-599d928ab941"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine\n"]}],"source":["import os\n","\n","\n","# Change the current directory to 4D-FedGNN-Plus\n","os.chdir(\"/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine\")\n","\n","# Print the current directory to confirm\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21397,"status":"ok","timestamp":1690732558794,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"7q88jl49dAZx","outputId":"2fd3a870-019a-4ad4-f902-562311c5e4a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["running on GPU\n","running on GPU\n"]}],"source":["from dataset import *\n","from demo import *\n","import main_functions as mf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4ul3h7Whe6m"},"outputs":[],"source":["import torch\n","import copy\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm\n","from torch_geometric.nn import NNConv\n","from torch_geometric.data import Data, InMemoryDataset, DataLoader\n","from torch.nn import Linear, Sequential, ReLU\n","from data_utils import create_edge_index_attribute\n","from torch.nn.parameter import Parameter\n","from torch import mm as mm\n","from torch.nn import Tanh\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","from collections import defaultdict\n","from itertools import combinations"]},{"cell_type":"markdown","metadata":{"id":"SuyWYNaidZie"},"source":["# Load the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRdM-5Xp9-Vp"},"outputs":[],"source":["args = get_args()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJfqLrF4bbv1"},"outputs":[],"source":["import scipy.io\n","\n","data_0 = scipy.io.loadmat('/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Data/data_t0.mat')\n","data_1 = scipy.io.loadmat('/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Data/data_t1.mat')\n","data_2 = scipy.io.loadmat('/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Data/data_t2.mat')\n","\n","data_0 = data_0['data_t0']\n","data_1 = data_1['data_t1']\n","data_2 = data_2['data_t2']\n","\n","X = np.zeros([len(data_0),3,35,35])\n","for idx,(sample_0,sample_1,sample_2) in enumerate(zip(data_0,data_1,data_2)):\n","\n","        matrix_0 = antiVectorize(sample_0,35)\n","        matrix_1 = antiVectorize(sample_1,35)\n","        matrix_2 = antiVectorize(sample_2,35)\n","\n","        X[idx,0,:,:]=matrix_0\n","        X[idx,1,:,:]=matrix_1\n","        X[idx,2,:,:]=matrix_2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzSpEdcy9-Vq"},"outputs":[],"source":["# Convert X to tensor\n","X = torch.from_numpy(X)\n","X = X.type(torch.FloatTensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuN0VG2g9-Vq"},"outputs":[],"source":["# Create table\n","table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","table = random_table(args, 4/8)\n","dataset = copy.deepcopy(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690732560310,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"eG540Mt3cvlt","outputId":"80b0cd8f-aa73-46b6-a520-d0d56f947265"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape:torch.Size([113, 3, 35, 35])\n"]}],"source":["# remove element 88\n","dataset = np.delete(dataset,88,axis=0)\n","# dataset shape\n","print(f'Dataset shape:{dataset.shape}')"]},{"cell_type":"markdown","metadata":{"id":"90ooXFhVd5Ci"},"source":["# Check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERoS0XqAd8Nh"},"outputs":[],"source":["# function to check our results\n","def check(name='4D-FED-GNN++',seed=200,ratio='4_8'):\n","   total_final = []\n","   for h in range(4):\n","      total = np.array([0,0],dtype=np.float32)\n","      for f in range(5):\n","            total += np.load(f\"/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_{seed}/{ratio}/global_test/GNNs/{name}/test_mae_losses/mae_test_loss_hospital_{h}_fold_{f}.npy\")\n","      print(f'Hospital {h}:')\n","      print(total/5)\n","      total_final.append(total/5)\n","\n","   total_final = np.array(total_final)\n","   total_final = total_final.sum(axis=0)/4\n","   print()\n","   print(f'Total sum:{total_final}')\n"]},{"cell_type":"markdown","metadata":{"id":"wq-tRUzrK_Zw"},"source":["# Set up the network architecture and train-validate"]},{"cell_type":"markdown","metadata":{"id":"ljZ0c4g_uu9g"},"source":["**Train-validate-functions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrLLuBr3uu9y"},"outputs":[],"source":["from torch_geometric.utils import dense_to_sparse\n","from torch_geometric.data import Data\n","import networkx as nx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CO9APLTuuu9y"},"outputs":[],"source":["def node_features_from_adj_matrix(adj_matrix,device):\n","\n","      if device.type=='cpu':\n","          # Create a NetworkX graph from the adjacency matrix\n","          G = nx.from_numpy_array(adj_matrix.numpy())\n","      elif device.type=='cuda':\n","          # Create a NetworkX graph from the adjacency matrix\n","          G = nx.from_numpy_array(adj_matrix.detach().cpu().numpy())\n","\n","      # Compute the weighted degree (strength) for each node\n","      strength = dict(G.degree(weight='weight'))\n","\n","      # Compute the degree for each node\n","      degree = dict(G.degree())\n","\n","      # Compute the clustering coefficient for each node\n","      clustering = nx.clustering(G, weight='weight')\n","\n","      # Compute the closeness centrality for each node\n","      closeness_centrality = nx.closeness_centrality(G)\n","\n","      # Compute the PageRank for each node\n","      pagerank = nx.pagerank(G, weight='weight')\n","\n","      # Let's convert these features into numpy arrays so we can stack them together\n","      strength_array = np.array(list(strength.values()))\n","      degree_array = np.array(list(degree.values()))\n","      clustering_array = np.array(list(clustering.values()))\n","      closeness_centrality_array = np.array(list(closeness_centrality.values()))\n","      pagerank_array = np.array(list(pagerank.values()))\n","\n","      # Now we can stack these features together to get a node feature matrix\n","      x = torch.Tensor(np.vstack([strength_array, degree_array, clustering_array, closeness_centrality_array, pagerank_array]).T)\n","      return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTF32DI3uu9z"},"outputs":[],"source":["def adj_matrix_to_pytorch_geometric_data(adj_matrix,device):\n","\n","    # calculate edge_index and edge_weights\n","    edge_indices, edge_weights = dense_to_sparse(adj_matrix)\n","\n","    #edge attributes\n","    edge_attr = torch.cat([edge_indices.T,edge_weights.view(len(edge_weights),1)],1)\n","\n","    # calculate the node features\n","    x = node_features_from_adj_matrix(adj_matrix,device)\n","\n","    data = Data(x=x.to(device),edge_index=edge_indices.to(device),edge_weights=edge_weights.to(device),adj_matrix=adj_matrix.to(device),edge_attr=edge_attr.to(device))\n","\n","    return data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsNWjhnAuu9z"},"outputs":[],"source":["def create_edge_index_attribute_new(adj_matrix):\n","    \"\"\"\n","    Given an adjacency matrix, this function creates the edge index and edge attribute matrix\n","    suitable to graph representation in PyTorch Geometric.\n","    \"\"\"\n","\n","    rows, cols = adj_matrix.shape[0], adj_matrix.shape[1]\n","    edge_index = [[],[]]\n","    edge_attr = []\n","\n","    for i in range(rows):\n","        for j in range(cols):\n","\n","              if adj_matrix[i,j] > 0:\n","\n","                    edge_index[0].append(i)\n","                    edge_index[1].append(j)\n","                    edge_attr.append(adj_matrix[i,j])\n","\n","    return torch.tensor(edge_index), torch.Tensor(edge_attr), rows, cols"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPVn3pNfuu9z"},"outputs":[],"source":["def get_adjacency_matrix(num_nodes, edge_indices, edge_weights):\n","    # Initialize an empty adjacency matrix\n","    adjacency_matrix = np.zeros((num_nodes, num_nodes))\n","\n","    # Fill in the adjacency matrix\n","    for ((node1, node2), weight) in zip(edge_indices, edge_weights):\n","        adjacency_matrix[node1, node2] = weight\n","\n","    return adjacency_matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cn3S3J0puu9z"},"outputs":[],"source":["def train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = False\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_meta(args,device))\n","                  print('GAT')\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    #if h_i ==3:\n","                     # return args,hospital,train_data,table_hospital\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0KtIuGQuu90"},"outputs":[],"source":["def exchange_models_based_on_order_gnns(hospitals, order):\n","    \"\"\"\n","        This function exchanges GNN-layer weights of paired hospitals at timepoint t with each other\n","    \"\"\"\n","    pre_model = None\n","    for i, h_i in enumerate(order):\n","        next_model = copy.deepcopy(hospitals[h_i].model.state_dict())\n","\n","        if not pre_model is None:\n","            hospitals[h_i].model.load_state_dict(pre_model)\n","\n","        pre_model = copy.deepcopy(next_model)\n","\n","        if i == 0:\n","            hospitals[h_i].model.load_state_dict(copy.deepcopy(hospitals[order[-1]].model.state_dict()))\n","\n","    return hospitals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sT4qEvJGuu90"},"outputs":[],"source":["def update_main_by_average_gnns(hospitals):\n","    \"\"\"\n","        This function takes the GNN-layer weights of the GNN at timepoint t and computes the global model by averaging,\n","        then broadcats the weights to the hospitals (updates each GNN with the global model)\n","    \"\"\"\n","    target_state_dict = copy.deepcopy(hospitals[0].model.state_dict())\n","    mux = 1 / len(hospitals)\n","\n","\n","    model_state_dict_list = [copy.deepcopy(hospitals[i].model.state_dict()) for i in range(1, len(hospitals))]\n","\n","\n","    for key in target_state_dict:\n","        if target_state_dict[key].data.dtype == torch.float32:\n","            target_state_dict[key].data = target_state_dict[key].data.clone() * mux\n","            for model_state_dict in model_state_dict_list:\n","                target_state_dict[key].data += mux * model_state_dict[key].data.clone()\n","\n","    for i in range(len(hospitals)):\n","        hospitals[i].model.load_state_dict(target_state_dict)\n","\n","    return hospitals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phwlKsRAuu90"},"outputs":[],"source":["def validate_during_training_gnns(args, hospitals, test_data):\n","    \"\"\"\n","    This function calculates the average MAE of predicted brain graphs during validation.\n","    \"\"\"\n","    mael = torch.nn.L1Loss()\n","\n","    hloss=np.array([np.zeros(args.num_timepoints-1) for i in range(args.num_hospitals)])\n","    for h_i, hospital in enumerate(hospitals):\n","           hospital.model.eval()\n","\n","           with torch.no_grad():\n","\n","\n","                for data in test_data:\n","\n","                    input = data[0]\n","                    mae_loss_hospital = np.zeros(args.num_timepoints-1)\n","\n","                    for t in range(args.num_timepoints-1):\n","\n","                        pred = hospital.model(input)\n","                        hloss[h_i,t] +=  mael(pred, data[t+1])\n","                        input = pred\n","\n","\n","    # Calculate and save the average MAE Loss for each hospital\n","    avg_hloss = hloss/len(test_data)\n","    avg_hloss_mean = np.mean(avg_hloss,axis=1)\n","    return avg_hloss,avg_hloss_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7su6wVRPuu90"},"outputs":[],"source":["def validate_gnns(args, hospitals, test_data,f,verbose=False):\n","    \"\"\"\n","    This function calculates the average MAE of predicted brain graphs during validation.\n","    \"\"\"\n","    mael = torch.nn.L1Loss()\n","\n","    hloss=np.array([np.zeros(args.num_timepoints-1) for i in range(args.num_hospitals)])\n","    for h_i, hospital in enumerate(hospitals):\n","           hospital.model.eval()\n","\n","           with torch.no_grad():\n","\n","\n","                for subject_index,data in enumerate(test_data):\n","\n","                    input = data[0]\n","                    mae_loss_hospital = np.zeros(args.num_timepoints-1)\n","\n","                    for t in range(args.num_timepoints-1):\n","\n","                        pred = hospital.model(input)\n","                        hloss[h_i,t] +=  mael(pred, data[t+1])\n","                        input = pred\n","\n","                        #plot and save the brain graph of patient(sample) i through all the timepoints\n","                        plot_matrix(data[t+1].cpu().detach().numpy(),f'Real Graph, Hospital:{h_i}, Subject:{subject_index}, Timepoint:{t+1}',\n","                                  args.save_path+'real_and_predicted_graphs/'+f'hospital_{h_i}_subject_{subject_index}_timepoint_{t+1}_fold_{f}_real_graph',verbose)\n","                        plot_matrix(pred.cpu().detach().numpy(),f'Predicted Graph, Hospital:{h_i}, Subject:{subject_index}, Timepoint:{t+1}',\n","                                  args.save_path+'real_and_predicted_graphs/'+f'hospital_{h_i}_subject_{subject_index}_timepoint_{t+1}_fold_{f}_predicted_graph',verbose)\n","\n","\n","\n","    # Calculate and save the average MAE Loss for each hospital\n","    avg_hloss = hloss/len(test_data)\n","    for h_i,loss in enumerate(avg_hloss):\n","\n","        # Save the MAE Loss\n","        np.save(args.save_path+f\"test_mae_losses/mae_test_loss_hospital_{h_i}_fold_{f}\", loss)\n","    # Save the loss\n","\n","    print(avg_hloss)\n","    return avg_hloss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7s1e_dY3uu90"},"outputs":[],"source":["class Hospital_gnns():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = RGCN(in_channels=3,hidden_size=32,num_nodes=35,window=1,dropout=0.4,device=device).to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EwPTvSI7uu91"},"outputs":[],"source":["device = torch.device('cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFsR7CK_uu91"},"outputs":[],"source":["def train_one_epoch_gnns(args,hospital,train_data,table_hospital):\n","      # Set the model in training mode\n","      hospital.model.train()\n","\n","      num_timepoints = train_data.shape[1]\n","\n","      #loss types definition\n","      MAE_loss = torch.nn.L1Loss()\n","\n","      # this is our loss for all the data\n","\n","      mae_loss_overall = []\n","\n","      # loop through the data batches\n","      for data_id,data in enumerate(train_data):\n","\n","            # zero the gradients\n","            hospital.optimizer.zero_grad()\n","\n","            num_preds=0 # the number of times that we are able to predict\n","            mae_loss = 0\n","            t=0\n","\n","            # loop through the time dependent adj matrices in the batches\n","            while t < num_timepoints-1:\n","                #print(data_id,t)\n","\n","                # check if the next timepoint is available\n","                if table_hospital[t+1]==1:\n","\n","\n","                    pred = hospital.model(data[t])\n","                    real = data[t+1]\n","\n","                    mae_loss += MAE_loss(pred,real)\n","                    #print(f'MAE LOSS FROM CURRENT PREDICTION:{MAE_loss(pred,real) }')\n","                    num_preds+=1\n","                    t+=1\n","\n","                # if the next timepoint is not available\n","                elif table_hospital[t+1]==0:\n","\n","                    pred = hospital.model(data[t])\n","                    #return pred\n","\n","\n","                    # find the next closes available 1 to use it for a label. We use the pred as input until then\n","                    reached = False\n","                    for t in range(t+1,num_timepoints-1):\n","                        #print(f'Search t:{t}')\n","\n","\n","                        # using the previous prediction as input\n","                        pred = hospital.model(pred)\n","\n","                        # if the next timepoint has data available we break\n","                        if table_hospital[t+1]==1:\n","\n","                                real = data[t+1]\n","                                mae_loss += MAE_loss(pred,real)\n","                                #print(f'MAE LOSS FROM CURRENT PREDICTION:{MAE_loss(pred,real)}')\n","                                num_preds+=1\n","                                t+=1\n","                                reached = True\n","                                break\n","\n","                    # if there are no more 1s in our table, we break from the loop and use whatever losses we accumulated in this loop\n","                    if not reached:\n","                      break\n","\n","                #print()\n","                #print(f'the new t is:{t}')\n","                #print()\n","\n","            #print(mae_loss)\n","            # Calculate the total MAE Loss for the current batch\n","\n","            if num_preds == 0:\n","              return 100,hospital\n","\n","            mae_loss=mae_loss/num_preds\n","            # Append to the total MAE Los\n","            mae_loss_overall.append(mae_loss.item())\n","            #print(f'Num Predictions:{num_preds}')\n","\n","            # Update the weights of the neural network\n","            mae_loss.backward()\n","            hospital.optimizer.step()\n","\n","\n","\n","      mae_loss_overall = np.mean(np.array(mae_loss_overall))\n","      return mae_loss_overall,hospital\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nT9Gm-3Yuu91"},"outputs":[],"source":["def get_order_gnns(table):\n","\n","    hospital_distances=[]\n","    hospital_number_of_points=[]\n","    for row in table:\n","\n","        hospital_number_of_points.append(int(row.sum()))\n","        ones_indexes = np.where(row==1)[0]\n","        distances = np.diff(ones_indexes)-1\n","        distance = distances.sum()\n","        hospital_distances.append(distance)\n","\n","\n","    hospital_number_of_points = np.array(hospital_number_of_points)\n","    hospital_distances = np.array(hospital_distances)\n","    order = np.lexsort((-hospital_distances,hospital_number_of_points))\n","    order = np.flip(order)\n","\n","\n","\n","    return order\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WR_nEeTSuu91"},"outputs":[],"source":["def random_table_gnns(args, ratio):\n","    # Creating the initial table filled with zeros\n","    table = np.zeros((args.num_hospitals, args.num_timepoints))\n","\n","    # Filling out the first column with ones\n","    table[:,0] = 1\n","\n","    # Randomly assigning a one to each row\n","    for i in range(args.num_hospitals):\n","        random_index = np.random.randint(1, args.num_timepoints)\n","        table[i, random_index] = 1\n","\n","    # Getting the indices of the remaining zeros\n","    zero_indices = np.argwhere(table == 0)\n","\n","    # Shuffling the indices\n","    np.random.shuffle(zero_indices)\n","\n","    # Calculating how many of the zeros should be replaced with ones\n","    num_replace = int(len(zero_indices) * ratio)\n","\n","    # Replacing the zeros with ones\n","    for i in range(num_replace):\n","        table[zero_indices[i,0], zero_indices[i,1]] = 1\n","\n","    return table"]},{"cell_type":"markdown","metadata":{"id":"VyNXSEppLzXg"},"source":["## GAT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4-HNGHJNdR2"},"outputs":[],"source":["from torch_geometric.nn import GATConv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aBPSr8uaYfqM"},"outputs":[],"source":["class GAT(torch.nn.Module):\n","    def __init__(self,device):\n","        super(GAT, self).__init__()\n","        self.in_channels = 5\n","        self.hidden_channels = 64\n","        self.heads=8\n","        self.out_head = 1\n","        self.device=device\n","        self.num_nodes = 35\n","\n","\n","        self.conv1 = GATConv(self.in_channels, self.hidden_channels, heads=self.heads, dropout=0.6)\n","        self.conv2 = GATConv(self.hidden_channels*self.heads, self.hidden_channels, concat=False,\n","                             heads=self.out_head, dropout=0.6)\n","        self.fc = nn.Linear(self.hidden_channels,self.num_nodes)\n","\n","\n","    def forward(self, adj_matrix):\n","\n","\n","        data = adj_matrix_to_pytorch_geometric_data(adj_matrix,self.device)\n","        edge_index = data.edge_index\n","        #print(data.x)\n","        x = F.dropout(data.x, p=0.6, training=self.training)\n","        data = self.conv1(x, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.6, training=self.training)\n","        data = self.conv2(data, edge_index)\n","        out = F.relu(self.fc(data))\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AwIFXNEqkcA"},"outputs":[],"source":["class GAT(torch.nn.Module):\n","    def __init__(self,device):\n","        super(GAT, self).__init__()\n","        self.in_channels = 35\n","        self.hidden_channels = 120\n","        self.heads=16\n","        self.out_head = 1\n","        self.device=device\n","        self.num_nodes = 35\n","\n","        self.conv1 = GATConv(self.in_channels, self.hidden_channels, heads=self.heads, dropout=0.6)\n","        self.conv2 = GATConv(self.hidden_channels*self.heads, self.hidden_channels, heads=self.heads, dropout=0.6)\n","        self.conv3 = GATConv(self.hidden_channels*self.heads, self.hidden_channels, concat=False,\n","                             heads=self.out_head, dropout=0.6)\n","        self.fc = nn.Linear(self.hidden_channels,self.num_nodes)\n","\n","    def forward(self, adj_matrix):\n","\n","        data = adj_matrix_to_pytorch_geometric_data(adj_matrix,self.device)\n","        edge_index = data.edge_index\n","\n","        x = F.dropout(data.adj_matrix, p=0.6, training=self.training)\n","        data = self.conv1(x, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.6, training=self.training)\n","        data = self.conv2(data, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.6, training=self.training)\n","        data = self.conv3(data, edge_index)\n","        out = F.relu(self.fc(data))\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fkd33oaDqoyH"},"outputs":[],"source":["torch.use_deterministic_algorithms(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3882,"status":"ok","timestamp":1690630393287,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"fya66Y9ObRLY","outputId":"d1b8087a-a45d-464d-9458-f8f8e4fe47af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([35, 35])"]},"metadata":{},"execution_count":30}],"source":["data = dataset[0][0].to(device)\n","gat = GAT(device).to(device)\n","gat(data).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":796,"status":"ok","timestamp":1690633289440,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"5UzcqQazfuKE","outputId":"21a11255-71cb-4331-f8a3-92395514ab09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 4000115\n","Trainable parameters: 4000115\n"]}],"source":["model = GAT(device)\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8E6iGHO7Neog"},"outputs":[],"source":["device = torch.device('cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgeCfm0Re7NO"},"outputs":[],"source":["class Hospital_gnns():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = GAT(device=device).to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ao8tLxUbia1h"},"outputs":[],"source":["def train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = False\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_gnns(args,device))\n","                  print('GAT')\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUTzn0RprGAO","outputId":"603c7e53-1182-4bc2-ce6d-50377aff29bf","executionInfo":{"status":"ok","timestamp":1690633107963,"user_tz":-60,"elapsed":2706778,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.06554328 0.06549242]\n"," [0.0786098  0.07541355]\n"," [0.07372822 0.08049995]\n"," [0.08754892 0.06277961]]\n","\n","Average MAE Loss:\n","[0.06551785 0.07701167 0.07711408 0.07516426]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06488417 0.06339394]\n"," [0.07860979 0.07541355]\n"," [0.06911475 0.07178723]\n"," [0.08752068 0.06818917]]\n","\n","Average MAE Loss:\n","[0.06413906 0.07701167 0.07045099 0.07785493]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06155229 0.06012526]\n"," [0.0786098  0.07541355]\n"," [0.06904044 0.07044032]\n"," [0.08754892 0.06089133]]\n","\n","Average MAE Loss:\n","[0.06083878 0.07701167 0.06974038 0.07422013]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.05983779 0.06056769]\n"," [0.0786098  0.07541355]\n"," [0.06377345 0.06774707]\n"," [0.08754892 0.060596  ]]\n","\n","Average MAE Loss:\n","[0.06020274 0.07701167 0.06576026 0.07407246]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.06034413 0.06609443]\n"," [0.0786098  0.07541355]\n"," [0.06845908 0.07092353]\n"," [0.08754892 0.06211524]]\n","\n","Average MAE Loss:\n","[0.06321928 0.07701167 0.06969131 0.07483208]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.05694972 0.05575815]\n"," [0.0786098  0.07541355]\n"," [0.0680601  0.07680329]\n"," [0.08754892 0.06264924]]\n","\n","Average MAE Loss:\n","[0.05635393 0.07701167 0.0724317  0.07509908]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.05616511 0.05575048]\n"," [0.07860979 0.07541355]\n"," [0.06788274 0.06850646]\n"," [0.08754892 0.06704082]]\n","\n","Average MAE Loss:\n","[0.0559578  0.07701167 0.0681946  0.07729487]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07858574 0.07207952]\n"," [0.08754892 0.06704082]\n"," [0.05752302 0.05388391]\n"," [0.08754892 0.05963609]]\n","\n","Average MAE Loss:\n","[0.07533263 0.07729487 0.05570347 0.07359251]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07355585 0.07660041]\n"," [0.08754892 0.06704082]\n"," [0.05733379 0.05533952]\n"," [0.08754892 0.05635274]]\n","\n","Average MAE Loss:\n","[0.07507813 0.07729487 0.05633665 0.07195083]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06588901 0.0645887 ]\n"," [0.08754892 0.06704082]\n"," [0.05848734 0.05599837]\n"," [0.08754892 0.05661466]]\n","\n","Average MAE Loss:\n","[0.06523885 0.07729487 0.05724286 0.07208179]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06199157 0.06287565]\n"," [0.08754892 0.06704082]\n"," [0.05955824 0.05589163]\n"," [0.08754892 0.05397195]]\n","\n","Average MAE Loss:\n","[0.06243361 0.07729487 0.05772493 0.07076043]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05824097 0.057972  ]\n"," [0.08754892 0.06704082]\n"," [0.05639018 0.05253502]\n"," [0.08754892 0.05466191]]\n","\n","Average MAE Loss:\n","[0.05810649 0.07729487 0.0544626  0.07110541]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05627896 0.05812849]\n"," [0.08754892 0.06704082]\n"," [0.05646813 0.05827012]\n"," [0.08754892 0.05426872]]\n","\n","Average MAE Loss:\n","[0.05720372 0.07729487 0.05736912 0.07090882]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05545456 0.05513501]\n"," [0.08754892 0.06704082]\n"," [0.05593131 0.05498503]\n"," [0.08754892 0.0547237 ]]\n","\n","Average MAE Loss:\n","[0.05529478 0.07729487 0.05545817 0.07113631]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0535383  0.05144609]\n"," [0.07215432 0.06449971]\n"," [0.05472733 0.05266451]\n"," [0.05444764 0.05237224]]\n","\n","Average MAE Loss:\n","[0.0524922  0.06832701 0.05369592 0.05340994]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05304204 0.05127146]\n"," [0.07215432 0.06449971]\n"," [0.05392252 0.05191499]\n"," [0.05468368 0.05159197]]\n","\n","Average MAE Loss:\n","[0.05215675 0.06832701 0.05291876 0.05313782]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05315968 0.05152957]\n"," [0.07215432 0.06449971]\n"," [0.05350073 0.05158468]\n"," [0.055528   0.05144846]]\n","\n","Average MAE Loss:\n","[0.05234463 0.06832701 0.0525427  0.05348823]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05308757 0.05138314]\n"," [0.07215432 0.06449971]\n"," [0.05446367 0.05187417]\n"," [0.05431642 0.05125127]]\n","\n","Average MAE Loss:\n","[0.05223536 0.06832701 0.05316892 0.05278385]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.05296872 0.05118607]\n"," [0.07215432 0.06449971]\n"," [0.05469079 0.05197061]\n"," [0.05439929 0.05118699]]\n","\n","Average MAE Loss:\n","[0.05207739 0.06832701 0.0533307  0.05279314]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.05297926 0.05126199]\n"," [0.07215432 0.06449971]\n"," [0.05337123 0.05151802]\n"," [0.055204   0.05115579]]\n","\n","Average MAE Loss:\n","[0.05212063 0.06832701 0.05244462 0.0531799 ]\n","\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0529515  0.05113621]\n"," [0.07215432 0.06449971]\n"," [0.05325311 0.05148116]\n"," [0.05515175 0.05101565]]\n","\n","Average MAE Loss:\n","[0.05204386 0.06832701 0.05236714 0.0530837 ]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05346434 0.05189587]\n"," [0.05515175 0.05101565]\n"," [0.05357574 0.05192535]\n"," [0.05352719 0.05148987]]\n","\n","Average MAE Loss:\n","[0.0526801  0.0530837  0.05275055 0.05250853]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.05317147 0.05145008]\n"," [0.05515175 0.05101565]\n"," [0.0541959  0.05205969]\n"," [0.05410627 0.05197242]]\n","\n","Average MAE Loss:\n","[0.05231077 0.0530837  0.05312779 0.05303935]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0529299  0.05108185]\n"," [0.05515175 0.05101565]\n"," [0.05362958 0.0515733 ]\n"," [0.05389054 0.05141956]]\n","\n","Average MAE Loss:\n","[0.05200587 0.0530837  0.05260144 0.05265505]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05299994 0.05127476]\n"," [0.05515175 0.05101565]\n"," [0.05347209 0.05134338]\n"," [0.05411437 0.05133132]]\n","\n","Average MAE Loss:\n","[0.05213735 0.0530837  0.05240773 0.05272284]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.05290505 0.0510686 ]\n"," [0.05515175 0.05101565]\n"," [0.05329816 0.05174927]\n"," [0.05374931 0.05122854]]\n","\n","Average MAE Loss:\n","[0.05198683 0.0530837  0.05252371 0.05248892]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.05285999 0.05107446]\n"," [0.05515175 0.05101565]\n"," [0.0529144  0.05129913]\n"," [0.05364314 0.05130674]]\n","\n","Average MAE Loss:\n","[0.05196722 0.0530837  0.05210676 0.05247494]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.05301134 0.05130309]\n"," [0.05515175 0.05101565]\n"," [0.05298981 0.05119517]\n"," [0.05366772 0.0513377 ]]\n","\n","Average MAE Loss:\n","[0.05215721 0.0530837  0.05209249 0.05250271]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05327371 0.05143204]\n"," [0.0534852  0.05126681]\n"," [0.05345402 0.05180742]\n"," [0.05356275 0.05127374]]\n","\n","Average MAE Loss:\n","[0.05235288 0.05237601 0.05263072 0.05241824]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05326455 0.0514291 ]\n"," [0.0534852  0.05126681]\n"," [0.05321207 0.05143377]\n"," [0.05358856 0.05124303]]\n","\n","Average MAE Loss:\n","[0.05234683 0.05237601 0.05232292 0.05241579]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05313125 0.05132476]\n"," [0.0534852  0.05126681]\n"," [0.05316263 0.05151541]\n"," [0.05366601 0.05121207]]\n","\n","Average MAE Loss:\n","[0.052228   0.05237601 0.05233902 0.05243904]\n","\n","Epoch 00031: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05307569 0.05135224]\n"," [0.0534852  0.05126681]\n"," [0.05308421 0.05113078]\n"," [0.05378568 0.05119023]]\n","\n","Average MAE Loss:\n","[0.05221397 0.05237601 0.0521075  0.05248795]\n","\n","Epoch 00032: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05302946 0.05131234]\n"," [0.0534852  0.05126681]\n"," [0.05309747 0.05148076]\n"," [0.05368562 0.0513495 ]]\n","\n","Average MAE Loss:\n","[0.0521709  0.05237601 0.05228912 0.05251756]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05299994 0.05130224]\n"," [0.0534852  0.05126681]\n"," [0.05295179 0.05149988]\n"," [0.05362316 0.05141121]]\n","\n","Average MAE Loss:\n","[0.05215109 0.05237601 0.05222583 0.05251718]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05297993 0.05125114]\n"," [0.0534852  0.05126681]\n"," [0.05307085 0.05126778]\n"," [0.05363988 0.05138365]]\n","\n","Average MAE Loss:\n","[0.05211553 0.05237601 0.05216931 0.05251177]\n","\n","Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05326034 0.05125268]\n"," [0.05363988 0.05138365]\n"," [0.05288009 0.05135429]\n"," [0.05316551 0.05114731]]\n","\n","Average MAE Loss:\n","[0.05225651 0.05251177 0.05211719 0.05215641]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05325193 0.05139523]\n"," [0.05363988 0.05138365]\n"," [0.05286996 0.05138789]\n"," [0.05345604 0.05127437]]\n","\n","Average MAE Loss:\n","[0.05232358 0.05251177 0.05212893 0.05236521]\n","\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05325273 0.05141916]\n"," [0.05363988 0.05138366]\n"," [0.05286572 0.05141469]\n"," [0.0532755  0.05137932]]\n","\n","Average MAE Loss:\n","[0.05233594 0.05251177 0.0521402  0.05232741]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05323521 0.0513965 ]\n"," [0.05363988 0.05138365]\n"," [0.05284467 0.05139654]\n"," [0.05336285 0.05126044]]\n","\n","Average MAE Loss:\n","[0.05231586 0.05251177 0.05212061 0.05231164]\n","\n","Epoch 00039: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05323991 0.0514128 ]\n"," [0.05363988 0.05138365]\n"," [0.05281521 0.05132371]\n"," [0.05350925 0.05126052]]\n","\n","Average MAE Loss:\n","[0.05232635 0.05251177 0.05206946 0.05238488]\n","\n","Epoch 00040: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05323908 0.0514008 ]\n"," [0.05363988 0.05138365]\n"," [0.0527827  0.05129213]\n"," [0.05338082 0.05136794]]\n","\n","Average MAE Loss:\n","[0.05231994 0.05251177 0.05203741 0.05237438]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05323101 0.05140822]\n"," [0.05363988 0.05138365]\n"," [0.05278367 0.05123264]\n"," [0.0533567  0.05140065]]\n","\n","Average MAE Loss:\n","[0.05231962 0.05251177 0.05200816 0.05237867]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05315274 0.05128604]\n"," [0.0531873  0.05124061]\n"," [0.05314703 0.05133055]\n"," [0.05320362 0.05130153]]\n","\n","Average MAE Loss:\n","[0.05221939 0.05221396 0.05223879 0.05225258]\n","\n","Epoch 00043: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.0531451  0.05131254]\n"," [0.0531873  0.05124061]\n"," [0.05319893 0.05150161]\n"," [0.05324714 0.05146233]]\n","\n","Average MAE Loss:\n","[0.05222882 0.05221396 0.05235027 0.05235474]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0531451  0.05133075]\n"," [0.0531873  0.05124061]\n"," [0.05316572 0.0514937 ]\n"," [0.05325285 0.05144551]]\n","\n","Average MAE Loss:\n","[0.05223792 0.05221396 0.05232971 0.05234918]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05314453 0.0513499 ]\n"," [0.0531873  0.05124061]\n"," [0.05313713 0.051512  ]\n"," [0.05323327 0.05136669]]\n","\n","Average MAE Loss:\n","[0.05224721 0.05221396 0.05232457 0.05229998]\n","\n","Epoch 00046: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05314775 0.05135844]\n"," [0.0531873  0.05124061]\n"," [0.05310412 0.05149962]\n"," [0.05326174 0.05130129]]\n","\n","Average MAE Loss:\n","[0.05225309 0.05221396 0.05230187 0.05228152]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05314898 0.05136488]\n"," [0.0531873  0.05124061]\n"," [0.05310433 0.05156258]\n"," [0.0532637  0.05135949]]\n","\n","Average MAE Loss:\n","[0.05225693 0.05221396 0.05233346 0.0523116 ]\n","\n","Epoch 00048: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05314911 0.05137042]\n"," [0.0531873  0.05124061]\n"," [0.05310182 0.05157543]\n"," [0.05324992 0.05139893]]\n","\n","Average MAE Loss:\n","[0.05225976 0.05221396 0.05233863 0.05232442]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05317333 0.05125273]\n"," [0.05324992 0.05139893]\n"," [0.05314664 0.05140961]\n"," [0.0530426  0.05131544]]\n","\n","Average MAE Loss:\n","[0.05221303 0.05232443 0.05227813 0.05217902]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0531603  0.05126818]\n"," [0.05324992 0.05139893]\n"," [0.0531459  0.05145362]\n"," [0.05303461 0.05117602]]\n","\n","Average MAE Loss:\n","[0.05221424 0.05232443 0.05229976 0.05210531]\n","\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0531553  0.05127526]\n"," [0.05324992 0.05139893]\n"," [0.05314881 0.05147946]\n"," [0.05303441 0.05117096]]\n","\n","Average MAE Loss:\n","[0.05221528 0.05232442 0.05231414 0.05210268]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05315272 0.05128297]\n"," [0.05324992 0.05139893]\n"," [0.05314934 0.05152271]\n"," [0.05304993 0.05119927]]\n","\n","Average MAE Loss:\n","[0.05221784 0.05232442 0.05233603 0.0521246 ]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05315086 0.05128803]\n"," [0.05324992 0.05139893]\n"," [0.05314357 0.05154034]\n"," [0.05302943 0.05118381]]\n","\n","Average MAE Loss:\n","[0.05221944 0.05232442 0.05234196 0.05210662]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05315017 0.05129358]\n"," [0.05324992 0.05139893]\n"," [0.05313811 0.05154729]\n"," [0.05302724 0.05120165]]\n","\n","Average MAE Loss:\n","[0.05222188 0.05232442 0.0523427  0.05211444]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05314952 0.05129627]\n"," [0.05324992 0.05139893]\n"," [0.05313689 0.05155413]\n"," [0.05301608 0.05124261]]\n","\n","Average MAE Loss:\n","[0.0522229  0.05232443 0.05234551 0.05212935]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05308859 0.05133522]\n"," [0.05309041 0.05133547]\n"," [0.05308468 0.05134552]\n"," [0.0530924  0.05134368]]\n","\n","Average MAE Loss:\n","[0.0522119  0.05221294 0.0522151  0.05221804]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05308697 0.05133285]\n"," [0.05309041 0.05133547]\n"," [0.05308512 0.05137528]\n"," [0.05309724 0.05132969]]\n","\n","Average MAE Loss:\n","[0.05220991 0.05221294 0.0522302  0.05221346]\n","\n","Epoch 00058: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05308547 0.0513305 ]\n"," [0.05309041 0.05133547]\n"," [0.05308501 0.05138725]\n"," [0.05310972 0.05132031]]\n","\n","Average MAE Loss:\n","[0.05220799 0.05221294 0.05223613 0.05221502]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00059: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05308467 0.05132906]\n"," [0.05309041 0.05133547]\n"," [0.0530853  0.05139819]\n"," [0.0531128  0.05131583]]\n","\n","Average MAE Loss:\n","[0.05220686 0.05221294 0.05224175 0.05221432]\n","\n","\n","epochs finished with time:528.6316599845886\n","\n","[[0.05308467 0.05132906]\n"," [0.05309041 0.05133547]\n"," [0.0530853  0.05139819]\n"," [0.0531128  0.05131583]]\n","00:09:8.333255414000064\n","------------------------------------Fold [2/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.09233737 0.09797873]\n"," [0.1072953  0.10620574]\n"," [0.09861679 0.09825733]\n"," [0.11803927 0.09421885]]\n","\n","Average MAE Loss:\n","[0.09515805 0.10675052 0.09843706 0.10612906]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.087795   0.09032544]\n"," [0.1072953  0.10620574]\n"," [0.09929941 0.10621307]\n"," [0.11803927 0.09410137]]\n","\n","Average MAE Loss:\n","[0.08906022 0.10675052 0.10275624 0.10607032]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08635981 0.09316585]\n"," [0.1072953  0.10620574]\n"," [0.08994011 0.10216143]\n"," [0.11803927 0.09817752]]\n","\n","Average MAE Loss:\n","[0.08976283 0.10675052 0.09605077 0.1081084 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08360782 0.08701304]\n"," [0.1072953  0.10620574]\n"," [0.09738162 0.10710704]\n"," [0.11803927 0.08944443]]\n","\n","Average MAE Loss:\n","[0.08531043 0.10675052 0.10224433 0.10374185]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08045261 0.08709522]\n"," [0.1072953  0.10620574]\n"," [0.0915925  0.09497571]\n"," [0.11803927 0.09193775]]\n","\n","Average MAE Loss:\n","[0.08377392 0.10675052 0.0932841  0.10498851]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07979423 0.08150229]\n"," [0.1072953  0.10620574]\n"," [0.08844892 0.09387121]\n"," [0.11803927 0.10279112]]\n","\n","Average MAE Loss:\n","[0.08064826 0.10675052 0.09116007 0.1104152 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07864363 0.08165178]\n"," [0.1072953  0.10620574]\n"," [0.08917223 0.09244999]\n"," [0.11803927 0.10299829]]\n","\n","Average MAE Loss:\n","[0.0801477  0.10675052 0.09081111 0.11051878]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0955961  0.09544954]\n"," [0.11803927 0.10299829]\n"," [0.07991346 0.08159397]\n"," [0.11697937 0.08904181]]\n","\n","Average MAE Loss:\n","[0.09552282 0.11051878 0.08075372 0.10301059]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.10183735 0.10399081]\n"," [0.11803927 0.10299829]\n"," [0.08169888 0.0841439 ]\n"," [0.11803927 0.07960293]]\n","\n","Average MAE Loss:\n","[0.10291408 0.11051878 0.08292139 0.0988211 ]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.09125187 0.09624523]\n"," [0.11803927 0.10299829]\n"," [0.079119   0.08029067]\n"," [0.11803927 0.0977788 ]]\n","\n","Average MAE Loss:\n","[0.09374855 0.11051878 0.07970483 0.10790904]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.09425726 0.10031207]\n"," [0.11803927 0.10299829]\n"," [0.0823751  0.0833064 ]\n"," [0.11803927 0.0823835 ]]\n","\n","Average MAE Loss:\n","[0.09728466 0.11051878 0.08284075 0.10021139]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08448868 0.08690202]\n"," [0.11803927 0.10299829]\n"," [0.08088764 0.08000489]\n"," [0.11803927 0.09257368]]\n","\n","Average MAE Loss:\n","[0.08569535 0.11051878 0.08044627 0.10530648]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.08220877 0.08253073]\n"," [0.11803927 0.10299829]\n"," [0.0788112  0.08065796]\n"," [0.11803927 0.09917814]]\n","\n","Average MAE Loss:\n","[0.08236975 0.11051878 0.07973458 0.10860871]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.08161285 0.08196533]\n"," [0.11803927 0.10299829]\n"," [0.07844891 0.08153726]\n"," [0.11803927 0.08956821]]\n","\n","Average MAE Loss:\n","[0.08178909 0.11051878 0.07999308 0.10380374]\n","\n","Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07858715 0.07990978]\n"," [0.1065668  0.09619753]\n"," [0.07889754 0.08008761]\n"," [0.09435508 0.08370612]]\n","\n","Average MAE Loss:\n","[0.07924846 0.10138216 0.07949257 0.0890306 ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07918087 0.08051482]\n"," [0.1065668  0.09619753]\n"," [0.07874186 0.07980587]\n"," [0.08279042 0.07927449]]\n","\n","Average MAE Loss:\n","[0.07984785 0.10138216 0.07927387 0.08103246]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07840388 0.0797769 ]\n"," [0.1065668  0.09619753]\n"," [0.07802914 0.07942805]\n"," [0.08456265 0.07984645]]\n","\n","Average MAE Loss:\n","[0.07909039 0.10138216 0.0787286  0.08220455]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0788581  0.08015462]\n"," [0.1065668  0.09619753]\n"," [0.07803746 0.07939779]\n"," [0.08180161 0.07916469]]\n","\n","Average MAE Loss:\n","[0.07950636 0.10138216 0.07871762 0.08048315]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07920308 0.08040206]\n"," [0.1065668  0.09619753]\n"," [0.07827086 0.07963588]\n"," [0.08532941 0.07949848]]\n","\n","Average MAE Loss:\n","[0.07980257 0.10138216 0.07895337 0.08241394]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07914778 0.08019975]\n"," [0.1065668  0.09619753]\n"," [0.07848317 0.07975354]\n"," [0.08640032 0.07946807]]\n","\n","Average MAE Loss:\n","[0.07967376 0.10138216 0.07911835 0.08293419]\n","\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07847247 0.07933637]\n"," [0.1065668  0.09619753]\n"," [0.07869818 0.08000162]\n"," [0.09017004 0.07966397]]\n","\n","Average MAE Loss:\n","[0.07890442 0.10138216 0.0793499  0.08491701]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0796829  0.0814425 ]\n"," [0.09017004 0.07966397]\n"," [0.07798102 0.0787717 ]\n"," [0.07945644 0.08083193]]\n","\n","Average MAE Loss:\n","[0.0805627  0.08491701 0.07837636 0.08014418]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07822528 0.07965291]\n"," [0.09017004 0.07966397]\n"," [0.07809905 0.07909094]\n"," [0.08145137 0.08227431]]\n","\n","Average MAE Loss:\n","[0.0789391  0.08491701 0.07859499 0.08186284]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07774615 0.07911887]\n"," [0.09017004 0.07966397]\n"," [0.07740165 0.07853363]\n"," [0.08115141 0.08132238]]\n","\n","Average MAE Loss:\n","[0.07843251 0.08491701 0.07796764 0.08123689]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07826601 0.07956312]\n"," [0.09017004 0.07966397]\n"," [0.07748357 0.0785643 ]\n"," [0.07857358 0.07900317]]\n","\n","Average MAE Loss:\n","[0.07891456 0.08491701 0.07802394 0.07878838]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07847049 0.07961048]\n"," [0.09017004 0.07966397]\n"," [0.07775043 0.07872592]\n"," [0.0797821  0.07972038]]\n","\n","Average MAE Loss:\n","[0.07904048 0.08491701 0.07823817 0.07975124]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07927434 0.08017471]\n"," [0.09017004 0.07966397]\n"," [0.07755917 0.07858702]\n"," [0.07891726 0.07913949]]\n","\n","Average MAE Loss:\n","[0.07972453 0.08491701 0.07807309 0.07902838]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07971289 0.08036267]\n"," [0.09017004 0.07966397]\n"," [0.07751039 0.07855949]\n"," [0.07929741 0.07927028]]\n","\n","Average MAE Loss:\n","[0.08003778 0.08491701 0.07803494 0.07928384]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.07763613 0.07804388]\n"," [0.07904401 0.07934771]\n"," [0.0773297  0.07807437]\n"," [0.07841517 0.07890946]]\n","\n","Average MAE Loss:\n","[0.07784    0.07919586 0.07770204 0.07866232]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0780736  0.07837147]\n"," [0.07904401 0.07934771]\n"," [0.07798629 0.07853937]\n"," [0.07760962 0.07828224]]\n","\n","Average MAE Loss:\n","[0.07822254 0.07919586 0.07826283 0.07794593]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.07783002 0.07823914]\n"," [0.07904401 0.07934771]\n"," [0.07747748 0.07828053]\n"," [0.07767344 0.07824622]]\n","\n","Average MAE Loss:\n","[0.07803458 0.07919586 0.07787901 0.07795983]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.0779799  0.07840939]\n"," [0.07904401 0.07934771]\n"," [0.07768121 0.07841723]\n"," [0.07813666 0.07851817]]\n","\n","Average MAE Loss:\n","[0.07819464 0.07919586 0.07804922 0.07832742]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07777398 0.07829516]\n"," [0.07904401 0.07934771]\n"," [0.07734818 0.07823885]\n"," [0.07782773 0.07829441]]\n","\n","Average MAE Loss:\n","[0.07803457 0.07919586 0.07779351 0.07806107]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07764092 0.07819675]\n"," [0.07904401 0.07934771]\n"," [0.07754112 0.07838215]\n"," [0.07790803 0.07825842]]\n","\n","Average MAE Loss:\n","[0.07791884 0.07919586 0.07796164 0.07808323]\n","\n","Epoch 00034: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0777688  0.07830873]\n"," [0.07904401 0.07934771]\n"," [0.07758728 0.07840961]\n"," [0.07805656 0.0783362 ]]\n","\n","Average MAE Loss:\n","[0.07803877 0.07919586 0.07799845 0.07819638]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.07790768 0.0783396 ]\n"," [0.07805656 0.0783362 ]\n"," [0.07755387 0.07811563]\n"," [0.07827901 0.07903924]]\n","\n","Average MAE Loss:\n","[0.07812364 0.07819638 0.07783475 0.07865912]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.07783775 0.07823804]\n"," [0.07805656 0.0783362 ]\n"," [0.07755027 0.07815653]\n"," [0.07706088 0.0779708 ]]\n","\n","Average MAE Loss:\n","[0.0780379  0.07819638 0.0778534  0.07751584]\n","\n","Epoch 00037: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00037: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.07782432 0.07824337]\n"," [0.07805656 0.0783362 ]\n"," [0.07762161 0.07823008]\n"," [0.07760013 0.07841142]]\n","\n","Average MAE Loss:\n","[0.07803385 0.07819638 0.07792585 0.07800578]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.07779903 0.07821948]\n"," [0.07805656 0.0783362 ]\n"," [0.07750937 0.07819219]\n"," [0.07715231 0.07807271]]\n","\n","Average MAE Loss:\n","[0.07800925 0.07819638 0.07785078 0.07761251]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.07775561 0.07818249]\n"," [0.07805656 0.0783362 ]\n"," [0.07746936 0.07819338]\n"," [0.07744942 0.07827988]]\n","\n","Average MAE Loss:\n","[0.07796905 0.07819638 0.07783137 0.07786465]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.07778759 0.07822966]\n"," [0.07805656 0.0783362 ]\n"," [0.07764692 0.07831028]\n"," [0.07729629 0.07813   ]]\n","\n","Average MAE Loss:\n","[0.07800862 0.07819638 0.0779786  0.07771315]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00041: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.07776591 0.07821869]\n"," [0.07805656 0.0783362 ]\n"," [0.07765465 0.07831537]\n"," [0.07731034 0.07817081]]\n","\n","Average MAE Loss:\n","[0.0779923  0.07819638 0.07798501 0.07774057]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.07771145 0.07824187]\n"," [0.0778432  0.07835833]\n"," [0.07770691 0.07826286]\n"," [0.07757319 0.0781632 ]]\n","\n","Average MAE Loss:\n","[0.07797666 0.07810076 0.07798488 0.07786819]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.07762609 0.07815629]\n"," [0.0778432  0.07835833]\n"," [0.07757788 0.07818452]\n"," [0.07734623 0.07804751]]\n","\n","Average MAE Loss:\n","[0.07789119 0.07810076 0.0778812  0.07769687]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.07757909 0.07810218]\n"," [0.0778432  0.07835833]\n"," [0.07752997 0.07816206]\n"," [0.07741511 0.07812954]]\n","\n","Average MAE Loss:\n","[0.07784064 0.07810076 0.07784601 0.07777233]\n","\n","Epoch 00045: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00045: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00045: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.07757466 0.07809004]\n"," [0.0778432  0.07835833]\n"," [0.07751128 0.07815481]\n"," [0.07753789 0.07822998]]\n","\n","Average MAE Loss:\n","[0.07783235 0.07810076 0.07783304 0.07788393]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.07758072 0.07808957]\n"," [0.0778432  0.07835833]\n"," [0.07749656 0.07815265]\n"," [0.07749943 0.07820773]]\n","\n","Average MAE Loss:\n","[0.07783515 0.07810076 0.07782461 0.07785358]\n","\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.07756838 0.0780742 ]\n"," [0.0778432  0.07835833]\n"," [0.07747154 0.07814435]\n"," [0.07743334 0.07817401]]\n","\n","Average MAE Loss:\n","[0.07782129 0.07810076 0.07780795 0.07780367]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.07756801 0.07806727]\n"," [0.0778432  0.07835833]\n"," [0.07744805 0.07813451]\n"," [0.07747906 0.07820211]]\n","\n","Average MAE Loss:\n","[0.07781764 0.07810076 0.07779128 0.07784059]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00049: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.07779409 0.07831206]\n"," [0.07747906 0.07820211]\n"," [0.07755133 0.07805791]\n"," [0.07753516 0.07820199]]\n","\n","Average MAE Loss:\n","[0.07805307 0.07784059 0.07780462 0.07786858]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.07772178 0.07824295]\n"," [0.07747906 0.07820211]\n"," [0.07752003 0.07804133]\n"," [0.07757396 0.07824735]]\n","\n","Average MAE Loss:\n","[0.07798237 0.07784059 0.07778068 0.07791065]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.07767112 0.07818833]\n"," [0.07747906 0.07820211]\n"," [0.07749211 0.07802637]\n"," [0.07750136 0.07820659]]\n","\n","Average MAE Loss:\n","[0.07792973 0.07784059 0.07775924 0.07785397]\n","\n","Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.07765326 0.0781673 ]\n"," [0.07747906 0.07820211]\n"," [0.0774623  0.07801092]\n"," [0.07744764 0.07818168]]\n","\n","Average MAE Loss:\n","[0.07791028 0.07784059 0.07773661 0.07781466]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00053: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.07765329 0.07816118]\n"," [0.07747906 0.0782021 ]\n"," [0.07745396 0.07800749]\n"," [0.07744573 0.07818062]]\n","\n","Average MAE Loss:\n","[0.07790723 0.07784058 0.07773072 0.07781318]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.07764627 0.07814932]\n"," [0.07747906 0.07820211]\n"," [0.07745141 0.07800794]\n"," [0.07739585 0.07814862]]\n","\n","Average MAE Loss:\n","[0.07789779 0.07784059 0.07772968 0.07777224]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.07763442 0.07813475]\n"," [0.07747906 0.07820211]\n"," [0.07745265 0.07801205]\n"," [0.07738813 0.07814502]]\n","\n","Average MAE Loss:\n","[0.07788459 0.07784059 0.07773235 0.07776657]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.07748256 0.07810599]\n"," [0.07747793 0.07810733]\n"," [0.07747011 0.07810448]\n"," [0.07746599 0.07810568]]\n","\n","Average MAE Loss:\n","[0.07779427 0.07779263 0.07778729 0.07778584]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00057: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.07749316 0.07810801]\n"," [0.07747793 0.07810733]\n"," [0.07746297 0.07810022]\n"," [0.07746179 0.07810544]]\n","\n","Average MAE Loss:\n","[0.07780058 0.07779263 0.07778159 0.07778361]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.07750232 0.07810979]\n"," [0.07747793 0.07810733]\n"," [0.07745571 0.0780961 ]\n"," [0.07745553 0.07810314]]\n","\n","Average MAE Loss:\n","[0.07780605 0.07779263 0.0777759  0.07777934]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.07750912 0.07811029]\n"," [0.07747793 0.07810733]\n"," [0.07745006 0.07809277]\n"," [0.07745984 0.07810577]]\n","\n","Average MAE Loss:\n","[0.07780971 0.07779263 0.07777141 0.0777828 ]\n","\n","\n","epochs finished with time:529.4865312576294\n","\n","[[0.07750912 0.07811029]\n"," [0.07747793 0.07810733]\n"," [0.07745006 0.07809277]\n"," [0.07745984 0.07810577]]\n","00:09:9.066398427000081\n","------------------------------------Fold [3/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07264132 0.06575746]\n"," [0.09104088 0.08139092]\n"," [0.07958802 0.07977618]\n"," [0.1015135  0.06915464]]\n","\n","Average MAE Loss:\n","[0.06919939 0.0862159  0.0796821  0.08533407]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07016549 0.06861556]\n"," [0.09104088 0.08139092]\n"," [0.07918477 0.08497272]\n"," [0.1015135  0.06562588]]\n","\n","Average MAE Loss:\n","[0.06939053 0.0862159  0.08207874 0.08356969]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06655025 0.06053178]\n"," [0.09104088 0.08139092]\n"," [0.07666926 0.07109763]\n"," [0.1015135  0.06414844]]\n","\n","Average MAE Loss:\n","[0.06354102 0.0862159  0.07388344 0.08283097]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.06498945 0.06091007]\n"," [0.09104088 0.08139092]\n"," [0.07513639 0.07001984]\n"," [0.1015135  0.06455813]]\n","\n","Average MAE Loss:\n","[0.06294976 0.0862159  0.07257812 0.08303582]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0669916  0.05905962]\n"," [0.09104088 0.08139092]\n"," [0.07366508 0.07357483]\n"," [0.10038206 0.06490992]]\n","\n","Average MAE Loss:\n","[0.06302561 0.0862159  0.07361995 0.08264599]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.06594987 0.05925158]\n"," [0.09104088 0.08139092]\n"," [0.0732349  0.06999107]\n"," [0.10145931 0.06164931]]\n","\n","Average MAE Loss:\n","[0.06260073 0.0862159  0.07161299 0.08155431]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.06317212 0.05767567]\n"," [0.09104088 0.08139092]\n"," [0.07434443 0.07024884]\n"," [0.10125498 0.06348535]]\n","\n","Average MAE Loss:\n","[0.0604239  0.0862159  0.07229663 0.08237016]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07598795 0.07746652]\n"," [0.10125498 0.06348535]\n"," [0.06558248 0.06515265]\n"," [0.10098456 0.07265771]]\n","\n","Average MAE Loss:\n","[0.07672723 0.08237016 0.06536756 0.08682114]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06868296 0.0634577 ]\n"," [0.10125498 0.06348535]\n"," [0.06623328 0.06107317]\n"," [0.1015135  0.06239903]]\n","\n","Average MAE Loss:\n","[0.06607033 0.08237016 0.06365322 0.08195627]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07808527 0.07272655]\n"," [0.10125498 0.06348535]\n"," [0.06479123 0.06244483]\n"," [0.1015135  0.0609123 ]]\n","\n","Average MAE Loss:\n","[0.07540591 0.08237016 0.06361803 0.0812129 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07209138 0.06626388]\n"," [0.10125498 0.06348535]\n"," [0.06423168 0.05767955]\n"," [0.1015135  0.05957712]]\n","\n","Average MAE Loss:\n","[0.06917763 0.08237016 0.06095561 0.08054531]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.06731482 0.06139383]\n"," [0.10125498 0.06348535]\n"," [0.0653352  0.06727525]\n"," [0.10087004 0.05935998]]\n","\n","Average MAE Loss:\n","[0.06435433 0.08237016 0.06630522 0.08011501]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06358925 0.05850294]\n"," [0.10125498 0.06348535]\n"," [0.06439669 0.0585103 ]\n"," [0.10139348 0.06565033]]\n","\n","Average MAE Loss:\n","[0.06104609 0.08237016 0.0614535  0.08352191]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06361662 0.05801046]\n"," [0.10125498 0.06348535]\n"," [0.06446517 0.05999138]\n"," [0.1015135  0.05981584]]\n","\n","Average MAE Loss:\n","[0.06081354 0.08237016 0.06222827 0.08066467]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06409447 0.05723568]\n"," [0.08515687 0.06991691]\n"," [0.06586179 0.05820066]\n"," [0.06470128 0.05684377]]\n","\n","Average MAE Loss:\n","[0.06066508 0.07753689 0.06203122 0.06077253]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06364293 0.05640022]\n"," [0.08515687 0.06991691]\n"," [0.06287144 0.05549888]\n"," [0.06335721 0.05530327]]\n","\n","Average MAE Loss:\n","[0.06002158 0.07753689 0.05918516 0.05933024]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06316923 0.05615551]\n"," [0.08515687 0.06991691]\n"," [0.06243351 0.05512005]\n"," [0.06517366 0.05522528]]\n","\n","Average MAE Loss:\n","[0.05966237 0.07753689 0.05877678 0.06019947]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06262344 0.05546241]\n"," [0.08515687 0.06991691]\n"," [0.06239254 0.05528712]\n"," [0.06715544 0.0553153 ]]\n","\n","Average MAE Loss:\n","[0.05904292 0.07753689 0.05883983 0.06123537]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06237861 0.05534028]\n"," [0.08515687 0.06991691]\n"," [0.0627578  0.05518458]\n"," [0.06491835 0.05527114]]\n","\n","Average MAE Loss:\n","[0.05885944 0.07753689 0.05897119 0.06009475]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06262422 0.05547813]\n"," [0.08515687 0.06991691]\n"," [0.06231409 0.05524628]\n"," [0.06736515 0.05502544]]\n","\n","Average MAE Loss:\n","[0.05905117 0.07753689 0.05878019 0.0611953 ]\n","\n","Epoch 00020: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06252832 0.05540179]\n"," [0.08515687 0.06991691]\n"," [0.06250026 0.05512298]\n"," [0.06593804 0.05501821]]\n","\n","Average MAE Loss:\n","[0.05896506 0.07753689 0.05881162 0.06047812]\n","\n","Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06449167 0.05807269]\n"," [0.06593804 0.05501821]\n"," [0.06271794 0.05541988]\n"," [0.06228891 0.05488854]]\n","\n","Average MAE Loss:\n","[0.06128218 0.06047812 0.05906891 0.05858873]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06488661 0.05763736]\n"," [0.06593804 0.05501821]\n"," [0.06312425 0.05561662]\n"," [0.06272803 0.05528766]]\n","\n","Average MAE Loss:\n","[0.06126199 0.06047812 0.05937044 0.05900784]\n","\n","Epoch 00023: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06229278 0.05546167]\n"," [0.06593804 0.05501821]\n"," [0.06232399 0.05518502]\n"," [0.06334078 0.05557599]]\n","\n","Average MAE Loss:\n","[0.05887722 0.06047812 0.0587545  0.05945838]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0625792  0.05546912]\n"," [0.06593804 0.05501822]\n"," [0.06237411 0.05507766]\n"," [0.06339639 0.05552361]]\n","\n","Average MAE Loss:\n","[0.05902416 0.06047813 0.05872589 0.05946   ]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06237903 0.05532643]\n"," [0.06593804 0.05501821]\n"," [0.06243773 0.05506853]\n"," [0.06326108 0.05542377]]\n","\n","Average MAE Loss:\n","[0.05885273 0.06047812 0.05875313 0.05934243]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06233577 0.05529992]\n"," [0.06593804 0.05501822]\n"," [0.06220777 0.05511977]\n"," [0.06270396 0.05551546]]\n","\n","Average MAE Loss:\n","[0.05881785 0.06047813 0.05866377 0.05910971]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06263568 0.05552658]\n"," [0.06593804 0.05501821]\n"," [0.06259266 0.0550722 ]\n"," [0.06275138 0.05532581]]\n","\n","Average MAE Loss:\n","[0.05908113 0.06047812 0.05883243 0.0590386 ]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06248229 0.05519588]\n"," [0.06325654 0.05541573]\n"," [0.06229702 0.05558001]\n"," [0.06289865 0.05520038]]\n","\n","Average MAE Loss:\n","[0.05883909 0.05933613 0.05893851 0.05904952]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06240041 0.05524472]\n"," [0.06325654 0.05541573]\n"," [0.06246483 0.05533604]\n"," [0.0626883  0.05521876]]\n","\n","Average MAE Loss:\n","[0.05882256 0.05933613 0.05890043 0.05895353]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.0623747  0.05531412]\n"," [0.06325654 0.05541573]\n"," [0.06245709 0.0552606 ]\n"," [0.06282747 0.05523813]]\n","\n","Average MAE Loss:\n","[0.05884441 0.05933613 0.05885885 0.0590328 ]\n","\n","Epoch 00031: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00031: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06235503 0.05531922]\n"," [0.06325654 0.05541573]\n"," [0.06236072 0.05527653]\n"," [0.06266325 0.05529522]]\n","\n","Average MAE Loss:\n","[0.05883713 0.05933613 0.05881863 0.05897924]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06236088 0.05532684]\n"," [0.06325654 0.05541573]\n"," [0.06228253 0.05530097]\n"," [0.06287698 0.055252  ]]\n","\n","Average MAE Loss:\n","[0.05884386 0.05933613 0.05879175 0.05906449]\n","\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06234268 0.05533566]\n"," [0.06325654 0.05541573]\n"," [0.06236122 0.05521118]\n"," [0.06272295 0.05531792]]\n","\n","Average MAE Loss:\n","[0.05883917 0.05933613 0.0587862  0.05902043]\n","\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06233796 0.05533701]\n"," [0.06325654 0.05541573]\n"," [0.06228359 0.05531551]\n"," [0.06273229 0.05531881]]\n","\n","Average MAE Loss:\n","[0.05883748 0.05933613 0.05879955 0.05902555]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06296115 0.05527705]\n"," [0.06273229 0.05531881]\n"," [0.06226818 0.05535669]\n"," [0.06246197 0.05508952]]\n","\n","Average MAE Loss:\n","[0.0591191  0.05902555 0.05881243 0.05877574]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06266904 0.05519786]\n"," [0.06273229 0.05531881]\n"," [0.0622316  0.05539992]\n"," [0.06258259 0.05507121]]\n","\n","Average MAE Loss:\n","[0.05893345 0.05902555 0.05881576 0.0588269 ]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06253792 0.05519737]\n"," [0.06273229 0.05531881]\n"," [0.06226986 0.0553764 ]\n"," [0.06249142 0.05508658]]\n","\n","Average MAE Loss:\n","[0.05886764 0.05902555 0.05882313 0.058789  ]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06249865 0.05520804]\n"," [0.06273229 0.05531881]\n"," [0.06225558 0.05541348]\n"," [0.06247322 0.05509073]]\n","\n","Average MAE Loss:\n","[0.05885335 0.05902555 0.05883453 0.05878197]\n","\n","Epoch 00039: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00039: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06248344 0.05521525]\n"," [0.06273229 0.05531881]\n"," [0.06224412 0.0554452 ]\n"," [0.06254916 0.05508821]]\n","\n","Average MAE Loss:\n","[0.05884935 0.05902555 0.05884466 0.05881868]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.0624785  0.05522726]\n"," [0.06273229 0.05531881]\n"," [0.06224651 0.05544394]\n"," [0.06259892 0.05509473]]\n","\n","Average MAE Loss:\n","[0.05885288 0.05902555 0.05884523 0.05884683]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06247094 0.05523326]\n"," [0.06273229 0.05531881]\n"," [0.06226388 0.05541349]\n"," [0.06251573 0.05510852]]\n","\n","Average MAE Loss:\n","[0.0588521  0.05902555 0.05883869 0.05881213]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06247377 0.05518118]\n"," [0.06246572 0.05519347]\n"," [0.06238668 0.05523716]\n"," [0.06246908 0.0551975 ]]\n","\n","Average MAE Loss:\n","[0.05882747 0.0588296  0.05881192 0.05883329]\n","\n","Epoch 00043: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00043: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06246834 0.05517587]\n"," [0.06246572 0.05519347]\n"," [0.06236148 0.05526416]\n"," [0.0624878  0.05519589]]\n","\n","Average MAE Loss:\n","[0.0588221  0.0588296  0.05881282 0.05884185]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06246112 0.05517478]\n"," [0.06246572 0.05519347]\n"," [0.06235082 0.05528694]\n"," [0.06247181 0.05520599]]\n","\n","Average MAE Loss:\n","[0.05881795 0.0588296  0.05881888 0.0588389 ]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06246296 0.05517225]\n"," [0.06246572 0.05519347]\n"," [0.06234941 0.05529857]\n"," [0.06246563 0.05521908]]\n","\n","Average MAE Loss:\n","[0.05881761 0.0588296  0.05882399 0.05884236]\n","\n","Epoch 00046: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06245697 0.0551728 ]\n"," [0.06246572 0.05519347]\n"," [0.06233604 0.05532036]\n"," [0.06247136 0.0552191 ]]\n","\n","Average MAE Loss:\n","[0.05881489 0.0588296  0.0588282  0.05884523]\n","\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06244996 0.05517337]\n"," [0.06246572 0.05519347]\n"," [0.06232727 0.05533616]\n"," [0.06247649 0.05521528]]\n","\n","Average MAE Loss:\n","[0.05881166 0.0588296  0.05883172 0.05884589]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06244356 0.05517557]\n"," [0.06246572 0.05519347]\n"," [0.06232477 0.05534927]\n"," [0.06248498 0.05521074]]\n","\n","Average MAE Loss:\n","[0.05880957 0.0588296  0.05883702 0.05884786]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06246502 0.05519137]\n"," [0.06248498 0.05521074]\n"," [0.06240779 0.05518486]\n"," [0.06235839 0.05529056]]\n","\n","Average MAE Loss:\n","[0.05882819 0.05884786 0.05879633 0.05882448]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06246401 0.05518738]\n"," [0.06248498 0.05521074]\n"," [0.06236687 0.05520288]\n"," [0.06238709 0.05525497]]\n","\n","Average MAE Loss:\n","[0.0588257  0.05884786 0.05878487 0.05882103]\n","\n","Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0624615  0.05518515]\n"," [0.06248498 0.05521074]\n"," [0.06235157 0.05521182]\n"," [0.06240758 0.05523425]]\n","\n","Average MAE Loss:\n","[0.05882332 0.05884786 0.05878169 0.05882091]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06246341 0.05518363]\n"," [0.06248498 0.05521074]\n"," [0.06233884 0.05522015]\n"," [0.06242532 0.05521859]]\n","\n","Average MAE Loss:\n","[0.05882352 0.05884786 0.05877949 0.05882196]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06246506 0.0551829 ]\n"," [0.06248498 0.05521074]\n"," [0.06233048 0.05522759]\n"," [0.06243753 0.05520864]]\n","\n","Average MAE Loss:\n","[0.05882398 0.05884786 0.05877903 0.05882309]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06246521 0.05518234]\n"," [0.06248498 0.05521074]\n"," [0.06232536 0.05523357]\n"," [0.06244253 0.05520317]]\n","\n","Average MAE Loss:\n","[0.05882377 0.05884786 0.05877947 0.05882285]\n","\n","Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06246564 0.05518115]\n"," [0.06248498 0.05521074]\n"," [0.06232508 0.05523557]\n"," [0.06244765 0.05519802]]\n","\n","Average MAE Loss:\n","[0.05882339 0.05884786 0.05878033 0.05882283]\n","\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06242622 0.05519475]\n"," [0.06242453 0.05519685]\n"," [0.06242014 0.0551999 ]\n"," [0.06242897 0.05519514]]\n","\n","Average MAE Loss:\n","[0.05881048 0.05881069 0.05881002 0.05881205]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06242725 0.05519295]\n"," [0.06242453 0.05519685]\n"," [0.06241458 0.05520406]\n"," [0.06243246 0.05519378]]\n","\n","Average MAE Loss:\n","[0.0588101  0.05881069 0.05880932 0.05881312]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06242779 0.05519126]\n"," [0.06242453 0.05519685]\n"," [0.06240691 0.05520865]\n"," [0.06243432 0.05519267]]\n","\n","Average MAE Loss:\n","[0.05880952 0.05881069 0.05880778 0.05881349]\n","\n","Epoch 00059: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06242813 0.05518998]\n"," [0.06242453 0.05519685]\n"," [0.06240176 0.05521151]\n"," [0.06243457 0.05519236]]\n","\n","Average MAE Loss:\n","[0.05880906 0.05881069 0.05880664 0.05881346]\n","\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","\n","epochs finished with time:511.6850554943085\n","\n","[[0.06242813 0.05518998]\n"," [0.06242453 0.05519685]\n"," [0.06240176 0.05521151]\n"," [0.06243457 0.05519236]]\n","00:08:47.40592096399996\n","------------------------------------Fold [4/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.0739117  0.09698736]\n"," [0.07516487 0.09693794]\n"," [0.07028911 0.09205604]\n"," [0.08410832 0.08128686]]\n","\n","Average MAE Loss:\n","[0.08544953 0.0860514  0.08117257 0.08269759]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06347818 0.08714736]\n"," [0.07516487 0.09693794]\n"," [0.06443725 0.08888495]\n"," [0.08410832 0.08238509]]\n","\n","Average MAE Loss:\n","[0.07531277 0.0860514  0.0766611  0.0832467 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.05855922 0.07984275]\n"," [0.07516487 0.09693794]\n"," [0.06700489 0.08736797]\n"," [0.08298293 0.08139657]]\n","\n","Average MAE Loss:\n","[0.06920098 0.0860514  0.07718643 0.08218975]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.05555392 0.07432481]\n"," [0.07516487 0.09693794]\n"," [0.06176891 0.08300257]\n"," [0.08410832 0.08633368]]\n","\n","Average MAE Loss:\n","[0.06493936 0.0860514  0.07238574 0.085221  ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0567278  0.07381052]\n"," [0.07516487 0.09693794]\n"," [0.06113722 0.08200012]\n"," [0.08410832 0.08015744]]\n","\n","Average MAE Loss:\n","[0.06526916 0.0860514  0.07156867 0.08213288]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.0553426  0.07119023]\n"," [0.07516487 0.09693794]\n"," [0.0639656  0.08511586]\n"," [0.08363249 0.08088467]]\n","\n","Average MAE Loss:\n","[0.06326641 0.0860514  0.07454073 0.08225858]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.05249881 0.07143424]\n"," [0.07516487 0.09693794]\n"," [0.05805893 0.07576557]\n"," [0.08410832 0.07854971]]\n","\n","Average MAE Loss:\n","[0.06196653 0.0860514  0.06691225 0.08132902]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0628849  0.08876817]\n"," [0.08410832 0.07854971]\n"," [0.05392491 0.07012071]\n"," [0.08394922 0.08188152]]\n","\n","Average MAE Loss:\n","[0.07582654 0.08132902 0.06202281 0.08291537]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05873173 0.08163565]\n"," [0.08410832 0.07854971]\n"," [0.05440049 0.07291216]\n"," [0.08410832 0.07490057]]\n","\n","Average MAE Loss:\n","[0.07018369 0.08132902 0.06365632 0.07950445]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05761489 0.07423813]\n"," [0.08410832 0.07854971]\n"," [0.05447341 0.07245374]\n"," [0.08410832 0.07610254]]\n","\n","Average MAE Loss:\n","[0.06592651 0.08132902 0.06346357 0.08010543]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05651768 0.07277058]\n"," [0.08410832 0.07854971]\n"," [0.05459501 0.07400875]\n"," [0.08410832 0.07161535]]\n","\n","Average MAE Loss:\n","[0.06464413 0.08132902 0.06430188 0.07786183]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05406938 0.07079156]\n"," [0.08410832 0.07854971]\n"," [0.05248729 0.07139196]\n"," [0.085751   0.08895898]]\n","\n","Average MAE Loss:\n","[0.06243047 0.08132902 0.06193962 0.08735499]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05415045 0.07227687]\n"," [0.08410832 0.07854971]\n"," [0.05431165 0.07353631]\n"," [0.08410832 0.07151399]]\n","\n","Average MAE Loss:\n","[0.06321366 0.08132902 0.06392398 0.07781116]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05394607 0.0717473 ]\n"," [0.08410832 0.07854971]\n"," [0.05195532 0.06900969]\n"," [0.08340271 0.08175495]]\n","\n","Average MAE Loss:\n","[0.06284669 0.08132902 0.06048251 0.08257883]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05171422 0.07194907]\n"," [0.0692514  0.08562203]\n"," [0.05229083 0.07032593]\n"," [0.05271243 0.07046122]]\n","\n","Average MAE Loss:\n","[0.06183165 0.07743672 0.06130838 0.06158683]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05131232 0.06981895]\n"," [0.0692514  0.08562203]\n"," [0.05155763 0.06905698]\n"," [0.05177626 0.06988407]]\n","\n","Average MAE Loss:\n","[0.06056564 0.07743672 0.06030731 0.06083016]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0511414  0.06957585]\n"," [0.0692514  0.08562203]\n"," [0.05137819 0.0690585 ]\n"," [0.05211148 0.06964177]]\n","\n","Average MAE Loss:\n","[0.06035862 0.07743672 0.06021834 0.06087662]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05102664 0.06949732]\n"," [0.0692514  0.08562203]\n"," [0.05194509 0.06925293]\n"," [0.05173348 0.06919203]]\n","\n","Average MAE Loss:\n","[0.06026198 0.07743672 0.06059901 0.06046275]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.05109324 0.06942374]\n"," [0.0692514  0.08562203]\n"," [0.05143443 0.06931765]\n"," [0.05209288 0.06884943]]\n","\n","Average MAE Loss:\n","[0.06025849 0.07743672 0.06037604 0.06047115]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.05112285 0.0696483 ]\n"," [0.0692514  0.08562203]\n"," [0.05104969 0.06863499]\n"," [0.05166058 0.06898198]]\n","\n","Average MAE Loss:\n","[0.06038557 0.07743672 0.05984234 0.06032128]\n","\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.05106964 0.06953044]\n"," [0.0692514  0.08562203]\n"," [0.0511164  0.06932195]\n"," [0.05209484 0.06882826]]\n","\n","Average MAE Loss:\n","[0.06030004 0.07743672 0.06021917 0.06046155]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05166344 0.07177122]\n"," [0.05209484 0.06882826]\n"," [0.0512127  0.06948166]\n"," [0.05107866 0.07010591]]\n","\n","Average MAE Loss:\n","[0.06171733 0.06046155 0.06034718 0.06059229]\n","\n","Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.05119717 0.06935957]\n"," [0.05209484 0.06882826]\n"," [0.05099691 0.06912284]\n"," [0.05140144 0.0699988 ]]\n","\n","Average MAE Loss:\n","[0.06027837 0.06046155 0.06005988 0.06070012]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05123442 0.06871463]\n"," [0.05209484 0.06882826]\n"," [0.05092792 0.06919397]\n"," [0.05090035 0.06949398]]\n","\n","Average MAE Loss:\n","[0.05997452 0.06046155 0.06006094 0.06019717]\n","\n","Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05111869 0.0688767 ]\n"," [0.05209484 0.06882826]\n"," [0.0511089  0.07008798]\n"," [0.05127767 0.06925563]]\n","\n","Average MAE Loss:\n","[0.0599977  0.06046155 0.06059844 0.06026665]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.05127504 0.06856885]\n"," [0.05209484 0.06882826]\n"," [0.05103466 0.07000682]\n"," [0.05125513 0.06981959]]\n","\n","Average MAE Loss:\n","[0.05992195 0.06046155 0.06052074 0.06053736]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.05110251 0.0694287 ]\n"," [0.05209484 0.06882826]\n"," [0.0511945  0.07022555]\n"," [0.05116285 0.06898654]]\n","\n","Average MAE Loss:\n","[0.06026561 0.06046155 0.06071002 0.06007469]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0511332  0.06907786]\n"," [0.05209484 0.06882826]\n"," [0.05077174 0.06968514]\n"," [0.05127602 0.06900333]]\n","\n","Average MAE Loss:\n","[0.06010553 0.06046155 0.06022844 0.06013968]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05131683 0.06893483]\n"," [0.05103173 0.06955279]\n"," [0.05125118 0.06905638]\n"," [0.05122065 0.06982044]]\n","\n","Average MAE Loss:\n","[0.06012583 0.06029226 0.06015378 0.06052054]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05117051 0.06889345]\n"," [0.05103173 0.06955279]\n"," [0.05101332 0.06922209]\n"," [0.05125779 0.06974097]]\n","\n","Average MAE Loss:\n","[0.06003198 0.06029226 0.06011771 0.06049938]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05120697 0.06885708]\n"," [0.05103173 0.06955279]\n"," [0.05114491 0.0692711 ]\n"," [0.05119217 0.06917636]]\n","\n","Average MAE Loss:\n","[0.06003203 0.06029226 0.060208   0.06018426]\n","\n","Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05119515 0.06885048]\n"," [0.05103173 0.06955279]\n"," [0.05095017 0.06935821]\n"," [0.05124795 0.06908721]]\n","\n","Average MAE Loss:\n","[0.06002282 0.06029226 0.06015419 0.06016758]\n","\n","Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05118258 0.06886509]\n"," [0.05103173 0.06955279]\n"," [0.05094264 0.06935187]\n"," [0.05123185 0.06904261]]\n","\n","Average MAE Loss:\n","[0.06002384 0.06029226 0.06014726 0.06013723]\n","\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05116285 0.06885577]\n"," [0.05103173 0.06955279]\n"," [0.05097401 0.06933893]\n"," [0.05123768 0.06905305]]\n","\n","Average MAE Loss:\n","[0.06000931 0.06029226 0.06015647 0.06014537]\n","\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05117813 0.06883343]\n"," [0.05103173 0.06955279]\n"," [0.05092797 0.06938381]\n"," [0.05125127 0.06903625]]\n","\n","Average MAE Loss:\n","[0.06000578 0.06029226 0.06015589 0.06014376]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05102791 0.06922913]\n"," [0.05125127 0.06903625]\n"," [0.05112426 0.06889266]\n"," [0.0507603  0.06933918]]\n","\n","Average MAE Loss:\n","[0.06012852 0.06014376 0.06000846 0.06004974]\n","\n","Epoch 00036: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05117987 0.06901614]\n"," [0.05125127 0.06903625]\n"," [0.05113875 0.06894322]\n"," [0.05098136 0.0690787 ]]\n","\n","Average MAE Loss:\n","[0.06009801 0.06014376 0.06004099 0.06003003]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05121718 0.06894739]\n"," [0.05125127 0.06903625]\n"," [0.05114799 0.06900216]\n"," [0.0509037  0.06916596]]\n","\n","Average MAE Loss:\n","[0.06008229 0.06014376 0.06007507 0.06003483]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.0511913  0.06893263]\n"," [0.05125127 0.06903625]\n"," [0.05118408 0.06904873]\n"," [0.05095046 0.06908839]]\n","\n","Average MAE Loss:\n","[0.06006197 0.06014376 0.06011641 0.06001942]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05118077 0.06891896]\n"," [0.05125127 0.06903625]\n"," [0.05111927 0.06912237]\n"," [0.05093664 0.06907471]]\n","\n","Average MAE Loss:\n","[0.06004986 0.06014376 0.06012082 0.06000567]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05118228 0.06891067]\n"," [0.05125127 0.06903625]\n"," [0.05109199 0.0691543 ]\n"," [0.05098588 0.06911392]]\n","\n","Average MAE Loss:\n","[0.06004647 0.06014376 0.06012315 0.0600499 ]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05119997 0.06889611]\n"," [0.05125127 0.06903625]\n"," [0.05111606 0.06916773]\n"," [0.05100142 0.06919226]]\n","\n","Average MAE Loss:\n","[0.06004804 0.06014376 0.06014189 0.06009684]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05101109 0.06906943]\n"," [0.05099937 0.06909504]\n"," [0.05103976 0.06906828]\n"," [0.05110716 0.06908377]]\n","\n","Average MAE Loss:\n","[0.06004026 0.06004721 0.06005402 0.06009546]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05103115 0.06903945]\n"," [0.05099937 0.06909503]\n"," [0.05109838 0.06904516]\n"," [0.05112194 0.06905068]]\n","\n","Average MAE Loss:\n","[0.0600353  0.0600472  0.06007177 0.06008631]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00044: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05105547 0.06901242]\n"," [0.05099937 0.06909503]\n"," [0.05111748 0.0690463 ]\n"," [0.05111323 0.06902979]]\n","\n","Average MAE Loss:\n","[0.06003394 0.0600472  0.06008189 0.06007151]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05107638 0.06898756]\n"," [0.05099937 0.06909503]\n"," [0.05114241 0.06905095]\n"," [0.05110477 0.06905393]]\n","\n","Average MAE Loss:\n","[0.06003197 0.0600472  0.06009668 0.06007935]\n","\n","Epoch 00046: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05108263 0.06897783]\n"," [0.05099937 0.06909503]\n"," [0.05115637 0.06905699]\n"," [0.05115405 0.06905647]]\n","\n","Average MAE Loss:\n","[0.06003023 0.0600472  0.06010668 0.06010526]\n","\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05109395 0.06896906]\n"," [0.05099937 0.06909503]\n"," [0.05116331 0.06906579]\n"," [0.05116728 0.06905191]]\n","\n","Average MAE Loss:\n","[0.06003151 0.0600472  0.06011455 0.06010959]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00048: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05109759 0.0689622 ]\n"," [0.05099937 0.06909503]\n"," [0.05116947 0.06907075]\n"," [0.05117191 0.06903933]]\n","\n","Average MAE Loss:\n","[0.06002989 0.0600472  0.06012011 0.06010562]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05100869 0.06907833]\n"," [0.05117191 0.06903933]\n"," [0.05110493 0.06896305]\n"," [0.0510623  0.06907986]]\n","\n","Average MAE Loss:\n","[0.06004351 0.06010562 0.06003399 0.06007108]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05101504 0.06906908]\n"," [0.05117191 0.06903933]\n"," [0.05111264 0.06896499]\n"," [0.05102924 0.06910368]]\n","\n","Average MAE Loss:\n","[0.06004206 0.06010562 0.06003881 0.06006646]\n","\n","Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05102142 0.06905952]\n"," [0.05117191 0.06903933]\n"," [0.05112381 0.06896655]\n"," [0.05106027 0.06910932]]\n","\n","Average MAE Loss:\n","[0.06004047 0.06010562 0.06004518 0.0600848 ]\n","\n","Epoch 00052: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00052: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05102761 0.06905089]\n"," [0.05117191 0.06903933]\n"," [0.05112764 0.0689685 ]\n"," [0.05107383 0.06911005]]\n","\n","Average MAE Loss:\n","[0.06003925 0.06010562 0.06004807 0.06009194]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0510333  0.069043  ]\n"," [0.05117191 0.06903933]\n"," [0.0511325  0.0689705 ]\n"," [0.05109011 0.06910631]]\n","\n","Average MAE Loss:\n","[0.06003815 0.06010562 0.0600515  0.06009821]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0510362  0.06903896]\n"," [0.05117191 0.06903933]\n"," [0.05113719 0.06897306]\n"," [0.05109276 0.06910012]]\n","\n","Average MAE Loss:\n","[0.06003758 0.06010562 0.06005513 0.06009644]\n","\n","Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0510397  0.0690349 ]\n"," [0.05117191 0.06903933]\n"," [0.05114139 0.06897543]\n"," [0.05108234 0.06908788]]\n","\n","Average MAE Loss:\n","[0.0600373  0.06010562 0.06005841 0.06008511]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00056: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05109555 0.06902065]\n"," [0.05109341 0.06902431]\n"," [0.05109595 0.06902406]\n"," [0.05109087 0.06902227]]\n","\n","Average MAE Loss:\n","[0.0600581  0.06005886 0.06006    0.06005657]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05109708 0.06901761]\n"," [0.05109341 0.06902431]\n"," [0.05109868 0.06902337]\n"," [0.05109256 0.06902236]]\n","\n","Average MAE Loss:\n","[0.06005735 0.06005886 0.06006103 0.06005746]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05109735 0.06901594]\n"," [0.05109341 0.06902431]\n"," [0.05110087 0.06902318]\n"," [0.0510939  0.06902537]]\n","\n","Average MAE Loss:\n","[0.06005665 0.06005886 0.06006203 0.06005963]\n","\n","Epoch 00059: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05109833 0.06901424]\n"," [0.05109341 0.06902431]\n"," [0.05110244 0.06902303]\n"," [0.0510898  0.06902678]]\n","\n","Average MAE Loss:\n","[0.06005628 0.06005886 0.06006273 0.06005829]\n","\n","Epoch 00060: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00060: reducing learning rate of group 0 to 7.8125e-06.\n","\n","epochs finished with time:526.6788949966431\n","\n","[[0.05109833 0.06901424]\n"," [0.05109341 0.06902431]\n"," [0.05110244 0.06902303]\n"," [0.0510898  0.06902678]]\n","00:09:2.6820107089999965\n","------------------------------------Fold [5/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07978836 0.08271068]\n"," [0.0937808  0.09830491]\n"," [0.07903471 0.08163604]\n"," [0.10398609 0.0821456 ]]\n","\n","Average MAE Loss:\n","[0.08124952 0.09604286 0.08033537 0.09306585]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07577115 0.08393054]\n"," [0.0937808  0.09830491]\n"," [0.08520117 0.10215335]\n"," [0.10398609 0.08267982]]\n","\n","Average MAE Loss:\n","[0.07985085 0.09604286 0.09367726 0.09333295]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06973495 0.07652107]\n"," [0.0937808  0.09830491]\n"," [0.08322313 0.09035707]\n"," [0.10398609 0.079435  ]]\n","\n","Average MAE Loss:\n","[0.07312801 0.09604286 0.0867901  0.09171055]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0667175  0.07244072]\n"," [0.0937808  0.09830491]\n"," [0.08373332 0.09217203]\n"," [0.10398609 0.08167977]]\n","\n","Average MAE Loss:\n","[0.06957911 0.09604286 0.08795267 0.09283293]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.06602735 0.07127724]\n"," [0.0937808  0.09830491]\n"," [0.08397086 0.09661091]\n"," [0.10387367 0.07575317]]\n","\n","Average MAE Loss:\n","[0.06865229 0.09604286 0.09029089 0.08981342]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.065229   0.06831776]\n"," [0.0937808  0.09830491]\n"," [0.07469554 0.08052726]\n"," [0.10385625 0.07467377]]\n","\n","Average MAE Loss:\n","[0.06677338 0.09604286 0.0776114  0.08926501]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.06518707 0.06813806]\n"," [0.0937808  0.09830491]\n"," [0.07202935 0.07685537]\n"," [0.10398609 0.07737508]]\n","\n","Average MAE Loss:\n","[0.06666256 0.09604286 0.07444236 0.09068059]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08173012 0.09351007]\n"," [0.10398609 0.07737508]\n"," [0.06454416 0.06600571]\n"," [0.10339092 0.08135925]]\n","\n","Average MAE Loss:\n","[0.08762009 0.09068059 0.06527493 0.09237508]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08013294 0.08776541]\n"," [0.10398609 0.07737508]\n"," [0.06288518 0.06596453]\n"," [0.10398609 0.0709527 ]]\n","\n","Average MAE Loss:\n","[0.08394917 0.09068059 0.06442486 0.0874694 ]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07844178 0.08327746]\n"," [0.10398609 0.07737508]\n"," [0.06332549 0.06708512]\n"," [0.10398609 0.0707075 ]]\n","\n","Average MAE Loss:\n","[0.08085962 0.09068059 0.06520531 0.0873468 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0805795  0.08844602]\n"," [0.10398609 0.07737508]\n"," [0.06470297 0.06638904]\n"," [0.10398609 0.0723974 ]]\n","\n","Average MAE Loss:\n","[0.08451276 0.09068059 0.065546   0.08819175]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07134271 0.08155324]\n"," [0.10398609 0.07737508]\n"," [0.06329496 0.06607442]\n"," [0.10293197 0.07952434]]\n","\n","Average MAE Loss:\n","[0.07644797 0.09068059 0.06468469 0.09122815]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07180561 0.07705723]\n"," [0.10398609 0.07737508]\n"," [0.06339915 0.06527592]\n"," [0.10398609 0.06724714]]\n","\n","Average MAE Loss:\n","[0.07443142 0.09068059 0.06433754 0.08561661]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06855392 0.06918032]\n"," [0.10398609 0.07737508]\n"," [0.06219574 0.0650833 ]\n"," [0.10584676 0.0951199 ]]\n","\n","Average MAE Loss:\n","[0.06886712 0.09068059 0.06363952 0.10048333]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06238312 0.06856356]\n"," [0.09235631 0.08740689]\n"," [0.06318755 0.06539643]\n"," [0.06941438 0.06799594]]\n","\n","Average MAE Loss:\n","[0.06547334 0.0898816  0.06429199 0.06870516]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06251653 0.06802256]\n"," [0.09235631 0.08740689]\n"," [0.06229217 0.06628744]\n"," [0.08790037 0.06812989]]\n","\n","Average MAE Loss:\n","[0.06526954 0.0898816  0.0642898  0.07801513]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06232221 0.06793852]\n"," [0.09235631 0.08740689]\n"," [0.06225922 0.06596222]\n"," [0.08271001 0.06755301]]\n","\n","Average MAE Loss:\n","[0.06513036 0.0898816  0.06411072 0.07513151]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0621756  0.06766311]\n"," [0.09235631 0.08740689]\n"," [0.06206062 0.06613824]\n"," [0.0817138  0.06740274]]\n","\n","Average MAE Loss:\n","[0.06491936 0.0898816  0.06409943 0.07455827]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0620964  0.06710455]\n"," [0.09235631 0.08740689]\n"," [0.06246096 0.06678713]\n"," [0.09337761 0.06784822]]\n","\n","Average MAE Loss:\n","[0.06460048 0.0898816  0.06462405 0.08061291]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06246102 0.06794666]\n"," [0.09235631 0.08740689]\n"," [0.06206282 0.06541052]\n"," [0.10062776 0.06696517]]\n","\n","Average MAE Loss:\n","[0.06520384 0.0898816  0.06373667 0.08379646]\n","\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06227437 0.06746601]\n"," [0.09235631 0.08740689]\n"," [0.06197736 0.06517507]\n"," [0.10288232 0.06716985]]\n","\n","Average MAE Loss:\n","[0.06487019 0.0898816  0.06357621 0.08502608]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06321266 0.06894425]\n"," [0.10288232 0.06716985]\n"," [0.0616432  0.06517226]\n"," [0.06249456 0.06649468]]\n","\n","Average MAE Loss:\n","[0.06607845 0.08502608 0.06340773 0.06449462]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06232074 0.06771424]\n"," [0.10288232 0.06716985]\n"," [0.06190952 0.06522391]\n"," [0.06327208 0.0682521 ]]\n","\n","Average MAE Loss:\n","[0.06501749 0.08502608 0.06356671 0.06576209]\n","\n","Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0616522  0.06595407]\n"," [0.10288232 0.06716985]\n"," [0.06208483 0.06545891]\n"," [0.06414317 0.06901407]]\n","\n","Average MAE Loss:\n","[0.06380314 0.08502608 0.06377187 0.06657862]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.06164856 0.06625524]\n"," [0.10288232 0.06716985]\n"," [0.06197024 0.0649718 ]\n"," [0.06448785 0.06890277]]\n","\n","Average MAE Loss:\n","[0.0639519  0.08502608 0.06347102 0.06669531]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06147289 0.0655052 ]\n"," [0.10288232 0.06716985]\n"," [0.06188738 0.06484227]\n"," [0.06238955 0.066254  ]]\n","\n","Average MAE Loss:\n","[0.06348904 0.08502608 0.06336483 0.06432177]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06156491 0.06570104]\n"," [0.10288232 0.06716985]\n"," [0.06227696 0.06567186]\n"," [0.0621572  0.06583056]]\n","\n","Average MAE Loss:\n","[0.06363297 0.08502608 0.06397441 0.06399388]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06168994 0.06630272]\n"," [0.10288232 0.06716985]\n"," [0.06185328 0.06475742]\n"," [0.06284095 0.06649375]]\n","\n","Average MAE Loss:\n","[0.06399633 0.08502608 0.06330535 0.06466735]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06180113 0.0655181 ]\n"," [0.06362754 0.06758676]\n"," [0.06178043 0.06449899]\n"," [0.06315161 0.0671242 ]]\n","\n","Average MAE Loss:\n","[0.06365961 0.06560715 0.06313971 0.0651379 ]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06161101 0.06544276]\n"," [0.06362754 0.06758676]\n"," [0.06239028 0.06605973]\n"," [0.06270837 0.06644414]]\n","\n","Average MAE Loss:\n","[0.06352688 0.06560715 0.06422501 0.06457626]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06170035 0.06555756]\n"," [0.06362754 0.06758676]\n"," [0.06181601 0.06463891]\n"," [0.06243202 0.06596809]]\n","\n","Average MAE Loss:\n","[0.06362895 0.06560715 0.06322746 0.06420006]\n","\n","Epoch 00031: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06167411 0.06545731]\n"," [0.06362754 0.06758676]\n"," [0.06213126 0.06552083]\n"," [0.06258751 0.06614008]]\n","\n","Average MAE Loss:\n","[0.06356571 0.06560715 0.06382605 0.06436379]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06178533 0.06569195]\n"," [0.06362754 0.06758676]\n"," [0.06187824 0.0648164 ]\n"," [0.06249135 0.065938  ]]\n","\n","Average MAE Loss:\n","[0.06373864 0.06560715 0.06334732 0.06421467]\n","\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06173747 0.06558064]\n"," [0.06362754 0.06758676]\n"," [0.06193883 0.06496043]\n"," [0.06255016 0.06599919]]\n","\n","Average MAE Loss:\n","[0.06365905 0.06560715 0.06344963 0.06427468]\n","\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06172748 0.06555967]\n"," [0.06362754 0.06758676]\n"," [0.06188822 0.06474849]\n"," [0.06254535 0.06596628]]\n","\n","Average MAE Loss:\n","[0.06364357 0.06560715 0.06331836 0.06425581]\n","\n","Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06238526 0.06604442]\n"," [0.06254535 0.06596628]\n"," [0.06161707 0.06522103]\n"," [0.06248334 0.06621313]]\n","\n","Average MAE Loss:\n","[0.06421484 0.06425581 0.06341905 0.06434824]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06189414 0.0654086 ]\n"," [0.06254535 0.06596628]\n"," [0.06162832 0.06487884]\n"," [0.06208052 0.06552585]]\n","\n","Average MAE Loss:\n","[0.06365137 0.06425581 0.06325358 0.06380319]\n","\n","Epoch 00037: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06187813 0.06557019]\n"," [0.06254535 0.06596628]\n"," [0.06169987 0.06494822]\n"," [0.0622584  0.06589675]]\n","\n","Average MAE Loss:\n","[0.06372416 0.06425581 0.06332404 0.06407758]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06182067 0.06551547]\n"," [0.06254535 0.06596628]\n"," [0.06170242 0.06478035]\n"," [0.0622099  0.0658878 ]]\n","\n","Average MAE Loss:\n","[0.06366807 0.06425581 0.06324139 0.06404885]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06175735 0.06543418]\n"," [0.06254535 0.06596628]\n"," [0.06175649 0.06490702]\n"," [0.06222822 0.06593332]]\n","\n","Average MAE Loss:\n","[0.06359577 0.06425581 0.06333176 0.06408077]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06172733 0.06541322]\n"," [0.06254535 0.06596628]\n"," [0.06176551 0.06485351]\n"," [0.06210025 0.0656876 ]]\n","\n","Average MAE Loss:\n","[0.06357027 0.06425581 0.06330951 0.06389392]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06175521 0.06548934]\n"," [0.06254535 0.06596628]\n"," [0.06175253 0.06478366]\n"," [0.06212052 0.06571259]]\n","\n","Average MAE Loss:\n","[0.06362228 0.06425581 0.06326809 0.06391655]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06197214 0.06558396]\n"," [0.06199484 0.06559064]\n"," [0.06188254 0.06537949]\n"," [0.06203731 0.06566121]]\n","\n","Average MAE Loss:\n","[0.06377805 0.06379274 0.06363101 0.06384926]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06193871 0.06555826]\n"," [0.06199484 0.06559063]\n"," [0.06176702 0.06512453]\n"," [0.0620214  0.06562035]]\n","\n","Average MAE Loss:\n","[0.06374849 0.06379274 0.06344577 0.06382087]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0619271  0.06556693]\n"," [0.06199484 0.06559063]\n"," [0.06174126 0.06502106]\n"," [0.06201675 0.06561335]]\n","\n","Average MAE Loss:\n","[0.06374701 0.06379274 0.06338116 0.06381505]\n","\n","Epoch 00045: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00045: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06188533 0.06551791]\n"," [0.06199484 0.06559063]\n"," [0.06172787 0.06495775]\n"," [0.06205826 0.06569339]]\n","\n","Average MAE Loss:\n","[0.06370162 0.06379274 0.06334281 0.06387582]\n","\n","Epoch 00046: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.0618938  0.06553782]\n"," [0.06199484 0.06559064]\n"," [0.0617334  0.06494749]\n"," [0.0620656  0.06571014]]\n","\n","Average MAE Loss:\n","[0.06371581 0.06379274 0.06334045 0.06388787]\n","\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06188777 0.06553592]\n"," [0.06199484 0.06559063]\n"," [0.06174954 0.0649793 ]\n"," [0.06205063 0.06567165]]\n","\n","Average MAE Loss:\n","[0.06371184 0.06379274 0.06336442 0.06386114]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06189169 0.06555236]\n"," [0.06199484 0.06559063]\n"," [0.06174373 0.06494185]\n"," [0.06204135 0.06564832]]\n","\n","Average MAE Loss:\n","[0.06372202 0.06379274 0.06334279 0.06384483]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00049: reducing learning rate of group 0 to 1.5625e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06199016 0.06559695]\n"," [0.06204135 0.06564832]\n"," [0.06184288 0.06545976]\n"," [0.06181162 0.06514072]]\n","\n","Average MAE Loss:\n","[0.06379356 0.06384483 0.06365132 0.06347617]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0619881  0.06560081]\n"," [0.06204135 0.06564832]\n"," [0.0617842  0.06534761]\n"," [0.06187396 0.06530283]]\n","\n","Average MAE Loss:\n","[0.06379445 0.06384483 0.06356591 0.0635884 ]\n","\n","Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06198363 0.06560164]\n"," [0.06204135 0.06564832]\n"," [0.06173425 0.06524964]\n"," [0.06193737 0.0654469 ]]\n","\n","Average MAE Loss:\n","[0.06379263 0.06384483 0.06349194 0.06369213]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06197479 0.06559467]\n"," [0.06204135 0.06564832]\n"," [0.06170871 0.06518857]\n"," [0.06200869 0.06558732]]\n","\n","Average MAE Loss:\n","[0.06378473 0.06384483 0.06344864 0.063798  ]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06196506 0.06558555]\n"," [0.06204135 0.06564832]\n"," [0.06169646 0.06515929]\n"," [0.06203253 0.06563547]]\n","\n","Average MAE Loss:\n","[0.0637753  0.06384483 0.06342787 0.063834  ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00054: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06196074 0.06558138]\n"," [0.06204135 0.06564832]\n"," [0.06168842 0.06513552]\n"," [0.06203863 0.06564432]]\n","\n","Average MAE Loss:\n","[0.06377106 0.06384483 0.06341197 0.06384148]\n","\n","Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06195604 0.06557684]\n"," [0.06204135 0.06564832]\n"," [0.0616761  0.06510278]\n"," [0.06203358 0.06563352]]\n","\n","Average MAE Loss:\n","[0.06376644 0.06384483 0.06338944 0.06383355]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06190955 0.06548686]\n"," [0.06191063 0.06548438]\n"," [0.06188771 0.06543882]\n"," [0.06190415 0.065466  ]]\n","\n","Average MAE Loss:\n","[0.0636982  0.06369751 0.06366327 0.06368508]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06190821 0.06548869]\n"," [0.06191063 0.06548438]\n"," [0.06187855 0.06542005]\n"," [0.06191416 0.06548143]]\n","\n","Average MAE Loss:\n","[0.06369845 0.06369751 0.0636493  0.0636978 ]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00058: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06190837 0.06549118]\n"," [0.06191063 0.06548438]\n"," [0.06186966 0.06540099]\n"," [0.06192715 0.06550749]]\n","\n","Average MAE Loss:\n","[0.06369978 0.06369751 0.06363532 0.06371732]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06190697 0.06549106]\n"," [0.06191063 0.06548438]\n"," [0.06186254 0.06538498]\n"," [0.06195284 0.0655545 ]]\n","\n","Average MAE Loss:\n","[0.06369901 0.06369751 0.06362376 0.06375367]\n","\n","\n","epochs finished with time:521.9519410133362\n","\n","[[0.06190697 0.06549106]\n"," [0.06191063 0.06548438]\n"," [0.06186254 0.06538498]\n"," [0.06195284 0.0655545 ]]\n","00:08:58.114067087999956\n"]}],"source":["# seed = 10 table = 4/8 fixed folds\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='GAT_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJdXmZuYn53r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690635346588,"user_tz":-60,"elapsed":2016911,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"}},"outputId":"87780b75-ba34-418e-ef71-6cbe8aa4af1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.25\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.0731893  0.07081092]\n"," [0.0786098  0.07541355]\n"," [0.06903275 0.06734133]\n"," [0.08754892 0.06189463]]\n","\n","Average MAE Loss:\n","[0.07200011 0.07701167 0.06818704 0.07472178]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.0731893  0.07081092]\n"," [0.0786098  0.07541355]\n"," [0.06861527 0.07096716]\n"," [0.08754892 0.06226186]]\n","\n","Average MAE Loss:\n","[0.07200011 0.07701167 0.06979122 0.07490539]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0731893  0.07081092]\n"," [0.0786098  0.07541355]\n"," [0.0668805  0.06951075]\n"," [0.08754892 0.06103855]]\n","\n","Average MAE Loss:\n","[0.07200011 0.07701167 0.06819562 0.07429374]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0731893  0.07081092]\n"," [0.0786098  0.07541355]\n"," [0.06635877 0.068912  ]\n"," [0.09716048 0.08422055]]\n","\n","Average MAE Loss:\n","[0.07200011 0.07701167 0.06763538 0.09069051]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0731893  0.07081092]\n"," [0.0786098  0.07541355]\n"," [0.06663882 0.06641331]\n"," [0.08754892 0.06043851]]\n","\n","Average MAE Loss:\n","[0.07200011 0.07701167 0.06652606 0.07399372]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.0731893  0.07081092]\n"," [0.0786098  0.07541355]\n"," [0.06453599 0.06253343]\n"," [0.08754892 0.06294379]]\n","\n","Average MAE Loss:\n","[0.07200011 0.07701167 0.06353471 0.07524635]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07318929 0.07081092]\n"," [0.0786098  0.07541355]\n"," [0.06424373 0.06772138]\n"," [0.08754892 0.05820012]]\n","\n","Average MAE Loss:\n","[0.07200011 0.07701167 0.06598256 0.07287452]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0786098  0.07541355]\n"," [0.08754892 0.05820012]\n"," [0.07748672 0.07271156]\n"," [0.08635663 0.06165311]]\n","\n","Average MAE Loss:\n","[0.07701167 0.07287452 0.07509914 0.07400487]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0786098  0.07541355]\n"," [0.08754892 0.05820012]\n"," [0.07124071 0.0695558 ]\n"," [0.08754892 0.05534181]]\n","\n","Average MAE Loss:\n","[0.07701167 0.07287452 0.07039825 0.07144537]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0786098  0.07541355]\n"," [0.08754892 0.05820012]\n"," [0.06923175 0.07312467]\n"," [0.08754892 0.0563526 ]]\n","\n","Average MAE Loss:\n","[0.07701167 0.07287452 0.07117821 0.07195076]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0786098  0.07541355]\n"," [0.08754892 0.05820012]\n"," [0.06928349 0.06730622]\n"," [0.08754892 0.05318623]]\n","\n","Average MAE Loss:\n","[0.07701167 0.07287452 0.06829485 0.07036757]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0786098  0.07541355]\n"," [0.08754892 0.05820012]\n"," [0.06454179 0.06339801]\n"," [0.08754892 0.05390713]]\n","\n","Average MAE Loss:\n","[0.07701167 0.07287452 0.0639699  0.07072803]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0786098  0.07541355]\n"," [0.08754892 0.05820012]\n"," [0.06081012 0.06040523]\n"," [0.08754892 0.05236855]]\n","\n","Average MAE Loss:\n","[0.07701167 0.07287452 0.06060767 0.06995874]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0786098  0.07541355]\n"," [0.08754892 0.05820012]\n"," [0.05809625 0.0581205 ]\n"," [0.08754892 0.05295573]]\n","\n","Average MAE Loss:\n","[0.07701167 0.07287452 0.05810838 0.07025233]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.07423969 0.06923292]\n"," [0.05795711 0.05535833]\n"," [0.05675977 0.05444715]]\n","\n","Average MAE Loss:\n","[0.07173631 0.07173631 0.05665772 0.05560346]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.07423969 0.06923292]\n"," [0.05590416 0.05433514]\n"," [0.05654923 0.05392968]]\n","\n","Average MAE Loss:\n","[0.07173631 0.07173631 0.05511965 0.05523945]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.07423969 0.06923292]\n"," [0.05710166 0.05670561]\n"," [0.05816689 0.05413657]]\n","\n","Average MAE Loss:\n","[0.07173631 0.07173631 0.05690363 0.05615173]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.07423969 0.06923292]\n"," [0.05608811 0.05452912]\n"," [0.06034467 0.0540861 ]]\n","\n","Average MAE Loss:\n","[0.07173631 0.07173631 0.05530862 0.05721538]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.07423969 0.06923292]\n"," [0.05560315 0.05353987]\n"," [0.05943543 0.05358577]]\n","\n","Average MAE Loss:\n","[0.0717363  0.07173631 0.05457151 0.0565106 ]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.07423969 0.06923292]\n"," [0.05559835 0.05362859]\n"," [0.06240409 0.05348209]]\n","\n","Average MAE Loss:\n","[0.07173631 0.07173631 0.05461347 0.05794309]\n","\n","Epoch 00020: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.07423969 0.06923292]\n"," [0.0557183  0.05378749]\n"," [0.06257761 0.05353941]]\n","\n","Average MAE Loss:\n","[0.07173631 0.07173631 0.0547529  0.05805851]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.06257761 0.05353941]\n"," [0.05735829 0.05442147]\n"," [0.05661693 0.05493035]]\n","\n","Average MAE Loss:\n","[0.07173631 0.05805851 0.05588988 0.05577364]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.06257761 0.05353941]\n"," [0.05580297 0.05384986]\n"," [0.05711934 0.05504865]]\n","\n","Average MAE Loss:\n","[0.07173631 0.05805851 0.05482642 0.056084  ]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.06257761 0.05353941]\n"," [0.05636017 0.05555843]\n"," [0.055988   0.0536988 ]]\n","\n","Average MAE Loss:\n","[0.07173631 0.05805851 0.0559593  0.0548434 ]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.06257761 0.05353941]\n"," [0.05563931 0.05338966]\n"," [0.05587949 0.05368307]]\n","\n","Average MAE Loss:\n","[0.07173631 0.05805851 0.05451448 0.05478128]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.06257761 0.05353941]\n"," [0.05574293 0.0541102 ]\n"," [0.05611922 0.05372173]]\n","\n","Average MAE Loss:\n","[0.07173631 0.05805851 0.05492657 0.05492047]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.06257761 0.05353941]\n"," [0.05559754 0.05357399]\n"," [0.05606787 0.05379776]]\n","\n","Average MAE Loss:\n","[0.07173631 0.05805851 0.05458577 0.05493281]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07423969 0.06923292]\n"," [0.06257761 0.05353941]\n"," [0.05561336 0.05352229]\n"," [0.0563353  0.05366786]]\n","\n","Average MAE Loss:\n","[0.07173631 0.05805851 0.05456783 0.05500158]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.0580807  0.05526649]\n"," [0.05626382 0.05404682]\n"," [0.05600561 0.05386835]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05667359 0.05515532 0.05493698]\n","\n","Epoch 00029: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00029: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.0580807  0.05526649]\n"," [0.05580333 0.05359382]\n"," [0.056033   0.05378768]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05667359 0.05469858 0.05491034]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.0580807  0.05526649]\n"," [0.05579699 0.05360804]\n"," [0.0560598  0.05383127]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05667359 0.05470251 0.05494553]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.0580807  0.05526649]\n"," [0.05583779 0.05365098]\n"," [0.05610247 0.05380406]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05667359 0.05474438 0.05495327]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.0580807  0.05526649]\n"," [0.05581114 0.05363744]\n"," [0.05619836 0.05374287]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05667359 0.05472429 0.05497061]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.0580807  0.05526649]\n"," [0.05580691 0.05363038]\n"," [0.05616685 0.05377014]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05667359 0.05471865 0.05496849]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.0580807  0.05526649]\n"," [0.05580118 0.05363949]\n"," [0.05619012 0.053765  ]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05667359 0.05472034 0.05497756]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.05619012 0.053765  ]\n"," [0.05646727 0.05417358]\n"," [0.05571221 0.05359145]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05497756 0.05532042 0.05465183]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.05619012 0.053765  ]\n"," [0.05577581 0.05358678]\n"," [0.05573422 0.0536376 ]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05497756 0.05468129 0.05468591]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.05619012 0.053765  ]\n"," [0.05584594 0.05365409]\n"," [0.05576807 0.05363826]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05497756 0.05475001 0.05470316]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.05619012 0.053765  ]\n"," [0.05582431 0.05362856]\n"," [0.05579123 0.05367044]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05497756 0.05472643 0.05473084]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.05619012 0.053765  ]\n"," [0.05581179 0.05361856]\n"," [0.05581434 0.05366921]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05497756 0.05471517 0.05474178]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.05619012 0.053765  ]\n"," [0.0558049  0.0536181 ]\n"," [0.0558239  0.05366635]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05497756 0.0547115  0.05474513]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.0580807  0.05526649]\n"," [0.05619013 0.053765  ]\n"," [0.05581258 0.05362763]\n"," [0.05583515 0.05366099]]\n","\n","Average MAE Loss:\n","[0.05667359 0.05497756 0.05472011 0.05474807]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05603278 0.05370479]\n"," [0.05594551 0.05365982]\n"," [0.05590098 0.05362327]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05486878 0.05480267 0.05476212]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05603278 0.0537048 ]\n"," [0.05586293 0.05362089]\n"," [0.05589005 0.05364652]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05486879 0.05474191 0.05476829]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05603278 0.0537048 ]\n"," [0.05582825 0.0536195 ]\n"," [0.05590125 0.05365442]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05486879 0.05472388 0.05477784]\n","\n","Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05603278 0.0537048 ]\n"," [0.05582355 0.05362787]\n"," [0.05590913 0.05366353]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05486879 0.05472571 0.05478633]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05603278 0.0537048 ]\n"," [0.05582273 0.05363844]\n"," [0.05591233 0.05366426]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05486879 0.05473059 0.05478829]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.05370479]\n"," [0.05603278 0.05370479]\n"," [0.05582889 0.0536526 ]\n"," [0.05591728 0.05367607]]\n","\n","Average MAE Loss:\n","[0.05486878 0.05486878 0.05474074 0.05479667]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05603278 0.0537048 ]\n"," [0.05583863 0.05366827]\n"," [0.05592172 0.05368107]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05486879 0.05475345 0.05480139]\n","\n","Epoch 00049: reducing learning rate of group 0 to 3.9063e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05592172 0.05368107]\n"," [0.05600362 0.05368951]\n"," [0.05583543 0.05366353]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05480139 0.05484657 0.05474948]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.05370479]\n"," [0.05592172 0.05368107]\n"," [0.0559671  0.05367016]\n"," [0.05583091 0.05365437]]\n","\n","Average MAE Loss:\n","[0.05486878 0.05480139 0.05481863 0.05474264]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05592172 0.05368107]\n"," [0.05593485 0.05365385]\n"," [0.05583249 0.05365093]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05480139 0.05479435 0.05474171]\n","\n","Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05592172 0.05368107]\n"," [0.05590864 0.05363951]\n"," [0.05583333 0.05365078]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05480139 0.05477408 0.05474206]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05592172 0.05368107]\n"," [0.05589795 0.05363434]\n"," [0.05583338 0.05364679]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05480139 0.05476614 0.05474008]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05592172 0.05368107]\n"," [0.05588658 0.05362927]\n"," [0.05583185 0.0536387 ]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05480139 0.05475793 0.05473527]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05603278 0.0537048 ]\n"," [0.05592172 0.05368107]\n"," [0.05587649 0.0536248 ]\n"," [0.05583379 0.0536362 ]]\n","\n","Average MAE Loss:\n","[0.05486879 0.05480139 0.05475065 0.05473499]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.0558822  0.05362474]\n"," [0.0558822  0.05362474]\n"," [0.05587456 0.05362325]\n"," [0.05588121 0.05362426]]\n","\n","Average MAE Loss:\n","[0.05475347 0.05475347 0.0547489  0.05475273]\n","\n","Epoch 00057: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.0558822  0.05362474]\n"," [0.0558822  0.05362474]\n"," [0.05587121 0.05362289]\n"," [0.05587832 0.05362498]]\n","\n","Average MAE Loss:\n","[0.05475347 0.05475347 0.05474705 0.05475165]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.0558822  0.05362474]\n"," [0.0558822  0.05362474]\n"," [0.05586817 0.0536226 ]\n"," [0.05587776 0.05362618]]\n","\n","Average MAE Loss:\n","[0.05475347 0.05475347 0.05474539 0.05475197]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.0558822  0.05362474]\n"," [0.0558822  0.05362474]\n"," [0.05586525 0.05362247]\n"," [0.05587586 0.05362785]]\n","\n","Average MAE Loss:\n","[0.05475347 0.05475347 0.05474386 0.05475185]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.9531e-06.\n","\n","epochs finished with time:405.54618525505066\n","\n","[[0.0558822  0.05362474]\n"," [0.0558822  0.05362474]\n"," [0.05586525 0.05362247]\n"," [0.05587586 0.05362785]]\n","00:07:11.286144809000007\n","------------------------------------Fold [2/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10214096 0.10191492]\n"," [0.10729529 0.10620574]\n"," [0.09274958 0.09463484]\n"," [0.11801491 0.0979555 ]]\n","\n","Average MAE Loss:\n","[0.10202794 0.10675051 0.09369221 0.1079852 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10214096 0.10191492]\n"," [0.10729529 0.10620574]\n"," [0.09075654 0.09272015]\n"," [0.11709981 0.09532983]]\n","\n","Average MAE Loss:\n","[0.10202794 0.10675051 0.09173834 0.10621482]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10214096 0.10191492]\n"," [0.10729529 0.10620574]\n"," [0.08955244 0.09402963]\n"," [0.11778611 0.09035347]]\n","\n","Average MAE Loss:\n","[0.10202794 0.10675051 0.09179104 0.10406979]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10214096 0.10191492]\n"," [0.10729529 0.10620574]\n"," [0.08775445 0.09446427]\n"," [0.11803927 0.08980535]]\n","\n","Average MAE Loss:\n","[0.10202794 0.10675051 0.09110936 0.10392231]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10214096 0.10191492]\n"," [0.10729529 0.10620574]\n"," [0.08547947 0.08917649]\n"," [0.11803927 0.08848376]]\n","\n","Average MAE Loss:\n","[0.10202794 0.10675051 0.08732798 0.10326152]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10214096 0.10191492]\n"," [0.10729529 0.10620574]\n"," [0.08915203 0.08908904]\n"," [0.11803927 0.08549398]]\n","\n","Average MAE Loss:\n","[0.10202794 0.10675051 0.08912053 0.10176663]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10214096 0.10191492]\n"," [0.10729529 0.10620574]\n"," [0.08488101 0.08826293]\n"," [0.11803927 0.08330056]]\n","\n","Average MAE Loss:\n","[0.10202794 0.10675051 0.08657197 0.10066992]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.10729529 0.10620574]\n"," [0.11803927 0.08330056]\n"," [0.10673183 0.10497202]\n"," [0.11652149 0.09064717]]\n","\n","Average MAE Loss:\n","[0.10675051 0.10066992 0.10585192 0.10358433]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.10729529 0.10620574]\n"," [0.11803927 0.08330056]\n"," [0.09072213 0.09969223]\n"," [0.11803927 0.08484451]]\n","\n","Average MAE Loss:\n","[0.10675051 0.10066992 0.09520718 0.10144189]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.1072953  0.10620574]\n"," [0.11803927 0.08330056]\n"," [0.09678773 0.09830347]\n"," [0.11803927 0.08637764]]\n","\n","Average MAE Loss:\n","[0.10675052 0.10066992 0.0975456  0.10220846]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.10729529 0.10620574]\n"," [0.11803927 0.08330056]\n"," [0.09281077 0.09960334]\n"," [0.11803927 0.08015168]]\n","\n","Average MAE Loss:\n","[0.10675051 0.10066992 0.09620706 0.09909548]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.10729529 0.10620574]\n"," [0.11803927 0.08330056]\n"," [0.09208481 0.09344459]\n"," [0.11803927 0.08023631]]\n","\n","Average MAE Loss:\n","[0.10675051 0.10066992 0.0927647  0.09913779]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.10729529 0.10620574]\n"," [0.11803927 0.08330056]\n"," [0.08442171 0.08714178]\n"," [0.11803927 0.08303073]]\n","\n","Average MAE Loss:\n","[0.10675051 0.10066992 0.08578175 0.100535  ]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.1072953  0.10620574]\n"," [0.11803927 0.08330056]\n"," [0.08336728 0.08655006]\n"," [0.11803927 0.07892385]]\n","\n","Average MAE Loss:\n","[0.10675052 0.10066992 0.08495867 0.09848156]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10555279 0.10087219]\n"," [0.08028031 0.08348563]\n"," [0.08024371 0.08101887]]\n","\n","Average MAE Loss:\n","[0.10321249 0.10321249 0.08188297 0.08063129]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10555279 0.10087219]\n"," [0.08011153 0.08130059]\n"," [0.08519688 0.08281358]]\n","\n","Average MAE Loss:\n","[0.10321249 0.10321249 0.08070606 0.08400523]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10555279 0.10087219]\n"," [0.07938256 0.08124728]\n"," [0.08786641 0.08254496]]\n","\n","Average MAE Loss:\n","[0.10321249 0.10321249 0.08031492 0.08520569]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10555279 0.10087219]\n"," [0.07973206 0.08126005]\n"," [0.09150548 0.08120018]]\n","\n","Average MAE Loss:\n","[0.10321249 0.10321249 0.08049605 0.08635283]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10555279 0.10087219]\n"," [0.0794264  0.08087062]\n"," [0.09305976 0.0807712 ]]\n","\n","Average MAE Loss:\n","[0.10321249 0.10321249 0.08014851 0.08691548]\n","\n","Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.1055528  0.10087219]\n"," [0.10555279 0.10087219]\n"," [0.07944595 0.08091722]\n"," [0.09864011 0.08046603]]\n","\n","Average MAE Loss:\n","[0.1032125  0.10321249 0.08018159 0.08955307]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10555279 0.10087219]\n"," [0.07977922 0.08109639]\n"," [0.10074584 0.08001902]]\n","\n","Average MAE Loss:\n","[0.10321249 0.10321249 0.08043781 0.09038243]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10074584 0.08001902]\n"," [0.08035028 0.08142864]\n"," [0.08037168 0.08221259]]\n","\n","Average MAE Loss:\n","[0.10321249 0.09038243 0.08088946 0.08129214]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10074584 0.08001902]\n"," [0.07925944 0.0808199 ]\n"," [0.08044925 0.0817331 ]]\n","\n","Average MAE Loss:\n","[0.10321249 0.09038243 0.08003967 0.08109118]\n","\n","Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.1055528  0.10087219]\n"," [0.10074584 0.08001902]\n"," [0.07879856 0.08046023]\n"," [0.07938646 0.08048113]]\n","\n","Average MAE Loss:\n","[0.1032125  0.09038243 0.07962939 0.0799338 ]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10074584 0.08001902]\n"," [0.07889818 0.08041428]\n"," [0.07989001 0.0808565 ]]\n","\n","Average MAE Loss:\n","[0.10321249 0.09038243 0.07965623 0.08037325]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10074584 0.08001902]\n"," [0.07934638 0.08118978]\n"," [0.07934469 0.08012879]]\n","\n","Average MAE Loss:\n","[0.10321249 0.09038243 0.08026808 0.07973674]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10074584 0.08001902]\n"," [0.07938802 0.08131268]\n"," [0.07976733 0.08051881]]\n","\n","Average MAE Loss:\n","[0.10321249 0.09038243 0.08035035 0.08014307]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.10555279 0.10087219]\n"," [0.10074584 0.08001902]\n"," [0.07904424 0.0808296 ]\n"," [0.07919368 0.07999258]]\n","\n","Average MAE Loss:\n","[0.10321249 0.09038243 0.07993692 0.07959313]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.08459765 0.084638  ]\n"," [0.0787599  0.07979327]\n"," [0.07844384 0.07914208]]\n","\n","Average MAE Loss:\n","[0.08461783 0.08461783 0.07927658 0.07879296]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.08459765 0.084638  ]\n"," [0.08022997 0.08082327]\n"," [0.07871644 0.07940326]]\n","\n","Average MAE Loss:\n","[0.08461783 0.08461783 0.08052662 0.07905985]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.08459765 0.084638  ]\n"," [0.07950777 0.08027133]\n"," [0.07819567 0.0790616 ]]\n","\n","Average MAE Loss:\n","[0.08461783 0.08461782 0.07988955 0.07862864]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.08459765 0.084638  ]\n"," [0.07972389 0.08042139]\n"," [0.07854756 0.07916055]]\n","\n","Average MAE Loss:\n","[0.08461783 0.08461783 0.08007264 0.07885405]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.08459765 0.084638  ]\n"," [0.07931239 0.0801865 ]\n"," [0.07852619 0.07907329]]\n","\n","Average MAE Loss:\n","[0.08461783 0.08461783 0.07974944 0.07879974]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.08459765 0.084638  ]\n"," [0.07955436 0.08033314]\n"," [0.0784503  0.0789982 ]]\n","\n","Average MAE Loss:\n","[0.08461783 0.08461782 0.07994375 0.07872425]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.08459765 0.084638  ]\n"," [0.07926613 0.08014585]\n"," [0.07907116 0.07938293]]\n","\n","Average MAE Loss:\n","[0.08461782 0.08461783 0.07970599 0.07922705]\n","\n","Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.07907116 0.07938293]\n"," [0.07929801 0.08015953]\n"," [0.07931814 0.08014202]]\n","\n","Average MAE Loss:\n","[0.08461783 0.07922705 0.07972877 0.07973008]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.07907116 0.07938293]\n"," [0.07955606 0.08024232]\n"," [0.07942657 0.08018258]]\n","\n","Average MAE Loss:\n","[0.08461782 0.07922705 0.07989919 0.07980458]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00037: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.07907116 0.07938293]\n"," [0.07960469 0.08026363]\n"," [0.07934155 0.08014585]]\n","\n","Average MAE Loss:\n","[0.08461783 0.07922705 0.07993416 0.0797437 ]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.07907116 0.07938293]\n"," [0.07951586 0.08020192]\n"," [0.07960092 0.08036152]]\n","\n","Average MAE Loss:\n","[0.08461783 0.07922705 0.07985889 0.07998122]\n","\n","Epoch 00039: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.07907116 0.07938293]\n"," [0.07957676 0.08025787]\n"," [0.07941325 0.08020185]]\n","\n","Average MAE Loss:\n","[0.08461783 0.07922705 0.07991731 0.07980755]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.07907116 0.07938293]\n"," [0.07968743 0.08036239]\n"," [0.07916023 0.0799804 ]]\n","\n","Average MAE Loss:\n","[0.08461783 0.07922705 0.08002491 0.07957032]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.08459765 0.084638  ]\n"," [0.07907116 0.07938293]\n"," [0.07953131 0.08024073]\n"," [0.07944615 0.08021163]]\n","\n","Average MAE Loss:\n","[0.08461782 0.07922705 0.07988602 0.07982889]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.08036926 0.08081567]\n"," [0.07943454 0.08003591]\n"," [0.07943498 0.07996473]]\n","\n","Average MAE Loss:\n","[0.08059247 0.08059247 0.07973523 0.07969986]\n","\n","Epoch 00043: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.08036926 0.08081567]\n"," [0.07860978 0.07936795]\n"," [0.0789232  0.07952009]]\n","\n","Average MAE Loss:\n","[0.08059247 0.08059247 0.07898887 0.07922164]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.08036926 0.08081567]\n"," [0.07826923 0.07909856]\n"," [0.07859491 0.07924157]]\n","\n","Average MAE Loss:\n","[0.08059247 0.08059247 0.07868389 0.07891824]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.08036926 0.08081567]\n"," [0.078205   0.07906047]\n"," [0.07849187 0.07917118]]\n","\n","Average MAE Loss:\n","[0.08059247 0.08059247 0.07863273 0.07883153]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.08036926 0.08081567]\n"," [0.0782805  0.07911347]\n"," [0.07840263 0.07912045]]\n","\n","Average MAE Loss:\n","[0.08059247 0.08059247 0.07869698 0.07876154]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.08036926 0.08081567]\n"," [0.07821803 0.07907142]\n"," [0.07833334 0.07907197]]\n","\n","Average MAE Loss:\n","[0.08059247 0.08059247 0.07864473 0.07870266]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.08036926 0.08081567]\n"," [0.07819945 0.079079  ]\n"," [0.07832087 0.07906662]]\n","\n","Average MAE Loss:\n","[0.08059247 0.08059247 0.07863923 0.07869375]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.07832087 0.07906662]\n"," [0.07954919 0.08013462]\n"," [0.07832055 0.07916721]]\n","\n","Average MAE Loss:\n","[0.08059247 0.07869375 0.07984191 0.07874388]\n","\n","Epoch 00050: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.07832087 0.07906662]\n"," [0.07906624 0.07972605]\n"," [0.07832484 0.07916887]]\n","\n","Average MAE Loss:\n","[0.08059247 0.07869375 0.07939615 0.07874686]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.07832087 0.07906662]\n"," [0.07873245 0.07946258]\n"," [0.0783115  0.07915883]]\n","\n","Average MAE Loss:\n","[0.08059247 0.07869375 0.07909751 0.07873516]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.07832087 0.07906662]\n"," [0.07855654 0.07932045]\n"," [0.07829821 0.07914816]]\n","\n","Average MAE Loss:\n","[0.08059247 0.07869375 0.0789385  0.07872318]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.07832087 0.07906662]\n"," [0.07849262 0.07927212]\n"," [0.07827393 0.07912801]]\n","\n","Average MAE Loss:\n","[0.08059247 0.07869375 0.07888237 0.07870097]\n","\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00054: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.07832087 0.07906662]\n"," [0.0784457  0.07923471]\n"," [0.07824926 0.07911268]]\n","\n","Average MAE Loss:\n","[0.08059247 0.07869375 0.07884021 0.07868097]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.08036926 0.08081567]\n"," [0.07832087 0.07906662]\n"," [0.0784173  0.07921138]\n"," [0.07823404 0.07910089]]\n","\n","Average MAE Loss:\n","[0.08059247 0.07869375 0.07881434 0.07866747]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.07874292 0.07943662]\n"," [0.07874292 0.07943662]\n"," [0.07863757 0.07935612]\n"," [0.07872329 0.07941904]]\n","\n","Average MAE Loss:\n","[0.07908977 0.07908977 0.07899685 0.07907116]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.07874292 0.07943662]\n"," [0.07874292 0.07943662]\n"," [0.07854877 0.07929147]\n"," [0.0786956  0.07939578]]\n","\n","Average MAE Loss:\n","[0.07908977 0.07908977 0.07892012 0.07904569]\n","\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00058: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.07874292 0.07943662]\n"," [0.07874292 0.07943662]\n"," [0.07850819 0.07926202]\n"," [0.07864034 0.07935035]]\n","\n","Average MAE Loss:\n","[0.07908977 0.07908977 0.07888511 0.07899534]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.07874292 0.07943662]\n"," [0.07874292 0.07943662]\n"," [0.07846225 0.079228  ]\n"," [0.07860789 0.0793245 ]]\n","\n","Average MAE Loss:\n","[0.07908977 0.07908977 0.07884512 0.07896619]\n","\n","\n","epochs finished with time:395.061838388443\n","\n","[[0.07874292 0.07943662]\n"," [0.07874292 0.07943662]\n"," [0.07846225 0.07922799]\n"," [0.07860789 0.0793245 ]]\n","00:07:1.2936388310000666\n","------------------------------------Fold [3/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08601599 0.07681459]\n"," [0.09104088 0.08139092]\n"," [0.07733299 0.07232179]\n"," [0.1015135  0.06819829]]\n","\n","Average MAE Loss:\n","[0.08141529 0.0862159  0.07482739 0.0848559 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08601599 0.07681459]\n"," [0.09104088 0.08139092]\n"," [0.08274895 0.07154992]\n"," [0.1015135  0.06915514]]\n","\n","Average MAE Loss:\n","[0.08141529 0.0862159  0.07714944 0.08533432]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08601599 0.07681459]\n"," [0.09104088 0.08139092]\n"," [0.07853144 0.08558123]\n"," [0.1015135  0.06696691]]\n","\n","Average MAE Loss:\n","[0.08141529 0.0862159  0.08205634 0.0842402 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08601599 0.07681459]\n"," [0.09104088 0.08139092]\n"," [0.07504907 0.07680976]\n"," [0.1015135  0.06546602]]\n","\n","Average MAE Loss:\n","[0.08141529 0.0862159  0.07592942 0.08348976]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08601599 0.07681459]\n"," [0.09104088 0.08139092]\n"," [0.07983624 0.08649614]\n"," [0.1015135  0.0672131 ]]\n","\n","Average MAE Loss:\n","[0.08141529 0.0862159  0.08316619 0.0843633 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08601599 0.07681459]\n"," [0.09104088 0.08139092]\n"," [0.07611393 0.06766233]\n"," [0.1015135  0.07670729]]\n","\n","Average MAE Loss:\n","[0.08141529 0.0862159  0.07188813 0.08911039]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08601599 0.07681459]\n"," [0.09104088 0.08139092]\n"," [0.07549927 0.06831744]\n"," [0.1015135  0.07063525]]\n","\n","Average MAE Loss:\n","[0.08141529 0.0862159  0.07190835 0.08607437]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.09104088 0.08139092]\n"," [0.1015135  0.07063525]\n"," [0.08564647 0.07813095]\n"," [0.10011529 0.06701924]]\n","\n","Average MAE Loss:\n","[0.0862159  0.08607437 0.08188871 0.08356727]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.09104088 0.08139092]\n"," [0.1015135  0.07063525]\n"," [0.0857489  0.07724239]\n"," [0.1015135  0.06307383]]\n","\n","Average MAE Loss:\n","[0.0862159  0.08607437 0.08149564 0.08229366]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.09104088 0.08139092]\n"," [0.1015135  0.07063525]\n"," [0.08702782 0.07487494]\n"," [0.1015135  0.06156196]]\n","\n","Average MAE Loss:\n","[0.0862159  0.08607437 0.08095138 0.08153773]\n","\n","Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.09104088 0.08139092]\n"," [0.1015135  0.07063525]\n"," [0.08169177 0.07224163]\n"," [0.1015135  0.06103264]]\n","\n","Average MAE Loss:\n","[0.0862159  0.08607437 0.0769667  0.08127307]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.09104088 0.08139092]\n"," [0.1015135  0.07063525]\n"," [0.06948953 0.06675757]\n"," [0.1015135  0.06005313]]\n","\n","Average MAE Loss:\n","[0.0862159  0.08607437 0.06812355 0.08078332]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.09104088 0.08139092]\n"," [0.1015135  0.07063525]\n"," [0.06624818 0.06006204]\n"," [0.1015135  0.06028423]]\n","\n","Average MAE Loss:\n","[0.0862159  0.08607437 0.06315511 0.08089886]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.09104088 0.08139092]\n"," [0.1015135  0.07063525]\n"," [0.06618482 0.06044103]\n"," [0.1015135  0.06037676]]\n","\n","Average MAE Loss:\n","[0.0862159  0.08607437 0.06331293 0.08094513]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.0940567  0.07643232]\n"," [0.07253939 0.06534821]\n"," [0.08011454 0.06537946]]\n","\n","Average MAE Loss:\n","[0.08524451 0.08524451 0.0689438  0.072747  ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.0940567  0.07643232]\n"," [0.06743089 0.05952238]\n"," [0.07016926 0.05954047]]\n","\n","Average MAE Loss:\n","[0.08524451 0.08524451 0.06347664 0.06485486]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.0940567  0.07643232]\n"," [0.06524014 0.05786594]\n"," [0.06987542 0.05873885]]\n","\n","Average MAE Loss:\n","[0.08524451 0.08524451 0.06155304 0.06430713]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.0940567  0.07643232]\n"," [0.06619063 0.05867034]\n"," [0.07160666 0.05863684]]\n","\n","Average MAE Loss:\n","[0.08524451 0.08524451 0.06243049 0.06512175]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.0940567  0.07643232]\n"," [0.06526984 0.05771883]\n"," [0.07145854 0.05850856]]\n","\n","Average MAE Loss:\n","[0.08524451 0.08524451 0.06149433 0.06498355]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.0940567  0.07643232]\n"," [0.0653518  0.05761477]\n"," [0.07538745 0.05840345]]\n","\n","Average MAE Loss:\n","[0.08524451 0.08524451 0.06148329 0.06689545]\n","\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.0940567  0.07643232]\n"," [0.06497221 0.05734364]\n"," [0.07812055 0.05767208]]\n","\n","Average MAE Loss:\n","[0.08524451 0.08524451 0.06115792 0.06789632]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.07812055 0.05767208]\n"," [0.07150664 0.06410192]\n"," [0.06590747 0.05813149]]\n","\n","Average MAE Loss:\n","[0.08524451 0.06789632 0.06780428 0.06201948]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.07812055 0.05767208]\n"," [0.06700856 0.05968873]\n"," [0.06603701 0.05819965]]\n","\n","Average MAE Loss:\n","[0.08524451 0.06789632 0.06334865 0.06211833]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.07812055 0.05767208]\n"," [0.06553349 0.05801516]\n"," [0.06576083 0.05794676]]\n","\n","Average MAE Loss:\n","[0.08524451 0.06789632 0.06177432 0.06185379]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.07812055 0.05767208]\n"," [0.06531601 0.05760012]\n"," [0.06614864 0.0581917 ]]\n","\n","Average MAE Loss:\n","[0.08524451 0.06789632 0.06145807 0.06217017]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.07812055 0.05767208]\n"," [0.06524997 0.05752866]\n"," [0.06604614 0.05795603]]\n","\n","Average MAE Loss:\n","[0.08524451 0.06789632 0.06138932 0.06200108]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.07812055 0.05767208]\n"," [0.06491899 0.05732512]\n"," [0.06612793 0.05798197]]\n","\n","Average MAE Loss:\n","[0.08524451 0.06789632 0.06112206 0.06205495]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0940567  0.07643232]\n"," [0.07812055 0.05767208]\n"," [0.06511023 0.05738828]\n"," [0.06562205 0.05776748]]\n","\n","Average MAE Loss:\n","[0.08524451 0.06789632 0.06124925 0.06169476]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06976905 0.06064646]\n"," [0.0652243  0.05861025]\n"," [0.06524833 0.05798995]]\n","\n","Average MAE Loss:\n","[0.06520776 0.06520775 0.06191727 0.06161914]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06976905 0.06064646]\n"," [0.065533   0.05772644]\n"," [0.06576008 0.05777183]]\n","\n","Average MAE Loss:\n","[0.06520776 0.06520776 0.06162972 0.06176595]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06976905 0.06064646]\n"," [0.06517605 0.05777799]\n"," [0.06601033 0.05781595]]\n","\n","Average MAE Loss:\n","[0.06520775 0.06520776 0.06147702 0.06191314]\n","\n","Epoch 00031: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06976905 0.06064646]\n"," [0.06510685 0.05781957]\n"," [0.06602886 0.0578168 ]]\n","\n","Average MAE Loss:\n","[0.06520776 0.06520776 0.06146321 0.06192283]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06976905 0.06064646]\n"," [0.06519948 0.05765264]\n"," [0.06583728 0.05775789]]\n","\n","Average MAE Loss:\n","[0.06520775 0.06520776 0.06142606 0.06179758]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06976905 0.06064646]\n"," [0.06513162 0.05766385]\n"," [0.06579461 0.05777858]]\n","\n","Average MAE Loss:\n","[0.06520776 0.06520776 0.06139773 0.06178659]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06976905 0.06064646]\n"," [0.06509408 0.05766537]\n"," [0.06601982 0.05781975]]\n","\n","Average MAE Loss:\n","[0.06520776 0.06520775 0.06137973 0.06191978]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06601982 0.05781975]\n"," [0.06723246 0.05889783]\n"," [0.06553499 0.05760765]]\n","\n","Average MAE Loss:\n","[0.06520776 0.06191978 0.06306515 0.06157132]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06601982 0.05781975]\n"," [0.06548437 0.05776559]\n"," [0.06573283 0.05773149]]\n","\n","Average MAE Loss:\n","[0.06520775 0.06191978 0.06162498 0.06173216]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06601982 0.05781975]\n"," [0.06521499 0.05780584]\n"," [0.06537165 0.05758781]]\n","\n","Average MAE Loss:\n","[0.06520776 0.06191978 0.06151042 0.06147973]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06601982 0.05781975]\n"," [0.06522231 0.05774961]\n"," [0.06549951 0.05765894]]\n","\n","Average MAE Loss:\n","[0.06520775 0.06191978 0.06148596 0.06157923]\n","\n","Epoch 00039: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06601982 0.05781975]\n"," [0.06524821 0.05770861]\n"," [0.06626198 0.05805195]]\n","\n","Average MAE Loss:\n","[0.06520775 0.06191978 0.06147841 0.06215697]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06601982 0.05781975]\n"," [0.065231   0.05770119]\n"," [0.06560574 0.0577703 ]]\n","\n","Average MAE Loss:\n","[0.06520775 0.06191978 0.06146609 0.06168802]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06976905 0.06064646]\n"," [0.06601982 0.05781975]\n"," [0.06519152 0.057709  ]\n"," [0.06589955 0.05781629]]\n","\n","Average MAE Loss:\n","[0.06520775 0.06191978 0.06145026 0.06185792]\n","\n","Epoch 00042: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06637567 0.05810042]\n"," [0.06595312 0.05785456]\n"," [0.06606298 0.05789127]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06223805 0.06190384 0.06197713]\n","\n","Epoch 00043: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06637567 0.05810042]\n"," [0.06572834 0.05775522]\n"," [0.06572454 0.05774268]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06223805 0.06174178 0.06173361]\n","\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06637567 0.05810042]\n"," [0.06557401 0.05770973]\n"," [0.06563994 0.05773106]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06223805 0.06164187 0.0616855 ]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06637567 0.05810042]\n"," [0.06546144 0.05770415]\n"," [0.06575154 0.0577441 ]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06223805 0.06158279 0.06174782]\n","\n","Epoch 00046: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06637567 0.05810042]\n"," [0.06538686 0.05772176]\n"," [0.06572386 0.05774033]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06223805 0.06155431 0.06173209]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06637567 0.05810042]\n"," [0.06536293 0.05773208]\n"," [0.06567558 0.0577342 ]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06223805 0.0615475  0.06170489]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06637567 0.05810042]\n"," [0.06534912 0.05773765]\n"," [0.06566944 0.05774953]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06223805 0.06154338 0.06170949]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06566944 0.05774953]\n"," [0.06624994 0.05802152]\n"," [0.06546064 0.05766796]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06170949 0.06213573 0.0615643 ]\n","\n","Epoch 00050: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06566944 0.05774953]\n"," [0.06608499 0.05792329]\n"," [0.0655434  0.05767136]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06170949 0.06200414 0.06160738]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06566944 0.05774953]\n"," [0.06600753 0.05788062]\n"," [0.06561811 0.05768273]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06170949 0.06194407 0.06165042]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06566944 0.05774953]\n"," [0.06593557 0.05784589]\n"," [0.06561935 0.05767966]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06170949 0.06189073 0.06164951]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06566944 0.05774953]\n"," [0.06587392 0.05781673]\n"," [0.06560639 0.05767684]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06170949 0.06184532 0.06164161]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00054: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06566944 0.05774953]\n"," [0.06581638 0.0577912 ]\n"," [0.06559767 0.05767187]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06170949 0.06180379 0.06163477]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06637567 0.05810042]\n"," [0.06566944 0.05774953]\n"," [0.06578783 0.05777904]\n"," [0.06557442 0.05766724]]\n","\n","Average MAE Loss:\n","[0.06223805 0.06170949 0.06178344 0.06162083]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.0658169  0.0577746 ]\n"," [0.0658169  0.0577746 ]\n"," [0.06578973 0.05776367]\n"," [0.06577526 0.05775552]]\n","\n","Average MAE Loss:\n","[0.06179575 0.06179575 0.0617767  0.06176539]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.0658169  0.0577746 ]\n"," [0.0658169  0.0577746 ]\n"," [0.06576308 0.05775315]\n"," [0.06574783 0.0577435 ]]\n","\n","Average MAE Loss:\n","[0.06179575 0.06179575 0.06175812 0.06174567]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00058: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.0658169  0.0577746 ]\n"," [0.0658169  0.0577746 ]\n"," [0.06573793 0.05774366]\n"," [0.0657353  0.05773948]]\n","\n","Average MAE Loss:\n","[0.06179575 0.06179575 0.0617408  0.06173739]\n","\n","Epoch 00059: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.0658169  0.0577746 ]\n"," [0.0658169  0.0577746 ]\n"," [0.0657264  0.05773965]\n"," [0.06571967 0.05773479]]\n","\n","Average MAE Loss:\n","[0.06179575 0.06179575 0.06173303 0.06172723]\n","\n","\n","epochs finished with time:364.4798777103424\n","\n","[[0.0658169  0.0577746 ]\n"," [0.0658169  0.05777459]\n"," [0.0657264  0.05773965]\n"," [0.06571967 0.05773479]]\n","00:06:24.04806552900004\n","------------------------------------Fold [4/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07008551 0.09225553]\n"," [0.07516487 0.09693794]\n"," [0.07035616 0.09170125]\n"," [0.08404089 0.08412736]]\n","\n","Average MAE Loss:\n","[0.08117052 0.0860514  0.08102871 0.08408412]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07008551 0.09225553]\n"," [0.07516487 0.09693794]\n"," [0.06812425 0.08714022]\n"," [0.08410832 0.08028961]]\n","\n","Average MAE Loss:\n","[0.08117052 0.0860514  0.07763223 0.08219897]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07008551 0.09225553]\n"," [0.07516487 0.09693794]\n"," [0.06796008 0.08461957]\n"," [0.08410832 0.07915427]]\n","\n","Average MAE Loss:\n","[0.08117052 0.0860514  0.07628983 0.0816313 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07008551 0.09225553]\n"," [0.07516487 0.09693794]\n"," [0.0689024  0.09675545]\n"," [0.08410832 0.07861266]]\n","\n","Average MAE Loss:\n","[0.08117052 0.0860514  0.08282893 0.08136049]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07008551 0.09225553]\n"," [0.07516487 0.09693794]\n"," [0.07035656 0.0926997 ]\n"," [0.08410832 0.0771442 ]]\n","\n","Average MAE Loss:\n","[0.08117052 0.0860514  0.08152813 0.08062626]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07008551 0.09225553]\n"," [0.07516487 0.09693794]\n"," [0.06518078 0.0908474 ]\n"," [0.08410832 0.07879096]]\n","\n","Average MAE Loss:\n","[0.08117052 0.0860514  0.07801409 0.08144964]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07008551 0.09225553]\n"," [0.07516487 0.09693794]\n"," [0.06952818 0.10105535]\n"," [0.08410832 0.07612756]]\n","\n","Average MAE Loss:\n","[0.08117052 0.0860514  0.08529177 0.08011794]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07516487 0.09693794]\n"," [0.08410832 0.07612756]\n"," [0.06592881 0.09221999]\n"," [0.08404359 0.08583438]]\n","\n","Average MAE Loss:\n","[0.0860514  0.08011794 0.0790744  0.08493898]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07516487 0.09693794]\n"," [0.08410832 0.07612756]\n"," [0.07310677 0.09554839]\n"," [0.08410832 0.07583921]]\n","\n","Average MAE Loss:\n","[0.0860514  0.08011794 0.08432758 0.07997377]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07516487 0.09693794]\n"," [0.08410832 0.07612756]\n"," [0.06795222 0.08750574]\n"," [0.08410832 0.0766223 ]]\n","\n","Average MAE Loss:\n","[0.0860514  0.08011794 0.07772898 0.08036531]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07516487 0.09693794]\n"," [0.08410832 0.07612756]\n"," [0.06076453 0.08071025]\n"," [0.08410832 0.07591071]]\n","\n","Average MAE Loss:\n","[0.0860514  0.08011794 0.07073739 0.08000952]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07516487 0.09693794]\n"," [0.08410832 0.07612756]\n"," [0.05837476 0.07603336]\n"," [0.08410832 0.10341229]]\n","\n","Average MAE Loss:\n","[0.0860514  0.08011794 0.06720406 0.0937603 ]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07516487 0.09693794]\n"," [0.08410832 0.07612756]\n"," [0.05655591 0.07681303]\n"," [0.08410832 0.10618569]]\n","\n","Average MAE Loss:\n","[0.0860514  0.08011794 0.06668447 0.09514701]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07516487 0.09693794]\n"," [0.08410832 0.07612756]\n"," [0.055691   0.07347911]\n"," [0.08410832 0.10313027]]\n","\n","Average MAE Loss:\n","[0.0860514  0.08011794 0.06458505 0.0936193 ]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07673834 0.09346756]\n"," [0.07673835 0.09346756]\n"," [0.05882915 0.0739342 ]\n"," [0.06563081 0.08216482]]\n","\n","Average MAE Loss:\n","[0.08510295 0.08510296 0.06638168 0.07389781]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07673834 0.09346756]\n"," [0.07673835 0.09346756]\n"," [0.05357244 0.07053525]\n"," [0.05662832 0.07429855]]\n","\n","Average MAE Loss:\n","[0.08510295 0.08510296 0.06205384 0.06546343]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.07673834 0.09346756]\n"," [0.05248335 0.07160509]\n"," [0.05455776 0.07227644]]\n","\n","Average MAE Loss:\n","[0.08510296 0.08510295 0.06204422 0.0634171 ]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.07673835 0.09346756]\n"," [0.05261158 0.07287593]\n"," [0.05625964 0.07185046]]\n","\n","Average MAE Loss:\n","[0.08510296 0.08510296 0.06274376 0.06405505]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.07673835 0.09346756]\n"," [0.0523472  0.07193596]\n"," [0.06064915 0.07145754]]\n","\n","Average MAE Loss:\n","[0.08510296 0.08510296 0.06214158 0.06605334]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.07673835 0.09346756]\n"," [0.05262827 0.07237116]\n"," [0.06377849 0.0703704 ]]\n","\n","Average MAE Loss:\n","[0.08510296 0.08510296 0.06249972 0.06707444]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07673834 0.09346756]\n"," [0.07673834 0.09346756]\n"," [0.05225043 0.07120804]\n"," [0.06515823 0.07007938]]\n","\n","Average MAE Loss:\n","[0.08510295 0.08510295 0.06172924 0.06761881]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.06515823 0.07007938]\n"," [0.05838349 0.07460088]\n"," [0.05217996 0.07071064]]\n","\n","Average MAE Loss:\n","[0.08510296 0.06761881 0.06649218 0.0614453 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.06515823 0.07007938]\n"," [0.05560382 0.0730134 ]\n"," [0.05234443 0.07136444]]\n","\n","Average MAE Loss:\n","[0.08510296 0.06761881 0.06430861 0.06185443]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07673834 0.09346756]\n"," [0.06515823 0.07007938]\n"," [0.05390362 0.07332909]\n"," [0.05212988 0.07083399]]\n","\n","Average MAE Loss:\n","[0.08510295 0.0676188  0.06361636 0.06148194]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.06515823 0.07007938]\n"," [0.05403665 0.07375494]\n"," [0.05217148 0.07091963]]\n","\n","Average MAE Loss:\n","[0.08510296 0.06761881 0.0638958  0.06154555]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07673835 0.09346756]\n"," [0.06515823 0.07007938]\n"," [0.05248404 0.07112497]\n"," [0.05235906 0.07134613]]\n","\n","Average MAE Loss:\n","[0.08510296 0.06761881 0.0618045  0.0618526 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07673834 0.09346756]\n"," [0.06515823 0.07007938]\n"," [0.05213876 0.07111334]\n"," [0.05216678 0.07082321]]\n","\n","Average MAE Loss:\n","[0.08510295 0.06761881 0.06162605 0.061495  ]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07673834 0.09346756]\n"," [0.06515823 0.07007938]\n"," [0.05224965 0.07103729]\n"," [0.05215331 0.07073189]]\n","\n","Average MAE Loss:\n","[0.08510295 0.06761881 0.06164347 0.0614426 ]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05454676 0.07448278]\n"," [0.05281465 0.0705563 ]\n"," [0.05221755 0.07065287]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06451477 0.06168547 0.06143521]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05454676 0.07448278]\n"," [0.05231425 0.07153065]\n"," [0.05213326 0.07087774]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06451477 0.06192245 0.0615055 ]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05454676 0.07448278]\n"," [0.05215535 0.07112385]\n"," [0.05210818 0.0708818 ]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06451477 0.0616396  0.06149499]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05454676 0.07448278]\n"," [0.0521368  0.07084634]\n"," [0.05210597 0.07072085]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06451477 0.06149157 0.06141341]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05454676 0.07448278]\n"," [0.05217267 0.07090726]\n"," [0.05214717 0.07076176]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06451477 0.06153997 0.06145447]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05454676 0.07448278]\n"," [0.05212998 0.07099751]\n"," [0.05216366 0.07076985]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06451477 0.06156374 0.06146676]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05454676 0.07448278]\n"," [0.05210624 0.07105028]\n"," [0.05219425 0.07086807]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06451477 0.06157826 0.06153116]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05219425 0.07086807]\n"," [0.05244662 0.07037488]\n"," [0.05206197 0.07103993]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06153116 0.06141075 0.06155095]\n","\n","Epoch 00036: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05219425 0.07086807]\n"," [0.05216536 0.07112101]\n"," [0.05204002 0.07096667]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06153116 0.06164319 0.06150334]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05219425 0.07086807]\n"," [0.052241   0.07081535]\n"," [0.05203678 0.07083458]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06153116 0.06152818 0.06143568]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05219425 0.07086807]\n"," [0.05213849 0.07108858]\n"," [0.05204109 0.07089725]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06153116 0.06161354 0.06146917]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05219425 0.07086807]\n"," [0.05213682 0.0707699 ]\n"," [0.05207064 0.07092711]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06153116 0.06145336 0.06149888]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05219425 0.07086807]\n"," [0.05213535 0.07087099]\n"," [0.05205277 0.07081427]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06153116 0.06150317 0.06143352]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05454676 0.07448278]\n"," [0.05219425 0.07086807]\n"," [0.05217403 0.07091862]\n"," [0.05205132 0.07074719]]\n","\n","Average MAE Loss:\n","[0.06451477 0.06153116 0.06154633 0.06139925]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05221291 0.07133052]\n"," [0.05209685 0.07069229]\n"," [0.05213209 0.07105536]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06177171 0.06139457 0.06159373]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05221291 0.07133052]\n"," [0.05214807 0.07069198]\n"," [0.05209465 0.07079404]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06177171 0.06142002 0.06144434]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05221291 0.07133052]\n"," [0.05213742 0.0708428 ]\n"," [0.05208711 0.0707786 ]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06177171 0.06149011 0.06143286]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05221291 0.07133052]\n"," [0.05216882 0.07085389]\n"," [0.05208107 0.07072316]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06177171 0.06151135 0.06140212]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05221291 0.07133052]\n"," [0.05214505 0.07085195]\n"," [0.05208506 0.07068665]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06177171 0.0614985  0.06138586]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05221291 0.07133052]\n"," [0.05215165 0.07086551]\n"," [0.05208659 0.07070374]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06177171 0.06150858 0.06139517]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05221291 0.07133052]\n"," [0.05214246 0.07089905]\n"," [0.05208441 0.0707027 ]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06177171 0.06152076 0.06139356]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05208441 0.07070271]\n"," [0.0520594  0.07089703]\n"," [0.05209088 0.07094979]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06139356 0.06147822 0.06152033]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05208441 0.07070271]\n"," [0.05207962 0.07066815]\n"," [0.05206421 0.07101113]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06139356 0.06137389 0.06153767]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05208441 0.0707027 ]\n"," [0.05212917 0.07068185]\n"," [0.05205938 0.07099567]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06139356 0.06140551 0.06152753]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05208441 0.0707027 ]\n"," [0.05212145 0.07074761]\n"," [0.0520571  0.07097073]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06139356 0.06143453 0.06151391]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05208441 0.0707027 ]\n"," [0.05213602 0.07078452]\n"," [0.05205503 0.07093574]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06139356 0.06146027 0.06149539]\n","\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05208441 0.0707027 ]\n"," [0.05215034 0.07080539]\n"," [0.05204961 0.07097819]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06139356 0.06147787 0.0615139 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05221291 0.07133052]\n"," [0.05208441 0.0707027 ]\n"," [0.05215369 0.07081662]\n"," [0.05204777 0.07099178]]\n","\n","Average MAE Loss:\n","[0.06177171 0.06139356 0.06148515 0.06151978]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05205781 0.0708993 ]\n"," [0.05205781 0.0708993 ]\n"," [0.05205782 0.07078721]\n"," [0.05205692 0.07088988]]\n","\n","Average MAE Loss:\n","[0.06147856 0.06147856 0.06142251 0.0614734 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05205781 0.0708993 ]\n"," [0.05205781 0.0708993 ]\n"," [0.05207791 0.07070207]\n"," [0.05205622 0.07086062]]\n","\n","Average MAE Loss:\n","[0.06147856 0.06147856 0.06138999 0.06145842]\n","\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05205781 0.0708993 ]\n"," [0.05205781 0.0708993 ]\n"," [0.05210088 0.07069324]\n"," [0.05205524 0.07083534]]\n","\n","Average MAE Loss:\n","[0.06147856 0.06147856 0.06139706 0.06144529]\n","\n","Epoch 00059: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00059: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05205781 0.0708993 ]\n"," [0.05205781 0.0708993 ]\n"," [0.05211395 0.07069193]\n"," [0.05205529 0.0708347 ]]\n","\n","Average MAE Loss:\n","[0.06147856 0.06147856 0.06140294 0.06144499]\n","\n","\n","epochs finished with time:369.6365964412689\n","\n","[[0.05205781 0.0708993 ]\n"," [0.05205781 0.0708993 ]\n"," [0.05211395 0.07069193]\n"," [0.05205529 0.0708347 ]]\n","00:06:29.239966656999968\n","------------------------------------Fold [5/5]-----------------------------------------\n","GAT\n","GAT\n","GAT\n","GAT\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08723602 0.09310579]\n"," [0.0937808  0.09830491]\n"," [0.08611272 0.09214191]\n"," [0.10383698 0.0883541 ]]\n","\n","Average MAE Loss:\n","[0.0901709  0.09604286 0.08912732 0.09609554]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08723602 0.09310579]\n"," [0.0937808  0.09830491]\n"," [0.07666846 0.08348309]\n"," [0.10398609 0.08401935]]\n","\n","Average MAE Loss:\n","[0.0901709  0.09604286 0.08007577 0.09400272]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08723602 0.09310579]\n"," [0.0937808  0.09830491]\n"," [0.08156312 0.08216327]\n"," [0.10398609 0.08173525]]\n","\n","Average MAE Loss:\n","[0.0901709  0.09604286 0.08186319 0.09286067]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08723602 0.09310579]\n"," [0.0937808  0.09830491]\n"," [0.07387978 0.084104  ]\n"," [0.10369623 0.07948971]]\n","\n","Average MAE Loss:\n","[0.0901709  0.09604286 0.07899189 0.09159297]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08723602 0.09310579]\n"," [0.0937808  0.09830491]\n"," [0.07828791 0.08695257]\n"," [0.10398609 0.08342582]]\n","\n","Average MAE Loss:\n","[0.0901709  0.09604286 0.08262024 0.09370596]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08723602 0.09310579]\n"," [0.0937808  0.09830491]\n"," [0.07549943 0.08044735]\n"," [0.10398609 0.07229455]]\n","\n","Average MAE Loss:\n","[0.0901709  0.09604286 0.07797339 0.08814032]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08723602 0.09310579]\n"," [0.0937808  0.09830491]\n"," [0.07588957 0.08109757]\n"," [0.10398609 0.07572705]]\n","\n","Average MAE Loss:\n","[0.0901709  0.09604286 0.07849357 0.08985657]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0937808  0.09830491]\n"," [0.10398609 0.07572705]\n"," [0.09129032 0.09248006]\n"," [0.10384768 0.07783739]]\n","\n","Average MAE Loss:\n","[0.09604286 0.08985657 0.09188519 0.09084254]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0937808  0.09830491]\n"," [0.10398609 0.07572705]\n"," [0.08324886 0.1003882 ]\n"," [0.10398609 0.07122181]]\n","\n","Average MAE Loss:\n","[0.09604286 0.08985657 0.09181853 0.08760395]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0937808  0.09830491]\n"," [0.10398609 0.07572705]\n"," [0.08042103 0.08644677]\n"," [0.10398609 0.07519069]]\n","\n","Average MAE Loss:\n","[0.09604286 0.08985657 0.0834339  0.08958839]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0937808  0.09830491]\n"," [0.10398609 0.07572705]\n"," [0.07883664 0.08623117]\n"," [0.10398609 0.07900082]]\n","\n","Average MAE Loss:\n","[0.09604286 0.08985657 0.08253391 0.09149345]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0937808  0.09830491]\n"," [0.10398609 0.07572705]\n"," [0.0762919  0.07959647]\n"," [0.10398609 0.06718057]]\n","\n","Average MAE Loss:\n","[0.09604286 0.08985657 0.07794419 0.08558333]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0937808  0.09830491]\n"," [0.10398609 0.07572705]\n"," [0.06981568 0.07594302]\n"," [0.10398609 0.06608789]]\n","\n","Average MAE Loss:\n","[0.09604286 0.08985657 0.07287935 0.08503699]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0937808  0.09830491]\n"," [0.10398609 0.07572705]\n"," [0.06734803 0.06869137]\n"," [0.10398609 0.06821522]]\n","\n","Average MAE Loss:\n","[0.09604286 0.08985657 0.0680197  0.08610066]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.09343091 0.09097303]\n"," [0.06805913 0.0679749 ]\n"," [0.06780191 0.07078844]]\n","\n","Average MAE Loss:\n","[0.09220197 0.09220197 0.06801702 0.06929518]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.09343091 0.09097303]\n"," [0.06561304 0.06840505]\n"," [0.07528678 0.07008691]]\n","\n","Average MAE Loss:\n","[0.09220197 0.09220197 0.06700904 0.07268684]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.09343091 0.09097303]\n"," [0.0659102  0.07099586]\n"," [0.08212364 0.06886826]]\n","\n","Average MAE Loss:\n","[0.09220197 0.09220197 0.06845303 0.07549595]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.09343091 0.09097303]\n"," [0.06588491 0.07167275]\n"," [0.08977329 0.06825128]]\n","\n","Average MAE Loss:\n","[0.09220197 0.09220197 0.06877883 0.07901229]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.09343091 0.09097303]\n"," [0.06587004 0.07086329]\n"," [0.09959897 0.06729472]]\n","\n","Average MAE Loss:\n","[0.09220197 0.09220197 0.06836667 0.08344685]\n","\n","Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.09343091 0.09097303]\n"," [0.06552236 0.06980253]\n"," [0.10090242 0.06806511]]\n","\n","Average MAE Loss:\n","[0.09220197 0.09220197 0.06766244 0.08448377]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.09343091 0.09097303]\n"," [0.06513873 0.06743649]\n"," [0.10310239 0.06722134]]\n","\n","Average MAE Loss:\n","[0.09220197 0.09220197 0.06628761 0.08516186]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.10310239 0.06722134]\n"," [0.0677076  0.07056426]\n"," [0.06564844 0.07056124]]\n","\n","Average MAE Loss:\n","[0.09220197 0.08516186 0.06913593 0.06810484]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.10310239 0.06722134]\n"," [0.06722613 0.07375609]\n"," [0.06595374 0.07124101]]\n","\n","Average MAE Loss:\n","[0.09220197 0.08516186 0.07049111 0.06859738]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.10310239 0.06722134]\n"," [0.06524083 0.06847844]\n"," [0.06480554 0.0688296 ]]\n","\n","Average MAE Loss:\n","[0.09220197 0.08516186 0.06685964 0.06681757]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.10310239 0.06722134]\n"," [0.06534085 0.0689415 ]\n"," [0.0650408  0.06883086]]\n","\n","Average MAE Loss:\n","[0.09220197 0.08516186 0.06714118 0.06693583]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.10310239 0.06722134]\n"," [0.06525334 0.06892408]\n"," [0.06526615 0.06895047]]\n","\n","Average MAE Loss:\n","[0.09220197 0.08516186 0.06708871 0.06710831]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.10310239 0.06722134]\n"," [0.06518678 0.06844135]\n"," [0.06565172 0.0689375 ]]\n","\n","Average MAE Loss:\n","[0.09220197 0.08516186 0.06681406 0.06729461]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.09343091 0.09097303]\n"," [0.10310239 0.06722134]\n"," [0.06525514 0.06828535]\n"," [0.06533409 0.06853407]]\n","\n","Average MAE Loss:\n","[0.09220197 0.08516186 0.06677025 0.06693408]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.07206833 0.07448448]\n"," [0.06336856 0.06662216]\n"," [0.06358217 0.06656004]]\n","\n","Average MAE Loss:\n","[0.0732764  0.0732764  0.06499536 0.0650711 ]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.07206833 0.07448448]\n"," [0.06288216 0.06626303]\n"," [0.06416436 0.06791386]]\n","\n","Average MAE Loss:\n","[0.0732764  0.0732764  0.0645726  0.06603911]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.07206833 0.07448448]\n"," [0.06281925 0.06625003]\n"," [0.06353576 0.06725816]]\n","\n","Average MAE Loss:\n","[0.0732764  0.0732764  0.06453464 0.06539696]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.07206833 0.07448448]\n"," [0.06271338 0.06596795]\n"," [0.06343387 0.06708558]]\n","\n","Average MAE Loss:\n","[0.0732764  0.0732764  0.06434066 0.06525972]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.07206833 0.07448448]\n"," [0.06274409 0.06592582]\n"," [0.06385344 0.06734475]]\n","\n","Average MAE Loss:\n","[0.0732764  0.0732764  0.06433496 0.0655991 ]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.07206833 0.07448448]\n"," [0.06274673 0.06597355]\n"," [0.0634764  0.06688425]]\n","\n","Average MAE Loss:\n","[0.0732764  0.0732764  0.06436014 0.06518032]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.07206833 0.07448448]\n"," [0.06273183 0.06596918]\n"," [0.06348095 0.06685913]]\n","\n","Average MAE Loss:\n","[0.0732764  0.0732764  0.06435051 0.06517004]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.06348095 0.06685913]\n"," [0.06338786 0.06664665]\n"," [0.06338243 0.06742685]]\n","\n","Average MAE Loss:\n","[0.0732764  0.06517004 0.06501726 0.06540464]\n","\n","Epoch 00036: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.06348095 0.06685913]\n"," [0.062888   0.06594145]\n"," [0.06284278 0.06640735]]\n","\n","Average MAE Loss:\n","[0.0732764  0.06517004 0.06441472 0.06462507]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.06348095 0.06685913]\n"," [0.06283531 0.06622355]\n"," [0.06319056 0.06723361]]\n","\n","Average MAE Loss:\n","[0.0732764  0.06517004 0.06452943 0.06521209]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.06348095 0.06685913]\n"," [0.06281238 0.06625025]\n"," [0.06295459 0.06682207]]\n","\n","Average MAE Loss:\n","[0.0732764  0.06517004 0.06453132 0.06488833]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.06348095 0.06685913]\n"," [0.06267448 0.06581143]\n"," [0.06303328 0.06685928]]\n","\n","Average MAE Loss:\n","[0.0732764  0.06517004 0.06424295 0.06494628]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.06348095 0.06685913]\n"," [0.06269497 0.06579495]\n"," [0.06305885 0.06687891]]\n","\n","Average MAE Loss:\n","[0.0732764  0.06517004 0.06424496 0.06496888]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.07206833 0.07448448]\n"," [0.06348095 0.06685913]\n"," [0.06271543 0.0658225 ]\n"," [0.06310319 0.06694254]]\n","\n","Average MAE Loss:\n","[0.0732764  0.06517004 0.06426897 0.06502287]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.0643858  0.06826795]\n"," [0.06312837 0.06665725]\n"," [0.06334889 0.06692058]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06632688 0.06489281 0.06513473]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.0643858  0.06826795]\n"," [0.06277207 0.06605876]\n"," [0.06317019 0.06672578]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06632688 0.06441542 0.06494799]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.0643858  0.06826795]\n"," [0.06275368 0.06602653]\n"," [0.0632793  0.06694657]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06632688 0.0643901  0.06511293]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.0643858  0.06826795]\n"," [0.06275763 0.06602689]\n"," [0.06329837 0.06700435]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06632688 0.06439226 0.06515136]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.0643858  0.06826795]\n"," [0.06282626 0.06614614]\n"," [0.06315708 0.06679547]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06632688 0.0644862  0.06497627]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.0643858  0.06826795]\n"," [0.06279635 0.06607505]\n"," [0.06319815 0.06686059]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06632688 0.0644357  0.06502937]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.0643858  0.06826795]\n"," [0.06275816 0.06598226]\n"," [0.06313415 0.06677993]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06632688 0.06437021 0.06495704]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.5625e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.06313415 0.06677993]\n"," [0.06389105 0.0676925 ]\n"," [0.0628389  0.06621726]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06495704 0.06579177 0.06452808]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.06313415 0.06677993]\n"," [0.06339275 0.06705385]\n"," [0.06294847 0.066469  ]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06495704 0.0652233  0.06470874]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.06313416 0.06677993]\n"," [0.0630951  0.06661346]\n"," [0.06302202 0.06661855]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06495704 0.06485428 0.06482028]\n","\n","Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.06313415 0.06677993]\n"," [0.06300446 0.06646015]\n"," [0.06303644 0.06665964]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06495704 0.0647323  0.06484804]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.06313415 0.06677993]\n"," [0.06293831 0.06634729]\n"," [0.06305281 0.0666977 ]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06495704 0.0646428  0.06487526]\n","\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00054: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.06313415 0.06677993]\n"," [0.06289206 0.06625687]\n"," [0.06308196 0.06675486]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06495704 0.06457447 0.06491841]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0643858  0.06826795]\n"," [0.06313415 0.06677993]\n"," [0.0628655  0.06620648]\n"," [0.06305969 0.06672393]]\n","\n","Average MAE Loss:\n","[0.06632688 0.06495704 0.06453599 0.06489181]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06326533 0.06694897]\n"," [0.06326533 0.06694897]\n"," [0.06320879 0.06686568]\n"," [0.06322777 0.06689367]]\n","\n","Average MAE Loss:\n","[0.06510715 0.06510715 0.06503724 0.06506072]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06326533 0.06694897]\n"," [0.06326533 0.06694897]\n"," [0.06313374 0.06675284]\n"," [0.06323151 0.06689929]]\n","\n","Average MAE Loss:\n","[0.06510715 0.06510715 0.06494329 0.0650654 ]\n","\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00058: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06326533 0.06694897]\n"," [0.06326533 0.06694897]\n"," [0.06307538 0.06666396]\n"," [0.06321524 0.06687417]]\n","\n","Average MAE Loss:\n","[0.06510715 0.06510715 0.06486967 0.0650447 ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06326533 0.06694897]\n"," [0.06326533 0.06694897]\n"," [0.0630233  0.0665824 ]\n"," [0.06320919 0.06686866]]\n","\n","Average MAE Loss:\n","[0.06510715 0.06510715 0.06480285 0.06503892]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.9531e-06.\n","\n","epochs finished with time:368.02757596969604\n","\n","[[0.06326533 0.06694897]\n"," [0.06326533 0.06694897]\n"," [0.0630233  0.0665824 ]\n"," [0.06320919 0.06686866]]\n","00:06:27.720077554\n"]}],"source":["# seed = 10 table = 2/8 fixed folds\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='GAT_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"sf-WvtWAntpD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v7zuUNtIu26b"},"source":["## ChebNet\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6a8lMZnZvaZl"},"outputs":[],"source":["from torch_geometric.nn.conv import ChebConv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2B4n_y-vjez"},"outputs":[],"source":["class DeepChebNet(torch.nn.Module):\n","    def __init__(self,device):\n","        super(DeepChebNet, self).__init__()\n","        in_features = 35\n","        out_features = 140\n","        self.conv1 = ChebConv(in_features, out_features, K=2)\n","        self.conv2 = ChebConv(out_features, 2*out_features, K=2)\n","        self.fc = nn.Linear(2*out_features,35)\n","        self.device = device\n","\n","    def forward(self, adj_matrix):\n","\n","        data = adj_matrix_to_pytorch_geometric_data(adj_matrix,self.device)\n","        edge_index = data.edge_index\n","        data = self.conv1(data.adj_matrix, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.3, training=self.training)\n","        data = self.conv2(data, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.5, training=self.training)\n","        out = F.relu(self.fc(data))\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690552814136,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"lDtElL8xyhrM","outputId":"d56e7d1b-b92f-4037-fec6-408c86c0eb78"},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1690554530929,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"pl605rk5vjib","outputId":"f28666e0-349b-4927-8d34-eb76e3f3ca52"},"outputs":[{"data":{"text/plain":["tensor([[0.0000, 0.0142, 0.0430,  ..., 0.0000, 0.0237, 0.1471],\n","        [0.0748, 0.0300, 0.0344,  ..., 0.0000, 0.0000, 0.0879],\n","        [0.1373, 0.0453, 0.0000,  ..., 0.0000, 0.0000, 0.0661],\n","        ...,\n","        [0.0000, 0.0000, 0.1067,  ..., 0.0000, 0.0000, 0.0389],\n","        [0.0300, 0.0151, 0.0690,  ..., 0.0031, 0.0000, 0.0898],\n","        [0.1029, 0.0396, 0.0046,  ..., 0.0000, 0.0000, 0.0000]],\n","       device='cuda:0', grad_fn=<ReluBackward0>)"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["data = dataset[0][0].to(device)\n","gat = DeepChebNet(device).to(device)\n","gat(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1690554533433,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"FarPaN5vzg71","outputId":"5cb630c2-02a4-4b49-e8a4-317446311c94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 98455\n","Trainable parameters: 98455\n"]}],"source":["model = DeepChebNet(device)\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AlNaYDnVzRwI"},"outputs":[],"source":["class Hospital_gnns():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = DeepChebNet(device=device).to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiWOdaKNzVRc"},"outputs":[],"source":["def train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = False\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_gnns(args,device))\n","                  print('DeepChebNet')\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2700575,"status":"ok","timestamp":1690559464419,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"661Hbgs83CMC","outputId":"830cd73b-559f-4347-fd2e-cf2e2091e6f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.04572127 0.0473866 ]\n"," [0.07827297 0.07513477]\n"," [0.04737947 0.0493499 ]\n"," [0.07588218 0.06153284]]\n","\n","Average MAE Loss:\n","[0.04655393 0.07670387 0.04836469 0.06870751]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.03840624 0.03943488]\n"," [0.07827297 0.07513476]\n"," [0.04450329 0.04762704]\n"," [0.07928729 0.05333582]]\n","\n","Average MAE Loss:\n","[0.03892056 0.07670387 0.04606516 0.06631155]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.03776373 0.03822628]\n"," [0.07827297 0.07513477]\n"," [0.04186774 0.0437093 ]\n"," [0.08359349 0.04695408]]\n","\n","Average MAE Loss:\n","[0.037995   0.07670387 0.04278852 0.06527378]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.036448   0.0378933 ]\n"," [0.07827297 0.07513477]\n"," [0.04147982 0.04118885]\n"," [0.08473877 0.04299527]]\n","\n","Average MAE Loss:\n","[0.03717065 0.07670387 0.04133433 0.06386702]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.03652274 0.03751233]\n"," [0.07827297 0.07513475]\n"," [0.0425214  0.04154283]\n"," [0.08491895 0.0425288 ]]\n","\n","Average MAE Loss:\n","[0.03701754 0.07670386 0.04203211 0.06372388]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.03709158 0.04066107]\n"," [0.07827297 0.07513477]\n"," [0.04148533 0.04014088]\n"," [0.08578058 0.04329427]]\n","\n","Average MAE Loss:\n","[0.03887633 0.07670387 0.0408131  0.06453743]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.03898067 0.04523253]\n"," [0.07827297 0.07513477]\n"," [0.04288096 0.04019095]\n"," [0.08603086 0.04512334]]\n","\n","Average MAE Loss:\n","[0.0421066  0.07670387 0.04153595 0.0655771 ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05044226 0.05402699]\n"," [0.08603086 0.04512334]\n"," [0.03901452 0.04099964]\n"," [0.04377259 0.0392442 ]]\n","\n","Average MAE Loss:\n","[0.05223463 0.0655771  0.04000708 0.0415084 ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04221677 0.04633956]\n"," [0.08603086 0.04512334]\n"," [0.0371267  0.03849106]\n"," [0.05462627 0.03875548]]\n","\n","Average MAE Loss:\n","[0.04427816 0.0655771  0.03780888 0.04669088]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.03813462 0.03952584]\n"," [0.08603086 0.04512334]\n"," [0.03892948 0.03906833]\n"," [0.06465603 0.03728761]]\n","\n","Average MAE Loss:\n","[0.03883023 0.0655771  0.0389989  0.05097182]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.03677478 0.03856102]\n"," [0.08603086 0.04512334]\n"," [0.04137781 0.04093842]\n"," [0.07402754 0.04195836]]\n","\n","Average MAE Loss:\n","[0.0376679  0.0655771  0.04115811 0.05799295]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0367013  0.03812256]\n"," [0.08603086 0.04512334]\n"," [0.04096087 0.03993707]\n"," [0.07220106 0.03956651]]\n","\n","Average MAE Loss:\n","[0.03741193 0.0655771  0.04044897 0.05588378]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.03702491 0.03883891]\n"," [0.08603086 0.04512334]\n"," [0.03838145 0.03955052]\n"," [0.06910045 0.03884043]]\n","\n","Average MAE Loss:\n","[0.03793191 0.0655771  0.03896599 0.05397044]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.03714198 0.04048417]\n"," [0.08603086 0.04512334]\n"," [0.03770902 0.03794515]\n"," [0.06674043 0.04015999]]\n","\n","Average MAE Loss:\n","[0.03881308 0.0655771  0.03782708 0.05345021]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05255097 0.05312073]\n"," [0.06486836 0.06157593]\n"," [0.04997515 0.05069978]\n"," [0.05520414 0.05428196]]\n","\n","Average MAE Loss:\n","[0.05283585 0.06322215 0.05033747 0.05474305]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04837178 0.0504266 ]\n"," [0.06486836 0.06157593]\n"," [0.04617033 0.04938354]\n"," [0.0505464  0.05111489]]\n","\n","Average MAE Loss:\n","[0.04939919 0.06322215 0.04777694 0.05083064]\n","\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04459586 0.04801651]\n"," [0.06486836 0.06157593]\n"," [0.04027066 0.04347183]\n"," [0.04994518 0.0503462 ]]\n","\n","Average MAE Loss:\n","[0.04630618 0.06322215 0.04187124 0.05014569]\n","\n","Epoch 00017: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04237728 0.04611168]\n"," [0.06486836 0.06157593]\n"," [0.03807138 0.03917251]\n"," [0.0496924  0.0502611 ]]\n","\n","Average MAE Loss:\n","[0.04424448 0.06322215 0.03862194 0.04997675]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04030294 0.04326084]\n"," [0.06486836 0.06157593]\n"," [0.03760665 0.03811264]\n"," [0.04936157 0.05015773]]\n","\n","Average MAE Loss:\n","[0.04178189 0.06322215 0.03785965 0.04975965]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03883939 0.04051894]\n"," [0.06486836 0.06157593]\n"," [0.0369976  0.03740055]\n"," [0.04895613 0.05001329]]\n","\n","Average MAE Loss:\n","[0.03967917 0.06322215 0.03719907 0.04948471]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03806385 0.0390216 ]\n"," [0.06486836 0.06157593]\n"," [0.03675488 0.03708052]\n"," [0.04870473 0.0499397 ]]\n","\n","Average MAE Loss:\n","[0.03854272 0.06322215 0.0369177  0.04932222]\n","\n","Epoch 00021: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06048059 0.05919134]\n"," [0.04870473 0.0499397 ]\n"," [0.03752729 0.03835677]\n"," [0.03635957 0.03676284]]\n","\n","Average MAE Loss:\n","[0.05983597 0.04932221 0.03794203 0.0365612 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.05519912 0.05556219]\n"," [0.04870473 0.0499397 ]\n"," [0.03699643 0.03765739]\n"," [0.03615152 0.03653135]]\n","\n","Average MAE Loss:\n","[0.05538065 0.04932222 0.03732691 0.03634143]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05135826 0.05227256]\n"," [0.04870473 0.0499397 ]\n"," [0.03664971 0.037133  ]\n"," [0.03598448 0.03642486]]\n","\n","Average MAE Loss:\n","[0.05181541 0.04932221 0.03689136 0.03620467]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.049371   0.05047905]\n"," [0.04870473 0.0499397 ]\n"," [0.03636652 0.03684322]\n"," [0.03574202 0.03618366]]\n","\n","Average MAE Loss:\n","[0.04992502 0.04932221 0.03660487 0.03596284]\n","\n","Epoch 00025: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04882535 0.05015073]\n"," [0.04870473 0.0499397 ]\n"," [0.03618433 0.03654255]\n"," [0.03569949 0.03612067]]\n","\n","Average MAE Loss:\n","[0.04948804 0.04932222 0.03636344 0.03591008]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04833208 0.04993361]\n"," [0.04870473 0.0499397 ]\n"," [0.03612208 0.03642433]\n"," [0.03562858 0.03607655]]\n","\n","Average MAE Loss:\n","[0.04913285 0.04932221 0.03627321 0.03585257]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04784126 0.04972687]\n"," [0.04870473 0.0499397 ]\n"," [0.03609951 0.03642642]\n"," [0.0355532  0.03610297]]\n","\n","Average MAE Loss:\n","[0.04878406 0.04932222 0.03626296 0.03582809]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04080003 0.04490271]\n"," [0.04145992 0.04560117]\n"," [0.03914691 0.04271621]\n"," [0.04033968 0.04434157]]\n","\n","Average MAE Loss:\n","[0.04285137 0.04353054 0.04093156 0.04234063]\n","\n","Epoch 00029: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04045769 0.04451385]\n"," [0.04145992 0.04560117]\n"," [0.03683051 0.03779066]\n"," [0.03842579 0.04146861]]\n","\n","Average MAE Loss:\n","[0.04248577 0.04353054 0.03731059 0.0399472 ]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04013155 0.04411082]\n"," [0.04145992 0.04560117]\n"," [0.03643142 0.03679819]\n"," [0.03696409 0.0381371 ]]\n","\n","Average MAE Loss:\n","[0.04212119 0.04353054 0.0366148  0.03755059]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03980823 0.04369434]\n"," [0.04145992 0.04560117]\n"," [0.03613755 0.0364445 ]\n"," [0.03648759 0.03721712]]\n","\n","Average MAE Loss:\n","[0.04175128 0.04353054 0.03629103 0.03685235]\n","\n","Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03949876 0.04326389]\n"," [0.04145992 0.04560117]\n"," [0.03609678 0.03643553]\n"," [0.03625541 0.0369657 ]]\n","\n","Average MAE Loss:\n","[0.04138133 0.04353054 0.03626615 0.03661055]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03935093 0.04304432]\n"," [0.04145992 0.04560117]\n"," [0.03615201 0.03646028]\n"," [0.03608907 0.03680847]]\n","\n","Average MAE Loss:\n","[0.04119762 0.04353054 0.03630615 0.03644877]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03921032 0.04282497]\n"," [0.04145992 0.04560117]\n"," [0.03612648 0.03643358]\n"," [0.03599461 0.03666474]]\n","\n","Average MAE Loss:\n","[0.04101765 0.04353054 0.03628003 0.03632967]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04128105 0.04541816]\n"," [0.03599461 0.03666474]\n"," [0.03832888 0.04140239]\n"," [0.03597096 0.03632084]]\n","\n","Average MAE Loss:\n","[0.04334961 0.03632967 0.03986564 0.0361459 ]\n","\n","Epoch 00036: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04109343 0.04522861]\n"," [0.03599461 0.03666474]\n"," [0.03770298 0.04009718]\n"," [0.03592647 0.03627868]]\n","\n","Average MAE Loss:\n","[0.04316102 0.03632967 0.03890008 0.03610258]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04100255 0.0451327 ]\n"," [0.03599461 0.03666474]\n"," [0.03722501 0.03896113]\n"," [0.03585076 0.03624034]]\n","\n","Average MAE Loss:\n","[0.04306762 0.03632967 0.03809307 0.03604555]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04091126 0.04503579]\n"," [0.03599461 0.03666474]\n"," [0.03688203 0.03817876]\n"," [0.03576996 0.03617531]]\n","\n","Average MAE Loss:\n","[0.04297353 0.03632967 0.03753039 0.03597264]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04082229 0.04493887]\n"," [0.03599461 0.03666474]\n"," [0.0366832  0.03764797]\n"," [0.03576485 0.03615906]]\n","\n","Average MAE Loss:\n","[0.04288058 0.03632967 0.03716559 0.03596195]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04073506 0.04484067]\n"," [0.03599462 0.03666474]\n"," [0.03660968 0.03743306]\n"," [0.03575717 0.03617145]]\n","\n","Average MAE Loss:\n","[0.04278786 0.03632968 0.03702137 0.03596431]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04069136 0.04479048]\n"," [0.03599462 0.03666474]\n"," [0.03656151 0.03729658]\n"," [0.03572579 0.03616869]]\n","\n","Average MAE Loss:\n","[0.04274092 0.03632968 0.03692905 0.03594724]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03670276 0.03824328]\n"," [0.03671828 0.03828247]\n"," [0.03659595 0.0380119 ]\n"," [0.03659737 0.03807666]]\n","\n","Average MAE Loss:\n","[0.03747302 0.03750037 0.03730392 0.03733702]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03669253 0.03821537]\n"," [0.03671828 0.03828247]\n"," [0.03648428 0.03772002]\n"," [0.03644237 0.03774677]]\n","\n","Average MAE Loss:\n","[0.03745395 0.03750037 0.03710215 0.03709457]\n","\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00044: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03668495 0.03819119]\n"," [0.03671828 0.03828247]\n"," [0.03642481 0.03756954]\n"," [0.03637852 0.03759433]]\n","\n","Average MAE Loss:\n","[0.03743807 0.03750037 0.03699717 0.03698643]\n","\n","Epoch 00045: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03668101 0.03817949]\n"," [0.03671828 0.03828247]\n"," [0.03638477 0.03744021]\n"," [0.03632245 0.03744719]]\n","\n","Average MAE Loss:\n","[0.03743025 0.03750037 0.03691249 0.03688482]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03667676 0.038167  ]\n"," [0.03671828 0.03828247]\n"," [0.03635055 0.03734142]\n"," [0.03626909 0.03732654]]\n","\n","Average MAE Loss:\n","[0.03742188 0.03750037 0.03684599 0.03679782]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03667249 0.03815514]\n"," [0.03671828 0.03828247]\n"," [0.03629345 0.03721308]\n"," [0.03622729 0.03722948]]\n","\n","Average MAE Loss:\n","[0.03741381 0.03750037 0.03675327 0.03672839]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03666939 0.03814436]\n"," [0.03671828 0.03828247]\n"," [0.03627003 0.03716151]\n"," [0.03620638 0.03718653]]\n","\n","Average MAE Loss:\n","[0.03740688 0.03750037 0.03671577 0.03669645]\n","\n","Epoch 00049: reducing learning rate of group 0 to 4.8828e-07.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03671637 0.03827633]\n"," [0.03620638 0.03718653]\n"," [0.03663046 0.03806332]\n"," [0.0362451  0.03711475]]\n","\n","Average MAE Loss:\n","[0.03749635 0.03669645 0.03734689 0.03667993]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03671452 0.03827006]\n"," [0.03620638 0.03718653]\n"," [0.03659012 0.03797099]\n"," [0.03621584 0.03706181]]\n","\n","Average MAE Loss:\n","[0.03749229 0.03669645 0.03728056 0.03663882]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03671238 0.03826366]\n"," [0.03620638 0.03718653]\n"," [0.03655422 0.0378765 ]\n"," [0.03618868 0.03700696]]\n","\n","Average MAE Loss:\n","[0.03748802 0.03669645 0.03721536 0.03659782]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00052: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03670994 0.03825718]\n"," [0.03620638 0.03718653]\n"," [0.03653795 0.03783467]\n"," [0.03617695 0.03698125]]\n","\n","Average MAE Loss:\n","[0.03748356 0.03669645 0.03718631 0.0365791 ]\n","\n","Epoch 00053: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03670855 0.0382538 ]\n"," [0.03620638 0.03718653]\n"," [0.0365204  0.03779445]\n"," [0.03616345 0.03695719]]\n","\n","Average MAE Loss:\n","[0.03748117 0.03669645 0.03715743 0.03656032]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03670731 0.03825064]\n"," [0.03620638 0.03718653]\n"," [0.03650407 0.03775539]\n"," [0.0361517  0.03693622]]\n","\n","Average MAE Loss:\n","[0.03747897 0.03669645 0.03712973 0.03654396]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03670594 0.03824716]\n"," [0.03620638 0.03718653]\n"," [0.03648967 0.03771761]\n"," [0.03613977 0.03691645]]\n","\n","Average MAE Loss:\n","[0.03747655 0.03669645 0.03710364 0.03652811]\n","\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00056: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03636034 0.03746719]\n"," [0.03636098 0.03746978]\n"," [0.03635541 0.03745437]\n"," [0.03635314 0.03745407]]\n","\n","Average MAE Loss:\n","[0.03691377 0.03691538 0.03690489 0.0369036 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03636014 0.03746504]\n"," [0.03636098 0.03746978]\n"," [0.03634931 0.03743991]\n"," [0.03634497 0.0374381 ]]\n","\n","Average MAE Loss:\n","[0.03691259 0.03691538 0.03689461 0.03689153]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03635987 0.03746283]\n"," [0.03636098 0.03746978]\n"," [0.03634406 0.03742476]\n"," [0.03633728 0.03742115]]\n","\n","Average MAE Loss:\n","[0.03691135 0.03691538 0.03688441 0.03687921]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03635951 0.03746085]\n"," [0.03636098 0.03746978]\n"," [0.03633982 0.03741113]\n"," [0.03632949 0.03740456]]\n","\n","Average MAE Loss:\n","[0.03691018 0.03691538 0.03687548 0.03686703]\n","\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00060: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","\n","epochs finished with time:528.3750631809235\n","\n","[[0.03635951 0.03746085]\n"," [0.03636098 0.03746978]\n"," [0.03633983 0.03741113]\n"," [0.03632949 0.03740456]]\n","00:09:8.201928812999995\n","------------------------------------Fold [2/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.06748901 0.07163185]\n"," [0.10415368 0.10648478]\n"," [0.06747851 0.07071254]\n"," [0.10956199 0.08992325]]\n","\n","Average MAE Loss:\n","[0.06956043 0.10531923 0.06909553 0.09974262]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06391211 0.06674129]\n"," [0.10415368 0.10648478]\n"," [0.06480535 0.0683413 ]\n"," [0.11043953 0.0770717 ]]\n","\n","Average MAE Loss:\n","[0.0653267  0.10531923 0.06657332 0.09375561]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06197508 0.06382436]\n"," [0.10415368 0.10648478]\n"," [0.06143926 0.06289752]\n"," [0.11538497 0.07600202]]\n","\n","Average MAE Loss:\n","[0.06289972 0.10531923 0.06216839 0.0956935 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.06049847 0.06276159]\n"," [0.10415368 0.10648478]\n"," [0.06042171 0.06105705]\n"," [0.11660689 0.07630293]]\n","\n","Average MAE Loss:\n","[0.06163003 0.10531923 0.06073938 0.09645491]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.05978716 0.06278678]\n"," [0.10415368 0.10648478]\n"," [0.06076942 0.06206328]\n"," [0.11538782 0.07087523]]\n","\n","Average MAE Loss:\n","[0.06128697 0.10531923 0.06141635 0.09313152]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.05990987 0.06362683]\n"," [0.10415368 0.10648478]\n"," [0.06046418 0.06567705]\n"," [0.11361622 0.06804451]]\n","\n","Average MAE Loss:\n","[0.06176835 0.10531923 0.06307062 0.09083036]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.06008075 0.06417239]\n"," [0.10415368 0.10648478]\n"," [0.06325254 0.06615928]\n"," [0.11276841 0.06741432]]\n","\n","Average MAE Loss:\n","[0.06212657 0.10531923 0.06470591 0.09009137]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07468479 0.08146768]\n"," [0.11276841 0.06741432]\n"," [0.06056262 0.06330841]\n"," [0.06822768 0.06596454]]\n","\n","Average MAE Loss:\n","[0.07807623 0.09009137 0.06193552 0.06709611]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0674786  0.07078789]\n"," [0.11276841 0.06741432]\n"," [0.05940149 0.06135057]\n"," [0.077293   0.07620514]]\n","\n","Average MAE Loss:\n","[0.06913324 0.09009137 0.06037603 0.07674907]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0650797  0.06952585]\n"," [0.11276841 0.06741432]\n"," [0.05918853 0.0608883 ]\n"," [0.07654436 0.0658178 ]]\n","\n","Average MAE Loss:\n","[0.06730278 0.09009137 0.06003842 0.07118108]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06400201 0.06783909]\n"," [0.11276841 0.06741432]\n"," [0.05980844 0.06126818]\n"," [0.08222327 0.06545694]]\n","\n","Average MAE Loss:\n","[0.06592055 0.09009137 0.06053831 0.07384011]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0636436  0.06859533]\n"," [0.11276841 0.06741432]\n"," [0.06024107 0.0620616 ]\n"," [0.08675193 0.06237414]]\n","\n","Average MAE Loss:\n","[0.06611947 0.09009137 0.06115133 0.07456304]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06381482 0.06799244]\n"," [0.11276841 0.06741432]\n"," [0.06049197 0.06146352]\n"," [0.09353452 0.06364254]]\n","\n","Average MAE Loss:\n","[0.06590363 0.09009137 0.06097775 0.07858853]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06123233 0.06425377]\n"," [0.11276841 0.06741432]\n"," [0.06067474 0.06233858]\n"," [0.09341763 0.06247226]]\n","\n","Average MAE Loss:\n","[0.06274305 0.09009137 0.06150666 0.07794495]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.08027264 0.0835027 ]\n"," [0.09488779 0.09381196]\n"," [0.08045434 0.08362955]\n"," [0.08391007 0.08478118]]\n","\n","Average MAE Loss:\n","[0.08188767 0.09434987 0.08204195 0.08434562]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07154741 0.07600458]\n"," [0.09488779 0.09381196]\n"," [0.07186468 0.07616395]\n"," [0.07405201 0.07672697]]\n","\n","Average MAE Loss:\n","[0.073776   0.09434987 0.07401432 0.07538949]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07010422 0.07563369]\n"," [0.09488779 0.09381196]\n"," [0.07068275 0.07608631]\n"," [0.0748447  0.07721317]]\n","\n","Average MAE Loss:\n","[0.07286896 0.09434987 0.07338453 0.07602893]\n","\n","Epoch 00017: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06717866 0.07314873]\n"," [0.09488779 0.09381196]\n"," [0.06579751 0.07186318]\n"," [0.07471545 0.07718408]]\n","\n","Average MAE Loss:\n","[0.07016369 0.09434987 0.06883035 0.07594976]\n","\n","Epoch 00018: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06473259 0.07004721]\n"," [0.09488779 0.09381196]\n"," [0.0637861  0.06882951]\n"," [0.07425864 0.07697613]]\n","\n","Average MAE Loss:\n","[0.0673899  0.09434987 0.0663078  0.07561738]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06279576 0.06655774]\n"," [0.09488779 0.09381196]\n"," [0.06246931 0.06590446]\n"," [0.07425058 0.07704204]]\n","\n","Average MAE Loss:\n","[0.06467675 0.09434987 0.06418689 0.07564631]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06155665 0.06405727]\n"," [0.09488779 0.09381196]\n"," [0.06134848 0.06372536]\n"," [0.07409475 0.07698029]]\n","\n","Average MAE Loss:\n","[0.06280696 0.09434987 0.06253692 0.07553752]\n","\n","Epoch 00021: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.08980488 0.09094593]\n"," [0.07409475 0.07698029]\n"," [0.06107102 0.06308354]\n"," [0.06126152 0.06303052]]\n","\n","Average MAE Loss:\n","[0.09037541 0.07553752 0.06207728 0.06214602]\n","\n","Epoch 00022: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.08320093 0.08621075]\n"," [0.07409475 0.07698029]\n"," [0.06074208 0.06264397]\n"," [0.06071134 0.06228554]]\n","\n","Average MAE Loss:\n","[0.08470584 0.07553752 0.06169302 0.06149844]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07748439 0.08121028]\n"," [0.07409475 0.07698029]\n"," [0.06059231 0.0624058 ]\n"," [0.06063498 0.06220194]]\n","\n","Average MAE Loss:\n","[0.07934734 0.07553752 0.06149906 0.06141846]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07388716 0.07780514]\n"," [0.07409475 0.07698029]\n"," [0.06040349 0.06216251]\n"," [0.06025981 0.06166771]]\n","\n","Average MAE Loss:\n","[0.07584615 0.07553752 0.061283   0.06096376]\n","\n","Epoch 00025: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07303753 0.07711958]\n"," [0.07409475 0.07698029]\n"," [0.06044723 0.06222969]\n"," [0.06025115 0.06152527]]\n","\n","Average MAE Loss:\n","[0.07507856 0.07553752 0.06133846 0.06088821]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07251262 0.07678716]\n"," [0.07409475 0.07698029]\n"," [0.06032446 0.06211258]\n"," [0.06014586 0.06138706]]\n","\n","Average MAE Loss:\n","[0.07464989 0.07553752 0.06121852 0.06076646]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07204824 0.07653977]\n"," [0.07409475 0.07698029]\n"," [0.06018367 0.06192588]\n"," [0.05986812 0.06101954]]\n","\n","Average MAE Loss:\n","[0.074294   0.07553752 0.06105478 0.06044383]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06607505 0.07198468]\n"," [0.06681771 0.07278119]\n"," [0.06616507 0.07216687]\n"," [0.06565452 0.07151977]]\n","\n","Average MAE Loss:\n","[0.06902986 0.06979945 0.06916597 0.06858715]\n","\n","Epoch 00029: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0656355  0.07149217]\n"," [0.06681772 0.07278119]\n"," [0.06530842 0.07126967]\n"," [0.06363928 0.06879987]]\n","\n","Average MAE Loss:\n","[0.06856384 0.06979946 0.06828904 0.06621957]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06522192 0.07099519]\n"," [0.06681771 0.07278119]\n"," [0.06493493 0.07082419]\n"," [0.06176904 0.06489044]]\n","\n","Average MAE Loss:\n","[0.06810856 0.06979945 0.06787956 0.06332974]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06482787 0.07049012]\n"," [0.06681772 0.07278119]\n"," [0.06459143 0.07037176]\n"," [0.06098316 0.06276181]]\n","\n","Average MAE Loss:\n","[0.06765899 0.06979946 0.0674816  0.06187249]\n","\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06444365 0.06997401]\n"," [0.06681772 0.07278119]\n"," [0.06425229 0.0699051 ]\n"," [0.06066891 0.06239195]]\n","\n","Average MAE Loss:\n","[0.06720883 0.06979946 0.0670787  0.06153043]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.0642705  0.06972454]\n"," [0.06681772 0.07278119]\n"," [0.06393113 0.06943479]\n"," [0.06053603 0.06221575]]\n","\n","Average MAE Loss:\n","[0.06699752 0.06979946 0.06668296 0.06137589]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06411077 0.06948445]\n"," [0.06681772 0.07278119]\n"," [0.06378127 0.06920598]\n"," [0.06047634 0.06205935]]\n","\n","Average MAE Loss:\n","[0.06679761 0.06979946 0.06649363 0.06126785]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.0666078  0.07256184]\n"," [0.06047634 0.06205935]\n"," [0.06393667 0.06923383]\n"," [0.06325662 0.06825423]]\n","\n","Average MAE Loss:\n","[0.06958482 0.06126785 0.06658525 0.06575542]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06637504 0.07231799]\n"," [0.06047634 0.06205935]\n"," [0.06377014 0.06898621]\n"," [0.06282108 0.06735854]]\n","\n","Average MAE Loss:\n","[0.06934651 0.06126785 0.06637817 0.06508981]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06625764 0.07219235]\n"," [0.06047634 0.06205935]\n"," [0.06359113 0.06873041]\n"," [0.06229858 0.06623411]]\n","\n","Average MAE Loss:\n","[0.069225   0.06126785 0.06616077 0.06426635]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06614504 0.07206998]\n"," [0.06047634 0.06205935]\n"," [0.06350141 0.06860105]\n"," [0.06180406 0.06513022]]\n","\n","Average MAE Loss:\n","[0.06910751 0.06126785 0.06605123 0.06346714]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06603429 0.07194875]\n"," [0.06047634 0.06205935]\n"," [0.06340987 0.06846828]\n"," [0.06146033 0.06423344]]\n","\n","Average MAE Loss:\n","[0.06899152 0.06126785 0.06593908 0.06284689]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06592476 0.07182603]\n"," [0.06047634 0.06205935]\n"," [0.06331974 0.0683348 ]\n"," [0.06128153 0.06382019]]\n","\n","Average MAE Loss:\n","[0.0688754  0.06126785 0.06582727 0.06255086]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06587168 0.07176558]\n"," [0.06047634 0.06205936]\n"," [0.06324235 0.06821019]\n"," [0.06116433 0.06351683]]\n","\n","Average MAE Loss:\n","[0.06881863 0.06126785 0.06572627 0.06234058]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.9531e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06231712 0.06626913]\n"," [0.06235884 0.06634456]\n"," [0.06231962 0.06627619]\n"," [0.06210624 0.06585039]]\n","\n","Average MAE Loss:\n","[0.06429312 0.0643517  0.06429791 0.06397832]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06228238 0.06620407]\n"," [0.06235884 0.06634456]\n"," [0.06227729 0.06620809]\n"," [0.06185778 0.06531896]]\n","\n","Average MAE Loss:\n","[0.06424323 0.0643517  0.06424269 0.06358837]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06225087 0.0661431 ]\n"," [0.06235884 0.06634456]\n"," [0.06223388 0.0661404 ]\n"," [0.06174992 0.06507263]]\n","\n","Average MAE Loss:\n","[0.06419699 0.0643517  0.06418714 0.06341127]\n","\n","Epoch 00045: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0622368  0.06611528]\n"," [0.06235884 0.06634456]\n"," [0.0621981  0.06608039]\n"," [0.06165404 0.06483927]]\n","\n","Average MAE Loss:\n","[0.06417604 0.0643517  0.06413924 0.06324666]\n","\n","Epoch 00046: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06222197 0.0660862 ]\n"," [0.06235884 0.06634456]\n"," [0.06218104 0.06605127]\n"," [0.06156176 0.06460969]]\n","\n","Average MAE Loss:\n","[0.06415409 0.0643517  0.06411615 0.06308573]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06220855 0.06605784]\n"," [0.06235884 0.06634456]\n"," [0.06216161 0.06601973]\n"," [0.06146542 0.06437943]]\n","\n","Average MAE Loss:\n","[0.06413319 0.0643517  0.06409067 0.06292242]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06219503 0.06602949]\n"," [0.06235884 0.06634456]\n"," [0.06214291 0.06598872]\n"," [0.06141575 0.06426461]]\n","\n","Average MAE Loss:\n","[0.06411226 0.0643517  0.06406581 0.06284018]\n","\n","Epoch 00049: reducing learning rate of group 0 to 4.8828e-07.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06235083 0.06632934]\n"," [0.06141575 0.06426461]\n"," [0.0621757  0.0659973 ]\n"," [0.0620885  0.06586504]]\n","\n","Average MAE Loss:\n","[0.06434008 0.06284018 0.0640865  0.06397677]\n","\n","Epoch 00050: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06234347 0.06631467]\n"," [0.06141575 0.06426461]\n"," [0.06216659 0.06598183]\n"," [0.0620285  0.06573057]]\n","\n","Average MAE Loss:\n","[0.06432907 0.06284018 0.06407421 0.06387953]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06233582 0.06629973]\n"," [0.06141575 0.06426461]\n"," [0.06215827 0.06596695]\n"," [0.0619684  0.06559331]]\n","\n","Average MAE Loss:\n","[0.06431777 0.06284018 0.06406261 0.06378085]\n","\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0623282  0.06628456]\n"," [0.06141575 0.06426461]\n"," [0.0621493  0.06595166]\n"," [0.06193635 0.06552301]]\n","\n","Average MAE Loss:\n","[0.06430638 0.06284018 0.06405048 0.06372968]\n","\n","Epoch 00053: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06232427 0.06627678]\n"," [0.06141575 0.06426461]\n"," [0.06213945 0.06593556]\n"," [0.06190532 0.06545467]]\n","\n","Average MAE Loss:\n","[0.06430053 0.06284018 0.0640375  0.06368   ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06232029 0.0662691 ]\n"," [0.06141574 0.06426461]\n"," [0.06213432 0.06592727]\n"," [0.06187572 0.06538907]]\n","\n","Average MAE Loss:\n","[0.06429469 0.06284018 0.06403079 0.06363239]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0623164  0.06626149]\n"," [0.06141575 0.06426461]\n"," [0.06213017 0.06591984]\n"," [0.06184569 0.06532369]]\n","\n","Average MAE Loss:\n","[0.06428895 0.06284018 0.064025   0.06358469]\n","\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06190791 0.06541698]\n"," [0.06191183 0.06542431]\n"," [0.06190738 0.0654168 ]\n"," [0.06189806 0.06539177]]\n","\n","Average MAE Loss:\n","[0.06366244 0.06366807 0.06366209 0.06364492]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06190623 0.06541368]\n"," [0.06191183 0.06542431]\n"," [0.06190252 0.06540912]\n"," [0.0618804  0.06535633]]\n","\n","Average MAE Loss:\n","[0.06365996 0.06366807 0.06365582 0.06361837]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.0619049  0.0654107 ]\n"," [0.06191183 0.06542431]\n"," [0.06190032 0.06540552]\n"," [0.06186481 0.06532294]]\n","\n","Average MAE Loss:\n","[0.0636578  0.06366807 0.06365292 0.06359387]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06190336 0.06540759]\n"," [0.06191183 0.06542431]\n"," [0.0618978  0.06540165]\n"," [0.06184785 0.0652883 ]]\n","\n","Average MAE Loss:\n","[0.06365547 0.06366807 0.06364973 0.06356808]\n","\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","\n","epochs finished with time:528.003582239151\n","\n","[[0.06190335 0.06540759]\n"," [0.06191183 0.06542431]\n"," [0.0618978  0.06540165]\n"," [0.06184785 0.0652883 ]]\n","00:09:7.428555610000103\n","------------------------------------Fold [3/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.04909564 0.05032358]\n"," [0.08913526 0.08132836]\n"," [0.05530195 0.05497015]\n"," [0.08783884 0.06346722]]\n","\n","Average MAE Loss:\n","[0.04970961 0.08523181 0.05513605 0.07565303]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04490949 0.04362469]\n"," [0.08913526 0.08132835]\n"," [0.04940265 0.04878997]\n"," [0.08936981 0.05130721]]\n","\n","Average MAE Loss:\n","[0.04426709 0.08523181 0.04909631 0.07033851]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04218278 0.04262412]\n"," [0.08913526 0.08132835]\n"," [0.04668186 0.04617142]\n"," [0.09736674 0.0533628 ]]\n","\n","Average MAE Loss:\n","[0.04240345 0.08523181 0.04642664 0.07536477]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04199432 0.04165683]\n"," [0.08913526 0.08132835]\n"," [0.04445411 0.04504607]\n"," [0.09912591 0.05069556]]\n","\n","Average MAE Loss:\n","[0.04182557 0.08523181 0.04475009 0.07491074]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04179066 0.04577849]\n"," [0.08913526 0.08132835]\n"," [0.04386551 0.046304  ]\n"," [0.09893338 0.04514293]]\n","\n","Average MAE Loss:\n","[0.04378458 0.08523181 0.04508475 0.07203815]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04173385 0.04674885]\n"," [0.08913526 0.08132836]\n"," [0.04351076 0.04671657]\n"," [0.09930359 0.04543617]]\n","\n","Average MAE Loss:\n","[0.04424135 0.08523181 0.04511366 0.07236988]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04174869 0.04284237]\n"," [0.08913526 0.08132835]\n"," [0.04306224 0.04696732]\n"," [0.0988484  0.0443216 ]]\n","\n","Average MAE Loss:\n","[0.04229553 0.08523181 0.04501478 0.071585  ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0570493  0.05769948]\n"," [0.0988484  0.0443216 ]\n"," [0.04314623 0.04678571]\n"," [0.04763214 0.04575067]]\n","\n","Average MAE Loss:\n","[0.05737439 0.071585   0.04496597 0.0466914 ]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04819161 0.04970329]\n"," [0.0988484  0.0443216 ]\n"," [0.04415694 0.04293017]\n"," [0.05147508 0.04322257]]\n","\n","Average MAE Loss:\n","[0.04894745 0.071585   0.04354355 0.04734883]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04734155 0.0478088 ]\n"," [0.0988484  0.0443216 ]\n"," [0.04343657 0.04321876]\n"," [0.04952003 0.04228355]]\n","\n","Average MAE Loss:\n","[0.04757518 0.071585   0.04332766 0.04590179]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04656244 0.04686084]\n"," [0.0988484  0.0443216 ]\n"," [0.04365199 0.04285936]\n"," [0.0568138  0.04164724]]\n","\n","Average MAE Loss:\n","[0.04671164 0.071585   0.04325568 0.04923052]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04577848 0.04553023]\n"," [0.0988484  0.0443216 ]\n"," [0.04309142 0.04354617]\n"," [0.06006876 0.04196795]]\n","\n","Average MAE Loss:\n","[0.04565435 0.071585   0.0433188  0.05101836]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04358151 0.04322325]\n"," [0.0988484  0.0443216 ]\n"," [0.04304393 0.04706953]\n"," [0.06169141 0.04296292]]\n","\n","Average MAE Loss:\n","[0.04340238 0.071585   0.04505673 0.05232716]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04296936 0.04252878]\n"," [0.0988484  0.0443216 ]\n"," [0.04274255 0.04947428]\n"," [0.06420057 0.04331687]]\n","\n","Average MAE Loss:\n","[0.04274907 0.071585   0.04610841 0.05375872]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0615624  0.05714748]\n"," [0.07691599 0.06799641]\n"," [0.05836576 0.05586457]\n"," [0.06927201 0.06219419]]\n","\n","Average MAE Loss:\n","[0.05935494 0.0724562  0.05711516 0.0657331 ]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05696306 0.05503465]\n"," [0.07691599 0.06799641]\n"," [0.05664496 0.05396509]\n"," [0.06018978 0.05510832]]\n","\n","Average MAE Loss:\n","[0.05599885 0.0724562  0.05530503 0.05764905]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05566642 0.05366033]\n"," [0.07691599 0.06799641]\n"," [0.05393872 0.05270251]\n"," [0.05927652 0.05457422]]\n","\n","Average MAE Loss:\n","[0.05466337 0.0724562  0.05332061 0.05692537]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05343932 0.05258553]\n"," [0.07691599 0.06799641]\n"," [0.04978117 0.05010121]\n"," [0.05950992 0.05435783]]\n","\n","Average MAE Loss:\n","[0.05301243 0.0724562  0.04994119 0.05693387]\n","\n","Epoch 00018: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.05089462 0.05100892]\n"," [0.07691599 0.06799641]\n"," [0.04573971 0.04576451]\n"," [0.05933199 0.05438086]]\n","\n","Average MAE Loss:\n","[0.05095177 0.0724562  0.04575211 0.05685643]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04838639 0.0486567 ]\n"," [0.07691599 0.06799641]\n"," [0.04464418 0.04448418]\n"," [0.05918171 0.0543225 ]]\n","\n","Average MAE Loss:\n","[0.04852155 0.0724562  0.04456418 0.0567521 ]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04718224 0.0474184 ]\n"," [0.07691599 0.06799641]\n"," [0.04410178 0.04395464]\n"," [0.05908902 0.05422818]]\n","\n","Average MAE Loss:\n","[0.04730032 0.0724562  0.04402821 0.0566586 ]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07130787 0.06483085]\n"," [0.05908902 0.05422818]\n"," [0.04582348 0.04612867]\n"," [0.04407309 0.04329918]]\n","\n","Average MAE Loss:\n","[0.06806936 0.0566586  0.04597607 0.04368614]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06449994 0.06003334]\n"," [0.05908902 0.05422818]\n"," [0.04482402 0.04494984]\n"," [0.04354674 0.0429403 ]]\n","\n","Average MAE Loss:\n","[0.06226664 0.0566586  0.04488693 0.04324352]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05999827 0.0561037 ]\n"," [0.05908902 0.05422818]\n"," [0.04445549 0.04449482]\n"," [0.04322937 0.04262968]]\n","\n","Average MAE Loss:\n","[0.05805098 0.0566586  0.04447515 0.04292953]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05881193 0.0551758 ]\n"," [0.05908902 0.05422818]\n"," [0.04419074 0.04419613]\n"," [0.04313867 0.0421343 ]]\n","\n","Average MAE Loss:\n","[0.05699386 0.0566586  0.04419343 0.04263649]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.05807833 0.05478489]\n"," [0.05908902 0.05422818]\n"," [0.04393059 0.04398782]\n"," [0.04286111 0.04176778]]\n","\n","Average MAE Loss:\n","[0.05643161 0.0566586  0.0439592  0.04231445]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.05752061 0.05453523]\n"," [0.05908902 0.05422818]\n"," [0.04375888 0.04373869]\n"," [0.04272276 0.04142316]]\n","\n","Average MAE Loss:\n","[0.05602792 0.0566586  0.04374879 0.04207296]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.05698801 0.0543313 ]\n"," [0.05908901 0.05422818]\n"," [0.04365448 0.04364365]\n"," [0.0425683  0.04122784]]\n","\n","Average MAE Loss:\n","[0.05565965 0.0566586  0.04364906 0.04189807]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05024683 0.05034471]\n"," [0.05060768 0.0506135 ]\n"," [0.05016845 0.05030384]\n"," [0.04929902 0.04946244]]\n","\n","Average MAE Loss:\n","[0.05029577 0.05061059 0.05023615 0.04938073]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04983173 0.05003665]\n"," [0.05060768 0.0506135 ]\n"," [0.04952377 0.04985059]\n"," [0.04715104 0.04702283]]\n","\n","Average MAE Loss:\n","[0.04993419 0.05061059 0.04968718 0.04708693]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04942542 0.04969925]\n"," [0.05060768 0.0506135 ]\n"," [0.04886677 0.04933153]\n"," [0.04514138 0.04418744]]\n","\n","Average MAE Loss:\n","[0.04956234 0.05061059 0.04909915 0.04466441]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04902879 0.04935586]\n"," [0.05060768 0.0506135 ]\n"," [0.04854012 0.04904743]\n"," [0.04406349 0.04341787]]\n","\n","Average MAE Loss:\n","[0.04919232 0.05061059 0.04879378 0.04374068]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04883439 0.04918065]\n"," [0.05060768 0.0506135 ]\n"," [0.04822632 0.04874905]\n"," [0.04384923 0.04307869]]\n","\n","Average MAE Loss:\n","[0.04900752 0.05061059 0.04848769 0.04346396]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04864635 0.04900317]\n"," [0.05060768 0.0506135 ]\n"," [0.04793925 0.04842632]\n"," [0.04367143 0.04274246]]\n","\n","Average MAE Loss:\n","[0.04882476 0.05061059 0.04818279 0.04320695]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0484497  0.04882351]\n"," [0.05060768 0.0506135 ]\n"," [0.04763526 0.04812138]\n"," [0.04336368 0.04259708]]\n","\n","Average MAE Loss:\n","[0.0486366  0.05061059 0.04787832 0.04298038]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.0504048  0.05046611]\n"," [0.04336368 0.04259708]\n"," [0.0482714  0.04867487]\n"," [0.04680184 0.0469118 ]]\n","\n","Average MAE Loss:\n","[0.05043546 0.04298038 0.04847313 0.04685682]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05030334 0.05038986]\n"," [0.04336368 0.04259709]\n"," [0.04809087 0.04852485]\n"," [0.04613058 0.04604499]]\n","\n","Average MAE Loss:\n","[0.0503466  0.04298038 0.04830786 0.04608779]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05020328 0.05031012]\n"," [0.04336368 0.04259708]\n"," [0.04792594 0.04837877]\n"," [0.04550194 0.04518304]]\n","\n","Average MAE Loss:\n","[0.0502567  0.04298038 0.04815235 0.04534249]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05010292 0.0502323 ]\n"," [0.04336367 0.04259708]\n"," [0.04776096 0.0482305 ]\n"," [0.04491886 0.04442752]]\n","\n","Average MAE Loss:\n","[0.05016761 0.04298038 0.04799573 0.04467319]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04999703 0.05015121]\n"," [0.04336367 0.04259709]\n"," [0.04767723 0.04815798]\n"," [0.04451811 0.04385882]]\n","\n","Average MAE Loss:\n","[0.05007412 0.04298038 0.04791761 0.04418847]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04994442 0.05011093]\n"," [0.04336368 0.04259708]\n"," [0.04759251 0.0480855 ]\n"," [0.0443605  0.043643  ]]\n","\n","Average MAE Loss:\n","[0.05002768 0.04298038 0.047839   0.04400175]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04989265 0.050071  ]\n"," [0.04336368 0.04259708]\n"," [0.04750697 0.04801652]\n"," [0.04419814 0.04347716]]\n","\n","Average MAE Loss:\n","[0.04998183 0.04298038 0.04776174 0.04383765]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04573455 0.04566942]\n"," [0.04577269 0.04571285]\n"," [0.04570368 0.04564834]\n"," [0.0454894  0.04535157]]\n","\n","Average MAE Loss:\n","[0.04570198 0.04574277 0.04567601 0.04542049]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04570284 0.04563139]\n"," [0.04577268 0.04571285]\n"," [0.04567237 0.04562165]\n"," [0.04520527 0.04492668]]\n","\n","Average MAE Loss:\n","[0.04566712 0.04574276 0.04564701 0.04506598]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04568922 0.04561093]\n"," [0.04577269 0.04571285]\n"," [0.04564104 0.04559289]\n"," [0.04506804 0.04472478]]\n","\n","Average MAE Loss:\n","[0.04565007 0.04574277 0.04561696 0.04489641]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04567333 0.04559149]\n"," [0.04577268 0.04571285]\n"," [0.04561155 0.0455627 ]\n"," [0.04496035 0.04452606]]\n","\n","Average MAE Loss:\n","[0.04563241 0.04574276 0.04558712 0.0447432 ]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04565798 0.04557323]\n"," [0.04577269 0.04571285]\n"," [0.04558527 0.04553235]\n"," [0.04483676 0.04434716]]\n","\n","Average MAE Loss:\n","[0.0456156  0.04574277 0.04555881 0.04459196]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04564384 0.04555468]\n"," [0.04577269 0.04571285]\n"," [0.04557179 0.04551716]\n"," [0.04472491 0.04418079]]\n","\n","Average MAE Loss:\n","[0.04559926 0.04574277 0.04554447 0.04445285]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04563583 0.04554547]\n"," [0.04577269 0.04571285]\n"," [0.04555888 0.04550119]\n"," [0.0446675  0.04410593]]\n","\n","Average MAE Loss:\n","[0.04559065 0.04574277 0.04553004 0.04438672]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04576472 0.0457033 ]\n"," [0.0446675  0.04410593]\n"," [0.04562031 0.04553089]\n"," [0.04548433 0.04540267]]\n","\n","Average MAE Loss:\n","[0.04573401 0.04438672 0.0455756  0.0454435 ]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04575654 0.04569352]\n"," [0.0446675  0.04410593]\n"," [0.04560364 0.04551586]\n"," [0.04540957 0.04528666]]\n","\n","Average MAE Loss:\n","[0.04572503 0.04438672 0.04555975 0.04534811]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04574898 0.04568342]\n"," [0.0446675  0.04410593]\n"," [0.04559579 0.04550804]\n"," [0.04533799 0.04517464]]\n","\n","Average MAE Loss:\n","[0.0457162  0.04438672 0.04555191 0.04525632]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04574518 0.04567811]\n"," [0.0446675  0.04410593]\n"," [0.04558755 0.04550049]\n"," [0.04530528 0.04512018]]\n","\n","Average MAE Loss:\n","[0.04571164 0.04438672 0.04554402 0.04521273]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0457411  0.04567298]\n"," [0.0446675  0.04410593]\n"," [0.04557956 0.04549284]\n"," [0.0452702  0.04506664]]\n","\n","Average MAE Loss:\n","[0.04570704 0.04438672 0.0455362  0.04516842]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0457366  0.04566779]\n"," [0.0446675  0.04410593]\n"," [0.04557144 0.04548531]\n"," [0.04523375 0.04501385]]\n","\n","Average MAE Loss:\n","[0.04570219 0.04438672 0.04552837 0.0451238 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04573223 0.04566289]\n"," [0.0446675  0.04410593]\n"," [0.04556738 0.04548147]\n"," [0.04520012 0.04496192]]\n","\n","Average MAE Loss:\n","[0.04569756 0.04438672 0.04552442 0.04508102]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.0452674  0.04502785]\n"," [0.04526915 0.04503014]\n"," [0.04526566 0.04502673]\n"," [0.04525143 0.04500378]]\n","\n","Average MAE Loss:\n","[0.04514762 0.04514964 0.0451462  0.0451276 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04526571 0.04502564]\n"," [0.04526915 0.04503014]\n"," [0.04526228 0.04502342]\n"," [0.04523347 0.04497829]]\n","\n","Average MAE Loss:\n","[0.04514568 0.04514964 0.04514285 0.04510588]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04526398 0.04502356]\n"," [0.04526915 0.04503014]\n"," [0.04525898 0.04502005]\n"," [0.04521438 0.04495264]]\n","\n","Average MAE Loss:\n","[0.04514377 0.04514964 0.04513952 0.04508351]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.0452622  0.04502137]\n"," [0.04526915 0.04503014]\n"," [0.04525718 0.04501835]\n"," [0.04519761 0.04492713]]\n","\n","Average MAE Loss:\n","[0.04514178 0.04514964 0.04513777 0.04506237]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","\n","epochs finished with time:508.3335268497467\n","\n","[[0.0452622  0.04502137]\n"," [0.04526915 0.04503014]\n"," [0.04525718 0.04501835]\n"," [0.04519761 0.04492713]]\n","00:08:44.47802006499978\n","------------------------------------Fold [4/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.04209311 0.06235512]\n"," [0.07500252 0.09722316]\n"," [0.04726049 0.06618903]\n"," [0.07573043 0.07965057]]\n","\n","Average MAE Loss:\n","[0.05222411 0.08611284 0.05672476 0.0776905 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.03943115 0.06045804]\n"," [0.07500252 0.09722316]\n"," [0.04114618 0.05976409]\n"," [0.07868512 0.0728826 ]]\n","\n","Average MAE Loss:\n","[0.0499446  0.08611284 0.05045514 0.07578386]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.03813261 0.05702787]\n"," [0.07500252 0.09722316]\n"," [0.0390067  0.05771439]\n"," [0.0800481  0.05991826]]\n","\n","Average MAE Loss:\n","[0.04758024 0.08611284 0.04836055 0.06998318]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.03795641 0.05539604]\n"," [0.07500252 0.09722316]\n"," [0.03949863 0.05968007]\n"," [0.08190938 0.05997904]]\n","\n","Average MAE Loss:\n","[0.04667622 0.08611284 0.04958935 0.07094421]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.03909777 0.05694262]\n"," [0.07500252 0.09722316]\n"," [0.03840492 0.05828352]\n"," [0.08245433 0.06276554]]\n","\n","Average MAE Loss:\n","[0.04802019 0.08611284 0.04834422 0.07260993]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.03966057 0.05843345]\n"," [0.07500252 0.09722316]\n"," [0.03634512 0.05745677]\n"," [0.08235285 0.06003992]]\n","\n","Average MAE Loss:\n","[0.04904701 0.08611284 0.04690094 0.07119638]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.03886551 0.05543086]\n"," [0.07500252 0.09722316]\n"," [0.03715258 0.05862385]\n"," [0.08238468 0.06125513]]\n","\n","Average MAE Loss:\n","[0.04714818 0.08611284 0.04788822 0.0718199 ]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04698936 0.07089932]\n"," [0.08238468 0.06125513]\n"," [0.03737234 0.05568218]\n"," [0.04172853 0.0623875 ]]\n","\n","Average MAE Loss:\n","[0.05894434 0.0718199  0.04652726 0.05205802]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04136652 0.06361967]\n"," [0.08238468 0.06125513]\n"," [0.03848826 0.05988248]\n"," [0.0424072  0.06460731]]\n","\n","Average MAE Loss:\n","[0.0524931  0.0718199  0.04918537 0.05350726]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04047423 0.06534471]\n"," [0.08238468 0.06125513]\n"," [0.03689931 0.05628118]\n"," [0.04440127 0.0620669 ]]\n","\n","Average MAE Loss:\n","[0.05290947 0.0718199  0.04659024 0.05323408]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.03958304 0.06301846]\n"," [0.08238468 0.06125513]\n"," [0.03705674 0.05945071]\n"," [0.04149304 0.05688588]]\n","\n","Average MAE Loss:\n","[0.05130075 0.0718199  0.04825373 0.04918946]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.03967978 0.06109437]\n"," [0.08238468 0.06125513]\n"," [0.03623777 0.05719872]\n"," [0.04681049 0.05527499]]\n","\n","Average MAE Loss:\n","[0.05038707 0.0718199  0.04671825 0.05104274]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.03871267 0.05848017]\n"," [0.08238468 0.06125513]\n"," [0.03825128 0.05840404]\n"," [0.04705802 0.05517495]]\n","\n","Average MAE Loss:\n","[0.04859642 0.0718199  0.04832766 0.05111649]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.03878139 0.05818625]\n"," [0.08238468 0.06125513]\n"," [0.03945621 0.05998348]\n"," [0.0485879  0.05733673]]\n","\n","Average MAE Loss:\n","[0.04848382 0.0718199  0.04971984 0.05296232]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04912843 0.07065254]\n"," [0.0619199  0.08372593]\n"," [0.04803314 0.0670962 ]\n"," [0.05140307 0.07269306]]\n","\n","Average MAE Loss:\n","[0.05989049 0.07282291 0.05756467 0.06204807]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.0460382  0.0664678 ]\n"," [0.0619199  0.08372593]\n"," [0.04441945 0.06848191]\n"," [0.04880536 0.06766554]]\n","\n","Average MAE Loss:\n","[0.056253   0.07282291 0.05645068 0.05823545]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04447553 0.06637688]\n"," [0.0619199  0.08372593]\n"," [0.04055069 0.06320423]\n"," [0.04815868 0.06763145]]\n","\n","Average MAE Loss:\n","[0.0554262  0.07282291 0.05187746 0.05789506]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.042703   0.06501057]\n"," [0.0619199  0.08372593]\n"," [0.03765804 0.05938514]\n"," [0.04791027 0.067642  ]]\n","\n","Average MAE Loss:\n","[0.05385679 0.07282291 0.04852159 0.05777613]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04061187 0.06285818]\n"," [0.06191989 0.08372593]\n"," [0.03646875 0.05637685]\n"," [0.04772536 0.06750172]]\n","\n","Average MAE Loss:\n","[0.05173502 0.07282291 0.0464228  0.05761354]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03875379 0.06010545]\n"," [0.0619199  0.08372593]\n"," [0.03601822 0.05555593]\n"," [0.04759689 0.06754373]]\n","\n","Average MAE Loss:\n","[0.04942962 0.07282291 0.04578708 0.05757031]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03808378 0.0587691 ]\n"," [0.0619199  0.08372593]\n"," [0.03558776 0.05504435]\n"," [0.04746761 0.06749967]]\n","\n","Average MAE Loss:\n","[0.04842644 0.07282291 0.04531605 0.05748364]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05692099 0.08019744]\n"," [0.04746761 0.06749967]\n"," [0.03704522 0.05786989]\n"," [0.03545949 0.0543735 ]]\n","\n","Average MAE Loss:\n","[0.06855921 0.05748364 0.04745755 0.0449165 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.05150133 0.074687  ]\n"," [0.04746761 0.06749967]\n"," [0.03610151 0.05561383]\n"," [0.03539992 0.05378822]]\n","\n","Average MAE Loss:\n","[0.06309417 0.05748364 0.04585767 0.04459407]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04817254 0.06957906]\n"," [0.04746761 0.06749967]\n"," [0.03581579 0.05526211]\n"," [0.03534683 0.0538778 ]]\n","\n","Average MAE Loss:\n","[0.0588758  0.05748364 0.04553895 0.04461232]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04743833 0.068227  ]\n"," [0.04746761 0.06749968]\n"," [0.03560309 0.05453951]\n"," [0.03533664 0.05357827]]\n","\n","Average MAE Loss:\n","[0.05783267 0.05748365 0.0450713  0.04445746]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04692768 0.06751739]\n"," [0.04746761 0.06749968]\n"," [0.03538239 0.05438459]\n"," [0.03534121 0.05342636]]\n","\n","Average MAE Loss:\n","[0.05722253 0.05748365 0.04488349 0.04438378]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04647938 0.06716149]\n"," [0.04746761 0.06749968]\n"," [0.03540027 0.05430639]\n"," [0.03534229 0.05351399]]\n","\n","Average MAE Loss:\n","[0.05682043 0.05748365 0.04485333 0.04442814]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0460558  0.06691416]\n"," [0.04746761 0.06749967]\n"," [0.03536909 0.05390939]\n"," [0.03534281 0.05371865]]\n","\n","Average MAE Loss:\n","[0.05648498 0.05748364 0.04463924 0.04453073]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04001394 0.06335508]\n"," [0.04030927 0.06372205]\n"," [0.03787737 0.06050312]\n"," [0.03927533 0.06245651]]\n","\n","Average MAE Loss:\n","[0.05168451 0.05201566 0.04919025 0.05086592]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03969112 0.06293037]\n"," [0.04030927 0.06372205]\n"," [0.0357717  0.05586836]\n"," [0.0374436  0.05965515]]\n","\n","Average MAE Loss:\n","[0.05131074 0.05201566 0.04582003 0.04854937]\n","\n","Epoch 00030: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03937317 0.06252245]\n"," [0.04030927 0.06372205]\n"," [0.0354601  0.05442971]\n"," [0.03663037 0.05765484]]\n","\n","Average MAE Loss:\n","[0.05094781 0.05201566 0.04494491 0.0471426 ]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03906104 0.06209616]\n"," [0.04030927 0.06372205]\n"," [0.03536741 0.05397286]\n"," [0.03610574 0.05597724]]\n","\n","Average MAE Loss:\n","[0.0505786  0.05201566 0.04467013 0.04604149]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03890939 0.06188011]\n"," [0.04030927 0.06372205]\n"," [0.03533805 0.0540068 ]\n"," [0.03592252 0.05509358]]\n","\n","Average MAE Loss:\n","[0.05039475 0.05201566 0.04467242 0.04550805]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03876133 0.06165742]\n"," [0.04030927 0.06372205]\n"," [0.03531526 0.05375673]\n"," [0.03582797 0.0546606 ]]\n","\n","Average MAE Loss:\n","[0.05020938 0.05201566 0.04453599 0.04524428]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03861582 0.06143884]\n"," [0.04030927 0.06372205]\n"," [0.03535003 0.05404659]\n"," [0.03577816 0.05460039]]\n","\n","Average MAE Loss:\n","[0.05002733 0.05201566 0.04469831 0.04518928]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04014784 0.06352684]\n"," [0.03577816 0.05460039]\n"," [0.03745055 0.05968817]\n"," [0.03534377 0.05387453]]\n","\n","Average MAE Loss:\n","[0.05183734 0.04518928 0.04856936 0.04460915]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.0400655  0.06342341]\n"," [0.03577816 0.05460039]\n"," [0.03626107 0.05714066]\n"," [0.03531294 0.05365205]]\n","\n","Average MAE Loss:\n","[0.05174445 0.04518928 0.04670087 0.04448249]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03998399 0.06332288]\n"," [0.03577816 0.05460039]\n"," [0.03573541 0.05561536]\n"," [0.03532302 0.05358898]]\n","\n","Average MAE Loss:\n","[0.05165343 0.04518928 0.04567538 0.044456  ]\n","\n","Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00038: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03990081 0.06321849]\n"," [0.03577816 0.05460039]\n"," [0.03559263 0.05508962]\n"," [0.03532682 0.05364833]]\n","\n","Average MAE Loss:\n","[0.05155965 0.04518928 0.04534112 0.04448758]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03981896 0.06311367]\n"," [0.03577816 0.05460039]\n"," [0.03548355 0.05475369]\n"," [0.03531808 0.053596  ]]\n","\n","Average MAE Loss:\n","[0.05146632 0.04518928 0.04511862 0.04445704]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03977769 0.06305954]\n"," [0.03577816 0.05460039]\n"," [0.03541966 0.05457768]\n"," [0.03530584 0.05352972]]\n","\n","Average MAE Loss:\n","[0.05141861 0.04518928 0.04499867 0.04441778]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03973702 0.06300458]\n"," [0.03577816 0.05460039]\n"," [0.03539734 0.05442785]\n"," [0.03530545 0.05351961]]\n","\n","Average MAE Loss:\n","[0.0513708  0.04518928 0.0449126  0.04441253]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00042: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03585379 0.05616318]\n"," [0.03586645 0.05621074]\n"," [0.03577476 0.05594516]\n"," [0.03582543 0.05608117]]\n","\n","Average MAE Loss:\n","[0.04600848 0.0460386  0.04585996 0.0459533 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03584676 0.0561344 ]\n"," [0.03586645 0.05621074]\n"," [0.03567811 0.05560833]\n"," [0.03576346 0.05587279]]\n","\n","Average MAE Loss:\n","[0.04599058 0.0460386  0.04564322 0.04581813]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03584117 0.05611148]\n"," [0.03586645 0.05621074]\n"," [0.03559944 0.05533543]\n"," [0.03570721 0.05566671]]\n","\n","Average MAE Loss:\n","[0.04597632 0.0460386  0.04546743 0.04568696]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0358356  0.0560886 ]\n"," [0.03586645 0.05621074]\n"," [0.03554541 0.05509991]\n"," [0.03565975 0.0554879 ]]\n","\n","Average MAE Loss:\n","[0.0459621  0.0460386  0.04532266 0.04557383]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00046: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03583044 0.05606703]\n"," [0.03586645 0.05621074]\n"," [0.03551133 0.05498244]\n"," [0.03563764 0.05540047]]\n","\n","Average MAE Loss:\n","[0.04594874 0.0460386  0.04524688 0.04551905]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03582487 0.05604339]\n"," [0.03586645 0.05621074]\n"," [0.03548819 0.05493376]\n"," [0.03561511 0.05530747]]\n","\n","Average MAE Loss:\n","[0.04593413 0.0460386  0.04521097 0.04546129]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03581898 0.05601609]\n"," [0.03586645 0.05621074]\n"," [0.03546651 0.05488331]\n"," [0.03559478 0.05523183]]\n","\n","Average MAE Loss:\n","[0.04591754 0.0460386  0.04517491 0.0454133 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03585951 0.05618342]\n"," [0.03559478 0.05523183]\n"," [0.03575808 0.05582321]\n"," [0.03545013 0.0548214 ]]\n","\n","Average MAE Loss:\n","[0.04602147 0.0454133  0.04579064 0.04513576]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00050: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03585284 0.0561577 ]\n"," [0.03559478 0.05523183]\n"," [0.03572892 0.05572115]\n"," [0.03544245 0.0547895 ]]\n","\n","Average MAE Loss:\n","[0.04600527 0.0454133  0.04572503 0.04511597]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03584609 0.05612937]\n"," [0.03559478 0.05523183]\n"," [0.0357013  0.05561957]\n"," [0.03543476 0.05475957]]\n","\n","Average MAE Loss:\n","[0.04598773 0.0454133  0.04566043 0.04509717]\n","\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03583867 0.05609954]\n"," [0.03559478 0.05523183]\n"," [0.03567449 0.05552967]\n"," [0.0354278  0.05473133]]\n","\n","Average MAE Loss:\n","[0.04596911 0.0454133  0.04560208 0.04507957]\n","\n","Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03583573 0.0560859 ]\n"," [0.03559478 0.05523183]\n"," [0.03565045 0.05545289]\n"," [0.03542131 0.05470353]]\n","\n","Average MAE Loss:\n","[0.04596082 0.0454133  0.04555167 0.04506242]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03583267 0.05607428]\n"," [0.03559478 0.05523183]\n"," [0.03563866 0.05541741]\n"," [0.03541804 0.05468992]]\n","\n","Average MAE Loss:\n","[0.04595347 0.0454133  0.04552804 0.04505398]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03582979 0.0560624 ]\n"," [0.03559478 0.05523183]\n"," [0.03562754 0.05538253]\n"," [0.03541467 0.05467514]]\n","\n","Average MAE Loss:\n","[0.0459461  0.0454133  0.04550504 0.04504491]\n","\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03559887 0.05530581]\n"," [0.03560016 0.0553145 ]\n"," [0.03558913 0.05528156]\n"," [0.03559528 0.05529502]]\n","\n","Average MAE Loss:\n","[0.04545234 0.04545733 0.04543534 0.04544515]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03559783 0.05530024]\n"," [0.03560016 0.05531449]\n"," [0.03557846 0.05524234]\n"," [0.03559009 0.05527462]]\n","\n","Average MAE Loss:\n","[0.04544904 0.04545733 0.0454104  0.04543236]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03559648 0.05529183]\n"," [0.03560017 0.0553145 ]\n"," [0.03557367 0.05522463]\n"," [0.0355875  0.05526499]]\n","\n","Average MAE Loss:\n","[0.04544416 0.04545733 0.04539915 0.04542624]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03559542 0.05528542]\n"," [0.03560017 0.05531449]\n"," [0.03556942 0.05521001]\n"," [0.03558469 0.05525417]]\n","\n","Average MAE Loss:\n","[0.04544042 0.04545733 0.04538972 0.04541943]\n","\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","\n","epochs finished with time:522.3602576255798\n","\n","[[0.03559542 0.05528542]\n"," [0.03560017 0.0553145 ]\n"," [0.03556942 0.05521001]\n"," [0.0355847  0.05525417]]\n","00:08:58.13630725400026\n","------------------------------------Fold [5/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05103825 0.06035576]\n"," [0.09164903 0.09799358]\n"," [0.05716019 0.06390756]\n"," [0.09048265 0.07192094]]\n","\n","Average MAE Loss:\n","[0.055697   0.0948213  0.06053387 0.0812018 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04584695 0.05258396]\n"," [0.09164903 0.09799358]\n"," [0.05007762 0.05839217]\n"," [0.09575333 0.06097345]]\n","\n","Average MAE Loss:\n","[0.04921545 0.0948213  0.0542349  0.07836339]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04361854 0.05078385]\n"," [0.09164903 0.09799358]\n"," [0.04682702 0.05272854]\n"," [0.09933713 0.05722303]]\n","\n","Average MAE Loss:\n","[0.0472012  0.0948213  0.04977778 0.07828008]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04379318 0.04688606]\n"," [0.09164903 0.09799358]\n"," [0.0474724  0.05487492]\n"," [0.10186042 0.06051696]]\n","\n","Average MAE Loss:\n","[0.04533962 0.0948213  0.05117366 0.08118869]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04371086 0.05166556]\n"," [0.09164903 0.09799358]\n"," [0.0458736  0.05329804]\n"," [0.10157841 0.05888462]]\n","\n","Average MAE Loss:\n","[0.04768821 0.0948213  0.04958582 0.08023152]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04604088 0.05884582]\n"," [0.09164903 0.09799358]\n"," [0.04559812 0.05331127]\n"," [0.10191801 0.05622264]]\n","\n","Average MAE Loss:\n","[0.05244335 0.0948213  0.04945469 0.07907033]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04406321 0.049164  ]\n"," [0.09164903 0.09799358]\n"," [0.04813348 0.0561649 ]\n"," [0.10143647 0.05562992]]\n","\n","Average MAE Loss:\n","[0.04661361 0.0948213  0.05214919 0.07853319]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05823734 0.06950562]\n"," [0.10143647 0.05562992]\n"," [0.04433517 0.04865541]\n"," [0.04684292 0.05457545]]\n","\n","Average MAE Loss:\n","[0.06387148 0.07853319 0.04649529 0.05070919]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04944596 0.05945779]\n"," [0.10143647 0.05562992]\n"," [0.04468583 0.04757518]\n"," [0.0458541  0.05279453]]\n","\n","Average MAE Loss:\n","[0.05445188 0.07853319 0.04613051 0.04932431]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04737232 0.05924232]\n"," [0.10143647 0.05562992]\n"," [0.04536469 0.0486028 ]\n"," [0.04518258 0.05300664]]\n","\n","Average MAE Loss:\n","[0.05330732 0.07853319 0.04698374 0.04909461]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04556948 0.05627049]\n"," [0.10143647 0.05562992]\n"," [0.04346532 0.0496823 ]\n"," [0.04602051 0.05244434]]\n","\n","Average MAE Loss:\n","[0.05091999 0.07853319 0.04657381 0.04923242]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04427909 0.0519788 ]\n"," [0.10143647 0.05562992]\n"," [0.04549677 0.05297403]\n"," [0.04642651 0.04941017]]\n","\n","Average MAE Loss:\n","[0.04812895 0.07853319 0.0492354  0.04791834]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04338721 0.0480429 ]\n"," [0.10143647 0.05562992]\n"," [0.04478445 0.0496581 ]\n"," [0.04807161 0.0494545 ]]\n","\n","Average MAE Loss:\n","[0.04571506 0.07853319 0.04722128 0.04876305]\n","\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04322711 0.04805003]\n"," [0.10143647 0.05562992]\n"," [0.04470136 0.05285779]\n"," [0.05137442 0.05070639]]\n","\n","Average MAE Loss:\n","[0.04563857 0.07853319 0.04877957 0.05104041]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.063657   0.07254258]\n"," [0.08032271 0.08495189]\n"," [0.05780974 0.06430738]\n"," [0.06656522 0.07327706]]\n","\n","Average MAE Loss:\n","[0.06809979 0.0826373  0.06105856 0.06992114]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05559443 0.06339385]\n"," [0.08032271 0.0849519 ]\n"," [0.0550564  0.06546956]\n"," [0.05885696 0.06445266]]\n","\n","Average MAE Loss:\n","[0.05949414 0.08263731 0.06026298 0.06165481]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05422816 0.06372091]\n"," [0.08032271 0.08495189]\n"," [0.04542024 0.05309733]\n"," [0.05907366 0.06519978]]\n","\n","Average MAE Loss:\n","[0.05897453 0.0826373  0.04925879 0.06213672]\n","\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05197695 0.06233152]\n"," [0.08032271 0.08495189]\n"," [0.0447941  0.05075262]\n"," [0.05864167 0.0647417 ]]\n","\n","Average MAE Loss:\n","[0.05715423 0.0826373  0.04777336 0.06169169]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04918606 0.05979452]\n"," [0.08032271 0.0849519 ]\n"," [0.04373485 0.04978289]\n"," [0.05826215 0.06460919]]\n","\n","Average MAE Loss:\n","[0.05449029 0.08263731 0.04675887 0.06143567]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04668707 0.05655798]\n"," [0.08032271 0.0849519 ]\n"," [0.04322199 0.04940934]\n"," [0.05799579 0.06472435]]\n","\n","Average MAE Loss:\n","[0.05162253 0.08263731 0.04631567 0.06136007]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0458025  0.05486836]\n"," [0.08032271 0.0849519 ]\n"," [0.04290055 0.04912828]\n"," [0.05768622 0.06455013]]\n","\n","Average MAE Loss:\n","[0.05033543 0.08263731 0.04601441 0.06111818]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07465586 0.08181776]\n"," [0.05768622 0.06455013]\n"," [0.04458673 0.05198689]\n"," [0.04246186 0.04810244]]\n","\n","Average MAE Loss:\n","[0.07823681 0.06111818 0.04828681 0.04528215]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06737094 0.07656736]\n"," [0.05768622 0.06455013]\n"," [0.043981   0.05046128]\n"," [0.04250778 0.04804021]]\n","\n","Average MAE Loss:\n","[0.07196915 0.06111818 0.04722114 0.045274  ]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06138758 0.07083597]\n"," [0.05768622 0.06455013]\n"," [0.04344609 0.05003234]\n"," [0.04264253 0.0480029 ]]\n","\n","Average MAE Loss:\n","[0.06611177 0.06111818 0.04673921 0.04532271]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05930272 0.06851776]\n"," [0.05768622 0.06455013]\n"," [0.04299576 0.04917686]\n"," [0.0426469  0.04757271]]\n","\n","Average MAE Loss:\n","[0.06391024 0.06111818 0.04608631 0.0451098 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.05783652 0.06688187]\n"," [0.05768622 0.06455013]\n"," [0.04245495 0.04829053]\n"," [0.04280753 0.04796821]]\n","\n","Average MAE Loss:\n","[0.0623592  0.06111818 0.04537274 0.04538787]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0568169  0.06575605]\n"," [0.05768622 0.06455013]\n"," [0.04245654 0.04843415]\n"," [0.04272392 0.04765183]]\n","\n","Average MAE Loss:\n","[0.06128647 0.06111818 0.04544535 0.04518787]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.05605174 0.06502021]\n"," [0.05768622 0.06455014]\n"," [0.04227796 0.04794071]\n"," [0.04283254 0.04796689]]\n","\n","Average MAE Loss:\n","[0.06053598 0.06111818 0.04510934 0.04539972]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04810564 0.0593337 ]\n"," [0.04858133 0.05985812]\n"," [0.04647334 0.05736222]\n"," [0.04668227 0.05754225]]\n","\n","Average MAE Loss:\n","[0.05371967 0.05421973 0.05191778 0.05211226]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04757037 0.05875355]\n"," [0.04858133 0.05985812]\n"," [0.04406755 0.05344193]\n"," [0.04531309 0.05546025]]\n","\n","Average MAE Loss:\n","[0.05316196 0.05421973 0.04875474 0.05038667]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04707891 0.05820813]\n"," [0.04858133 0.05985812]\n"," [0.04305285 0.05033012]\n"," [0.04414353 0.0527653 ]]\n","\n","Average MAE Loss:\n","[0.05264352 0.05421973 0.04669148 0.04845441]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04663754 0.05768456]\n"," [0.04858133 0.05985812]\n"," [0.04294702 0.04931872]\n"," [0.04346415 0.05039575]]\n","\n","Average MAE Loss:\n","[0.05216105 0.05421973 0.04613287 0.04692995]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04643273 0.05743482]\n"," [0.04858133 0.05985812]\n"," [0.04275027 0.04880162]\n"," [0.04339568 0.04953879]]\n","\n","Average MAE Loss:\n","[0.05193377 0.05421973 0.04577595 0.04646723]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04623014 0.05718022]\n"," [0.04858133 0.05985812]\n"," [0.04255578 0.04827707]\n"," [0.04331117 0.04918998]]\n","\n","Average MAE Loss:\n","[0.05170518 0.05421973 0.04541643 0.04625057]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04602574 0.05691011]\n"," [0.04858133 0.05985812]\n"," [0.04249998 0.04822595]\n"," [0.04322124 0.04902118]]\n","\n","Average MAE Loss:\n","[0.05146793 0.05421973 0.04536297 0.04612121]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04832398 0.05959217]\n"," [0.04322124 0.04902118]\n"," [0.04512625 0.05555127]\n"," [0.04248297 0.04825373]]\n","\n","Average MAE Loss:\n","[0.05395807 0.04612121 0.05033876 0.04536835]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04819191 0.05945491]\n"," [0.04322124 0.04902118]\n"," [0.04451517 0.05447549]\n"," [0.04247637 0.04816167]]\n","\n","Average MAE Loss:\n","[0.05382341 0.04612121 0.04949533 0.04531902]\n","\n","Epoch 00037: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04806272 0.05932088]\n"," [0.04322124 0.04902118]\n"," [0.04401811 0.05345634]\n"," [0.04247155 0.04806492]]\n","\n","Average MAE Loss:\n","[0.0536918  0.04612121 0.04873722 0.04526824]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04792573 0.05917504]\n"," [0.04322124 0.04902118]\n"," [0.04359328 0.05247508]\n"," [0.04251173 0.04813152]]\n","\n","Average MAE Loss:\n","[0.05355038 0.04612121 0.04803418 0.04532162]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04779533 0.05903716]\n"," [0.04322124 0.04902118]\n"," [0.04323886 0.05150863]\n"," [0.04249091 0.04804823]]\n","\n","Average MAE Loss:\n","[0.05341624 0.04612121 0.04737374 0.04526957]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04773371 0.05897261]\n"," [0.04322124 0.04902118]\n"," [0.04312339 0.05113265]\n"," [0.04246492 0.04794029]]\n","\n","Average MAE Loss:\n","[0.05335316 0.04612121 0.04712802 0.0452026 ]\n","\n","Epoch 00041: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04767102 0.0589051 ]\n"," [0.04322124 0.04902118]\n"," [0.04302627 0.05080385]\n"," [0.04248102 0.04796658]]\n","\n","Average MAE Loss:\n","[0.05328806 0.04612121 0.04691506 0.0452238 ]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04316306 0.05120489]\n"," [0.04318543 0.05125545]\n"," [0.04303605 0.05086496]\n"," [0.04311814 0.05106217]]\n","\n","Average MAE Loss:\n","[0.04718397 0.04722044 0.0469505  0.04709016]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04315177 0.0511823 ]\n"," [0.04318543 0.05125545]\n"," [0.04289308 0.05048479]\n"," [0.04299911 0.05072508]]\n","\n","Average MAE Loss:\n","[0.04716703 0.04722044 0.04668893 0.0468621 ]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04314743 0.05117055]\n"," [0.04318543 0.05125545]\n"," [0.04283614 0.05032526]\n"," [0.04290432 0.05038317]]\n","\n","Average MAE Loss:\n","[0.04715899 0.04722044 0.0465807  0.04664375]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04314392 0.05116352]\n"," [0.04318543 0.05125545]\n"," [0.04279472 0.05020046]\n"," [0.04287484 0.05025122]]\n","\n","Average MAE Loss:\n","[0.04715372 0.04722044 0.04649759 0.04656303]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04314023 0.05115282]\n"," [0.04318543 0.05125546]\n"," [0.04274908 0.05005909]\n"," [0.04285172 0.05014167]]\n","\n","Average MAE Loss:\n","[0.04714653 0.04722044 0.04640409 0.04649669]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04313479 0.0511423 ]\n"," [0.04318543 0.05125545]\n"," [0.04272098 0.04996468]\n"," [0.04282761 0.05002293]]\n","\n","Average MAE Loss:\n","[0.04713855 0.04722044 0.04634283 0.04642527]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04313277 0.05113907]\n"," [0.04318543 0.05125545]\n"," [0.04270524 0.04991575]\n"," [0.0428029  0.04987759]]\n","\n","Average MAE Loss:\n","[0.04713592 0.04722044 0.04631049 0.04634025]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.9531e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04318316 0.05125163]\n"," [0.0428029  0.04987759]\n"," [0.0431017  0.05106305]\n"," [0.04269326 0.04985778]]\n","\n","Average MAE Loss:\n","[0.04721739 0.04634025 0.04708237 0.04627552]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04318171 0.0512492 ]\n"," [0.0428029  0.04987759]\n"," [0.04305325 0.05094901]\n"," [0.04268195 0.04979359]]\n","\n","Average MAE Loss:\n","[0.04721545 0.04634025 0.04700113 0.04623777]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04317878 0.05124365]\n"," [0.0428029  0.04987759]\n"," [0.04300978 0.05083942]\n"," [0.04267357 0.04973693]]\n","\n","Average MAE Loss:\n","[0.04721121 0.04634025 0.0469246  0.04620525]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04317741 0.05124108]\n"," [0.0428029  0.04987759]\n"," [0.04298851 0.05078231]\n"," [0.04266481 0.04969044]]\n","\n","Average MAE Loss:\n","[0.04720924 0.04634025 0.04688541 0.04617762]\n","\n","Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04317591 0.05123798]\n"," [0.0428029  0.04987759]\n"," [0.04297204 0.05073738]\n"," [0.04266113 0.04966702]]\n","\n","Average MAE Loss:\n","[0.04720695 0.04634025 0.04685471 0.04616407]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04317438 0.0512348 ]\n"," [0.0428029  0.04987759]\n"," [0.04295544 0.05069231]\n"," [0.0426579  0.04963912]]\n","\n","Average MAE Loss:\n","[0.04720459 0.04634025 0.04682387 0.04614851]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04317309 0.05123224]\n"," [0.0428029  0.04987759]\n"," [0.04293662 0.05064053]\n"," [0.04265444 0.04961585]]\n","\n","Average MAE Loss:\n","[0.04720266 0.04634025 0.04678858 0.04613514]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04286128 0.05032687]\n"," [0.04286156 0.05032731]\n"," [0.04285455 0.05030648]\n"," [0.04285244 0.05029507]]\n","\n","Average MAE Loss:\n","[0.04659408 0.04659444 0.04658052 0.04657375]\n","\n","Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04286103 0.05032672]\n"," [0.04286156 0.05032731]\n"," [0.04284727 0.05028604]\n"," [0.04284825 0.05028033]]\n","\n","Average MAE Loss:\n","[0.04659387 0.04659444 0.04656665 0.04656429]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04286081 0.05032661]\n"," [0.04286156 0.05032731]\n"," [0.04283988 0.05026502]\n"," [0.04284415 0.05026486]]\n","\n","Average MAE Loss:\n","[0.04659371 0.04659444 0.04655245 0.04655451]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04286099 0.0503273 ]\n"," [0.04286156 0.05032731]\n"," [0.04283214 0.05024267]\n"," [0.04283972 0.0502377 ]]\n","\n","Average MAE Loss:\n","[0.04659414 0.04659444 0.0465374  0.04653871]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","\n","epochs finished with time:523.9097678661346\n","\n","[[0.04286099 0.0503273 ]\n"," [0.04286156 0.05032731]\n"," [0.04283214 0.05024267]\n"," [0.04283972 0.0502377 ]]\n","00:08:59.62454304699986\n"]}],"source":["# seed = 10 table = 4/8 fixed folds 3 layers adj\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Cheb_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LpZnLEEKzVW2","outputId":"6dc6384d-fc5d-48b8-f721-ffd2923ecccb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.25\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07840958 0.07396808]\n"," [0.07827297 0.07513475]\n"," [0.04662903 0.04930476]\n"," [0.07608913 0.06248086]]\n","\n","Average MAE Loss:\n","[0.07618883 0.07670386 0.0479669  0.069285  ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07840958 0.07396808]\n"," [0.07827297 0.07513475]\n"," [0.042727   0.04538441]\n"," [0.07781408 0.05591736]]\n","\n","Average MAE Loss:\n","[0.07618883 0.07670386 0.0440557  0.06686572]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07840958 0.07396808]\n"," [0.07827297 0.07513477]\n"," [0.04216056 0.04465297]\n"," [0.08511374 0.04672095]]\n","\n","Average MAE Loss:\n","[0.07618883 0.07670387 0.04340676 0.06591734]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07840958 0.07396808]\n"," [0.07827297 0.07513475]\n"," [0.04154528 0.04290692]\n"," [0.08546145 0.04443548]]\n","\n","Average MAE Loss:\n","[0.07618883 0.07670386 0.0422261  0.06494846]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07840958 0.07396808]\n"," [0.07827297 0.07513477]\n"," [0.04032011 0.03968448]\n"," [0.08493776 0.04208969]]\n","\n","Average MAE Loss:\n","[0.07618883 0.07670387 0.04000229 0.06351373]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07840958 0.07396808]\n"," [0.07827297 0.07513475]\n"," [0.03978442 0.03953252]\n"," [0.08554496 0.0435383 ]]\n","\n","Average MAE Loss:\n","[0.07618883 0.07670386 0.03965847 0.06454163]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07840958 0.07396808]\n"," [0.07827297 0.07513475]\n"," [0.04118207 0.03950005]\n"," [0.08552908 0.04447891]]\n","\n","Average MAE Loss:\n","[0.07618883 0.07670386 0.04034106 0.065004  ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07827297 0.07513477]\n"," [0.08552908 0.04447891]\n"," [0.05173832 0.05793184]\n"," [0.04488443 0.03897111]]\n","\n","Average MAE Loss:\n","[0.07670387 0.065004   0.05483508 0.04192777]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07827297 0.07513475]\n"," [0.08552908 0.04447891]\n"," [0.0466386  0.05106381]\n"," [0.05272515 0.03874678]]\n","\n","Average MAE Loss:\n","[0.07670386 0.065004   0.04885121 0.04573597]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07827297 0.07513477]\n"," [0.08552908 0.04447891]\n"," [0.04286805 0.04798735]\n"," [0.06243505 0.03874896]]\n","\n","Average MAE Loss:\n","[0.07670387 0.065004   0.0454277  0.05059201]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07827297 0.07513477]\n"," [0.08552908 0.04447891]\n"," [0.04047374 0.04304103]\n"," [0.07015824 0.03979626]]\n","\n","Average MAE Loss:\n","[0.07670387 0.065004   0.04175738 0.05497725]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07827297 0.07513477]\n"," [0.08552908 0.04447891]\n"," [0.0381541  0.03969217]\n"," [0.06950958 0.03846757]]\n","\n","Average MAE Loss:\n","[0.07670387 0.065004   0.03892313 0.05398858]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07827297 0.07513477]\n"," [0.08552908 0.04447891]\n"," [0.03754622 0.03829395]\n"," [0.07172738 0.03746507]]\n","\n","Average MAE Loss:\n","[0.07670387 0.065004   0.03792008 0.05459623]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07827297 0.07513477]\n"," [0.08552908 0.04447891]\n"," [0.03822517 0.03920372]\n"," [0.07187013 0.03747257]]\n","\n","Average MAE Loss:\n","[0.07670387 0.065004   0.03871445 0.05467135]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.07269852 0.06847153]\n"," [0.05212935 0.05282854]\n"," [0.05809096 0.05703698]]\n","\n","Average MAE Loss:\n","[0.07058502 0.07058502 0.05247894 0.05756397]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.07269852 0.06847153]\n"," [0.04928618 0.05212595]\n"," [0.05091221 0.05053498]]\n","\n","Average MAE Loss:\n","[0.07058502 0.07058502 0.05070607 0.05072359]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.07269852 0.06847153]\n"," [0.04241601 0.04584276]\n"," [0.05069333 0.05030361]]\n","\n","Average MAE Loss:\n","[0.07058502 0.07058502 0.04412939 0.05049847]\n","\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.07269852 0.06847153]\n"," [0.03950805 0.04175678]\n"," [0.05054099 0.05022137]]\n","\n","Average MAE Loss:\n","[0.07058502 0.07058502 0.04063241 0.05038118]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.07269852 0.06847153]\n"," [0.03820513 0.0387853 ]\n"," [0.05033305 0.05017981]]\n","\n","Average MAE Loss:\n","[0.07058502 0.07058502 0.03849522 0.05025643]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.07269852 0.06847153]\n"," [0.03743923 0.0377514 ]\n"," [0.05014008 0.05015919]]\n","\n","Average MAE Loss:\n","[0.07058502 0.07058502 0.03759532 0.05014963]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.07269852 0.06847153]\n"," [0.03705656 0.03729598]\n"," [0.05000229 0.0501517 ]]\n","\n","Average MAE Loss:\n","[0.07058502 0.07058502 0.03717627 0.05007699]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.05000229 0.0501517 ]\n"," [0.05411841 0.05548373]\n"," [0.0365542  0.03692117]]\n","\n","Average MAE Loss:\n","[0.07058502 0.05007699 0.05480107 0.03673768]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.05000229 0.0501517 ]\n"," [0.04970354 0.05064698]\n"," [0.03616298 0.03652371]]\n","\n","Average MAE Loss:\n","[0.07058502 0.05007699 0.05017526 0.03634334]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.05000229 0.0501517 ]\n"," [0.04747215 0.04978254]\n"," [0.03598146 0.0363515 ]]\n","\n","Average MAE Loss:\n","[0.07058502 0.05007699 0.04862734 0.03616648]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.05000229 0.0501517 ]\n"," [0.04456635 0.0479318 ]\n"," [0.03583102 0.03617443]]\n","\n","Average MAE Loss:\n","[0.07058502 0.05007699 0.04624907 0.03600273]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.05000229 0.0501517 ]\n"," [0.0428392  0.04666349]\n"," [0.03571107 0.03609787]]\n","\n","Average MAE Loss:\n","[0.07058502 0.05007699 0.04475135 0.03590447]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.05000229 0.0501517 ]\n"," [0.04105637 0.04465035]\n"," [0.03572428 0.03600874]]\n","\n","Average MAE Loss:\n","[0.07058502 0.05007699 0.04285336 0.03586651]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07269852 0.06847153]\n"," [0.05000229 0.0501517 ]\n"," [0.03934681 0.04213682]\n"," [0.03560702 0.03606097]]\n","\n","Average MAE Loss:\n","[0.07058502 0.05007699 0.04074181 0.035834  ]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.04654753 0.05040609]\n"," [0.04246891 0.04632856]\n"," [0.04363121 0.04732726]]\n","\n","Average MAE Loss:\n","[0.04847681 0.04847681 0.04439874 0.04547924]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.04654753 0.05040609]\n"," [0.04135614 0.04526667]\n"," [0.04224228 0.04611083]]\n","\n","Average MAE Loss:\n","[0.04847681 0.04847681 0.0433114  0.04417655]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.04654753 0.05040609]\n"," [0.04034529 0.04393977]\n"," [0.04054708 0.04401829]]\n","\n","Average MAE Loss:\n","[0.04847681 0.04847681 0.04214253 0.04228269]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04654754 0.05040609]\n"," [0.04654753 0.05040609]\n"," [0.03945916 0.04263206]\n"," [0.03881957 0.04074503]]\n","\n","Average MAE Loss:\n","[0.04847681 0.04847681 0.04104561 0.0397823 ]\n","\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.04654754 0.05040609]\n"," [0.03869497 0.04128093]\n"," [0.03815328 0.0391631 ]]\n","\n","Average MAE Loss:\n","[0.04847681 0.04847681 0.03998795 0.03865819]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.04654753 0.05040609]\n"," [0.03841967 0.04064653]\n"," [0.03776359 0.03829751]]\n","\n","Average MAE Loss:\n","[0.04847681 0.04847681 0.0395331  0.03803055]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.04654754 0.05040609]\n"," [0.03816307 0.04008662]\n"," [0.03741995 0.03790822]]\n","\n","Average MAE Loss:\n","[0.04847681 0.04847681 0.03912484 0.03766408]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04654754 0.05040609]\n"," [0.03741995 0.03790822]\n"," [0.04517491 0.04908584]\n"," [0.03754377 0.03893769]]\n","\n","Average MAE Loss:\n","[0.04847681 0.03766408 0.04713037 0.03824073]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04654754 0.05040609]\n"," [0.03741995 0.03790821]\n"," [0.04374322 0.0475646 ]\n"," [0.03722907 0.03825001]]\n","\n","Average MAE Loss:\n","[0.04847681 0.03766408 0.04565391 0.03773954]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00037: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.03741995 0.03790822]\n"," [0.0432943  0.04709382]\n"," [0.0370168  0.03775972]]\n","\n","Average MAE Loss:\n","[0.04847681 0.03766408 0.04519406 0.03738826]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04654754 0.05040609]\n"," [0.03741995 0.03790822]\n"," [0.04294862 0.04675757]\n"," [0.03683192 0.03746209]]\n","\n","Average MAE Loss:\n","[0.04847681 0.03766408 0.0448531  0.03714701]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.03741995 0.03790822]\n"," [0.0426369  0.04647467]\n"," [0.0366723  0.03722888]]\n","\n","Average MAE Loss:\n","[0.04847681 0.03766408 0.04455579 0.03695059]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04654754 0.05040609]\n"," [0.03741995 0.03790821]\n"," [0.04233573 0.04620986]\n"," [0.03660125 0.03712788]]\n","\n","Average MAE Loss:\n","[0.04847681 0.03766408 0.0442728  0.03686457]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04654753 0.05040609]\n"," [0.03741995 0.03790822]\n"," [0.0421894  0.04607974]\n"," [0.03656153 0.03705836]]\n","\n","Average MAE Loss:\n","[0.04847681 0.03766408 0.04413457 0.03680995]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292112]\n"," [0.03975744 0.04292113]\n"," [0.03958514 0.04267004]\n"," [0.03945394 0.04246392]]\n","\n","Average MAE Loss:\n","[0.04133928 0.04133928 0.04112759 0.04095893]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292112]\n"," [0.03975744 0.04292113]\n"," [0.03940534 0.0424097 ]\n"," [0.03909376 0.04189523]]\n","\n","Average MAE Loss:\n","[0.04133928 0.04133928 0.04090752 0.04049449]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292112]\n"," [0.03975744 0.04292112]\n"," [0.03924754 0.04217322]\n"," [0.03893454 0.04163376]]\n","\n","Average MAE Loss:\n","[0.04133928 0.04133928 0.04071038 0.04028415]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292112]\n"," [0.03975744 0.04292112]\n"," [0.03918035 0.04206775]\n"," [0.03878596 0.04138289]]\n","\n","Average MAE Loss:\n","[0.04133928 0.04133928 0.04062405 0.04008443]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03975744 0.04292113]\n"," [0.0391201  0.04196762]\n"," [0.038648   0.04113571]]\n","\n","Average MAE Loss:\n","[0.04133928 0.04133928 0.04054386 0.03989185]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03975744 0.04292112]\n"," [0.03906062 0.0418694 ]\n"," [0.03852035 0.04088759]]\n","\n","Average MAE Loss:\n","[0.04133928 0.04133928 0.04046501 0.03970397]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03975744 0.04292113]\n"," [0.03900212 0.04177346]\n"," [0.03845984 0.04076133]]\n","\n","Average MAE Loss:\n","[0.04133928 0.04133928 0.04038779 0.03961058]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.9531e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03845984 0.04076133]\n"," [0.0397112  0.04285477]\n"," [0.03892529 0.04164674]]\n","\n","Average MAE Loss:\n","[0.04133928 0.03961058 0.04128299 0.04028601]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03845984 0.04076133]\n"," [0.03966028 0.04278269]\n"," [0.03884544 0.04152146]]\n","\n","Average MAE Loss:\n","[0.04133928 0.03961058 0.04122149 0.04018345]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03845984 0.04076133]\n"," [0.03960966 0.04271235]\n"," [0.03876657 0.04139561]]\n","\n","Average MAE Loss:\n","[0.04133928 0.03961058 0.04116101 0.04008109]\n","\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03845984 0.04076133]\n"," [0.0395644  0.04264681]\n"," [0.03873121 0.04133288]]\n","\n","Average MAE Loss:\n","[0.04133928 0.03961058 0.0411056  0.04003204]\n","\n","Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03845984 0.04076133]\n"," [0.03954218 0.04261407]\n"," [0.03869733 0.04127001]]\n","\n","Average MAE Loss:\n","[0.04133928 0.03961058 0.04107813 0.03998367]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292112]\n"," [0.03845984 0.04076133]\n"," [0.03951935 0.04258161]\n"," [0.03866379 0.04120663]]\n","\n","Average MAE Loss:\n","[0.04133928 0.03961058 0.04105048 0.03993521]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03975744 0.04292113]\n"," [0.03845984 0.04076133]\n"," [0.03949686 0.04254937]\n"," [0.03863056 0.04114539]]\n","\n","Average MAE Loss:\n","[0.04133928 0.03961058 0.04102312 0.03988797]\n","\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03903699 0.04180089]\n"," [0.03903699 0.04180089]\n"," [0.03901967 0.04177307]\n"," [0.03901683 0.04176763]]\n","\n","Average MAE Loss:\n","[0.04041894 0.04041894 0.04039637 0.04039223]\n","\n","Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03903699 0.04180089]\n"," [0.03903699 0.04180089]\n"," [0.03901159 0.04175997]\n"," [0.03899494 0.04173191]]\n","\n","Average MAE Loss:\n","[0.04041894 0.04041894 0.04038578 0.04036343]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03903699 0.04180089]\n"," [0.03903699 0.04180089]\n"," [0.03900362 0.04174701]\n"," [0.03897342 0.04169705]]\n","\n","Average MAE Loss:\n","[0.04041894 0.04041894 0.04037531 0.04033523]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03903699 0.04180089]\n"," [0.03903699 0.04180089]\n"," [0.0389958  0.04173449]\n"," [0.03895149 0.04166244]]\n","\n","Average MAE Loss:\n","[0.04041894 0.04041894 0.04036515 0.04030696]\n","\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","\n","epochs finished with time:399.69935297966003\n","\n","[[0.03903699 0.04180089]\n"," [0.03903699 0.04180089]\n"," [0.0389958  0.04173449]\n"," [0.03895149 0.04166245]]\n","00:07:1.2999312330002795\n","------------------------------------Fold [2/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10669415 0.10465537]\n"," [0.10415368 0.10648478]\n"," [0.06799362 0.07174922]\n"," [0.11030372 0.09066629]]\n","\n","Average MAE Loss:\n","[0.10567476 0.10531923 0.06987142 0.10048501]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10669415 0.10465537]\n"," [0.10415368 0.10648478]\n"," [0.06356258 0.06703166]\n"," [0.11208581 0.07889954]]\n","\n","Average MAE Loss:\n","[0.10567476 0.10531923 0.06529712 0.09549267]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10669416 0.10465537]\n"," [0.10415368 0.10648478]\n"," [0.06091537 0.06241139]\n"," [0.11658111 0.07572227]]\n","\n","Average MAE Loss:\n","[0.10567477 0.10531923 0.06166338 0.09615169]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10669415 0.10465537]\n"," [0.10415368 0.10648478]\n"," [0.05991526 0.06243497]\n"," [0.11443824 0.07133863]]\n","\n","Average MAE Loss:\n","[0.10567476 0.10531923 0.06117511 0.09288843]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10669416 0.10465537]\n"," [0.10415368 0.10648478]\n"," [0.05991873 0.062224  ]\n"," [0.11582941 0.06971385]]\n","\n","Average MAE Loss:\n","[0.10567477 0.10531923 0.06107136 0.09277163]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10669416 0.10465537]\n"," [0.10415368 0.10648478]\n"," [0.06005453 0.06620246]\n"," [0.11572497 0.07004607]]\n","\n","Average MAE Loss:\n","[0.10567477 0.10531923 0.0631285  0.09288552]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10669416 0.10465537]\n"," [0.10415368 0.10648478]\n"," [0.05945406 0.06838459]\n"," [0.11443146 0.06730244]]\n","\n","Average MAE Loss:\n","[0.10567477 0.10531923 0.06391933 0.09086695]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.10415368 0.10648478]\n"," [0.11443146 0.06730244]\n"," [0.07713511 0.08870469]\n"," [0.08055018 0.07507896]]\n","\n","Average MAE Loss:\n","[0.10531923 0.09086695 0.0829199  0.07781457]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.10415368 0.10648478]\n"," [0.11443146 0.06730244]\n"," [0.0669624  0.0720093 ]\n"," [0.0884528  0.06916022]]\n","\n","Average MAE Loss:\n","[0.10531923 0.09086695 0.06948585 0.07880651]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.10415368 0.10648478]\n"," [0.11443146 0.06730244]\n"," [0.063572   0.06505216]\n"," [0.09466892 0.06555396]]\n","\n","Average MAE Loss:\n","[0.10531923 0.09086695 0.06431208 0.08011144]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.10415368 0.10648478]\n"," [0.11443146 0.06730244]\n"," [0.06323404 0.06514083]\n"," [0.09743417 0.06379085]]\n","\n","Average MAE Loss:\n","[0.10531923 0.09086695 0.06418743 0.08061251]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.10415368 0.10648478]\n"," [0.11443146 0.06730244]\n"," [0.06245005 0.06376001]\n"," [0.10178509 0.06308043]]\n","\n","Average MAE Loss:\n","[0.10531923 0.09086695 0.06310503 0.08243276]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.10415368 0.10648478]\n"," [0.11443146 0.06730244]\n"," [0.06218062 0.06361481]\n"," [0.10348744 0.063486  ]]\n","\n","Average MAE Loss:\n","[0.10531923 0.09086695 0.06289771 0.08348672]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.10415368 0.10648478]\n"," [0.11443146 0.06730244]\n"," [0.06258608 0.06408763]\n"," [0.10362318 0.06180597]]\n","\n","Average MAE Loss:\n","[0.10531923 0.09086695 0.06333686 0.08271457]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.1033478  0.10116181]\n"," [0.08015144 0.08455569]\n"," [0.08635009 0.0876922 ]]\n","\n","Average MAE Loss:\n","[0.10225481 0.10225481 0.08235357 0.08702115]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.1033478  0.10116181]\n"," [0.07213243 0.07640119]\n"," [0.07555372 0.07787068]]\n","\n","Average MAE Loss:\n","[0.10225481 0.10225481 0.07426681 0.0767122 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.1033478  0.10116181]\n"," [0.07199254 0.07720979]\n"," [0.07478473 0.0771557 ]]\n","\n","Average MAE Loss:\n","[0.10225481 0.10225481 0.07460117 0.07597021]\n","\n","Epoch 00017: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.1033478  0.10116181]\n"," [0.06955112 0.07542509]\n"," [0.0754661  0.07782   ]]\n","\n","Average MAE Loss:\n","[0.10225481 0.10225481 0.07248811 0.07664305]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.1033478  0.10116181]\n"," [0.06762093 0.07400378]\n"," [0.0741128  0.07696575]]\n","\n","Average MAE Loss:\n","[0.10225481 0.10225481 0.07081235 0.07553928]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.1033478  0.10116181]\n"," [0.06590371 0.07208964]\n"," [0.07410522 0.07714929]]\n","\n","Average MAE Loss:\n","[0.10225481 0.10225481 0.06899667 0.07562725]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.1033478  0.10116181]\n"," [0.0640089  0.06930498]\n"," [0.07294004 0.07652717]]\n","\n","Average MAE Loss:\n","[0.10225481 0.10225481 0.06665694 0.0747336 ]\n","\n","Epoch 00021: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.07294004 0.07652717]\n"," [0.09640934 0.09798168]\n"," [0.0676209  0.07215364]]\n","\n","Average MAE Loss:\n","[0.10225481 0.0747336  0.09719551 0.06988727]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.07294004 0.07652717]\n"," [0.08668759 0.09131247]\n"," [0.0637957  0.06594234]]\n","\n","Average MAE Loss:\n","[0.10225481 0.0747336  0.08900003 0.06486902]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.07294004 0.07652717]\n"," [0.07877204 0.08344496]\n"," [0.06101833 0.06353603]]\n","\n","Average MAE Loss:\n","[0.10225481 0.0747336  0.0811085  0.06227718]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.07294004 0.07652717]\n"," [0.07437866 0.07830023]\n"," [0.06249646 0.06400535]]\n","\n","Average MAE Loss:\n","[0.10225481 0.0747336  0.07633944 0.06325091]\n","\n","Epoch 00025: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.07294004 0.07652717]\n"," [0.07343628 0.07737269]\n"," [0.06333237 0.06482275]]\n","\n","Average MAE Loss:\n","[0.10225481 0.0747336  0.07540449 0.06407756]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.07294004 0.07652717]\n"," [0.07295111 0.07702047]\n"," [0.06362271 0.06490207]]\n","\n","Average MAE Loss:\n","[0.10225481 0.0747336  0.07498579 0.06426239]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.1033478  0.10116181]\n"," [0.07294004 0.07652717]\n"," [0.0726096  0.0768521 ]\n"," [0.06318066 0.06371298]]\n","\n","Average MAE Loss:\n","[0.10225481 0.0747336  0.07473085 0.06344682]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.07649489 0.08159274]\n"," [0.07423277 0.07957047]\n"," [0.07043282 0.07565172]]\n","\n","Average MAE Loss:\n","[0.07904381 0.07904381 0.07690162 0.07304227]\n","\n","Epoch 00029: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.07649489 0.08159274]\n"," [0.07279284 0.07820331]\n"," [0.06933286 0.07503934]]\n","\n","Average MAE Loss:\n","[0.07904381 0.07904381 0.07549807 0.0721861 ]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.07649489 0.08159273]\n"," [0.07157482 0.07706808]\n"," [0.06662837 0.07245225]]\n","\n","Average MAE Loss:\n","[0.07904381 0.07904381 0.07432145 0.06954031]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.07649489 0.08159274]\n"," [0.07065158 0.07624938]\n"," [0.0635637  0.06733336]]\n","\n","Average MAE Loss:\n","[0.07904381 0.07904381 0.07345048 0.06544853]\n","\n","Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.07649489 0.08159274]\n"," [0.06991265 0.07562902]\n"," [0.06208047 0.06446654]]\n","\n","Average MAE Loss:\n","[0.07904381 0.07904381 0.07277084 0.0632735 ]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.07649489 0.08159274]\n"," [0.06963662 0.07540434]\n"," [0.06141368 0.06344822]]\n","\n","Average MAE Loss:\n","[0.07904381 0.07904381 0.07252048 0.06243095]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.07649489 0.08159274]\n"," [0.06939117 0.0752103 ]\n"," [0.06121485 0.06321746]]\n","\n","Average MAE Loss:\n","[0.07904381 0.07904381 0.07230074 0.06221615]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159273]\n"," [0.06121485 0.06321746]\n"," [0.07579744 0.08098617]\n"," [0.06832319 0.07436131]]\n","\n","Average MAE Loss:\n","[0.07904381 0.06221615 0.0783918  0.07134225]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.06121485 0.06321746]\n"," [0.07490487 0.08017654]\n"," [0.06749644 0.07367699]]\n","\n","Average MAE Loss:\n","[0.07904381 0.06221615 0.0775407  0.07058672]\n","\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.06121485 0.06321746]\n"," [0.07447975 0.07978326]\n"," [0.06595469 0.07204664]]\n","\n","Average MAE Loss:\n","[0.07904381 0.06221615 0.0771315  0.06900066]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.06121485 0.06321746]\n"," [0.07407339 0.07940143]\n"," [0.06399837 0.06932744]]\n","\n","Average MAE Loss:\n","[0.07904381 0.06221615 0.07673741 0.06666291]\n","\n","Epoch 00039: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.06121485 0.06321746]\n"," [0.07369757 0.07904419]\n"," [0.06294218 0.06737123]]\n","\n","Average MAE Loss:\n","[0.07904381 0.06221615 0.07637088 0.06515671]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.06121485 0.06321745]\n"," [0.07333763 0.07869968]\n"," [0.06185524 0.06524182]]\n","\n","Average MAE Loss:\n","[0.07904381 0.06221615 0.07601865 0.06354853]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.07649489 0.08159274]\n"," [0.06121485 0.06321746]\n"," [0.07316673 0.078538  ]\n"," [0.06127617 0.0637203 ]]\n","\n","Average MAE Loss:\n","[0.07904381 0.06221615 0.07585237 0.06249824]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.06763695 0.07317748]\n"," [0.06747444 0.07301342]\n"," [0.06524738 0.0703628 ]]\n","\n","Average MAE Loss:\n","[0.07040722 0.07040722 0.07024393 0.06780509]\n","\n","Epoch 00043: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.06763695 0.07317748]\n"," [0.06731831 0.0728548 ]\n"," [0.06414872 0.06900014]]\n","\n","Average MAE Loss:\n","[0.07040722 0.07040722 0.07008655 0.06657443]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.06763695 0.07317748]\n"," [0.06716539 0.07269704]\n"," [0.0633757  0.06781914]]\n","\n","Average MAE Loss:\n","[0.07040722 0.07040722 0.06993122 0.06559742]\n","\n","Epoch 00045: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.06763695 0.07317748]\n"," [0.06709046 0.07261932]\n"," [0.06275717 0.06669658]]\n","\n","Average MAE Loss:\n","[0.07040722 0.07040722 0.06985489 0.06472688]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.06763695 0.07317749]\n"," [0.06701599 0.07254236]\n"," [0.06228874 0.06564346]]\n","\n","Average MAE Loss:\n","[0.07040722 0.07040722 0.06977918 0.0639661 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.06763695 0.07317748]\n"," [0.06694688 0.07247091]\n"," [0.0621042  0.06518587]]\n","\n","Average MAE Loss:\n","[0.07040722 0.07040722 0.06970889 0.06364504]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.06763695 0.07317748]\n"," [0.06687763 0.0723997 ]\n"," [0.0619432  0.06479121]]\n","\n","Average MAE Loss:\n","[0.07040722 0.07040722 0.06963867 0.06336721]\n","\n","Epoch 00049: reducing learning rate of group 0 to 4.8828e-07.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.0619432  0.06479121]\n"," [0.06759664 0.07313659]\n"," [0.06623402 0.07164148]]\n","\n","Average MAE Loss:\n","[0.07040722 0.06336721 0.07036661 0.06893775]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.0619432  0.06479121]\n"," [0.06755584 0.07309528]\n"," [0.06545386 0.07073699]]\n","\n","Average MAE Loss:\n","[0.07040722 0.06336721 0.07032556 0.06809543]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.0619432  0.06479121]\n"," [0.06751515 0.07305394]\n"," [0.0651172  0.07034083]]\n","\n","Average MAE Loss:\n","[0.07040722 0.06336721 0.07028455 0.06772902]\n","\n","Epoch 00052: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.0619432  0.06479121]\n"," [0.06747646 0.07301449]\n"," [0.06483765 0.06999492]]\n","\n","Average MAE Loss:\n","[0.07040722 0.06336721 0.07024547 0.06741628]\n","\n","Epoch 00053: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317749]\n"," [0.0619432  0.06479121]\n"," [0.06745718 0.07299477]\n"," [0.064586   0.06967082]]\n","\n","Average MAE Loss:\n","[0.07040722 0.06336721 0.07022597 0.06712841]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317748]\n"," [0.0619432  0.06479121]\n"," [0.06743816 0.07297543]\n"," [0.06433681 0.06934355]]\n","\n","Average MAE Loss:\n","[0.07040722 0.06336721 0.07020679 0.06684018]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06763695 0.07317749]\n"," [0.0619432  0.06479121]\n"," [0.06741896 0.07295581]\n"," [0.06423484 0.06919618]]\n","\n","Average MAE Loss:\n","[0.07040722 0.06336721 0.07018738 0.06671551]\n","\n","Epoch 00056: reducing learning rate of group 0 to 2.4414e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06514665 0.07004178]\n"," [0.06514665 0.07004178]\n"," [0.06513166 0.07002525]\n"," [0.06499239 0.06983987]]\n","\n","Average MAE Loss:\n","[0.06759422 0.06759422 0.06757846 0.06741613]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06514665 0.07004178]\n"," [0.06514665 0.07004178]\n"," [0.06512531 0.07001809]\n"," [0.06480309 0.06960658]]\n","\n","Average MAE Loss:\n","[0.06759422 0.06759422 0.0675717  0.06720484]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06514665 0.07004179]\n"," [0.06514665 0.07004178]\n"," [0.06511889 0.07001087]\n"," [0.06461347 0.06937346]]\n","\n","Average MAE Loss:\n","[0.06759422 0.06759422 0.06756488 0.06699347]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06514665 0.07004178]\n"," [0.06514665 0.07004178]\n"," [0.06511266 0.07000381]\n"," [0.06452426 0.06926029]]\n","\n","Average MAE Loss:\n","[0.06759422 0.06759422 0.06755823 0.06689228]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.2207e-07.\n","\n","epochs finished with time:398.21870946884155\n","\n","[[0.06514665 0.07004178]\n"," [0.06514665 0.07004178]\n"," [0.06511266 0.07000381]\n"," [0.06452426 0.06926029]]\n","00:06:57.75279310399992\n","------------------------------------Fold [3/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.09068221 0.08059703]\n"," [0.08913526 0.08132835]\n"," [0.05670045 0.05711552]\n"," [0.0925004  0.06493275]]\n","\n","Average MAE Loss:\n","[0.08563962 0.08523181 0.05690798 0.07871658]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.09068221 0.08059703]\n"," [0.08913526 0.08132836]\n"," [0.05011641 0.05011068]\n"," [0.0917946  0.05443561]]\n","\n","Average MAE Loss:\n","[0.08563962 0.08523181 0.05011355 0.07311511]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.09068221 0.08059703]\n"," [0.08913526 0.08132835]\n"," [0.0464584  0.04692008]\n"," [0.09432985 0.04951596]]\n","\n","Average MAE Loss:\n","[0.08563962 0.08523181 0.04668924 0.0719229 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.09068221 0.08059703]\n"," [0.08913526 0.08132836]\n"," [0.04371212 0.04542146]\n"," [0.09815102 0.04981025]]\n","\n","Average MAE Loss:\n","[0.08563962 0.08523181 0.04456679 0.07398064]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.09068221 0.08059703]\n"," [0.08913525 0.08132835]\n"," [0.04223738 0.04411273]\n"," [0.09807438 0.04679681]]\n","\n","Average MAE Loss:\n","[0.08563962 0.0852318  0.04317506 0.0724356 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.09068221 0.08059703]\n"," [0.08913526 0.08132835]\n"," [0.04373356 0.05095132]\n"," [0.09814106 0.04645094]]\n","\n","Average MAE Loss:\n","[0.08563962 0.08523181 0.04734244 0.072296  ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.09068221 0.08059703]\n"," [0.08913527 0.08132835]\n"," [0.04395721 0.05369544]\n"," [0.09891968 0.04696849]]\n","\n","Average MAE Loss:\n","[0.08563962 0.08523181 0.04882632 0.07294409]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08913526 0.08132835]\n"," [0.09891968 0.04696849]\n"," [0.05948848 0.06321993]\n"," [0.0477943  0.04374923]]\n","\n","Average MAE Loss:\n","[0.08523181 0.07294409 0.06135421 0.04577177]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08913526 0.08132836]\n"," [0.09891968 0.04696849]\n"," [0.05159206 0.05479471]\n"," [0.05168895 0.04297878]]\n","\n","Average MAE Loss:\n","[0.08523181 0.07294409 0.05319338 0.04733387]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.08913526 0.08132835]\n"," [0.09891968 0.04696849]\n"," [0.04842896 0.0476271 ]\n"," [0.04990644 0.04198135]]\n","\n","Average MAE Loss:\n","[0.08523181 0.07294409 0.04802803 0.0459439 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08913525 0.08132836]\n"," [0.09891968 0.04696849]\n"," [0.04654497 0.04474128]\n"," [0.05604599 0.04151146]]\n","\n","Average MAE Loss:\n","[0.08523181 0.07294409 0.04564312 0.04877873]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08913526 0.08132835]\n"," [0.09891968 0.04696849]\n"," [0.04447092 0.04263178]\n"," [0.0565697  0.04198578]]\n","\n","Average MAE Loss:\n","[0.08523181 0.07294409 0.04355135 0.04927774]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.08913526 0.08132835]\n"," [0.09891968 0.04696849]\n"," [0.04392979 0.0421683 ]\n"," [0.05721612 0.04443343]]\n","\n","Average MAE Loss:\n","[0.08523181 0.07294409 0.04304904 0.05082478]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.08913526 0.08132835]\n"," [0.09891968 0.04696849]\n"," [0.04344387 0.04177149]\n"," [0.0597508  0.04060312]]\n","\n","Average MAE Loss:\n","[0.08523181 0.07294409 0.04260768 0.05017696]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.08533051 0.07509531]\n"," [0.05970876 0.0603237 ]\n"," [0.07513363 0.06716049]]\n","\n","Average MAE Loss:\n","[0.08021291 0.08021291 0.06001623 0.07114706]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.08533051 0.07509531]\n"," [0.05892809 0.05565209]\n"," [0.06085605 0.05524448]]\n","\n","Average MAE Loss:\n","[0.08021291 0.08021291 0.05729009 0.05805027]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.08533051 0.07509531]\n"," [0.05025965 0.05047077]\n"," [0.05999257 0.05463437]]\n","\n","Average MAE Loss:\n","[0.08021291 0.08021291 0.05036521 0.05731347]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.08533051 0.07509531]\n"," [0.04464211 0.04429676]\n"," [0.06015837 0.0542781 ]]\n","\n","Average MAE Loss:\n","[0.08021291 0.08021291 0.04446944 0.05721824]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.08533051 0.07509531]\n"," [0.04367893 0.04359003]\n"," [0.06010057 0.05427237]]\n","\n","Average MAE Loss:\n","[0.08021291 0.08021291 0.04363448 0.05718647]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.08533051 0.07509531]\n"," [0.04335458 0.04275365]\n"," [0.06010395 0.05421649]]\n","\n","Average MAE Loss:\n","[0.08021291 0.08021291 0.04305412 0.05716022]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.08533051 0.07509531]\n"," [0.04280568 0.04227215]\n"," [0.06003617 0.05424184]]\n","\n","Average MAE Loss:\n","[0.08021291 0.08021291 0.04253892 0.05713901]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.06003617 0.05424184]\n"," [0.06291196 0.05894198]\n"," [0.04290594 0.04187244]]\n","\n","Average MAE Loss:\n","[0.08021291 0.057139   0.06092697 0.04238919]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.06003617 0.05424184]\n"," [0.05798461 0.05492647]\n"," [0.04281259 0.04165511]]\n","\n","Average MAE Loss:\n","[0.08021291 0.057139   0.05645554 0.04223385]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.06003617 0.05424184]\n"," [0.05630908 0.05336615]\n"," [0.04244418 0.04154526]]\n","\n","Average MAE Loss:\n","[0.08021291 0.057139   0.05483761 0.04199472]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.06003617 0.05424184]\n"," [0.05247992 0.05192589]\n"," [0.0425542  0.04132367]]\n","\n","Average MAE Loss:\n","[0.08021291 0.057139   0.05220291 0.04193893]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.06003617 0.05424184]\n"," [0.05040859 0.05040483]\n"," [0.04235559 0.04122385]]\n","\n","Average MAE Loss:\n","[0.08021291 0.057139   0.05040671 0.04178972]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.06003617 0.05424184]\n"," [0.04826117 0.04836804]\n"," [0.04219045 0.04119484]]\n","\n","Average MAE Loss:\n","[0.08021291 0.05713901 0.04831461 0.04169264]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.08533051 0.07509531]\n"," [0.06003617 0.05424184]\n"," [0.04640665 0.04625872]\n"," [0.04232049 0.04096179]]\n","\n","Average MAE Loss:\n","[0.08021291 0.057139   0.04633269 0.04164114]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.05627066 0.05487412]\n"," [0.05103425 0.05094221]\n"," [0.05359376 0.05229772]]\n","\n","Average MAE Loss:\n","[0.05557239 0.05557239 0.05098823 0.05294574]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.05627066 0.05487412]\n"," [0.04971866 0.05027469]\n"," [0.05207046 0.05164516]]\n","\n","Average MAE Loss:\n","[0.05557239 0.05557239 0.04999667 0.05185781]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.05627066 0.05487412]\n"," [0.04855637 0.04889151]\n"," [0.05139192 0.05086379]]\n","\n","Average MAE Loss:\n","[0.05557239 0.05557239 0.04872394 0.05112785]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.05627066 0.05487412]\n"," [0.04741766 0.04756861]\n"," [0.05057196 0.05011494]]\n","\n","Average MAE Loss:\n","[0.05557239 0.05557239 0.04749314 0.05034345]\n","\n","Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.05627066 0.05487412]\n"," [0.04642932 0.04636869]\n"," [0.05003467 0.04966591]]\n","\n","Average MAE Loss:\n","[0.05557239 0.05557239 0.046399   0.04985029]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.05627066 0.05487412]\n"," [0.04595691 0.0458472 ]\n"," [0.04943424 0.04914642]]\n","\n","Average MAE Loss:\n","[0.05557239 0.05557239 0.04590205 0.04929033]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.05627066 0.05487412]\n"," [0.04551595 0.04538007]\n"," [0.04881731 0.0485231 ]]\n","\n","Average MAE Loss:\n","[0.05557239 0.05557239 0.04544801 0.04867021]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.04881731 0.0485231 ]\n"," [0.05434455 0.05315894]\n"," [0.04505755 0.0445707 ]]\n","\n","Average MAE Loss:\n","[0.05557239 0.04867021 0.05375175 0.04481413]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.04881731 0.0485231 ]\n"," [0.0525629  0.05172306]\n"," [0.04484501 0.04424281]]\n","\n","Average MAE Loss:\n","[0.05557239 0.04867021 0.05214298 0.04454391]\n","\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00037: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.04881731 0.0485231 ]\n"," [0.0519832  0.05135876]\n"," [0.0446092  0.04398069]]\n","\n","Average MAE Loss:\n","[0.05557239 0.04867021 0.05167098 0.04429495]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.04881731 0.0485231 ]\n"," [0.05153914 0.05112206]\n"," [0.04440697 0.0437394 ]]\n","\n","Average MAE Loss:\n","[0.05557239 0.04867021 0.0513306  0.04407319]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.04881731 0.0485231 ]\n"," [0.05114283 0.05093173]\n"," [0.04424519 0.04354219]]\n","\n","Average MAE Loss:\n","[0.05557239 0.04867021 0.05103728 0.04389369]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.04881731 0.0485231 ]\n"," [0.05078879 0.05072758]\n"," [0.04418701 0.04346085]]\n","\n","Average MAE Loss:\n","[0.05557239 0.04867021 0.05075818 0.04382393]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00041: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05627066 0.05487412]\n"," [0.04881731 0.0485231 ]\n"," [0.05061594 0.05061126]\n"," [0.04412299 0.04339304]]\n","\n","Average MAE Loss:\n","[0.05557239 0.04867021 0.0506136  0.04375801]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.0494658  0.04909137]\n"," [0.04920406 0.04888145]\n"," [0.04924504 0.04890591]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04927858 0.04904275 0.04907547]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.0494658  0.04909137]\n"," [0.04893283 0.04868371]\n"," [0.04896063 0.04868748]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04927858 0.04880827 0.04882406]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.0494658  0.04909137]\n"," [0.04868481 0.04850721]\n"," [0.04883328 0.04858438]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04927858 0.04859601 0.04870883]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.0494658  0.04909137]\n"," [0.04856928 0.04842642]\n"," [0.04870979 0.04848478]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04927858 0.04849785 0.04859728]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04946579 0.04909137]\n"," [0.0494658  0.04909137]\n"," [0.04845876 0.04834674]\n"," [0.04859542 0.04838855]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04927858 0.04840275 0.04849199]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.0494658  0.04909137]\n"," [0.04835329 0.04826748]\n"," [0.04849117 0.04829309]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04927858 0.04831039 0.04839213]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.0494658  0.04909137]\n"," [0.04825173 0.04819122]\n"," [0.04843768 0.04824461]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04927858 0.04822147 0.04834114]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.9531e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.04843768 0.04824461]\n"," [0.04939444 0.04903502]\n"," [0.04820213 0.04814305]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04834114 0.04921473 0.04817259]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.04843768 0.04824461]\n"," [0.04931451 0.04897431]\n"," [0.0481532  0.04809473]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04834114 0.04914441 0.04812397]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.04843768 0.04824461]\n"," [0.04924102 0.04891849]\n"," [0.04810607 0.04804572]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04834114 0.04907975 0.0480759 ]\n","\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.04843768 0.04824461]\n"," [0.04917068 0.04886599]\n"," [0.04808357 0.04802087]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04834114 0.04901833 0.04805222]\n","\n","Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.04843768 0.04824461]\n"," [0.04913487 0.04884006]\n"," [0.04805964 0.04799537]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04834114 0.04898747 0.04802751]\n","\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.04843768 0.04824461]\n"," [0.04910031 0.04881514]\n"," [0.04803658 0.04796962]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04834114 0.04895772 0.0480031 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0494658  0.04909137]\n"," [0.04843768 0.04824461]\n"," [0.04906457 0.04879002]\n"," [0.0480147  0.04794408]]\n","\n","Average MAE Loss:\n","[0.04927858 0.04834114 0.04892729 0.04797939]\n","\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.048734   0.0484948 ]\n"," [0.048734   0.0484948 ]\n"," [0.04870254 0.0484718 ]\n"," [0.04872088 0.04848225]]\n","\n","Average MAE Loss:\n","[0.0486144  0.0486144  0.04858717 0.04860157]\n","\n","Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.048734   0.0484948 ]\n"," [0.048734   0.0484948 ]\n"," [0.04868648 0.04846046]\n"," [0.04870512 0.04846878]]\n","\n","Average MAE Loss:\n","[0.0486144  0.0486144  0.04857347 0.04858695]\n","\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.048734   0.0484948 ]\n"," [0.048734   0.0484948 ]\n"," [0.04867069 0.04844907]\n"," [0.04868864 0.0484552 ]]\n","\n","Average MAE Loss:\n","[0.0486144  0.0486144  0.04855988 0.04857192]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.048734   0.0484948 ]\n"," [0.048734   0.0484948 ]\n"," [0.04865479 0.04843792]\n"," [0.04867225 0.04844185]]\n","\n","Average MAE Loss:\n","[0.0486144  0.0486144  0.04854636 0.04855705]\n","\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","\n","epochs finished with time:385.14210987091064\n","\n","[[0.048734   0.0484948 ]\n"," [0.048734   0.0484948 ]\n"," [0.04865479 0.04843792]\n"," [0.04867225 0.04844185]]\n","00:06:40.796858183999575\n","------------------------------------Fold [4/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07429574 0.09416686]\n"," [0.07500252 0.09722316]\n"," [0.0444941  0.06410401]\n"," [0.072319   0.07279621]]\n","\n","Average MAE Loss:\n","[0.0842313  0.08611284 0.05429905 0.07255761]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07429574 0.09416686]\n"," [0.07500252 0.09722316]\n"," [0.04165596 0.06065975]\n"," [0.0746162  0.06434773]]\n","\n","Average MAE Loss:\n","[0.0842313  0.08611284 0.05115785 0.06948197]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07429574 0.09416686]\n"," [0.07500252 0.09722316]\n"," [0.04048953 0.06181976]\n"," [0.07972113 0.06253267]]\n","\n","Average MAE Loss:\n","[0.0842313  0.08611284 0.05115464 0.0711269 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07429574 0.09416686]\n"," [0.07500252 0.09722316]\n"," [0.04072382 0.06298023]\n"," [0.0805054  0.06039715]]\n","\n","Average MAE Loss:\n","[0.0842313  0.08611284 0.05185203 0.07045127]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07429574 0.09416686]\n"," [0.07500252 0.09722316]\n"," [0.03958655 0.05991145]\n"," [0.08119023 0.06249045]]\n","\n","Average MAE Loss:\n","[0.0842313  0.08611284 0.049749   0.07184034]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07429574 0.09416686]\n"," [0.07500252 0.09722316]\n"," [0.03833676 0.05806902]\n"," [0.08168501 0.06163992]]\n","\n","Average MAE Loss:\n","[0.0842313  0.08611284 0.04820289 0.07166247]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07429574 0.09416686]\n"," [0.07500252 0.09722316]\n"," [0.03802489 0.05967676]\n"," [0.07793862 0.05666161]]\n","\n","Average MAE Loss:\n","[0.0842313  0.08611284 0.04885083 0.06730011]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07500252 0.09722316]\n"," [0.07793862 0.05666161]\n"," [0.0472123  0.06836127]\n"," [0.04331671 0.06180492]]\n","\n","Average MAE Loss:\n","[0.08611284 0.06730011 0.05778679 0.05256081]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07500252 0.09722316]\n"," [0.07793862 0.0566616 ]\n"," [0.04334694 0.06264549]\n"," [0.04469623 0.06251182]]\n","\n","Average MAE Loss:\n","[0.08611284 0.06730011 0.05299621 0.05360403]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07500252 0.09722316]\n"," [0.07793862 0.05666161]\n"," [0.03995414 0.05912944]\n"," [0.04477529 0.0618697 ]]\n","\n","Average MAE Loss:\n","[0.08611284 0.06730011 0.04954179 0.05332249]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07500252 0.09722316]\n"," [0.07793862 0.0566616 ]\n"," [0.04009517 0.06124602]\n"," [0.04677283 0.06173088]]\n","\n","Average MAE Loss:\n","[0.08611284 0.06730011 0.0506706  0.05425185]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07500252 0.09722316]\n"," [0.07793862 0.05666161]\n"," [0.04147016 0.06492288]\n"," [0.04841949 0.06153503]]\n","\n","Average MAE Loss:\n","[0.08611284 0.06730011 0.05319652 0.05497726]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07500252 0.09722316]\n"," [0.07793862 0.0566616 ]\n"," [0.04023197 0.06337714]\n"," [0.0480264  0.05511452]]\n","\n","Average MAE Loss:\n","[0.08611284 0.06730011 0.05180456 0.05157046]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07500252 0.09722316]\n"," [0.07793862 0.0566616 ]\n"," [0.03944558 0.06005404]\n"," [0.05024446 0.05580319]]\n","\n","Average MAE Loss:\n","[0.08611284 0.06730011 0.04974981 0.05302382]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.06875592 0.09103521]\n"," [0.05069315 0.07342052]\n"," [0.05999993 0.0829461 ]]\n","\n","Average MAE Loss:\n","[0.07989557 0.07989557 0.06205684 0.07147301]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.06875592 0.09103521]\n"," [0.04720557 0.06687751]\n"," [0.04979386 0.06836711]]\n","\n","Average MAE Loss:\n","[0.07989557 0.07989557 0.05704154 0.05908048]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.06875592 0.09103521]\n"," [0.04561793 0.06730897]\n"," [0.04889331 0.0683495 ]]\n","\n","Average MAE Loss:\n","[0.07989557 0.07989557 0.05646345 0.0586214 ]\n","\n","Epoch 00017: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.06875592 0.09103521]\n"," [0.04323443 0.06548199]\n"," [0.04868327 0.06773927]]\n","\n","Average MAE Loss:\n","[0.07989557 0.07989557 0.05435821 0.05821127]\n","\n","Epoch 00018: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.06875592 0.09103521]\n"," [0.04173183 0.06446718]\n"," [0.04862884 0.06756078]]\n","\n","Average MAE Loss:\n","[0.07989557 0.07989557 0.05309951 0.05809481]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.06875592 0.09103521]\n"," [0.03999095 0.06279933]\n"," [0.0485675  0.06760751]]\n","\n","Average MAE Loss:\n","[0.07989557 0.07989557 0.05139514 0.0580875 ]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.06875592 0.09103521]\n"," [0.03839812 0.06055741]\n"," [0.04848503 0.06757583]]\n","\n","Average MAE Loss:\n","[0.07989557 0.07989557 0.04947776 0.05803043]\n","\n","Epoch 00021: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.04848503 0.06757583]\n"," [0.05682789 0.08271123]\n"," [0.03775216 0.05919029]]\n","\n","Average MAE Loss:\n","[0.07989557 0.05803043 0.06976956 0.04847123]\n","\n","Epoch 00022: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.04848503 0.06757583]\n"," [0.05102348 0.07507823]\n"," [0.03708252 0.05740009]]\n","\n","Average MAE Loss:\n","[0.07989557 0.05803043 0.06305086 0.0472413 ]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.04848503 0.06757583]\n"," [0.04823732 0.06936067]\n"," [0.03675829 0.05609829]]\n","\n","Average MAE Loss:\n","[0.07989557 0.05803043 0.05879899 0.04642829]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.04848503 0.06757583]\n"," [0.04719826 0.06764814]\n"," [0.03659973 0.05547691]]\n","\n","Average MAE Loss:\n","[0.07989557 0.05803043 0.0574232  0.04603832]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.04848503 0.06757583]\n"," [0.04653758 0.06726513]\n"," [0.03649538 0.05531368]]\n","\n","Average MAE Loss:\n","[0.07989557 0.05803043 0.05690135 0.04590453]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.04848503 0.06757583]\n"," [0.04625922 0.06716529]\n"," [0.03638908 0.05517497]]\n","\n","Average MAE Loss:\n","[0.07989557 0.05803043 0.05671226 0.04578202]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06875592 0.09103521]\n"," [0.04848503 0.06757583]\n"," [0.04597762 0.06704887]\n"," [0.03626507 0.05481746]]\n","\n","Average MAE Loss:\n","[0.07989557 0.05803043 0.05651324 0.04554127]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04739406 0.07217938]\n"," [0.04603895 0.07005855]\n"," [0.04545171 0.06823615]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05978672 0.05804875 0.05684393]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04739406 0.07217938]\n"," [0.04482283 0.06774743]\n"," [0.04470823 0.0659294 ]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05978672 0.05628513 0.05531881]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04739406 0.07217938]\n"," [0.04443109 0.06704254]\n"," [0.04436692 0.06598508]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05978672 0.05573681 0.055176  ]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04739406 0.07217938]\n"," [0.04413341 0.06658529]\n"," [0.04400508 0.06594871]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05978672 0.05535935 0.0549769 ]\n","\n","Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04739406 0.07217938]\n"," [0.0438679  0.06623577]\n"," [0.04377404 0.06573417]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05978672 0.05505183 0.05475411]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04739406 0.07217938]\n"," [0.04362255 0.06596406]\n"," [0.04354247 0.06549802]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05978672 0.0547933  0.05452025]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04739406 0.07217938]\n"," [0.04350293 0.06585775]\n"," [0.04331207 0.06536587]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05978672 0.05468034 0.05433897]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04331207 0.06536587]\n"," [0.04699606 0.07160547]\n"," [0.04321215 0.0655423 ]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05433897 0.05930077 0.05437723]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04331207 0.06536587]\n"," [0.04648391 0.07077227]\n"," [0.04304965 0.06532679]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05433897 0.05862809 0.05418822]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04331207 0.06536587]\n"," [0.04604491 0.06998406]\n"," [0.04289202 0.06519578]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05433897 0.05801448 0.0540439 ]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04331207 0.06536587]\n"," [0.0458458  0.06961491]\n"," [0.042734   0.0650923 ]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05433897 0.05773035 0.05391315]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04331207 0.06536587]\n"," [0.04566517 0.06927844]\n"," [0.04257935 0.06501018]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05433897 0.05747181 0.05379477]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04331207 0.06536587]\n"," [0.04550494 0.06897744]\n"," [0.04249335 0.06493802]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05433897 0.05724119 0.05371568]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04739406 0.07217938]\n"," [0.04331207 0.06536587]\n"," [0.04535383 0.06868652]\n"," [0.04240307 0.0648459 ]]\n","\n","Average MAE Loss:\n","[0.05978672 0.05433897 0.05702018 0.05362448]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.9531e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.0443416  0.0674654 ]\n"," [0.04428105 0.06735715]\n"," [0.04416878 0.06709835]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0559035  0.0558191  0.05563356]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.0443416  0.0674654 ]\n"," [0.04422465 0.06726068]\n"," [0.04398666 0.06666392]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0559035  0.05574267 0.05532529]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.0443416  0.0674654 ]\n"," [0.0441712  0.06717113]\n"," [0.04390907 0.06647714]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0559035  0.05567116 0.05519311]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.0443416  0.0674654 ]\n"," [0.04411698 0.06708167]\n"," [0.04384451 0.06632387]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0559035  0.05559933 0.05508419]\n","\n","Epoch 00046: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.0443416  0.0674654 ]\n"," [0.04409065 0.06703892]\n"," [0.04378532 0.06618262]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0559035  0.05556478 0.05498397]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.0443416  0.0674654 ]\n"," [0.04406539 0.06699921]\n"," [0.04373191 0.06606326]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0559035  0.0555323  0.05489758]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.0443416  0.0674654 ]\n"," [0.04403986 0.06695937]\n"," [0.04370673 0.06601106]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0559035  0.05549962 0.0548589 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.04370673 0.06601106]\n"," [0.04431298 0.06741682]\n"," [0.04399667 0.06686557]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0548589  0.0558649  0.05543112]\n","\n","Epoch 00050: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.04370673 0.06601107]\n"," [0.04429833 0.06739151]\n"," [0.04394924 0.06675634]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0548589  0.05584492 0.05535279]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.04370673 0.06601106]\n"," [0.04428355 0.06736578]\n"," [0.04390404 0.06664959]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0548589  0.05582467 0.05527681]\n","\n","Epoch 00052: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.04370673 0.06601106]\n"," [0.04426941 0.06734177]\n"," [0.04388274 0.06659998]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0548589  0.05580559 0.05524136]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.04370673 0.06601106]\n"," [0.04425495 0.06731719]\n"," [0.04386255 0.06655368]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0548589  0.05578607 0.05520812]\n","\n","Epoch 00054: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.04370673 0.06601106]\n"," [0.04424775 0.06730506]\n"," [0.04384225 0.06650635]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0548589  0.05577641 0.0551743 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0443416  0.0674654 ]\n"," [0.04370673 0.06601106]\n"," [0.04424056 0.06729279]\n"," [0.04382238 0.06646013]]\n","\n","Average MAE Loss:\n","[0.0559035  0.0548589  0.05576667 0.05514125]\n","\n","Epoch 00056: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04400475 0.06677561]\n"," [0.04400475 0.06677561]\n"," [0.04399853 0.06676637]\n"," [0.04399369 0.06675091]]\n","\n","Average MAE Loss:\n","[0.05539018 0.05539018 0.05538245 0.0553723 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04400475 0.06677561]\n"," [0.04400475 0.06677561]\n"," [0.04399209 0.06675652]\n"," [0.04398225 0.06672427]]\n","\n","Average MAE Loss:\n","[0.05539018 0.05539018 0.0553743  0.05535326]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04400475 0.06677561]\n"," [0.04400475 0.06677561]\n"," [0.04398908 0.06675208]\n"," [0.04397079 0.06669787]]\n","\n","Average MAE Loss:\n","[0.05539018 0.05539018 0.05537058 0.05533433]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04400475 0.06677561]\n"," [0.04400475 0.06677561]\n"," [0.04398594 0.06674729]\n"," [0.04395953 0.06667185]]\n","\n","Average MAE Loss:\n","[0.05539018 0.05539018 0.05536661 0.05531569]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","\n","epochs finished with time:388.9894859790802\n","\n","[[0.04400475 0.06677561]\n"," [0.04400475 0.06677561]\n"," [0.04398594 0.06674729]\n"," [0.04395953 0.06667185]]\n","00:06:44.52325385600034\n","------------------------------------Fold [5/5]-----------------------------------------\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","DeepChebNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.09219217 0.09606084]\n"," [0.09164903 0.09799358]\n"," [0.05667996 0.06588065]\n"," [0.09049209 0.07350625]]\n","\n","Average MAE Loss:\n","[0.0941265  0.0948213  0.0612803  0.08199917]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.09219217 0.09606084]\n"," [0.09164903 0.09799358]\n"," [0.04831105 0.05645401]\n"," [0.09007604 0.05864121]]\n","\n","Average MAE Loss:\n","[0.0941265  0.0948213  0.05238253 0.07435863]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.09219217 0.09606084]\n"," [0.09164903 0.09799358]\n"," [0.04533979 0.05349376]\n"," [0.09779595 0.0556103 ]]\n","\n","Average MAE Loss:\n","[0.0941265  0.0948213  0.04941677 0.07670312]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.09219217 0.09606084]\n"," [0.09164901 0.09799358]\n"," [0.04619044 0.05179368]\n"," [0.09952236 0.05451302]]\n","\n","Average MAE Loss:\n","[0.0941265  0.09482129 0.04899206 0.07701769]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.09219217 0.09606084]\n"," [0.09164901 0.09799358]\n"," [0.04576637 0.05192218]\n"," [0.10056396 0.05688017]]\n","\n","Average MAE Loss:\n","[0.0941265  0.09482129 0.04884428 0.07872207]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.09219217 0.09606084]\n"," [0.09164903 0.09799358]\n"," [0.0452417  0.05233637]\n"," [0.10059166 0.05605929]]\n","\n","Average MAE Loss:\n","[0.0941265  0.0948213  0.04878904 0.07832548]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.09219217 0.09606084]\n"," [0.09164903 0.09799358]\n"," [0.04530142 0.05283422]\n"," [0.0997385  0.05194085]]\n","\n","Average MAE Loss:\n","[0.0941265  0.0948213  0.04906782 0.07583968]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.09164903 0.09799358]\n"," [0.0997385  0.05194085]\n"," [0.06225695 0.06926034]\n"," [0.04572182 0.05581407]]\n","\n","Average MAE Loss:\n","[0.0948213  0.07583968 0.06575865 0.05076794]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.09164903 0.09799358]\n"," [0.0997385  0.05194085]\n"," [0.05288005 0.05847494]\n"," [0.04511467 0.05585475]]\n","\n","Average MAE Loss:\n","[0.0948213  0.07583968 0.0556775  0.05048471]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.09164903 0.09799358]\n"," [0.0997385  0.05194085]\n"," [0.04690505 0.05451176]\n"," [0.04601919 0.05647594]]\n","\n","Average MAE Loss:\n","[0.0948213  0.07583968 0.0507084  0.05124757]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.09164903 0.09799358]\n"," [0.0997385  0.05194085]\n"," [0.04760121 0.05583086]\n"," [0.04806857 0.05844521]]\n","\n","Average MAE Loss:\n","[0.0948213  0.07583968 0.05171604 0.05325689]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.09164903 0.09799358]\n"," [0.0997385  0.05194085]\n"," [0.04695033 0.0577829 ]\n"," [0.0472565  0.05541169]]\n","\n","Average MAE Loss:\n","[0.0948213  0.07583968 0.05236661 0.0513341 ]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.09164903 0.09799358]\n"," [0.0997385  0.05194085]\n"," [0.04512821 0.0517155 ]\n"," [0.04710872 0.05100241]]\n","\n","Average MAE Loss:\n","[0.0948213  0.07583968 0.04842185 0.04905557]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.09164903 0.09799358]\n"," [0.0997385  0.05194085]\n"," [0.04546892 0.05490869]\n"," [0.04936775 0.04969684]]\n","\n","Average MAE Loss:\n","[0.0948213  0.07583968 0.05018881 0.04953229]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.0883851  0.09221043]\n"," [0.05996301 0.06469733]\n"," [0.06808189 0.07578452]]\n","\n","Average MAE Loss:\n","[0.09029776 0.09029776 0.06233017 0.07193321]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.0883851  0.09221043]\n"," [0.05673804 0.06764103]\n"," [0.05988465 0.06608983]]\n","\n","Average MAE Loss:\n","[0.09029776 0.09029776 0.06218953 0.06298724]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.0883851  0.09221043]\n"," [0.04802284 0.05791482]\n"," [0.0591687  0.06446257]]\n","\n","Average MAE Loss:\n","[0.09029776 0.09029776 0.05296883 0.06181564]\n","\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.0883851  0.09221043]\n"," [0.0451071  0.05295024]\n"," [0.05907137 0.0649543 ]]\n","\n","Average MAE Loss:\n","[0.09029776 0.09029776 0.04902867 0.06201283]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.0883851  0.09221043]\n"," [0.04417974 0.05040009]\n"," [0.05870669 0.06470024]]\n","\n","Average MAE Loss:\n","[0.09029776 0.09029776 0.04728991 0.06170347]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.0883851  0.09221043]\n"," [0.04380765 0.05085994]\n"," [0.05826927 0.06439678]]\n","\n","Average MAE Loss:\n","[0.09029776 0.09029776 0.04733379 0.06133302]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.0883851  0.09221043]\n"," [0.04295565 0.04904532]\n"," [0.05796542 0.06457871]]\n","\n","Average MAE Loss:\n","[0.09029776 0.09029776 0.04600049 0.06127207]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.05796542 0.06457871]\n"," [0.06246909 0.07237935]\n"," [0.04244897 0.04794722]]\n","\n","Average MAE Loss:\n","[0.09029776 0.06127207 0.06742422 0.0451981 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.05796542 0.06457871]\n"," [0.05693016 0.06339087]\n"," [0.04248497 0.04774483]]\n","\n","Average MAE Loss:\n","[0.09029776 0.06127207 0.06016052 0.0451149 ]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.05796542 0.06457871]\n"," [0.0547035  0.06391369]\n"," [0.04258915 0.04776049]]\n","\n","Average MAE Loss:\n","[0.09029776 0.06127207 0.0593086  0.04517482]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.05796542 0.06457871]\n"," [0.05075696 0.06091807]\n"," [0.04253065 0.04702988]]\n","\n","Average MAE Loss:\n","[0.09029776 0.06127207 0.05583752 0.04478026]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.05796542 0.06457871]\n"," [0.04858935 0.05901941]\n"," [0.04269547 0.04739888]]\n","\n","Average MAE Loss:\n","[0.09029776 0.06127207 0.05380438 0.04504717]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.05796542 0.06457871]\n"," [0.04643185 0.05613084]\n"," [0.04271816 0.04770701]]\n","\n","Average MAE Loss:\n","[0.09029776 0.06127207 0.05128135 0.04521259]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0883851  0.09221043]\n"," [0.05796542 0.06457871]\n"," [0.04494734 0.05330459]\n"," [0.04258734 0.04683729]]\n","\n","Average MAE Loss:\n","[0.09029776 0.06127207 0.04912596 0.04471232]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.05491613 0.06795608]\n"," [0.04852386 0.05966251]\n"," [0.04970342 0.06061926]]\n","\n","Average MAE Loss:\n","[0.06143611 0.06143611 0.05409319 0.05516134]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.05491613 0.06795608]\n"," [0.04709403 0.05753347]\n"," [0.04763399 0.05811181]]\n","\n","Average MAE Loss:\n","[0.06143611 0.06143611 0.05231375 0.0528729 ]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.05491613 0.06795608]\n"," [0.04592027 0.05633437]\n"," [0.04535525 0.05427821]]\n","\n","Average MAE Loss:\n","[0.06143611 0.06143611 0.05112732 0.04981673]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.05491613 0.06795608]\n"," [0.04491282 0.05466932]\n"," [0.04428466 0.04993712]]\n","\n","Average MAE Loss:\n","[0.06143611 0.06143611 0.04979107 0.04711089]\n","\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.05491613 0.06795608]\n"," [0.04407429 0.05274206]\n"," [0.04418413 0.04956799]]\n","\n","Average MAE Loss:\n","[0.06143611 0.06143611 0.04840817 0.04687606]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.05491613 0.06795608]\n"," [0.04376304 0.05192542]\n"," [0.04393848 0.04909733]]\n","\n","Average MAE Loss:\n","[0.06143611 0.06143611 0.04784423 0.0465179 ]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.05491613 0.06795608]\n"," [0.04353197 0.05124788]\n"," [0.04370356 0.04889509]]\n","\n","Average MAE Loss:\n","[0.06143611 0.06143611 0.04738992 0.04629933]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.04370356 0.04889509]\n"," [0.05280846 0.06549143]\n"," [0.0432121  0.04994486]]\n","\n","Average MAE Loss:\n","[0.06143611 0.04629933 0.05914995 0.04657848]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.04370356 0.04889509]\n"," [0.05051213 0.06242403]\n"," [0.04318979 0.04936804]]\n","\n","Average MAE Loss:\n","[0.06143611 0.04629933 0.05646808 0.04627891]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00037: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.04370356 0.04889509]\n"," [0.04973406 0.06132358]\n"," [0.04321916 0.04902292]]\n","\n","Average MAE Loss:\n","[0.06143611 0.04629933 0.05552882 0.04612104]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.04370356 0.04889509]\n"," [0.04914387 0.06050354]\n"," [0.04319721 0.04868186]]\n","\n","Average MAE Loss:\n","[0.06143611 0.04629933 0.05482371 0.04593953]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.04370356 0.04889509]\n"," [0.04867062 0.05986147]\n"," [0.04318248 0.04861329]]\n","\n","Average MAE Loss:\n","[0.06143611 0.04629933 0.05426604 0.04589789]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.04370356 0.04889509]\n"," [0.04825773 0.05934042]\n"," [0.04316176 0.04850417]]\n","\n","Average MAE Loss:\n","[0.06143611 0.04629933 0.05379907 0.04583297]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05491613 0.06795608]\n"," [0.04370356 0.04889509]\n"," [0.04805918 0.05909192]\n"," [0.04312468 0.04830045]]\n","\n","Average MAE Loss:\n","[0.06143611 0.04629933 0.05357555 0.04571256]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.04570945 0.05638039]\n"," [0.04545982 0.05591699]\n"," [0.04514956 0.05521822]]\n","\n","Average MAE Loss:\n","[0.05104492 0.05104492 0.0506884  0.05018389]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.04570945 0.05638039]\n"," [0.04519166 0.05538602]\n"," [0.0445898  0.05384358]]\n","\n","Average MAE Loss:\n","[0.05104492 0.05104492 0.05028884 0.04921669]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.04570945 0.05638039]\n"," [0.04495153 0.05488589]\n"," [0.04438344 0.05330173]]\n","\n","Average MAE Loss:\n","[0.05104492 0.05104492 0.04991871 0.04884259]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.04570945 0.05638039]\n"," [0.04484499 0.05466413]\n"," [0.04422352 0.05286239]]\n","\n","Average MAE Loss:\n","[0.05104492 0.05104492 0.04975456 0.04854296]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.04570945 0.05638039]\n"," [0.0447479  0.05445462]\n"," [0.04409757 0.05248712]]\n","\n","Average MAE Loss:\n","[0.05104492 0.05104492 0.04960126 0.04829234]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.04570945 0.05638039]\n"," [0.04466286 0.05427179]\n"," [0.04397579 0.05210134]]\n","\n","Average MAE Loss:\n","[0.05104492 0.05104492 0.04946732 0.04803856]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.04570945 0.05638039]\n"," [0.04458703 0.0541082 ]\n"," [0.0439221  0.05191262]]\n","\n","Average MAE Loss:\n","[0.05104492 0.05104492 0.04934762 0.04791736]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.9531e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.0439221  0.05191262]\n"," [0.04563958 0.05625287]\n"," [0.0444814  0.05384221]]\n","\n","Average MAE Loss:\n","[0.05104492 0.04791736 0.05094622 0.0491618 ]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.0439221  0.05191262]\n"," [0.04555697 0.05609884]\n"," [0.0443766  0.05356637]]\n","\n","Average MAE Loss:\n","[0.05104492 0.04791736 0.05082791 0.04897149]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.0439221  0.05191262]\n"," [0.04547736 0.05594666]\n"," [0.04427175 0.05328171]]\n","\n","Average MAE Loss:\n","[0.05104492 0.04791736 0.05071201 0.04877673]\n","\n","Epoch 00052: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.0439221  0.05191262]\n"," [0.04539995 0.05579536]\n"," [0.04422461 0.05315433]]\n","\n","Average MAE Loss:\n","[0.05104492 0.04791736 0.05059766 0.04868947]\n","\n","Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.0439221  0.05191262]\n"," [0.04536425 0.05572516]\n"," [0.04417972 0.0530298 ]]\n","\n","Average MAE Loss:\n","[0.05104492 0.04791736 0.05054471 0.04860476]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.0439221  0.05191262]\n"," [0.04533076 0.05565916]\n"," [0.04413633 0.05290802]]\n","\n","Average MAE Loss:\n","[0.05104492 0.04791736 0.05049496 0.04852218]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04570945 0.05638039]\n"," [0.0439221  0.05191262]\n"," [0.04529499 0.05558721]\n"," [0.04409826 0.05280094]]\n","\n","Average MAE Loss:\n","[0.05104492 0.04791736 0.0504411  0.0484496 ]\n","\n","Epoch 00056: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04467189 0.05415706]\n"," [0.04467189 0.05415706]\n"," [0.04464681 0.05410137]\n"," [0.04464563 0.05409054]]\n","\n","Average MAE Loss:\n","[0.04941448 0.04941448 0.04937409 0.04936809]\n","\n","Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04467189 0.05415706]\n"," [0.04467189 0.05415706]\n"," [0.04463515 0.05407608]\n"," [0.04461254 0.05400709]]\n","\n","Average MAE Loss:\n","[0.04941448 0.04941448 0.04935562 0.04930981]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04467189 0.05415706]\n"," [0.04467189 0.05415706]\n"," [0.04462487 0.05405416]\n"," [0.04458035 0.05392494]]\n","\n","Average MAE Loss:\n","[0.04941448 0.04941448 0.04933951 0.04925265]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04467189 0.05415706]\n"," [0.04467189 0.05415706]\n"," [0.04461446 0.05403154]\n"," [0.04454891 0.05384448]]\n","\n","Average MAE Loss:\n","[0.04941448 0.04941448 0.049323   0.04919669]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00060: reducing learning rate of group 0 to 4.8828e-07.\n","\n","epochs finished with time:389.57608127593994\n","\n","[[0.04467189 0.05415706]\n"," [0.04467189 0.05415706]\n"," [0.04461446 0.05403155]\n"," [0.04454891 0.05384448]]\n","00:06:45.315829948999635\n"]}],"source":["# seed = 10 table = 2/8 fixed folds 3 layers adj\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='Cheb_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkibe0w5zVYt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"aHtyfjWEQe11"},"source":["## TAGConv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPOBdKRqQe19"},"outputs":[],"source":["from torch_geometric.nn.conv import TAGConv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ne0yrIisRAGA"},"outputs":[],"source":["class DeepTAGNet(torch.nn.Module):\n","    def __init__(self,device):\n","        super(DeepTAGNet, self).__init__()\n","        in_features = 35\n","        out_features = 140\n","        self.conv1 = TAGConv(in_features, out_features)\n","        self.conv2 = TAGConv(out_features, 2*out_features)\n","        self.fc1 = nn.Linear(2*out_features,35)\n","        self.device = device\n","\n","    def forward(self, adj_matrix):\n","\n","        data = adj_matrix_to_pytorch_geometric_data(adj_matrix,self.device)\n","        edge_index = data.edge_index\n","        data = self.conv1(data.adj_matrix, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.3, training=self.training)\n","\n","        data = self.conv2(data, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.5, training=self.training)\n","\n","        out = F.relu(self.fc1(data))\n","\n","\n","        return out"]},{"cell_type":"code","source":["model = DeepTAGNet(device).to(device)\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBzRNcBRSB_Z","executionInfo":{"status":"ok","timestamp":1690732596379,"user_tz":-60,"elapsed":314,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"}},"outputId":"f7ca18c6-8d24-479b-8add-59a0b3dfe3f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 186655\n","Trainable parameters: 186655\n"]}]},{"cell_type":"code","source":["model(dataset[0][0]).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EaI_3uI2dKm5","executionInfo":{"status":"ok","timestamp":1690732601223,"user_tz":-60,"elapsed":4538,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"}},"outputId":"acd95ea2-3cc7-4c9f-b604-2ca238d0518b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([35, 35])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["class Hospital_gnns():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = DeepTAGNet(device=device).to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"],"metadata":{"id":"huEdhb3BSt2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = False\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_gnns(args,device))\n","                  print('DeepTAGNet')\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n","\n"],"metadata":{"id":"sZwH35roTI2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# seed = 10 table = 4/8 fixed folds 2+1 layers adj 140\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='TAG_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umpWwby7dciA","outputId":"2dd382ad-7874-4172-bcb2-394a1a394b63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.04718883 0.05323763]\n"," [0.0762185  0.07197651]\n"," [0.05193045 0.05785387]\n"," [0.08247099 0.06363742]]\n","\n","Average MAE Loss:\n","[0.05021323 0.07409751 0.05489216 0.0730542 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04289958 0.04841904]\n"," [0.0762185  0.07197651]\n"," [0.04429915 0.04774465]\n"," [0.08677099 0.05083821]]\n","\n","Average MAE Loss:\n","[0.04565931 0.0740975  0.0460219  0.0688046 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.03921052 0.04293645]\n"," [0.0762185  0.07197651]\n"," [0.04076046 0.04571464]\n"," [0.08549808 0.04958681]]\n","\n","Average MAE Loss:\n","[0.04107348 0.0740975  0.04323755 0.06754245]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04338257 0.05723711]\n"," [0.0762185  0.07197651]\n"," [0.04058278 0.04518093]\n"," [0.08462437 0.04500822]]\n","\n","Average MAE Loss:\n","[0.05030984 0.07409751 0.04288186 0.0648163 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.03916704 0.0435534 ]\n"," [0.0762185  0.07197651]\n"," [0.0440313  0.04775369]\n"," [0.08497975 0.04138131]]\n","\n","Average MAE Loss:\n","[0.04136022 0.0740975  0.0458925  0.06318053]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.03911099 0.04335908]\n"," [0.0762185  0.07197651]\n"," [0.04242391 0.04333098]\n"," [0.08493388 0.03993101]]\n","\n","Average MAE Loss:\n","[0.04123503 0.0740975  0.04287744 0.06243244]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04224543 0.05828204]\n"," [0.0762185  0.07197651]\n"," [0.04256431 0.04395244]\n"," [0.08549327 0.04163515]]\n","\n","Average MAE Loss:\n","[0.05026374 0.0740975  0.04325837 0.06356421]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05407878 0.05881648]\n"," [0.08549327 0.04163515]\n"," [0.03817305 0.04404458]\n"," [0.05404329 0.04068725]]\n","\n","Average MAE Loss:\n","[0.05644763 0.06356421 0.04110881 0.04736527]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04443248 0.04789661]\n"," [0.08549327 0.04163515]\n"," [0.03964678 0.04894326]\n"," [0.07096341 0.04251751]]\n","\n","Average MAE Loss:\n","[0.04616455 0.06356421 0.04429502 0.05674046]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04143797 0.04243047]\n"," [0.08549327 0.04163515]\n"," [0.04065049 0.04683004]\n"," [0.07919542 0.04544414]]\n","\n","Average MAE Loss:\n","[0.04193422 0.06356421 0.04374026 0.06231978]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0387162  0.04107574]\n"," [0.08549327 0.04163515]\n"," [0.0402532  0.04911683]\n"," [0.08110749 0.0428132 ]]\n","\n","Average MAE Loss:\n","[0.03989597 0.06356421 0.04468502 0.06196034]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.03774951 0.03872979]\n"," [0.08549327 0.04163515]\n"," [0.04193373 0.04618426]\n"," [0.08033279 0.04465163]]\n","\n","Average MAE Loss:\n","[0.03823965 0.06356421 0.044059   0.06249221]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.03814383 0.03984374]\n"," [0.08549327 0.04163515]\n"," [0.04687798 0.04620171]\n"," [0.0798082  0.04115121]]\n","\n","Average MAE Loss:\n","[0.03899378 0.06356421 0.04653984 0.06047971]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.03818356 0.03962932]\n"," [0.08549327 0.04163515]\n"," [0.04047986 0.04252643]\n"," [0.08049133 0.04035028]]\n","\n","Average MAE Loss:\n","[0.03890644 0.06356421 0.04150314 0.06042081]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04953307 0.05190938]\n"," [0.066326   0.06227604]\n"," [0.05082995 0.05295848]\n"," [0.05194384 0.05134214]]\n","\n","Average MAE Loss:\n","[0.05072123 0.06430102 0.05189421 0.05164299]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04339522 0.04711529]\n"," [0.066326   0.06227604]\n"," [0.04619498 0.04971   ]\n"," [0.05219324 0.05183945]]\n","\n","Average MAE Loss:\n","[0.04525526 0.06430102 0.04795249 0.05201634]\n","\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04011609 0.04208285]\n"," [0.066326   0.06227605]\n"," [0.04256845 0.04594948]\n"," [0.05030042 0.05068583]]\n","\n","Average MAE Loss:\n","[0.04109947 0.06430102 0.04425897 0.05049312]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.03847733 0.0389045 ]\n"," [0.066326   0.06227604]\n"," [0.04007439 0.04217863]\n"," [0.05001448 0.05022759]]\n","\n","Average MAE Loss:\n","[0.03869091 0.06430102 0.04112651 0.05012104]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.03780547 0.03820788]\n"," [0.066326   0.06227604]\n"," [0.03882931 0.03944457]\n"," [0.04952895 0.05015271]]\n","\n","Average MAE Loss:\n","[0.03800668 0.06430102 0.03913694 0.04984083]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03746251 0.03808517]\n"," [0.06632599 0.06227604]\n"," [0.03830957 0.03873107]\n"," [0.04914499 0.0499433 ]]\n","\n","Average MAE Loss:\n","[0.03777384 0.06430102 0.03852032 0.04954415]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03708301 0.03785607]\n"," [0.066326   0.06227604]\n"," [0.03801254 0.03839421]\n"," [0.04884314 0.04988026]]\n","\n","Average MAE Loss:\n","[0.03746954 0.06430102 0.03820338 0.0493617 ]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04955218 0.05039378]\n"," [0.04884314 0.04988026]\n"," [0.03697359 0.0375688 ]\n"," [0.03820857 0.03862007]]\n","\n","Average MAE Loss:\n","[0.04997298 0.0493617  0.0372712  0.03841432]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04697366 0.0496152 ]\n"," [0.04884314 0.04988026]\n"," [0.03706589 0.03748702]\n"," [0.03756864 0.03795377]]\n","\n","Average MAE Loss:\n","[0.04829443 0.0493617  0.03727646 0.03776121]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04324638 0.04680679]\n"," [0.04884314 0.04988026]\n"," [0.03655405 0.03682366]\n"," [0.03693261 0.03741909]]\n","\n","Average MAE Loss:\n","[0.04502659 0.0493617  0.03668885 0.03717585]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.03952374 0.04147341]\n"," [0.04884314 0.04988026]\n"," [0.03623251 0.03636883]\n"," [0.03698751 0.03730207]]\n","\n","Average MAE Loss:\n","[0.04049857 0.0493617  0.03630067 0.03714479]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.03844027 0.03943545]\n"," [0.04884314 0.04988026]\n"," [0.03630353 0.03622797]\n"," [0.03645834 0.0369037 ]]\n","\n","Average MAE Loss:\n","[0.03893786 0.0493617  0.03626575 0.03668102]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03792349 0.03861067]\n"," [0.04884314 0.04988026]\n"," [0.03642034 0.03644035]\n"," [0.03633952 0.03659368]]\n","\n","Average MAE Loss:\n","[0.03826708 0.0493617  0.03643034 0.0364666 ]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03762048 0.03828155]\n"," [0.04884314 0.04988026]\n"," [0.03642838 0.03632399]\n"," [0.0363636  0.03645383]]\n","\n","Average MAE Loss:\n","[0.03795102 0.0493617  0.03637619 0.03640871]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03772457 0.04002583]\n"," [0.03889595 0.04163754]\n"," [0.03770321 0.03922618]\n"," [0.03755632 0.03944654]]\n","\n","Average MAE Loss:\n","[0.0388752  0.04026674 0.0384647  0.03850143]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03730632 0.03902922]\n"," [0.03889595 0.04163754]\n"," [0.03692359 0.03726947]\n"," [0.03680615 0.03746525]]\n","\n","Average MAE Loss:\n","[0.03816777 0.04026674 0.03709653 0.0371357 ]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03695135 0.03829161]\n"," [0.03889595 0.04163754]\n"," [0.03655668 0.03701563]\n"," [0.03650631 0.03697717]]\n","\n","Average MAE Loss:\n","[0.03762148 0.04026674 0.03678615 0.03674174]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03673161 0.03780337]\n"," [0.03889595 0.04163754]\n"," [0.03657073 0.03674836]\n"," [0.03639633 0.03675818]]\n","\n","Average MAE Loss:\n","[0.03726749 0.04026674 0.03665955 0.03657726]\n","\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.0365355  0.03744859]\n"," [0.03889595 0.04163754]\n"," [0.03647207 0.03664042]\n"," [0.03617248 0.03667471]]\n","\n","Average MAE Loss:\n","[0.03699204 0.04026674 0.03655625 0.03642359]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03641213 0.03723514]\n"," [0.03889595 0.04163754]\n"," [0.03647659 0.03648149]\n"," [0.03612439 0.03650105]]\n","\n","Average MAE Loss:\n","[0.03682364 0.04026674 0.03647904 0.03631272]\n","\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03632213 0.0370651 ]\n"," [0.03889595 0.04163754]\n"," [0.03638066 0.03643675]\n"," [0.03606043 0.03640525]]\n","\n","Average MAE Loss:\n","[0.03669361 0.04026674 0.03640871 0.03623284]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03812695 0.04078998]\n"," [0.03606043 0.03640525]\n"," [0.03615959 0.03691176]\n"," [0.03620511 0.03626851]]\n","\n","Average MAE Loss:\n","[0.03945846 0.03623284 0.03653567 0.03623681]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03753547 0.03948939]\n"," [0.03606043 0.03640525]\n"," [0.03608615 0.03669665]\n"," [0.03607941 0.03611339]]\n","\n","Average MAE Loss:\n","[0.03851243 0.03623284 0.0363914  0.0360964 ]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03715041 0.03856683]\n"," [0.03606043 0.03640525]\n"," [0.03603507 0.03655539]\n"," [0.03618839 0.03608156]]\n","\n","Average MAE Loss:\n","[0.03785862 0.03623284 0.03629523 0.03613498]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03682001 0.0379865 ]\n"," [0.03606043 0.03640525]\n"," [0.03604171 0.03651922]\n"," [0.03602452 0.03604403]]\n","\n","Average MAE Loss:\n","[0.03740325 0.03623284 0.03628047 0.03603427]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03670611 0.03775975]\n"," [0.03606043 0.03640525]\n"," [0.03599987 0.0365064 ]\n"," [0.03610836 0.03601288]]\n","\n","Average MAE Loss:\n","[0.03723293 0.03623284 0.03625313 0.03606062]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03661795 0.03761905]\n"," [0.03606043 0.03640525]\n"," [0.03601417 0.036425  ]\n"," [0.03594202 0.036029  ]]\n","\n","Average MAE Loss:\n","[0.0371185  0.03623284 0.03621958 0.03598551]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03657037 0.03748077]\n"," [0.03606043 0.03640525]\n"," [0.03603904 0.03640116]\n"," [0.03602484 0.03600428]]\n","\n","Average MAE Loss:\n","[0.03702557 0.03623284 0.0362201  0.03601456]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03597404 0.03633741]\n"," [0.03599515 0.03635191]\n"," [0.03594953 0.03633763]\n"," [0.03592187 0.03625712]]\n","\n","Average MAE Loss:\n","[0.03615572 0.03617353 0.03614358 0.0360895 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03599333 0.0363536 ]\n"," [0.03599514 0.03635191]\n"," [0.03593689 0.03629827]\n"," [0.03578182 0.0361771 ]]\n","\n","Average MAE Loss:\n","[0.03617347 0.03617353 0.03611758 0.03597946]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0359571  0.03635486]\n"," [0.03599515 0.03635191]\n"," [0.03595393 0.03629111]\n"," [0.03575935 0.03605449]]\n","\n","Average MAE Loss:\n","[0.03615598 0.03617353 0.03612252 0.03590692]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03594448 0.03636858]\n"," [0.03599515 0.03635191]\n"," [0.03596956 0.03625286]\n"," [0.03575448 0.03597751]]\n","\n","Average MAE Loss:\n","[0.03615653 0.03617353 0.03611121 0.035866  ]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.0359345  0.03636761]\n"," [0.03599515 0.03635191]\n"," [0.0359217  0.03626511]\n"," [0.03573859 0.03596167]]\n","\n","Average MAE Loss:\n","[0.03615106 0.03617353 0.03609341 0.03585013]\n","\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03593595 0.03634165]\n"," [0.03599515 0.03635191]\n"," [0.03592892 0.03622691]\n"," [0.03576528 0.03595362]]\n","\n","Average MAE Loss:\n","[0.0361388  0.03617353 0.03607792 0.03585945]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03593816 0.03632637]\n"," [0.03599515 0.03635191]\n"," [0.03596884 0.0361985 ]\n"," [0.03576927 0.03596758]]\n","\n","Average MAE Loss:\n","[0.03613226 0.03617353 0.03608367 0.03586842]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03600853 0.0363229 ]\n"," [0.03576927 0.03596758]\n"," [0.03585912 0.03628655]\n"," [0.03585626 0.03607818]]\n","\n","Average MAE Loss:\n","[0.03616572 0.03586842 0.03607283 0.03596722]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03601054 0.03634153]\n"," [0.03576927 0.03596758]\n"," [0.03580677 0.03622058]\n"," [0.03588011 0.0359822 ]]\n","\n","Average MAE Loss:\n","[0.03617604 0.03586842 0.03601367 0.03593115]\n","\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0359881  0.03635274]\n"," [0.03576927 0.03596758]\n"," [0.03579792 0.03612683]\n"," [0.03580831 0.0359739 ]]\n","\n","Average MAE Loss:\n","[0.03617042 0.03586842 0.03596238 0.0358911 ]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03597679 0.03635035]\n"," [0.03576927 0.03596758]\n"," [0.03578798 0.03610419]\n"," [0.03573051 0.0359994 ]]\n","\n","Average MAE Loss:\n","[0.03616357 0.03586842 0.03594609 0.03586496]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0359569  0.03633142]\n"," [0.03576927 0.03596758]\n"," [0.03579161 0.03607437]\n"," [0.03571661 0.03595819]]\n","\n","Average MAE Loss:\n","[0.03614416 0.03586842 0.03593299 0.0358374 ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03593895 0.03633487]\n"," [0.03576927 0.03596758]\n"," [0.03579604 0.03605492]\n"," [0.03570032 0.03592291]]\n","\n","Average MAE Loss:\n","[0.03613691 0.03586842 0.03592548 0.03581162]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03590615 0.03633476]\n"," [0.03576927 0.03596758]\n"," [0.03582134 0.0360584 ]\n"," [0.03569083 0.0359201 ]]\n","\n","Average MAE Loss:\n","[0.03612046 0.03586842 0.03593987 0.03580546]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03571296 0.03593764]\n"," [0.03570557 0.0359395 ]\n"," [0.03566986 0.03597714]\n"," [0.03569275 0.03593686]]\n","\n","Average MAE Loss:\n","[0.0358253  0.03582254 0.0358235  0.03581481]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03575417 0.03594468]\n"," [0.03570557 0.0359395 ]\n"," [0.03572139 0.03595609]\n"," [0.03566497 0.03590693]]\n","\n","Average MAE Loss:\n","[0.03584942 0.03582254 0.03583874 0.03578595]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03574952 0.03597364]\n"," [0.03570557 0.0359395 ]\n"," [0.03578542 0.03596466]\n"," [0.03563202 0.03590598]]\n","\n","Average MAE Loss:\n","[0.03586158 0.03582254 0.03587504 0.035769  ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03572374 0.03598719]\n"," [0.03570558 0.0359395 ]\n"," [0.03578174 0.03597326]\n"," [0.03565624 0.03588378]]\n","\n","Average MAE Loss:\n","[0.03585546 0.03582254 0.0358775  0.03577001]\n","\n","\n","epochs finished with time:697.3030531406403\n","\n","[[0.03572373 0.03598719]\n"," [0.03570557 0.0359395 ]\n"," [0.03578174 0.03597326]\n"," [0.03565624 0.03588378]]\n","00:12:40.16162915699999\n","------------------------------------Fold [2/5]-----------------------------------------\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07115094 0.08034179]\n"," [0.10347126 0.10364695]\n"," [0.07397121 0.08680835]\n"," [0.11317539 0.09383562]]\n","\n","Average MAE Loss:\n","[0.07574637 0.1035591  0.08038978 0.1035055 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06357882 0.07042629]\n"," [0.10347126 0.10364695]\n"," [0.06625707 0.07329422]\n"," [0.11564789 0.0840644 ]]\n","\n","Average MAE Loss:\n","[0.06700256 0.1035591  0.06977565 0.09985614]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06276627 0.06973619]\n"," [0.10347126 0.10364695]\n"," [0.06576164 0.06891868]\n"," [0.11655949 0.07651532]]\n","\n","Average MAE Loss:\n","[0.06625123 0.1035591  0.06734016 0.09653741]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0621624  0.06733791]\n"," [0.10347126 0.10364695]\n"," [0.06303315 0.06727318]\n"," [0.11495439 0.07388357]]\n","\n","Average MAE Loss:\n","[0.06475016 0.1035591  0.06515316 0.09441898]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.06026253 0.06621653]\n"," [0.10347126 0.10364695]\n"," [0.06563262 0.06592604]\n"," [0.11591483 0.07001973]]\n","\n","Average MAE Loss:\n","[0.06323953 0.1035591  0.06577933 0.09296728]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.06401928 0.0661575 ]\n"," [0.10347126 0.10364695]\n"," [0.06281312 0.0743794 ]\n"," [0.11499097 0.06967598]]\n","\n","Average MAE Loss:\n","[0.06508839 0.1035591  0.06859626 0.09233348]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.0612873  0.07311143]\n"," [0.10347126 0.10364695]\n"," [0.06571635 0.06707342]\n"," [0.11484323 0.06884539]]\n","\n","Average MAE Loss:\n","[0.06719936 0.1035591  0.06639489 0.09184431]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07429522 0.08443793]\n"," [0.11484323 0.06884539]\n"," [0.05954509 0.06845061]\n"," [0.08686232 0.07066841]]\n","\n","Average MAE Loss:\n","[0.07936658 0.09184431 0.06399785 0.07876537]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06701131 0.07465881]\n"," [0.11484323 0.06884539]\n"," [0.06500874 0.07196645]\n"," [0.09262964 0.06992343]]\n","\n","Average MAE Loss:\n","[0.07083506 0.09184431 0.06848759 0.08127653]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06455186 0.06849301]\n"," [0.11484323 0.06884539]\n"," [0.06111835 0.07070151]\n"," [0.10504009 0.06902152]]\n","\n","Average MAE Loss:\n","[0.06652243 0.09184431 0.06590993 0.0870308 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06166418 0.06348303]\n"," [0.11484323 0.06884539]\n"," [0.06495361 0.06900954]\n"," [0.10977405 0.06764932]]\n","\n","Average MAE Loss:\n","[0.0625736  0.09184431 0.06698157 0.08871169]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.06140023 0.06551852]\n"," [0.11484323 0.06884539]\n"," [0.0636295  0.06609835]\n"," [0.11334236 0.07138853]]\n","\n","Average MAE Loss:\n","[0.06345938 0.09184431 0.06486392 0.09236544]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06129339 0.06444438]\n"," [0.11484323 0.06884539]\n"," [0.06407728 0.06521429]\n"," [0.10803219 0.06935396]]\n","\n","Average MAE Loss:\n","[0.06286888 0.09184431 0.06464579 0.08869307]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0614573  0.06340663]\n"," [0.11484323 0.06884539]\n"," [0.06427963 0.06622432]\n"," [0.1069783  0.0684769 ]]\n","\n","Average MAE Loss:\n","[0.06243197 0.09184431 0.06525198 0.0877276 ]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07213521 0.07676946]\n"," [0.09729638 0.09454629]\n"," [0.07320297 0.0764513 ]\n"," [0.08297353 0.0827273 ]]\n","\n","Average MAE Loss:\n","[0.07445233 0.09592134 0.07482713 0.08285042]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06815492 0.07342414]\n"," [0.09729638 0.09454629]\n"," [0.07163699 0.076658  ]\n"," [0.07790697 0.07971966]]\n","\n","Average MAE Loss:\n","[0.07078953 0.09592134 0.0741475  0.07881331]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0627449  0.06433215]\n"," [0.09729638 0.09454629]\n"," [0.06652419 0.07156596]\n"," [0.07535542 0.07711018]]\n","\n","Average MAE Loss:\n","[0.06353852 0.09592134 0.06904507 0.0762328 ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06207808 0.06373544]\n"," [0.09729638 0.09454629]\n"," [0.06394591 0.0666932 ]\n"," [0.07549573 0.07715001]]\n","\n","Average MAE Loss:\n","[0.06290676 0.09592134 0.06531956 0.07632287]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0617979  0.06347558]\n"," [0.09729638 0.0945463 ]\n"," [0.06253454 0.06424057]\n"," [0.07513908 0.07690153]]\n","\n","Average MAE Loss:\n","[0.06263674 0.09592134 0.06338756 0.0760203 ]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06123433 0.06311553]\n"," [0.09729638 0.09454629]\n"," [0.06180768 0.06357449]\n"," [0.07486212 0.07687312]]\n","\n","Average MAE Loss:\n","[0.06217493 0.09592134 0.06269108 0.07586762]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.061342   0.0630964 ]\n"," [0.09729638 0.0945463 ]\n"," [0.06155602 0.06353389]\n"," [0.07444966 0.07672491]]\n","\n","Average MAE Loss:\n","[0.0622192  0.09592134 0.06254496 0.07558728]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07755861 0.08117936]\n"," [0.07444966 0.07672491]\n"," [0.06018226 0.06153971]\n"," [0.06545542 0.06816344]]\n","\n","Average MAE Loss:\n","[0.07936898 0.07558728 0.06086099 0.06680943]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07309413 0.07725555]\n"," [0.07444966 0.07672491]\n"," [0.060338   0.0614852 ]\n"," [0.06478808 0.0662054 ]]\n","\n","Average MAE Loss:\n","[0.07517484 0.07558728 0.0609116  0.06549674]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06912882 0.07441494]\n"," [0.07444966 0.07672491]\n"," [0.06013374 0.06121864]\n"," [0.06085339 0.06215623]]\n","\n","Average MAE Loss:\n","[0.07177188 0.07558728 0.06067619 0.06150481]\n","\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0669614  0.07209617]\n"," [0.07444966 0.07672491]\n"," [0.05976078 0.06082213]\n"," [0.06185473 0.06273831]]\n","\n","Average MAE Loss:\n","[0.06952878 0.07558728 0.06029145 0.06229652]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0649724  0.06904703]\n"," [0.07444966 0.07672491]\n"," [0.05944748 0.06073223]\n"," [0.06267896 0.06336706]]\n","\n","Average MAE Loss:\n","[0.06700972 0.07558728 0.06008986 0.06302301]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06332535 0.06597279]\n"," [0.07444966 0.07672491]\n"," [0.05995231 0.06109681]\n"," [0.06298174 0.06334694]]\n","\n","Average MAE Loss:\n","[0.06464907 0.07558728 0.06052456 0.06316434]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06248656 0.06436243]\n"," [0.07444966 0.07672491]\n"," [0.05982734 0.06097051]\n"," [0.06269706 0.06251788]]\n","\n","Average MAE Loss:\n","[0.06342449 0.07558728 0.06039892 0.06260747]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00028: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06263425 0.06616472]\n"," [0.06475316 0.06864609]\n"," [0.061633   0.06420194]\n"," [0.06249339 0.06525342]]\n","\n","Average MAE Loss:\n","[0.06439948 0.06669962 0.06291747 0.06387341]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06175729 0.06463001]\n"," [0.06475316 0.06864609]\n"," [0.06043679 0.06185688]\n"," [0.06163896 0.06282368]]\n","\n","Average MAE Loss:\n","[0.06319365 0.06669962 0.06114684 0.06223132]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06143498 0.06371425]\n"," [0.06475316 0.06864609]\n"," [0.06021304 0.06153536]\n"," [0.06121851 0.06227407]]\n","\n","Average MAE Loss:\n","[0.06257462 0.06669962 0.0608742  0.06174629]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.0610052  0.06293885]\n"," [0.06475316 0.06864609]\n"," [0.05997091 0.061154  ]\n"," [0.06110968 0.06207803]]\n","\n","Average MAE Loss:\n","[0.06197203 0.06669962 0.06056245 0.06159386]\n","\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.0606763  0.06240761]\n"," [0.06475316 0.06864609]\n"," [0.05993591 0.06110757]\n"," [0.06093566 0.06185926]]\n","\n","Average MAE Loss:\n","[0.06154196 0.06669962 0.06052174 0.06139746]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06050581 0.06209179]\n"," [0.06475316 0.06864609]\n"," [0.0600749  0.06114039]\n"," [0.0608204  0.06165729]]\n","\n","Average MAE Loss:\n","[0.0612988  0.06669962 0.06060764 0.06123885]\n","\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06031111 0.06185896]\n"," [0.06475316 0.06864609]\n"," [0.05995921 0.0609892 ]\n"," [0.06079539 0.06155977]]\n","\n","Average MAE Loss:\n","[0.06108504 0.06669962 0.0604742  0.06117758]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06263785 0.06632161]\n"," [0.06079539 0.06155977]\n"," [0.06016982 0.06162316]\n"," [0.06030695 0.0611817 ]]\n","\n","Average MAE Loss:\n","[0.06447973 0.06117758 0.06089649 0.06074432]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.061955   0.06488792]\n"," [0.06079539 0.06155977]\n"," [0.06024435 0.06155735]\n"," [0.06010101 0.06078326]]\n","\n","Average MAE Loss:\n","[0.06342146 0.06117758 0.06090085 0.06044213]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06146595 0.06375715]\n"," [0.06079539 0.06155977]\n"," [0.06019227 0.06138577]\n"," [0.06024938 0.06086515]]\n","\n","Average MAE Loss:\n","[0.06261155 0.06117758 0.06078902 0.06055726]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06089439 0.06279936]\n"," [0.06079539 0.06155977]\n"," [0.06010165 0.06125038]\n"," [0.06020906 0.06084024]]\n","\n","Average MAE Loss:\n","[0.06184688 0.06117758 0.06067602 0.06052465]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06074    0.0625228 ]\n"," [0.06079539 0.06155977]\n"," [0.06008523 0.06119303]\n"," [0.06024055 0.06086485]]\n","\n","Average MAE Loss:\n","[0.0616314  0.06117758 0.06063913 0.0605527 ]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06065217 0.06235271]\n"," [0.06079539 0.06155977]\n"," [0.06007769 0.06115274]\n"," [0.06013066 0.06079608]]\n","\n","Average MAE Loss:\n","[0.06150244 0.06117758 0.06061521 0.06046337]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06064289 0.06227061]\n"," [0.06079539 0.06155977]\n"," [0.06004131 0.06107772]\n"," [0.06028017 0.06092225]]\n","\n","Average MAE Loss:\n","[0.06145675 0.06117758 0.06055952 0.06060121]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06006558 0.0611206 ]\n"," [0.06028219 0.06132376]\n"," [0.06018046 0.06122087]\n"," [0.06029147 0.06131103]]\n","\n","Average MAE Loss:\n","[0.06059309 0.06080298 0.06070066 0.06080125]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06008307 0.06113597]\n"," [0.06028219 0.06132376]\n"," [0.06007621 0.06111996]\n"," [0.06027324 0.06120906]]\n","\n","Average MAE Loss:\n","[0.06060952 0.06080298 0.06059808 0.06074115]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0600955  0.06116669]\n"," [0.0602822  0.06132376]\n"," [0.06008242 0.06111519]\n"," [0.06021969 0.06108391]]\n","\n","Average MAE Loss:\n","[0.06063109 0.06080298 0.06059881 0.0606518 ]\n","\n","Epoch 00045: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06006272 0.06114061]\n"," [0.0602822  0.06132375]\n"," [0.06014458 0.06115779]\n"," [0.0602231  0.06106144]]\n","\n","Average MAE Loss:\n","[0.06060166 0.06080298 0.06065118 0.06064227]\n","\n","Epoch 00046: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05993714 0.06105033]\n"," [0.0602822  0.06132376]\n"," [0.06013558 0.06114683]\n"," [0.06035916 0.06115646]]\n","\n","Average MAE Loss:\n","[0.06049373 0.06080298 0.0606412  0.06075781]\n","\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06003728 0.06110879]\n"," [0.06028219 0.06132376]\n"," [0.06010534 0.06111082]\n"," [0.06035829 0.06113089]]\n","\n","Average MAE Loss:\n","[0.06057304 0.06080298 0.06060808 0.06074459]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06005201 0.06113098]\n"," [0.0602822  0.06132376]\n"," [0.06009582 0.06109899]\n"," [0.06034395 0.06110387]]\n","\n","Average MAE Loss:\n","[0.06059149 0.06080298 0.0605974  0.06072391]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0601889  0.06125252]\n"," [0.06034395 0.06110387]\n"," [0.0600248  0.06109662]\n"," [0.06010766 0.06109504]]\n","\n","Average MAE Loss:\n","[0.06072071 0.06072391 0.06056071 0.06060135]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06017462 0.06123586]\n"," [0.06034395 0.06110388]\n"," [0.06000023 0.06106668]\n"," [0.06018214 0.06114504]]\n","\n","Average MAE Loss:\n","[0.06070524 0.06072391 0.06053346 0.06066359]\n","\n","Epoch 00051: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06014103 0.06121065]\n"," [0.06034395 0.06110387]\n"," [0.0599828  0.06104249]\n"," [0.06026708 0.06120657]]\n","\n","Average MAE Loss:\n","[0.06067584 0.06072391 0.06051264 0.06073683]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06008333 0.06116511]\n"," [0.06034395 0.06110387]\n"," [0.05998055 0.06103106]\n"," [0.06029376 0.06121764]]\n","\n","Average MAE Loss:\n","[0.06062422 0.06072391 0.06050581 0.0607557 ]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06007428 0.06115211]\n"," [0.06034395 0.06110387]\n"," [0.05998676 0.06102762]\n"," [0.06029851 0.0612149 ]]\n","\n","Average MAE Loss:\n","[0.06061319 0.06072391 0.06050719 0.06075671]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06005399 0.06113594]\n"," [0.06034395 0.06110387]\n"," [0.0599857  0.06102285]\n"," [0.06027771 0.06118736]]\n","\n","Average MAE Loss:\n","[0.06059496 0.06072391 0.06050428 0.06073253]\n","\n","Epoch 00055: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06000656 0.06109565]\n"," [0.06034395 0.06110387]\n"," [0.05998348 0.0610176 ]\n"," [0.06026546 0.06116634]]\n","\n","Average MAE Loss:\n","[0.06055111 0.06072391 0.06050054 0.0607159 ]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06005954 0.06100456]\n"," [0.06011717 0.06104854]\n"," [0.06010069 0.06103095]\n"," [0.06011261 0.0610361 ]]\n","\n","Average MAE Loss:\n","[0.06053205 0.06058286 0.06056582 0.06057436]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06004483 0.06099744]\n"," [0.06011717 0.06104854]\n"," [0.06007391 0.06100589]\n"," [0.06011962 0.06103731]]\n","\n","Average MAE Loss:\n","[0.06052113 0.06058286 0.0605399  0.06057847]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06002949 0.06099309]\n"," [0.06011717 0.06104854]\n"," [0.06006581 0.06099759]\n"," [0.06012688 0.0610379 ]]\n","\n","Average MAE Loss:\n","[0.06051129 0.06058286 0.0605317  0.06058239]\n","\n","Epoch 00059: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06001744 0.06098585]\n"," [0.06011717 0.06104854]\n"," [0.06006177 0.06099291]\n"," [0.0601442  0.06104789]]\n","\n","Average MAE Loss:\n","[0.06050164 0.06058286 0.06052734 0.06059604]\n","\n","\n","epochs finished with time:684.1485495567322\n","\n","[[0.06001743 0.06098585]\n"," [0.06011717 0.06104854]\n"," [0.06006177 0.06099291]\n"," [0.0601442  0.06104789]]\n","00:11:51.081589000999884\n","------------------------------------Fold [3/5]-----------------------------------------\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05328942 0.05378283]\n"," [0.08831251 0.07784657]\n"," [0.05719306 0.06172043]\n"," [0.09508604 0.06900197]]\n","\n","Average MAE Loss:\n","[0.05353613 0.08307954 0.05945675 0.08204401]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.0461927  0.05817379]\n"," [0.08831251 0.07784657]\n"," [0.04697651 0.05544094]\n"," [0.09808514 0.05811863]]\n","\n","Average MAE Loss:\n","[0.05218324 0.08307954 0.05120872 0.07810189]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04276215 0.04895798]\n"," [0.08831251 0.07784657]\n"," [0.04732551 0.07329077]\n"," [0.099224   0.05399361]]\n","\n","Average MAE Loss:\n","[0.04586006 0.08307954 0.06030814 0.0766088 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04260926 0.04754886]\n"," [0.08831251 0.07784657]\n"," [0.04537843 0.06419934]\n"," [0.09919594 0.0513173 ]]\n","\n","Average MAE Loss:\n","[0.04507906 0.08307954 0.05478888 0.07525662]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04639034 0.0472809 ]\n"," [0.08831251 0.07784657]\n"," [0.04694243 0.05165523]\n"," [0.09791406 0.04754366]]\n","\n","Average MAE Loss:\n","[0.04683562 0.08307954 0.04929883 0.07272886]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04165299 0.04371909]\n"," [0.08831251 0.07784657]\n"," [0.0496886  0.07330193]\n"," [0.09813849 0.04532672]]\n","\n","Average MAE Loss:\n","[0.04268604 0.08307954 0.06149526 0.07173261]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.05093002 0.05577689]\n"," [0.08831251 0.07784657]\n"," [0.04991814 0.07843833]\n"," [0.09627019 0.04378016]]\n","\n","Average MAE Loss:\n","[0.05335345 0.08307954 0.06417823 0.07002517]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06028423 0.06309381]\n"," [0.09627019 0.04378016]\n"," [0.04828054 0.06064013]\n"," [0.06790425 0.05105627]]\n","\n","Average MAE Loss:\n","[0.06168902 0.07002517 0.05446034 0.05948026]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04803351 0.05253001]\n"," [0.09627019 0.04378016]\n"," [0.04616164 0.05143787]\n"," [0.08352867 0.04993998]]\n","\n","Average MAE Loss:\n","[0.05028176 0.07002517 0.04879976 0.06673432]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05117007 0.05212421]\n"," [0.09627019 0.04378016]\n"," [0.04566362 0.04730275]\n"," [0.0871647  0.04543753]]\n","\n","Average MAE Loss:\n","[0.05164714 0.07002517 0.04648319 0.06630111]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04230879 0.0443853 ]\n"," [0.09627019 0.04378016]\n"," [0.04583098 0.04411235]\n"," [0.08995341 0.04692292]]\n","\n","Average MAE Loss:\n","[0.04334704 0.07002517 0.04497166 0.06843816]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04285052 0.04321998]\n"," [0.09627019 0.04378016]\n"," [0.04385894 0.04370052]\n"," [0.09061165 0.04402371]]\n","\n","Average MAE Loss:\n","[0.04303525 0.07002517 0.04377973 0.06731768]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04143083 0.04200669]\n"," [0.09627019 0.04378016]\n"," [0.04363036 0.0460847 ]\n"," [0.09200808 0.04325917]]\n","\n","Average MAE Loss:\n","[0.04171876 0.07002517 0.04485753 0.06763363]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04081625 0.04323587]\n"," [0.09627019 0.04378016]\n"," [0.04708361 0.04847695]\n"," [0.09169221 0.04161026]]\n","\n","Average MAE Loss:\n","[0.04202606 0.07002517 0.04778028 0.06665123]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05760231 0.05719643]\n"," [0.07958177 0.06784804]\n"," [0.05798889 0.05483817]\n"," [0.06384673 0.05687264]]\n","\n","Average MAE Loss:\n","[0.05739937 0.0737149  0.05641353 0.06035969]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05139309 0.05136919]\n"," [0.07958177 0.06784804]\n"," [0.04723248 0.04754513]\n"," [0.06130231 0.05541967]]\n","\n","Average MAE Loss:\n","[0.05138114 0.0737149  0.04738881 0.05836099]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04597738 0.04524627]\n"," [0.07958177 0.06784804]\n"," [0.04428472 0.04612496]\n"," [0.05910657 0.05446704]]\n","\n","Average MAE Loss:\n","[0.04561182 0.0737149  0.04520484 0.05678681]\n","\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04499742 0.04459829]\n"," [0.07958177 0.06784804]\n"," [0.04288752 0.04565985]\n"," [0.05856003 0.0542088 ]]\n","\n","Average MAE Loss:\n","[0.04479786 0.0737149  0.04427369 0.05638442]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04462414 0.04442955]\n"," [0.07958177 0.06784804]\n"," [0.04232888 0.04510847]\n"," [0.05777266 0.05403236]]\n","\n","Average MAE Loss:\n","[0.04452685 0.0737149  0.04371868 0.05590251]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04425411 0.04389195]\n"," [0.07958177 0.06784804]\n"," [0.04211434 0.0440391 ]\n"," [0.05653721 0.05352358]]\n","\n","Average MAE Loss:\n","[0.04407303 0.0737149  0.04307672 0.05503039]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04369301 0.04319085]\n"," [0.07958177 0.06784804]\n"," [0.04182793 0.04414677]\n"," [0.05409552 0.05223483]]\n","\n","Average MAE Loss:\n","[0.04344193 0.0737149  0.04298735 0.05316518]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06738766 0.06148278]\n"," [0.05409552 0.05223483]\n"," [0.04359286 0.04331206]\n"," [0.04254825 0.04141931]]\n","\n","Average MAE Loss:\n","[0.06443522 0.05316518 0.04345246 0.04198378]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0581741  0.05502047]\n"," [0.05409552 0.05223483]\n"," [0.04410388 0.04292134]\n"," [0.04245109 0.04170643]]\n","\n","Average MAE Loss:\n","[0.05659729 0.05316518 0.04351261 0.04207876]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05700624 0.05409804]\n"," [0.05409552 0.05223483]\n"," [0.04220661 0.04165845]\n"," [0.04238326 0.04117687]]\n","\n","Average MAE Loss:\n","[0.05555214 0.05316518 0.04193253 0.04178007]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05512463 0.05320938]\n"," [0.05409552 0.05223483]\n"," [0.04203306 0.0417324 ]\n"," [0.04229215 0.04154848]]\n","\n","Average MAE Loss:\n","[0.054167   0.05316518 0.04188273 0.04192032]\n","\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.05386295 0.05261416]\n"," [0.05409552 0.05223483]\n"," [0.04166181 0.04254494]\n"," [0.04252499 0.04136986]]\n","\n","Average MAE Loss:\n","[0.05323856 0.05316518 0.04210338 0.04194742]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.05272794 0.05193359]\n"," [0.05409552 0.05223483]\n"," [0.04222641 0.04365855]\n"," [0.04271905 0.04182095]]\n","\n","Average MAE Loss:\n","[0.05233076 0.05316518 0.04294248 0.04227   ]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.05142722 0.05106143]\n"," [0.05409552 0.05223483]\n"," [0.04215985 0.04379309]\n"," [0.04310873 0.04138162]]\n","\n","Average MAE Loss:\n","[0.05124432 0.05316518 0.04297647 0.04224518]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.0457732  0.04610543]\n"," [0.04762514 0.04736552]\n"," [0.04358791 0.04373294]\n"," [0.04505956 0.04444478]]\n","\n","Average MAE Loss:\n","[0.04593932 0.04749533 0.04366042 0.04475217]\n","\n","Epoch 00029: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00029: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04517944 0.04562851]\n"," [0.04762514 0.04736552]\n"," [0.04247776 0.04320867]\n"," [0.04363436 0.0435448 ]]\n","\n","Average MAE Loss:\n","[0.04540398 0.04749533 0.04284321 0.04358958]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04481915 0.04510857]\n"," [0.04762514 0.04736552]\n"," [0.04204991 0.0428289 ]\n"," [0.04270349 0.04222924]]\n","\n","Average MAE Loss:\n","[0.04496386 0.04749533 0.0424394  0.04246636]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04444922 0.04471728]\n"," [0.04762514 0.04736552]\n"," [0.04190565 0.04233719]\n"," [0.0423497  0.04157636]]\n","\n","Average MAE Loss:\n","[0.04458325 0.04749533 0.04212142 0.04196303]\n","\n","Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04414228 0.04435304]\n"," [0.04762514 0.04736552]\n"," [0.04172782 0.04171353]\n"," [0.04191207 0.04151896]]\n","\n","Average MAE Loss:\n","[0.04424766 0.04749533 0.04172067 0.04171551]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04399486 0.04421287]\n"," [0.04762514 0.04736552]\n"," [0.04178753 0.04186279]\n"," [0.04193246 0.04125941]]\n","\n","Average MAE Loss:\n","[0.04410386 0.04749533 0.04182516 0.04159593]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04385564 0.04407025]\n"," [0.04762514 0.04736552]\n"," [0.041773   0.0424911 ]\n"," [0.0418731  0.04110583]]\n","\n","Average MAE Loss:\n","[0.04396295 0.04749533 0.04213205 0.04148947]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04711467 0.04703009]\n"," [0.0418731  0.04110583]\n"," [0.04319127 0.04308363]\n"," [0.04164148 0.04130184]]\n","\n","Average MAE Loss:\n","[0.04707238 0.04148947 0.04313745 0.04147166]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04652369 0.04670419]\n"," [0.0418731  0.04110583]\n"," [0.04231317 0.04295169]\n"," [0.04179844 0.04115973]]\n","\n","Average MAE Loss:\n","[0.04661394 0.04148947 0.04263243 0.04147908]\n","\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00037: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04629496 0.04656466]\n"," [0.0418731  0.04110583]\n"," [0.04203258 0.04283142]\n"," [0.0418045  0.04115727]]\n","\n","Average MAE Loss:\n","[0.04642981 0.04148947 0.042432   0.04148088]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04609606 0.04642916]\n"," [0.0418731  0.04110583]\n"," [0.04203523 0.04225134]\n"," [0.04189625 0.04110089]]\n","\n","Average MAE Loss:\n","[0.04626261 0.04148947 0.04214328 0.04149857]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04592892 0.0462852 ]\n"," [0.0418731  0.04110583]\n"," [0.04188579 0.04245318]\n"," [0.04186259 0.04111642]]\n","\n","Average MAE Loss:\n","[0.04610706 0.04148947 0.04216949 0.04148951]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04576598 0.04614498]\n"," [0.0418731  0.04110583]\n"," [0.04177668 0.04216535]\n"," [0.04186732 0.04142528]]\n","\n","Average MAE Loss:\n","[0.04595548 0.04148946 0.04197102 0.0416463 ]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04569093 0.04607235]\n"," [0.0418731  0.04110583]\n"," [0.04170865 0.0422464 ]\n"," [0.04189854 0.04139245]]\n","\n","Average MAE Loss:\n","[0.04588164 0.04148947 0.04197752 0.04164549]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04236051 0.04209064]\n"," [0.04239303 0.0421109 ]\n"," [0.04211532 0.04250516]\n"," [0.04233947 0.04172265]]\n","\n","Average MAE Loss:\n","[0.04222558 0.04225197 0.04231024 0.04203106]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04234696 0.04207583]\n"," [0.04239304 0.0421109 ]\n"," [0.04203717 0.04227192]\n"," [0.04206146 0.04163679]]\n","\n","Average MAE Loss:\n","[0.04221139 0.04225197 0.04215454 0.04184913]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04234118 0.04206688]\n"," [0.04239304 0.0421109 ]\n"," [0.04199769 0.04199639]\n"," [0.04195768 0.04159715]]\n","\n","Average MAE Loss:\n","[0.04220403 0.04225197 0.04199704 0.04177742]\n","\n","Epoch 00045: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00045: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04233294 0.04206475]\n"," [0.04239303 0.0421109 ]\n"," [0.04193243 0.04199102]\n"," [0.04195379 0.04148754]]\n","\n","Average MAE Loss:\n","[0.04219884 0.04225197 0.04196173 0.04172066]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04232461 0.04206496]\n"," [0.04239303 0.0421109 ]\n"," [0.04185265 0.04218626]\n"," [0.04191996 0.04142559]]\n","\n","Average MAE Loss:\n","[0.04219478 0.04225197 0.04201946 0.04167277]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04232146 0.04206222]\n"," [0.04239303 0.0421109 ]\n"," [0.04183028 0.04206536]\n"," [0.04191479 0.04135623]]\n","\n","Average MAE Loss:\n","[0.04219184 0.04225197 0.04194782 0.04163551]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04231744 0.04205599]\n"," [0.04239304 0.0421109 ]\n"," [0.04181514 0.04203432]\n"," [0.04190046 0.04135395]]\n","\n","Average MAE Loss:\n","[0.04218671 0.04225197 0.04192473 0.0416272 ]\n","\n","Epoch 00049: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00049: reducing learning rate of group 0 to 1.5625e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04239202 0.04210594]\n"," [0.04190046 0.04135395]\n"," [0.042165   0.04215803]\n"," [0.04183216 0.04163916]]\n","\n","Average MAE Loss:\n","[0.04224898 0.0416272  0.04216151 0.04173566]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04239072 0.0421016 ]\n"," [0.04190046 0.04135394]\n"," [0.04205647 0.04226554]\n"," [0.04197961 0.04133505]]\n","\n","Average MAE Loss:\n","[0.04224616 0.0416272  0.042161   0.04165733]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04238667 0.04209906]\n"," [0.04190046 0.04135395]\n"," [0.04200591 0.04227675]\n"," [0.04189539 0.04134855]]\n","\n","Average MAE Loss:\n","[0.04224287 0.0416272  0.04214133 0.04162197]\n","\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04238244 0.0420969 ]\n"," [0.04190046 0.04135394]\n"," [0.0419773  0.04227201]\n"," [0.04185994 0.04134457]]\n","\n","Average MAE Loss:\n","[0.04223967 0.0416272  0.04212466 0.04160225]\n","\n","Epoch 00053: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00053: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04238087 0.04209521]\n"," [0.04190046 0.04135394]\n"," [0.04196481 0.04224045]\n"," [0.04183681 0.04131826]]\n","\n","Average MAE Loss:\n","[0.04223804 0.0416272  0.04210263 0.04157754]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04237892 0.04209347]\n"," [0.04190046 0.04135394]\n"," [0.04195603 0.04221664]\n"," [0.04178911 0.04133642]]\n","\n","Average MAE Loss:\n","[0.04223619 0.0416272  0.04208633 0.04156276]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04237676 0.04209259]\n"," [0.04190046 0.04135395]\n"," [0.04193689 0.0422526 ]\n"," [0.04174885 0.0413811 ]]\n","\n","Average MAE Loss:\n","[0.04223467 0.0416272  0.04209475 0.04156497]\n","\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04192724 0.04166227]\n"," [0.04192791 0.04166382]\n"," [0.04186625 0.04173841]\n"," [0.04193215 0.04162699]]\n","\n","Average MAE Loss:\n","[0.04179475 0.04179586 0.04180233 0.04177957]\n","\n","Epoch 00057: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00057: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04192718 0.04166076]\n"," [0.04192791 0.04166382]\n"," [0.04183892 0.04179228]\n"," [0.04194613 0.04156746]]\n","\n","Average MAE Loss:\n","[0.04179397 0.04179586 0.0418156  0.04175679]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04192672 0.04166008]\n"," [0.04192791 0.04166382]\n"," [0.04182064 0.04183304]\n"," [0.04195365 0.04153499]]\n","\n","Average MAE Loss:\n","[0.0417934  0.04179586 0.04182684 0.04174432]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04192626 0.04165915]\n"," [0.04192791 0.04166381]\n"," [0.04180411 0.04187613]\n"," [0.04196423 0.04149709]]\n","\n","Average MAE Loss:\n","[0.04179271 0.04179586 0.04184012 0.04173066]\n","\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00060: reducing learning rate of group 0 to 1.9531e-06.\n","\n","epochs finished with time:635.6599946022034\n","\n","[[0.04192626 0.04165915]\n"," [0.04192791 0.04166382]\n"," [0.04180411 0.04187613]\n"," [0.04196423 0.04149709]]\n","00:10:56.53458188600007\n","------------------------------------Fold [4/5]-----------------------------------------\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.04895836 0.07154102]\n"," [0.07262255 0.09376136]\n"," [0.04904315 0.07461751]\n"," [0.07994147 0.08181076]]\n","\n","Average MAE Loss:\n","[0.06024969 0.08319196 0.06183033 0.08087612]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04299499 0.06123258]\n"," [0.07262255 0.09376137]\n"," [0.04159538 0.0633331 ]\n"," [0.08203728 0.06632009]]\n","\n","Average MAE Loss:\n","[0.05211379 0.08319196 0.05246424 0.07417869]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04212983 0.0578067 ]\n"," [0.07262255 0.09376136]\n"," [0.03965597 0.06204245]\n"," [0.08156426 0.06508865]]\n","\n","Average MAE Loss:\n","[0.04996826 0.08319196 0.05084921 0.07332646]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0419127  0.0642722 ]\n"," [0.07262255 0.09376137]\n"," [0.03905307 0.06094978]\n"," [0.08137691 0.06393001]]\n","\n","Average MAE Loss:\n","[0.05309245 0.08319196 0.05000143 0.07265346]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04026648 0.05839421]\n"," [0.07262255 0.09376135]\n"," [0.04068341 0.06589647]\n"," [0.0810688  0.06273947]]\n","\n","Average MAE Loss:\n","[0.04933035 0.08319195 0.05328994 0.07190413]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04173534 0.05864331]\n"," [0.07262255 0.09376135]\n"," [0.04226097 0.06711821]\n"," [0.0812739  0.05942529]]\n","\n","Average MAE Loss:\n","[0.05018933 0.08319195 0.05468959 0.0703496 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04024571 0.06164687]\n"," [0.07262255 0.09376135]\n"," [0.03962911 0.05988792]\n"," [0.08064945 0.05845033]]\n","\n","Average MAE Loss:\n","[0.05094629 0.08319195 0.04975851 0.06954989]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04847862 0.07218066]\n"," [0.08064945 0.05845033]\n"," [0.04071376 0.05941948]\n"," [0.05201876 0.05745047]]\n","\n","Average MAE Loss:\n","[0.06032964 0.06954989 0.05006662 0.05473461]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04349612 0.06451572]\n"," [0.08064945 0.05845033]\n"," [0.04141699 0.06288882]\n"," [0.07507974 0.0685398 ]]\n","\n","Average MAE Loss:\n","[0.05400592 0.06954989 0.05215291 0.07180977]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04193647 0.06594045]\n"," [0.08064945 0.05845032]\n"," [0.03751466 0.05539009]\n"," [0.07812258 0.06479091]]\n","\n","Average MAE Loss:\n","[0.05393846 0.06954989 0.04645238 0.07145675]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04113492 0.06299368]\n"," [0.08064945 0.05845032]\n"," [0.03768319 0.05855244]\n"," [0.07727714 0.05936908]]\n","\n","Average MAE Loss:\n","[0.0520643  0.06954988 0.04811782 0.06832311]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0402202  0.0592133 ]\n"," [0.08064945 0.05845032]\n"," [0.03684372 0.05822648]\n"," [0.07838516 0.05990505]]\n","\n","Average MAE Loss:\n","[0.04971675 0.06954988 0.0475351  0.06914511]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04007474 0.05871178]\n"," [0.08064945 0.05845032]\n"," [0.0368009  0.05730885]\n"," [0.0781666  0.05977774]]\n","\n","Average MAE Loss:\n","[0.04939326 0.06954988 0.04705488 0.06897217]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04122641 0.06031566]\n"," [0.08064945 0.05845032]\n"," [0.03815916 0.05998299]\n"," [0.07857797 0.05992422]]\n","\n","Average MAE Loss:\n","[0.05077103 0.06954988 0.04907107 0.06925109]\n","\n","Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04786249 0.06928922]\n"," [0.06313257 0.08412161]\n"," [0.04882725 0.06706797]\n"," [0.04974727 0.07080956]]\n","\n","Average MAE Loss:\n","[0.05857586 0.07362709 0.05794761 0.06027841]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04541782 0.06731678]\n"," [0.06313257 0.08412161]\n"," [0.04340508 0.06732209]\n"," [0.04909464 0.07128537]]\n","\n","Average MAE Loss:\n","[0.0563673  0.07362709 0.05536359 0.06019   ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04278639 0.06463276]\n"," [0.06313257 0.08412161]\n"," [0.03917322 0.06155317]\n"," [0.0473132  0.06759699]]\n","\n","Average MAE Loss:\n","[0.05370957 0.07362709 0.0503632  0.05745509]\n","\n","Epoch 00017: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04105981 0.06297881]\n"," [0.06313257 0.08412161]\n"," [0.03799689 0.05867315]\n"," [0.04689204 0.06770194]]\n","\n","Average MAE Loss:\n","[0.05201931 0.07362709 0.04833502 0.05729699]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.03959629 0.060592  ]\n"," [0.06313257 0.08412161]\n"," [0.03692687 0.05703591]\n"," [0.04645824 0.06734131]]\n","\n","Average MAE Loss:\n","[0.05009414 0.07362709 0.04698139 0.05689977]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03864513 0.05875487]\n"," [0.06313257 0.08412161]\n"," [0.0360163  0.05613364]\n"," [0.04571718 0.06723107]]\n","\n","Average MAE Loss:\n","[0.0487     0.07362709 0.04607497 0.05647412]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03814583 0.05765964]\n"," [0.06313257 0.08412161]\n"," [0.03580528 0.05580345]\n"," [0.04519545 0.06689096]]\n","\n","Average MAE Loss:\n","[0.04790273 0.07362709 0.04580437 0.05604321]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05206661 0.07654794]\n"," [0.04519545 0.06689096]\n"," [0.03762029 0.05806863]\n"," [0.0360688  0.05600353]]\n","\n","Average MAE Loss:\n","[0.06430727 0.05604321 0.04784446 0.04603617]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04724026 0.06655942]\n"," [0.04519545 0.06689096]\n"," [0.03707173 0.05773855]\n"," [0.03557527 0.05492226]]\n","\n","Average MAE Loss:\n","[0.05689984 0.05604321 0.04740514 0.04524876]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04525087 0.06677732]\n"," [0.04519545 0.06689096]\n"," [0.03615157 0.05641862]\n"," [0.03552794 0.05486083]]\n","\n","Average MAE Loss:\n","[0.05601409 0.05604321 0.0462851  0.04519438]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04376732 0.06574093]\n"," [0.04519545 0.06689096]\n"," [0.03584883 0.05590631]\n"," [0.03565726 0.05470724]]\n","\n","Average MAE Loss:\n","[0.05475412 0.05604321 0.04587757 0.04518225]\n","\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04290723 0.06490065]\n"," [0.04519545 0.06689096]\n"," [0.03549565 0.05504617]\n"," [0.03580829 0.05475586]]\n","\n","Average MAE Loss:\n","[0.05390394 0.05604321 0.04527091 0.04528208]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04191609 0.06405932]\n"," [0.04519545 0.06689096]\n"," [0.03547996 0.05512722]\n"," [0.0356658  0.05445338]]\n","\n","Average MAE Loss:\n","[0.05298771 0.05604321 0.04530359 0.04505959]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04092256 0.06287237]\n"," [0.04519545 0.06689096]\n"," [0.03544206 0.05496924]\n"," [0.03582732 0.05466841]]\n","\n","Average MAE Loss:\n","[0.05189746 0.05604321 0.04520565 0.04524786]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03741407 0.05938818]\n"," [0.03836227 0.0615284 ]\n"," [0.03736834 0.05945293]\n"," [0.03724187 0.05892615]]\n","\n","Average MAE Loss:\n","[0.04840113 0.04994533 0.04841064 0.04808401]\n","\n","Epoch 00029: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03714335 0.05872296]\n"," [0.03836227 0.06152839]\n"," [0.03653165 0.05783547]\n"," [0.03663215 0.05650591]]\n","\n","Average MAE Loss:\n","[0.04793315 0.04994533 0.04718356 0.04656903]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03691635 0.05821829]\n"," [0.03836227 0.06152839]\n"," [0.03591493 0.05645359]\n"," [0.03650037 0.0559816 ]]\n","\n","Average MAE Loss:\n","[0.04756732 0.04994533 0.04618426 0.04624098]\n","\n","Epoch 00031: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.0367287  0.05772986]\n"," [0.03836227 0.06152839]\n"," [0.03570419 0.05585439]\n"," [0.0363122  0.05577545]]\n","\n","Average MAE Loss:\n","[0.04722928 0.04994533 0.04577929 0.04604383]\n","\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03657671 0.05729406]\n"," [0.03836227 0.06152839]\n"," [0.03554494 0.05553219]\n"," [0.03611466 0.05549683]]\n","\n","Average MAE Loss:\n","[0.04693539 0.04994533 0.04553857 0.04580574]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03645644 0.05691759]\n"," [0.03836227 0.06152839]\n"," [0.03546559 0.05529574]\n"," [0.03600057 0.05535685]]\n","\n","Average MAE Loss:\n","[0.04668702 0.04994533 0.04538066 0.04567871]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03633533 0.05657386]\n"," [0.03836227 0.0615284 ]\n"," [0.0354027  0.05517943]\n"," [0.03585983 0.05520074]]\n","\n","Average MAE Loss:\n","[0.0464546  0.04994533 0.04529107 0.04553029]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.0378689  0.06052155]\n"," [0.03585983 0.05520074]\n"," [0.03613821 0.05630312]\n"," [0.03537908 0.05507744]]\n","\n","Average MAE Loss:\n","[0.04919523 0.04553029 0.04622066 0.04522826]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03749153 0.0595781 ]\n"," [0.03585983 0.05520074]\n"," [0.03603177 0.05612381]\n"," [0.03531887 0.05475205]]\n","\n","Average MAE Loss:\n","[0.04853482 0.04553029 0.04607779 0.04503546]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03717395 0.05889408]\n"," [0.03585983 0.05520074]\n"," [0.0359386  0.05595709]\n"," [0.03529072 0.05445428]]\n","\n","Average MAE Loss:\n","[0.04803402 0.04553029 0.04594784 0.0448725 ]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03691833 0.0582496 ]\n"," [0.03585983 0.05520074]\n"," [0.03586565 0.05584122]\n"," [0.03531343 0.05444896]]\n","\n","Average MAE Loss:\n","[0.04758397 0.04553029 0.04585343 0.04488119]\n","\n","Epoch 00039: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03681211 0.05797878]\n"," [0.03585983 0.05520074]\n"," [0.03578036 0.05565288]\n"," [0.03529396 0.05435976]]\n","\n","Average MAE Loss:\n","[0.04739544 0.04553029 0.04571662 0.04482686]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03671657 0.05773516]\n"," [0.03585983 0.05520074]\n"," [0.03574484 0.05558623]\n"," [0.0352577  0.05423783]]\n","\n","Average MAE Loss:\n","[0.04722587 0.04553029 0.04566553 0.04474776]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03663726 0.05750728]\n"," [0.03585983 0.05520074]\n"," [0.03571787 0.05554894]\n"," [0.03529694 0.05428306]]\n","\n","Average MAE Loss:\n","[0.04707227 0.04553029 0.04563341 0.04479   ]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03561021 0.05538644]\n"," [0.03563433 0.05550092]\n"," [0.03560188 0.05543602]\n"," [0.03559705 0.05539443]]\n","\n","Average MAE Loss:\n","[0.04549832 0.04556762 0.04551895 0.04549574]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03558749 0.05530632]\n"," [0.03563433 0.05550092]\n"," [0.03556983 0.05536176]\n"," [0.03551962 0.05505742]]\n","\n","Average MAE Loss:\n","[0.0454469  0.04556762 0.0454658  0.04528852]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03556206 0.055241  ]\n"," [0.03563432 0.05550092]\n"," [0.03555554 0.05533393]\n"," [0.03546208 0.05480103]]\n","\n","Average MAE Loss:\n","[0.04540153 0.04556762 0.04544473 0.04513155]\n","\n","Epoch 00045: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03554194 0.05516463]\n"," [0.03563433 0.05550092]\n"," [0.03554136 0.0553214 ]\n"," [0.03546345 0.05476206]]\n","\n","Average MAE Loss:\n","[0.04535329 0.04556762 0.04543138 0.04511275]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03553865 0.05514476]\n"," [0.03563432 0.05550092]\n"," [0.03552874 0.05529394]\n"," [0.03544195 0.05465204]]\n","\n","Average MAE Loss:\n","[0.0453417  0.04556762 0.04541134 0.04504699]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03553478 0.05510923]\n"," [0.03563432 0.05550092]\n"," [0.03552203 0.05528373]\n"," [0.03541506 0.05453403]]\n","\n","Average MAE Loss:\n","[0.045322   0.04556762 0.04540288 0.04497455]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03552799 0.05507005]\n"," [0.03563432 0.05550092]\n"," [0.03551722 0.055276  ]\n"," [0.03540016 0.05449102]]\n","\n","Average MAE Loss:\n","[0.04529902 0.04556762 0.04539661 0.04494559]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03561589 0.05543851]\n"," [0.03540016 0.05449102]\n"," [0.03551825 0.05503437]\n"," [0.03550206 0.05524962]]\n","\n","Average MAE Loss:\n","[0.0455272  0.04494559 0.04527631 0.04537584]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03558917 0.05533705]\n"," [0.03540016 0.05449102]\n"," [0.03551015 0.05501407]\n"," [0.03548994 0.05519605]]\n","\n","Average MAE Loss:\n","[0.04546311 0.04494559 0.04526211 0.04534299]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03556885 0.05525398]\n"," [0.03540016 0.05449102]\n"," [0.03550294 0.05499738]\n"," [0.03548029 0.05513694]]\n","\n","Average MAE Loss:\n","[0.04541141 0.04494559 0.04525016 0.04530862]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03555272 0.05517248]\n"," [0.03540016 0.05449102]\n"," [0.03549963 0.05499159]\n"," [0.03546723 0.05508375]]\n","\n","Average MAE Loss:\n","[0.0453626  0.04494559 0.04524561 0.04527549]\n","\n","Epoch 00053: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00053: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03554751 0.05515473]\n"," [0.03540016 0.05449102]\n"," [0.03549567 0.05498141]\n"," [0.03545764 0.05504474]]\n","\n","Average MAE Loss:\n","[0.04535112 0.04494559 0.04523854 0.04525119]\n","\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03554219 0.05515404]\n"," [0.03540016 0.05449102]\n"," [0.03549236 0.05497557]\n"," [0.03544818 0.0550066 ]]\n","\n","Average MAE Loss:\n","[0.04534812 0.04494559 0.04523396 0.04522739]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03553742 0.05513421]\n"," [0.03540016 0.05449102]\n"," [0.03548988 0.05497515]\n"," [0.03544368 0.05498501]]\n","\n","Average MAE Loss:\n","[0.04533582 0.04494559 0.04523252 0.04521434]\n","\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03542924 0.05484267]\n"," [0.03543384 0.05486235]\n"," [0.03543229 0.0548586 ]\n"," [0.03542996 0.05485165]]\n","\n","Average MAE Loss:\n","[0.04513596 0.04514809 0.04514545 0.0451408 ]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03542428 0.05481999]\n"," [0.03543384 0.05486235]\n"," [0.03543087 0.05485428]\n"," [0.03542934 0.05485024]]\n","\n","Average MAE Loss:\n","[0.04512213 0.04514809 0.04514258 0.04513979]\n","\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03542091 0.05478475]\n"," [0.03543384 0.05486235]\n"," [0.03542918 0.05485053]\n"," [0.03542935 0.05485094]]\n","\n","Average MAE Loss:\n","[0.04510283 0.04514809 0.04513985 0.04514015]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03542065 0.0547803 ]\n"," [0.03543384 0.05486235]\n"," [0.03542791 0.05484811]\n"," [0.03542859 0.05485116]]\n","\n","Average MAE Loss:\n","[0.04510047 0.04514809 0.04513801 0.04513988]\n","\n","\n","epochs finished with time:652.1611948013306\n","\n","[[0.03542065 0.0547803 ]\n"," [0.03543384 0.05486235]\n"," [0.03542791 0.0548481 ]\n"," [0.03542859 0.05485116]]\n","00:11:14.872130567000113\n","------------------------------------Fold [5/5]-----------------------------------------\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","DeepTAGNet\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05567095 0.068898  ]\n"," [0.0904028  0.0951113 ]\n"," [0.06057876 0.07503339]\n"," [0.09873527 0.08421589]]\n","\n","Average MAE Loss:\n","[0.06228447 0.09275705 0.06780608 0.09147558]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.05080413 0.05444801]\n"," [0.0904028  0.0951113 ]\n"," [0.05285475 0.0627739 ]\n"," [0.10153905 0.06999452]]\n","\n","Average MAE Loss:\n","[0.05262607 0.09275705 0.05781432 0.08576678]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04772882 0.05132536]\n"," [0.0904028  0.0951113 ]\n"," [0.05232126 0.05393425]\n"," [0.10317633 0.06485917]]\n","\n","Average MAE Loss:\n","[0.04952709 0.09275705 0.05312775 0.08401775]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04648894 0.04962903]\n"," [0.0904028  0.0951113 ]\n"," [0.05018483 0.05724816]\n"," [0.09850009 0.05800051]]\n","\n","Average MAE Loss:\n","[0.04805899 0.09275705 0.05371649 0.0782503 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0463293  0.05117589]\n"," [0.0904028  0.0951113 ]\n"," [0.05446665 0.07661065]\n"," [0.09917799 0.057225  ]]\n","\n","Average MAE Loss:\n","[0.04875259 0.09275705 0.06553865 0.07820149]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04745401 0.05137623]\n"," [0.0904028  0.0951113 ]\n"," [0.04912603 0.05874262]\n"," [0.10004109 0.05452763]]\n","\n","Average MAE Loss:\n","[0.04941512 0.09275705 0.05393432 0.07728436]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04666704 0.05724011]\n"," [0.0904028  0.0951113 ]\n"," [0.05223977 0.06572455]\n"," [0.10212074 0.05724563]]\n","\n","Average MAE Loss:\n","[0.05195358 0.09275705 0.05898216 0.07968319]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06133139 0.08211133]\n"," [0.10212074 0.05724563]\n"," [0.04864477 0.05540914]\n"," [0.0722436  0.06111364]]\n","\n","Average MAE Loss:\n","[0.07172136 0.07968319 0.05202695 0.06667862]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05215586 0.0652814 ]\n"," [0.10212074 0.05724563]\n"," [0.05105568 0.05854643]\n"," [0.08791992 0.05824601]]\n","\n","Average MAE Loss:\n","[0.05871863 0.07968319 0.05480105 0.07308297]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04797723 0.06097309]\n"," [0.10212074 0.05724563]\n"," [0.05198589 0.06071335]\n"," [0.09566733 0.06026294]]\n","\n","Average MAE Loss:\n","[0.05447516 0.07968319 0.05634962 0.07796513]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0476336  0.05498535]\n"," [0.10212074 0.05724563]\n"," [0.0498407  0.05927357]\n"," [0.09482196 0.05802549]]\n","\n","Average MAE Loss:\n","[0.05130947 0.07968319 0.05455714 0.07642373]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04503478 0.05306165]\n"," [0.10212074 0.05724563]\n"," [0.04625732 0.05654234]\n"," [0.09876246 0.06019779]]\n","\n","Average MAE Loss:\n","[0.04904822 0.07968319 0.05139983 0.07948012]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04516673 0.05516632]\n"," [0.10212074 0.05724563]\n"," [0.04525632 0.05540679]\n"," [0.09550628 0.05911041]]\n","\n","Average MAE Loss:\n","[0.05016652 0.07968319 0.05033156 0.07730835]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04422075 0.05245245]\n"," [0.10212074 0.05724563]\n"," [0.04651362 0.05571715]\n"," [0.0911824  0.05246496]]\n","\n","Average MAE Loss:\n","[0.0483366  0.07968319 0.05111538 0.07182368]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05996395 0.06891952]\n"," [0.08422329 0.08708352]\n"," [0.05916896 0.06483501]\n"," [0.06134298 0.06794254]]\n","\n","Average MAE Loss:\n","[0.06444173 0.0856534  0.06200199 0.06464276]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05575279 0.06510917]\n"," [0.08422329 0.08708352]\n"," [0.05409828 0.06449741]\n"," [0.06021516 0.06799613]]\n","\n","Average MAE Loss:\n","[0.06043098 0.0856534  0.05929784 0.06410564]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05368666 0.06337042]\n"," [0.08422329 0.08708352]\n"," [0.04961918 0.05932214]\n"," [0.05974342 0.06719104]]\n","\n","Average MAE Loss:\n","[0.05852854 0.0856534  0.05447066 0.06346723]\n","\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0515646  0.0617125 ]\n"," [0.08422328 0.08708352]\n"," [0.04680151 0.05336515]\n"," [0.05832919 0.06555648]]\n","\n","Average MAE Loss:\n","[0.05663855 0.0856534  0.05008333 0.06194284]\n","\n"]}]},{"cell_type":"code","source":["# seed = 10 table = 2/8 fixed folds 2+1 layers adj 140\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='TAG_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"],"metadata":{"id":"Aoy8JGs4WXto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"boAUzmyY1b45"},"source":["## GAT dream"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFwnGZ9q1b5B"},"outputs":[],"source":["from torch_geometric.nn import GATConv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJ1he4kz1b5B"},"outputs":[],"source":["class GAT_dream(torch.nn.Module):\n","    def __init__(self,device):\n","        super(GAT_dream, self).__init__()\n","        self.in_channels = 10\n","        self.hidden_channels = 64\n","        self.heads=8\n","        self.out_head = 1\n","        self.device=device\n","        self.num_nodes = 35\n","\n","\n","        self.conv1 = GATConv(self.in_channels, self.hidden_channels, heads=self.heads, dropout=0.6)\n","        self.conv2 = GATConv(self.hidden_channels*self.heads, self.hidden_channels, concat=False,\n","                             heads=self.out_head, dropout=0.6)\n","        self.fc = nn.Linear(self.hidden_channels,self.num_nodes)\n","        self.fc_dream = nn.Sequential(nn.Linear(35,10), nn.ELU(), nn.Linear(10,10), nn.ELU())\n","\n","\n","    def forward(self, adj_matrix):\n","\n","        # learning suitable node representation from adj matrix\n","        x = self.fc_dream(adj_matrix)\n","        data = adj_matrix_to_pytorch_geometric_data(adj_matrix,self.device)\n","        edge_index = data.edge_index\n","\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        data = self.conv1(x, edge_index)\n","        data = F.elu(data)\n","        data = F.dropout(data, p=0.6, training=self.training)\n","        data = self.conv2(data, edge_index)\n","        out = F.relu(self.fc(data))\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9SRbiVZ1b5C"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zdvS0zx1b5C"},"outputs":[],"source":["torch.use_deterministic_algorithms(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690540144598,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"6NDFeain1b5C","outputId":"5e2ce9a0-79dd-433e-f578-5d8c973ac319"},"outputs":[{"data":{"text/plain":["tensor(34.4247, device='cuda:0', grad_fn=<SumBackward0>)"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["data = dataset[0][0].to(device)\n","gat = GAT_dream(device).to(device)\n","gat(data).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1690540157682,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"LFvr5dVr1b5C","outputId":"80022d22-0f7e-4c3d-f4ff-fcc3cb9b4abc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 42251\n","Trainable parameters: 42251\n"]}],"source":["model = GAT(device)\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiB2OAMT1b5C"},"outputs":[],"source":["device = torch.device('cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZWPNdUf1b5C"},"outputs":[],"source":["class Hospital_gat_dream():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = GAT_dream(device=device).to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kq9AJIhG1b5D"},"outputs":[],"source":["def train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = False\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_gat_dream(args,device))\n","                  print('GAT dream')\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":893423,"status":"error","timestamp":1690541192752,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"ETYGBzvB1b5D","outputId":"281ed29d-3dbc-455d-f056-3c4cc5af9988"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","GAT dream\n","GAT dream\n","GAT dream\n","GAT dream\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.06138949 0.05815226]\n"," [0.0784388  0.07544469]\n"," [0.06316184 0.06054214]\n"," [0.06397959 0.061189  ]]\n","\n","Average MAE Loss:\n","[0.05977088 0.07694175 0.06185199 0.06258429]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06061132 0.05789622]\n"," [0.0784388  0.07544469]\n"," [0.06204481 0.05905726]\n"," [0.0627647  0.06066238]]\n","\n","Average MAE Loss:\n","[0.05925377 0.07694175 0.06055104 0.06171354]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.05979239 0.05687716]\n"," [0.0784388  0.07544469]\n"," [0.06133087 0.05876619]\n"," [0.0623493  0.06008061]]\n","\n","Average MAE Loss:\n","[0.05833477 0.07694175 0.06004853 0.06121495]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.06034056 0.05761187]\n"," [0.0784388  0.07544469]\n"," [0.06131377 0.05829144]\n"," [0.06243274 0.06028367]]\n","\n","Average MAE Loss:\n","[0.05897622 0.07694175 0.0598026  0.06135821]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.05932696 0.05674029]\n"," [0.0784388  0.07544469]\n"," [0.0611453  0.05853945]\n"," [0.06240481 0.06034547]]\n","\n","Average MAE Loss:\n","[0.05803362 0.07694175 0.05984237 0.06137514]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.05921668 0.05673197]\n"," [0.0784388  0.07544469]\n"," [0.06142271 0.05874606]\n"," [0.06229082 0.06041909]]\n","\n","Average MAE Loss:\n","[0.05797432 0.07694175 0.06008438 0.06135496]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.05918933 0.05681399]\n"," [0.0784388  0.07544469]\n"," [0.06096097 0.05835616]\n"," [0.06217572 0.06020639]]\n","\n","Average MAE Loss:\n","[0.05800166 0.07694175 0.05965857 0.06119106]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06440833 0.06194883]\n"," [0.06217572 0.06020639]\n"," [0.05968444 0.05711573]\n"," [0.06116295 0.05835871]]\n","\n","Average MAE Loss:\n","[0.06317858 0.06119106 0.05840009 0.05976083]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06345444 0.06123577]\n"," [0.06217572 0.06020639]\n"," [0.05916922 0.05678155]\n"," [0.06081668 0.0583196 ]]\n","\n","Average MAE Loss:\n","[0.06234511 0.06119106 0.05797539 0.05956814]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06308405 0.0608575 ]\n"," [0.06217572 0.06020639]\n"," [0.05843938 0.05591569]\n"," [0.06064201 0.05792355]]\n","\n","Average MAE Loss:\n","[0.06197078 0.06119106 0.05717753 0.05928278]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06292354 0.06054514]\n"," [0.06217572 0.06020639]\n"," [0.0581984  0.05558555]\n"," [0.06090415 0.05816532]]\n","\n","Average MAE Loss:\n","[0.06173434 0.06119106 0.05689197 0.05953473]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0628428  0.06048578]\n"," [0.06217572 0.06020639]\n"," [0.05780271 0.05550412]\n"," [0.06074873 0.05790998]]\n","\n","Average MAE Loss:\n","[0.06166429 0.06119106 0.05665341 0.05932935]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06278602 0.06044426]\n"," [0.06217572 0.06020639]\n"," [0.05761725 0.05582983]\n"," [0.06007488 0.05747796]]\n","\n","Average MAE Loss:\n","[0.06161514 0.06119106 0.05672354 0.05877642]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0628279  0.06048163]\n"," [0.06217572 0.06020639]\n"," [0.0578042  0.05537747]\n"," [0.05994482 0.05753036]]\n","\n","Average MAE Loss:\n","[0.06165476 0.06119106 0.05659083 0.05873759]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06668237 0.0637706 ]\n"," [0.07214824 0.06918426]\n"," [0.06098126 0.05896078]\n"," [0.0604978  0.05798591]]\n","\n","Average MAE Loss:\n","[0.06522649 0.07066625 0.05997102 0.05924185]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.0613043  0.05887893]\n"," [0.07214824 0.06918426]\n"," [0.05934833 0.05698953]\n"," [0.05676633 0.05434457]]\n","\n","Average MAE Loss:\n","[0.06009161 0.07066625 0.05816893 0.05555545]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05986657 0.05768541]\n"," [0.07214824 0.06918426]\n"," [0.05828681 0.0558995 ]\n"," [0.05651404 0.05399643]]\n","\n","Average MAE Loss:\n","[0.05877599 0.07066625 0.05709316 0.05525524]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05875015 0.05632589]\n"," [0.07214824 0.06918426]\n"," [0.05779163 0.05550914]\n"," [0.05649859 0.05409249]]\n","\n","Average MAE Loss:\n","[0.05753802 0.07066625 0.05665039 0.05529554]\n","\n","Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.05854337 0.05610741]\n"," [0.07214824 0.06918426]\n"," [0.05790905 0.0556182 ]\n"," [0.05642797 0.05401075]]\n","\n","Average MAE Loss:\n","[0.05732539 0.07066625 0.05676363 0.05521936]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.05853214 0.05613192]\n"," [0.07214824 0.06918426]\n"," [0.05796119 0.0556506 ]\n"," [0.05646272 0.05405269]]\n","\n","Average MAE Loss:\n","[0.05733203 0.07066625 0.0568059  0.05525771]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.05850517 0.05611096]\n"," [0.07214824 0.06918426]\n"," [0.05789832 0.05559981]\n"," [0.05644521 0.05403544]]\n","\n","Average MAE Loss:\n","[0.05730806 0.07066625 0.05674906 0.05524033]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06732628 0.06437189]\n"," [0.05644521 0.05403544]\n"," [0.05851093 0.05624943]\n"," [0.05768243 0.05522089]]\n","\n","Average MAE Loss:\n","[0.06584909 0.05524033 0.05738018 0.05645166]\n","\n","Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0619601  0.05942419]\n"," [0.05644521 0.05403544]\n"," [0.05859665 0.05636204]\n"," [0.05787246 0.05548331]]\n","\n","Average MAE Loss:\n","[0.06069214 0.05524033 0.05747935 0.05667789]\n","\n","Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05984941 0.05766326]\n"," [0.05644521 0.05403544]\n"," [0.05869718 0.05646062]\n"," [0.05737454 0.05497411]]\n","\n","Average MAE Loss:\n","[0.05875633 0.05524033 0.0575789  0.05617432]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05875213 0.05632064]\n"," [0.05644521 0.05403544]\n"," [0.05864646 0.056398  ]\n"," [0.05658437 0.05420895]]\n","\n","Average MAE Loss:\n","[0.05753639 0.05524033 0.05752223 0.05539666]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.05856381 0.05609616]\n"," [0.05644521 0.05403544]\n"," [0.05863507 0.05637864]\n"," [0.05649725 0.05407894]]\n","\n","Average MAE Loss:\n","[0.05732999 0.05524033 0.05750686 0.05528809]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.05852847 0.05609215]\n"," [0.05644521 0.05403544]\n"," [0.05865139 0.05640413]\n"," [0.05642349 0.05399972]]\n","\n","Average MAE Loss:\n","[0.05731031 0.05524033 0.05752776 0.0552116 ]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.05785891 0.05540664]\n"," [0.05644521 0.05403544]\n"," [0.05862957 0.05638833]\n"," [0.05643334 0.05402315]]\n","\n","Average MAE Loss:\n","[0.05663278 0.05524033 0.05750895 0.05522825]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05655076 0.05416935]\n"," [0.05674521 0.05436415]\n"," [0.05655952 0.05421388]\n"," [0.05645662 0.05406725]]\n","\n","Average MAE Loss:\n","[0.05536005 0.05555468 0.0553867  0.05526193]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05654826 0.05415824]\n"," [0.05674521 0.05436415]\n"," [0.05654835 0.05421751]\n"," [0.05647458 0.05403185]]\n","\n","Average MAE Loss:\n","[0.05535325 0.05555468 0.05538293 0.05525322]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05656209 0.05415137]\n"," [0.05674521 0.05436415]\n"," [0.05661569 0.05428746]\n"," [0.05639724 0.05396985]]\n","\n","Average MAE Loss:\n","[0.05535673 0.05555468 0.05545157 0.05518355]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05653266 0.05410864]\n"," [0.05674521 0.05436415]\n"," [0.05657758 0.05425333]\n"," [0.05646295 0.05403832]]\n","\n","Average MAE Loss:\n","[0.05532065 0.05555468 0.05541545 0.05525064]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05650247 0.05407551]\n"," [0.05674521 0.05436415]\n"," [0.05660157 0.05428101]\n"," [0.05533951 0.05307745]]\n","\n","Average MAE Loss:\n","[0.05528899 0.05555468 0.05544129 0.05420848]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05651976 0.05409334]\n"," [0.05674521 0.05436415]\n"," [0.05659412 0.05427533]\n"," [0.05533757 0.05305499]]\n","\n","Average MAE Loss:\n","[0.05530655 0.05555468 0.05543473 0.05419628]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05652949 0.05410163]\n"," [0.05674521 0.05436415]\n"," [0.05658645 0.0542676 ]\n"," [0.05543353 0.05313956]]\n","\n","Average MAE Loss:\n","[0.05531556 0.05555468 0.05542703 0.05428655]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05658599 0.05421668]\n"," [0.05543353 0.05313956]\n"," [0.05651385 0.05409931]\n"," [0.05637507 0.05392202]]\n","\n","Average MAE Loss:\n","[0.05540133 0.05428655 0.05530658 0.05514855]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05655725 0.05416611]\n"," [0.05543353 0.05313956]\n"," [0.05651136 0.05411665]\n"," [0.05646262 0.05402716]]\n","\n","Average MAE Loss:\n","[0.05536168 0.05428655 0.05531401 0.05524489]\n","\n","Epoch 00037: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05655916 0.05415418]\n"," [0.05543353 0.05313956]\n"," [0.05651052 0.05413851]\n"," [0.05640522 0.05399734]]\n","\n","Average MAE Loss:\n","[0.05535667 0.05428655 0.05532451 0.05520128]\n","\n","Epoch 00038: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05656157 0.05414776]\n"," [0.05543353 0.05313956]\n"," [0.05653998 0.05418486]\n"," [0.05644918 0.05405197]]\n","\n","Average MAE Loss:\n","[0.05535466 0.05428655 0.05536242 0.05525058]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05654688 0.05413033]\n"," [0.05543353 0.05313956]\n"," [0.05655782 0.05421281]\n"," [0.05650019 0.05409705]]\n","\n","Average MAE Loss:\n","[0.05533861 0.05428655 0.05538532 0.05529862]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05653412 0.05411444]\n"," [0.05543353 0.05313956]\n"," [0.05655876 0.05421818]\n"," [0.05650039 0.05409434]]\n","\n","Average MAE Loss:\n","[0.05532428 0.05428655 0.05538847 0.05529737]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05652999 0.05410791]\n"," [0.05543353 0.05313956]\n"," [0.05655689 0.05421932]\n"," [0.05648198 0.05406458]]\n","\n","Average MAE Loss:\n","[0.05531895 0.05428655 0.0553881  0.05527328]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05613669 0.05374439]\n"," [0.05627954 0.05388758]\n"," [0.05611997 0.05373533]\n"," [0.05573738 0.05337804]]\n","\n","Average MAE Loss:\n","[0.05494054 0.05508356 0.05492765 0.05455771]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05597148 0.05358433]\n"," [0.05627954 0.05388758]\n"," [0.0559523  0.05358438]\n"," [0.05539772 0.05312233]]\n","\n","Average MAE Loss:\n","[0.0547779  0.05508356 0.05476834 0.05426002]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05584567 0.05347471]\n"," [0.05627954 0.05388758]\n"," [0.05582961 0.05348554]\n"," [0.05536535 0.05308108]]\n","\n","Average MAE Loss:\n","[0.05466019 0.05508356 0.05465757 0.05422322]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05575463 0.05339969]\n"," [0.05627954 0.05388758]\n"," [0.05574313 0.05342346]\n"," [0.05535918 0.05306993]]\n","\n","Average MAE Loss:\n","[0.05457716 0.05508356 0.0545833  0.05421455]\n","\n","Epoch 00046: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05568259 0.05334417]\n"," [0.05627954 0.05388758]\n"," [0.05568053 0.05338497]\n"," [0.05535442 0.0530632 ]]\n","\n","Average MAE Loss:\n","[0.05451338 0.05508356 0.05453275 0.05420881]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05562831 0.05330449]\n"," [0.05627954 0.05388758]\n"," [0.05563921 0.05336797]\n"," [0.05535654 0.05306635]]\n","\n","Average MAE Loss:\n","[0.0544664  0.05508356 0.05450359 0.05421144]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.0555868  0.05327661]\n"," [0.05627954 0.05388758]\n"," [0.05560641 0.053354  ]\n"," [0.05536575 0.0530772 ]]\n","\n","Average MAE Loss:\n","[0.0544317  0.05508356 0.0544802  0.05422147]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05615815 0.05376731]\n"," [0.05536575 0.0530772 ]\n"," [0.055548   0.05325351]\n"," [0.0554984  0.05325639]]\n","\n","Average MAE Loss:\n","[0.05496273 0.05422148 0.05440076 0.05437739]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05602484 0.05363505]\n"," [0.05536575 0.0530772 ]\n"," [0.05551851 0.05324244]\n"," [0.05543776 0.05319761]]\n","\n","Average MAE Loss:\n","[0.05482995 0.05422147 0.05438048 0.05431769]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05591792 0.05353746]\n"," [0.05536575 0.0530772 ]\n"," [0.05549307 0.05323064]\n"," [0.05540105 0.05315399]]\n","\n","Average MAE Loss:\n","[0.05472769 0.05422147 0.05436186 0.05427752]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05582777 0.05345989]\n"," [0.05536575 0.0530772 ]\n"," [0.05547789 0.05322277]\n"," [0.05537567 0.05312211]]\n","\n","Average MAE Loss:\n","[0.05464383 0.05422147 0.05435033 0.05424889]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05579151 0.05343082]\n"," [0.05536575 0.0530772 ]\n"," [0.05547721 0.05322763]\n"," [0.0553555  0.05309765]]\n","\n","Average MAE Loss:\n","[0.05461116 0.05422148 0.05435242 0.05422658]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00054: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05576146 0.05340739]\n"," [0.05536575 0.0530772 ]\n"," [0.05547737 0.05323169]\n"," [0.05535166 0.05309223]]\n","\n","Average MAE Loss:\n","[0.05458442 0.05422148 0.05435453 0.05422195]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0557366  0.053388  ]\n"," [0.05536575 0.0530772 ]\n"," [0.05547053 0.05322934]\n"," [0.05534944 0.05308913]]\n","\n","Average MAE Loss:\n","[0.0545623  0.05422148 0.05434994 0.05421928]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05540272 0.05313442]\n"," [0.05540712 0.05314146]\n"," [0.05540406 0.05314391]\n"," [0.05539613 0.05312791]]\n","\n","Average MAE Loss:\n","[0.05426857 0.05427429 0.05427398 0.05426202]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05540024 0.05312886]\n"," [0.05540712 0.05314146]\n"," [0.05540911 0.05315387]\n"," [0.05538601 0.05311653]]\n","\n","Average MAE Loss:\n","[0.05426455 0.05427429 0.05428149 0.05425127]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00058: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05539923 0.05312545]\n"," [0.05540712 0.05314146]\n"," [0.0554172  0.05316827]\n"," [0.05537876 0.05310819]]\n","\n","Average MAE Loss:\n","[0.05426234 0.05427429 0.05429273 0.05424348]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05539995 0.05312431]\n"," [0.05540712 0.05314146]\n"," [0.05542597 0.05318326]\n"," [0.05537412 0.05310248]]\n","\n","Average MAE Loss:\n","[0.05426213 0.05427429 0.05430462 0.0542383 ]\n","\n","\n","epochs finished with time:536.678633928299\n","\n","[[0.05539995 0.05312431]\n"," [0.05540712 0.05314146]\n"," [0.05542597 0.05318326]\n"," [0.05537412 0.05310248]]\n","00:09:18.758402274999753\n","------------------------------------Fold [2/5]-----------------------------------------\n","GAT dream\n","GAT dream\n","GAT dream\n","GAT dream\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08776168 0.08670674]\n"," [0.10661513 0.10472541]\n"," [0.08837341 0.08814111]\n"," [0.0898912  0.08920274]]\n","\n","Average MAE Loss:\n","[0.08723421 0.10567027 0.08825726 0.08954697]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08731469 0.08562504]\n"," [0.10661513 0.10472541]\n"," [0.08459665 0.08455428]\n"," [0.08890479 0.08869753]]\n","\n","Average MAE Loss:\n","[0.08646986 0.10567027 0.08457547 0.08880116]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08671284 0.08573135]\n"," [0.10661513 0.10472541]\n"," [0.084408   0.08473267]\n"," [0.08920957 0.08893926]]\n","\n","Average MAE Loss:\n","[0.08622209 0.10567027 0.08457034 0.08907441]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0877026  0.08642734]\n"," [0.10661513 0.10472541]\n"," [0.08416206 0.08438684]\n"," [0.08801569 0.08791955]]\n","\n","Average MAE Loss:\n","[0.08706497 0.10567027 0.08427445 0.08796762]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08654385 0.08519567]\n"," [0.10661513 0.10472541]\n"," [0.08400136 0.08424064]\n"," [0.08803938 0.08807094]]\n","\n","Average MAE Loss:\n","[0.08586976 0.10567027 0.084121   0.08805516]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08688203 0.08538268]\n"," [0.10661513 0.10472541]\n"," [0.08508667 0.08508968]\n"," [0.08781894 0.08781526]]\n","\n","Average MAE Loss:\n","[0.08613235 0.10567027 0.08508818 0.0878171 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08647657 0.08505918]\n"," [0.10661513 0.10472541]\n"," [0.08512228 0.08498668]\n"," [0.08783796 0.08776997]]\n","\n","Average MAE Loss:\n","[0.08576787 0.10567027 0.08505448 0.08780397]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.09114474 0.09085109]\n"," [0.08783796 0.08776997]\n"," [0.08744739 0.08578917]\n"," [0.08615714 0.08625217]]\n","\n","Average MAE Loss:\n","[0.09099791 0.08780397 0.08661828 0.08620465]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08916104 0.08951862]\n"," [0.08783796 0.08776997]\n"," [0.08677554 0.08558464]\n"," [0.08480327 0.08522779]]\n","\n","Average MAE Loss:\n","[0.08933983 0.08780397 0.08618009 0.08501553]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.08939757 0.08925132]\n"," [0.08783796 0.08776997]\n"," [0.08613998 0.08516054]\n"," [0.08362921 0.08395386]]\n","\n","Average MAE Loss:\n","[0.08932445 0.08780397 0.08565026 0.08379154]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08906733 0.0888981 ]\n"," [0.08783796 0.08776997]\n"," [0.08634676 0.08558689]\n"," [0.08348782 0.08395383]]\n","\n","Average MAE Loss:\n","[0.08898272 0.08780397 0.08596682 0.08372082]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08944039 0.08915928]\n"," [0.08783796 0.08776997]\n"," [0.08613937 0.08555034]\n"," [0.08344634 0.0838442 ]]\n","\n","Average MAE Loss:\n","[0.08929983 0.08780397 0.08584485 0.08364527]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.08926624 0.08897513]\n"," [0.08783796 0.08776997]\n"," [0.08502507 0.08455881]\n"," [0.08341221 0.08394423]]\n","\n","Average MAE Loss:\n","[0.08912069 0.08780397 0.08479194 0.08367822]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0893371  0.08906712]\n"," [0.08783796 0.08776997]\n"," [0.08548068 0.08550773]\n"," [0.08349254 0.08397379]]\n","\n","Average MAE Loss:\n","[0.08920211 0.08780397 0.08549421 0.08373317]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.08996843 0.0893185 ]\n"," [0.10137439 0.10005689]\n"," [0.09589674 0.09469957]\n"," [0.08723452 0.08665345]]\n","\n","Average MAE Loss:\n","[0.08964347 0.10071564 0.09529816 0.08694398]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.08579631 0.08562863]\n"," [0.10137439 0.10005689]\n"," [0.08881783 0.0881563 ]\n"," [0.08341284 0.08361304]]\n","\n","Average MAE Loss:\n","[0.08571247 0.10071564 0.08848706 0.08351294]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.08470747 0.08453473]\n"," [0.10137439 0.10005689]\n"," [0.08518451 0.08506907]\n"," [0.08226238 0.08239973]]\n","\n","Average MAE Loss:\n","[0.0846211  0.10071564 0.08512679 0.08233106]\n","\n","Epoch 00017: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.08433235 0.08417447]\n"," [0.10137439 0.10005689]\n"," [0.084926   0.0849106 ]\n"," [0.08189087 0.08211319]]\n","\n","Average MAE Loss:\n","[0.08425341 0.10071564 0.0849183  0.08200203]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.08436253 0.08423113]\n"," [0.10137439 0.10005689]\n"," [0.08495771 0.08498477]\n"," [0.0809351  0.08097255]]\n","\n","Average MAE Loss:\n","[0.08429683 0.10071564 0.08497124 0.08095383]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.08435926 0.08422303]\n"," [0.10137439 0.10005689]\n"," [0.0849938  0.08504789]\n"," [0.08104903 0.08105284]]\n","\n","Average MAE Loss:\n","[0.08429115 0.10071564 0.08502084 0.08105093]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.08402224 0.08390779]\n"," [0.10137439 0.10005689]\n"," [0.0849572  0.08504662]\n"," [0.08096524 0.08097816]]\n","\n","Average MAE Loss:\n","[0.08396502 0.10071564 0.08500191 0.0809717 ]\n","\n","Epoch 00021: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09557385 0.09441342]\n"," [0.08096524 0.08097816]\n"," [0.08376493 0.08368664]\n"," [0.0843351  0.08438174]]\n","\n","Average MAE Loss:\n","[0.09499364 0.0809717  0.08372579 0.08435842]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.08733397 0.08688092]\n"," [0.08096524 0.08097816]\n"," [0.08348674 0.08344172]\n"," [0.08054514 0.08060523]]\n","\n","Average MAE Loss:\n","[0.08710744 0.0809717  0.08346423 0.08057518]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.08334765 0.08317052]\n"," [0.08096524 0.08097816]\n"," [0.08327901 0.08326774]\n"," [0.08007528 0.08018901]]\n","\n","Average MAE Loss:\n","[0.08325908 0.0809717  0.08327337 0.08013214]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.08329512 0.08303806]\n"," [0.08096524 0.08097816]\n"," [0.0831603  0.08317083]\n"," [0.08027298 0.08035403]]\n","\n","Average MAE Loss:\n","[0.08316659 0.0809717  0.08316556 0.08031351]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.08338934 0.08319782]\n"," [0.08096524 0.08097816]\n"," [0.08308367 0.08311377]\n"," [0.08007215 0.08020409]]\n","\n","Average MAE Loss:\n","[0.08329358 0.0809717  0.08309872 0.08013812]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.08316874 0.08304534]\n"," [0.08096524 0.08097816]\n"," [0.08303778 0.08308758]\n"," [0.08018777 0.08031872]]\n","\n","Average MAE Loss:\n","[0.08310704 0.0809717  0.08306268 0.08025324]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.08321518 0.08307842]\n"," [0.08096524 0.08097816]\n"," [0.08304085 0.08309968]\n"," [0.08013083 0.08027341]]\n","\n","Average MAE Loss:\n","[0.0831468  0.0809717  0.08307026 0.08020212]\n","\n","Epoch 00028: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.08016831 0.08011912]\n"," [0.08177434 0.08181893]\n"," [0.08122533 0.0812503 ]\n"," [0.07999228 0.08002722]]\n","\n","Average MAE Loss:\n","[0.08014371 0.08179663 0.08123781 0.08000975]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.08025465 0.08004255]\n"," [0.08177434 0.08181893]\n"," [0.08063225 0.08063838]\n"," [0.0800315  0.08022757]]\n","\n","Average MAE Loss:\n","[0.0801486  0.08179663 0.08063531 0.08012953]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.08026654 0.08009279]\n"," [0.08177434 0.08181893]\n"," [0.08032697 0.08033077]\n"," [0.08017951 0.08031757]]\n","\n","Average MAE Loss:\n","[0.08017966 0.08179663 0.08032887 0.08024854]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.0802471  0.08010489]\n"," [0.08177434 0.08181893]\n"," [0.08020797 0.08020849]\n"," [0.08013967 0.08023054]]\n","\n","Average MAE Loss:\n","[0.080176   0.08179663 0.08020823 0.08018511]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.08026149 0.08013248]\n"," [0.08177434 0.08181893]\n"," [0.08016884 0.08017211]\n"," [0.0801785  0.08028654]]\n","\n","Average MAE Loss:\n","[0.08019699 0.08179663 0.08017047 0.08023252]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.08022055 0.08008828]\n"," [0.08177434 0.08181893]\n"," [0.08017741 0.08018506]\n"," [0.08011091 0.08022268]]\n","\n","Average MAE Loss:\n","[0.08015441 0.08179663 0.08018123 0.0801668 ]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.08021237 0.08007753]\n"," [0.08177434 0.08181893]\n"," [0.08020997 0.08022059]\n"," [0.08006485 0.08018094]]\n","\n","Average MAE Loss:\n","[0.08014495 0.08179663 0.08021528 0.08012289]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.0807393  0.08069836]\n"," [0.08006485 0.08018094]\n"," [0.0802157  0.08008882]\n"," [0.08031366 0.08035212]]\n","\n","Average MAE Loss:\n","[0.08071883 0.08012289 0.08015226 0.08033289]\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-7adf3fb8b0fe>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GAT_dream_4D-FED-GNN++'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_gnns_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbosity_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-90-49510104076b>\u001b[0m in \u001b[0;36mtrain_gnns_final\u001b[0;34m(args, dataset, seed, ratio, verbose, train_validate_verbose, train_validate_verbosity_epochs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;31m# Train the current hospital at the current timepoint for 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mmae_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch_gnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_hospital\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0;31m# Updating the hospital\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-7904a58cbf49>\u001b[0m in \u001b[0;36mtrain_one_epoch_gnns\u001b[0;34m(args, hospital, train_data, table_hospital)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhospital\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-77-c3f89663b5f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, adj_matrix)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# learning suitable node representation from adj matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_dream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_matrix_to_pytorch_geometric_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-ba2b9f936167>\u001b[0m in \u001b[0;36madj_matrix_to_pytorch_geometric_data\u001b[0;34m(adj_matrix, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# calculate the node features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_features_from_adj_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madj_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-67cd9fe2c87a>\u001b[0m in \u001b[0;36mnode_features_from_adj_matrix\u001b[0;34m(adj_matrix, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# Compute the clustering coefficient for each node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mclustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;31m# Compute the closeness centrality for each node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/classes/backends.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0;34mf\"'{name}' not implemented by {plugin_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     )\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# Keep a handle to the original function to use when testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/cluster.py\u001b[0m in \u001b[0;36mclustering\u001b[0;34m(G, nodes, weight)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mtd_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_weighted_triangles_and_degree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mclusterc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mtd_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triangles_and_degree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/cluster.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mtd_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_weighted_triangles_and_degree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mclusterc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mtd_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triangles_and_degree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/cluster.py\u001b[0m in \u001b[0;36m_weighted_triangles_and_degree_iter\u001b[0;34m(G, nodes, weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mwij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             weighted_triangles += sum(\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwij\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minbrs\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mjnbrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             )\n\u001b[1;32m    122\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_triangles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/cluster.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mwij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             weighted_triangles += sum(\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwij\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minbrs\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mjnbrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             )\n\u001b[1;32m    122\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_triangles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/cluster.py\u001b[0m in \u001b[0;36mwt\u001b[0;34m(u, v)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes_nbrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# table = 4/8 fixed folds\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='GAT_dream_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"gCeqTVgMstrm"},"source":["## MetaLayer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9n9j1z6tJfd"},"outputs":[],"source":["from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n","from torch_geometric.utils import scatter\n","from torch_geometric.nn import MetaLayer\n","from typing import Optional, Tuple\n","from torch import Tensor\n","from torch_geometric.utils import dense_to_sparse\n","import networkx as nx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28DIOItGQ_TZ"},"outputs":[],"source":["def get_adjacency_matrix(num_nodes, edge_indices, edge_weights,device):\n","    # Initialize an empty adjacency matrix\n","    adjacency_matrix = torch.zeros((num_nodes, num_nodes))\n","\n","    # Fill in the adjacency matrix\n","    for ((node1, node2), weight) in zip(edge_indices.T, edge_weights):\n","        adjacency_matrix[node1, node2] = weight\n","\n","    return adjacency_matrix.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8kfL8Ul1HSs"},"outputs":[],"source":["class MetaLayer(torch.nn.Module):\n","    r\"\"\"A meta layer for building any kind of graph network, inspired by the\n","    `\"Relational Inductive Biases, Deep Learning, and Graph Networks\"\n","    <https://arxiv.org/abs/1806.01261>`_ paper.\n","\n","    A graph network takes a graph as input and returns an updated graph as\n","    output (with same connectivity).\n","    The input graph has node features :obj:`x`, edge features :obj:`edge_attr`\n","    as well as graph-level features :obj:`u`.\n","    The output graph has the same structure, but updated features.\n","\n","    Edge features, node features as well as global features are updated by\n","    calling the modules :obj:`edge_model`, :obj:`node_model` and\n","    :obj:`global_model`, respectively.\n","\n","    To allow for batch-wise graph processing, all callable functions take an\n","    additional argument :obj:`batch`, which determines the assignment of\n","    edges or nodes to their specific graphs.\n","\n","    Args:\n","        edge_model (torch.nn.Module, optional): A callable which updates a\n","            graph's edge features based on its source and target node features,\n","            its current edge features and its global features.\n","            (default: :obj:`None`)\n","        node_model (torch.nn.Module, optional): A callable which updates a\n","            graph's node features based on its current node features, its graph\n","            connectivity, its edge features and its global features.\n","            (default: :obj:`None`)\n","        global_model (torch.nn.Module, optional): A callable which updates a\n","            graph's global features based on its node features, its graph\n","            connectivity, its edge features and its current global features.\n","            (default: :obj:`None`)\n","\n","    .. code-block:: python\n","\n","        from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n","        from torch_geometric.utils import scatter\n","        from torch_geometric.nn import MetaLayer\n","\n","        class EdgeModel(torch.nn.Module):\n","            def __init__(self):\n","                super().__init__()\n","                self.edge_mlp = Seq(Lin(..., ...), ReLU(), Lin(..., ...))\n","\n","            def forward(self, src, dst, edge_attr, u, batch):\n","                # src, dst: [E, F_x], where E is the number of edges.\n","                # edge_attr: [E, F_e]\n","                # u: [B, F_u], where B is the number of graphs.\n","                # batch: [E] with max entry B - 1.\n","                out = torch.cat([src, dst, edge_attr, u[batch]], 1)\n","                return self.edge_mlp(out)\n","\n","        class NodeModel(torch.nn.Module):\n","            def __init__(self):\n","                super().__init__()\n","                self.node_mlp_1 = Seq(Lin(..., ...), ReLU(), Lin(..., ...))\n","                self.node_mlp_2 = Seq(Lin(..., ...), ReLU(), Lin(..., ...))\n","\n","            def forward(self, x, edge_index, edge_attr, u, batch):\n","                # x: [N, F_x], where N is the number of nodes.\n","                # edge_index: [2, E] with max entry N - 1.\n","                # edge_attr: [E, F_e]\n","                # u: [B, F_u]\n","                # batch: [N] with max entry B - 1.\n","                row, col = edge_index\n","                out = torch.cat([x[row], edge_attr], dim=1)\n","                out = self.node_mlp_1(out)\n","                out = scatter(out, col, dim=0, dim_size=x.size(0),\n","                              reduce='mean')\n","                out = torch.cat([x, out, u[batch]], dim=1)\n","                return self.node_mlp_2(out)\n","\n","        class GlobalModel(torch.nn.Module):\n","            def __init__(self):\n","                super().__init__()\n","                self.global_mlp = Seq(Lin(..., ...), ReLU(), Lin(..., ...))\n","\n","            def forward(self, x, edge_index, edge_attr, u, batch):\n","                # x: [N, F_x], where N is the number of nodes.\n","                # edge_index: [2, E] with max entry N - 1.\n","                # edge_attr: [E, F_e]\n","                # u: [B, F_u]\n","                # batch: [N] with max entry B - 1.\n","                out = torch.cat([\n","                    u,\n","                    scatter(x, batch, dim=0, reduce='mean'),\n","                ], dim=1)\n","                return self.global_mlp(out)\n","\n","        op = MetaLayer(EdgeModel(), NodeModel(), GlobalModel())\n","        x, edge_attr, u = op(x, edge_index, edge_attr, u, batch)\n","    \"\"\"\n","    def __init__(\n","        self,\n","        edge_model: Optional[torch.nn.Module] = None,\n","        node_model: Optional[torch.nn.Module] = None,\n","        global_model: Optional[torch.nn.Module] = None,\n","    ):\n","        super().__init__()\n","        self.edge_model = edge_model\n","        self.node_model = node_model\n","        self.global_model = global_model\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n","        for item in [self.node_model, self.edge_model, self.global_model]:\n","            if hasattr(item, 'reset_parameters'):\n","                item.reset_parameters()\n","\n","\n","    def forward(\n","        self,\n","        x: Tensor,\n","        edge_index: Tensor,\n","        edge_attr: Optional[Tensor] = None,\n","        u: Optional[Tensor] = None,\n","        batch: Optional[Tensor] = None,\n","    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n","        r\"\"\"\n","        Args:\n","            x (torch.Tensor): The node features.\n","            edge_index (torch.Tensor): The edge indices.\n","            edge_attr (torch.Tensor, optional): The edge features.\n","                (default: :obj:`None`)\n","            u (torch.Tensor, optional): The global graph features.\n","                (default: :obj:`None`)\n","            batch (torch.Tensor, optional): The batch vector\n","                :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns\n","                each node to a specific graph. (default: :obj:`None`)\n","        \"\"\"\n","        row = edge_index[0]\n","        col = edge_index[1]\n","\n","        if self.edge_model is not None:\n","            edge_attr = self.edge_model(x[row], x[col], edge_attr)\n","\n","\n","        if self.node_model is not None:\n","            x = self.node_model(x, edge_index, edge_attr)\n","\n","        if self.global_model is not None:\n","            u = self.global_model(x, edge_index, edge_attr, u, batch)\n","\n","        return x, edge_attr, u\n","\n","\n","    def __repr__(self) -> str:\n","        return (f'{self.__class__.__name__}(\\n'\n","                f'  edge_model={self.edge_model},\\n'\n","                f'  node_model={self.node_model},\\n'\n","                f'  global_model={self.global_model}\\n'\n","                f')')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVTNyW85tMtO"},"outputs":[],"source":["class EdgeModel(torch.nn.Module):\n","    def __init__(self,input=11,out1=15,out2=20):\n","        super().__init__()\n","        self.edge_mlp = Seq(Lin(input, out1), nn.ELU(), Lin(out1, out2))\n","\n","    def forward(self, src, dst, edge_attr):\n","        # src, dst: [E, F_x], where E is the number of edges.\n","        # edge_attr: [E, F_e]\n","        # u: [B, F_u], where B is the number of graphs.\n","        # batch: [E] with max entry B - 1.\n","        out = torch.cat([src, dst, edge_attr], 1)\n","        return self.edge_mlp(out)\n","\n","class NodeModel(torch.nn.Module):\n","    def __init__(self,input1,input2,out11,out12,out21,out22):\n","        super().__init__()\n","        self.node_mlp_1 = Seq(Lin(input1, out11), nn.ELU(), Lin(out11, out12))\n","        self.node_mlp_2 = Seq(Lin(input2, out21), nn.Sigmoid(), Lin(out21, out22))\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        # x: [N, F_x], where N is the number of nodes.\n","        # edge_index: [2, E] with max entry N - 1.\n","        # edge_attr: [E, F_e]\n","        # u: [B, F_u]\n","        # batch: [N] with max entry B - 1.\n","        row, col = edge_index\n","        out = torch.cat([x[row], edge_attr], dim=1)\n","        out = self.node_mlp_1(out)\n","        out = scatter(out, col, dim=0, dim_size=x.size(0),\n","                      reduce='mean')\n","        out = torch.cat([x, out], dim=1)\n","        return self.node_mlp_2(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljpE3Gy_3ZZN"},"outputs":[],"source":["class DeepMetaLayer(torch.nn.Module):\n","    def __init__(self,device):\n","        super().__init__()\n","        self.meta1 = MetaLayer(EdgeModel(input=13,out1=18,out2=20),NodeModel(input1=20+5,input2=5+35,out11=30,out12=35,out21=40,out22=45))\n","        self.meta2 = MetaLayer(EdgeModel(input=20+45*2,out1=120,out2=130),NodeModel(input1=130+45,input2=45+1,out11=1,out12=1,out21=1,out22=1))\n","        self.fc1 = nn.Linear(130,5)\n","        self.fc2 = nn.Linear(5,1)\n","\n","\n","        self.device = device\n","    def forward(self, adj_matrix):\n","\n","\n","        data = adj_matrix_to_pytorch_geometric_data(adj_matrix,self.device)\n","        x, edge_attr, u = self.meta1(data.x, data.edge_index, data.edge_attr)\n","        #print(edge_attr)\n","        x, edge_attr, u = self.meta2(x, data.edge_index, edge_attr)\n","        edge_attr = edge_attr\n","        #print(edge_attr)\n","        edge_weights = F.elu(self.fc1(edge_attr))\n","        #print(edge_weights)\n","        #print(edge_weights.shape)\n","        #print(edge_weights.sum())\n","\n","        edge_weights = F.sigmoid(self.fc2(edge_weights))\n","        #print(edge_weights)\n","        #print(edge_weights.shape)\n","        #print(edge_weights.sum())\n","\n","        out = get_adjacency_matrix(35, data.edge_index, edge_weights,device)\n","\n","\n","        return out\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8kxdGUARPMm"},"outputs":[],"source":["class DeepMetaLayer_adj(torch.nn.Module):\n","    def __init__(self,device):\n","        super().__init__()\n","        self.meta1 = MetaLayer(EdgeModel(input=70+3,out1=80,out2=80),NodeModel(input1=35+80,input2=35+115,out11=115,out12=115,out21=125,out22=125))\n","        self.meta2 = MetaLayer(EdgeModel(input=80+125*2,out1=350,out2=350),NodeModel(input1=125+350,input2=125+1,out11=1,out12=1,out21=1,out22=1))\n","        self.fc1 = nn.Linear(350,50)\n","        self.fc2 = nn.Linear(50,1)\n","\n","\n","        self.device = device\n","    def forward(self, adj_matrix):\n","\n","\n","        data = adj_matrix_to_pytorch_geometric_data(adj_matrix,self.device)\n","        x, edge_attr, u = self.meta1(data.adj_matrix, data.edge_index, data.edge_attr)\n","        #print(edge_attr)\n","        x, edge_attr, u = self.meta2(x, data.edge_index, edge_attr)\n","        edge_attr = edge_attr\n","        #print(edge_attr)\n","        edge_weights = F.elu(self.fc1(edge_attr))\n","        #print(edge_weights)\n","        #print(edge_weights.shape)\n","        #print(edge_weights.sum())\n","\n","        edge_weights = F.sigmoid(self.fc2(edge_weights))\n","        #print(edge_weights)\n","        #print(edge_weights.shape)\n","        #print(edge_weights.sum())\n","\n","        out = get_adjacency_matrix(35, data.edge_index, edge_weights,device)\n","\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PE478yMSZtzd"},"outputs":[],"source":["data = dataset[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3840,"status":"ok","timestamp":1690711806185,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"OaCvopjRksah","outputId":"7461619f-14d5-4bbb-ad43-0d7ebbce788d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0000, 0.5725, 0.5738,  ..., 0.0000, 0.6195, 0.6206],\n","        [0.5688, 0.0000, 0.5726,  ..., 0.6172, 0.6191, 0.6201],\n","        [0.5687, 0.5709, 0.0000,  ..., 0.6171, 0.6190, 0.6200],\n","        ...,\n","        [0.0000, 0.5920, 0.5929,  ..., 0.0000, 0.6351, 0.6361],\n","        [0.5916, 0.5926, 0.5935,  ..., 0.6345, 0.0000, 0.6367],\n","        [0.5922, 0.5932, 0.5941,  ..., 0.6349, 0.6361, 0.0000]],\n","       device='cuda:0', grad_fn=<ToCopyBackward0>)\n","tensor(711.3232, device='cuda:0', grad_fn=<SumBackward0>)\n"]}],"source":["model =  DeepMetaLayer(device).to(device)\n","out = model(data)\n","print(out)\n","print(out.sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXoXZYhJP9zR"},"outputs":[],"source":["class Hospital_meta():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model =  DeepMetaLayer(device).to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690711816905,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"VwwdqzqNQM00","outputId":"149f11d3-6cc0-450a-96b4-7ca588aca13e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 35920\n","Trainable parameters: 35920\n"]}],"source":["model = DeepMetaLayer(device).to(device)\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTpdPTyTaYaL"},"outputs":[],"source":["def train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = False\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_meta(args,device))\n","                  print('Meta 2 X')\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    #if h_i ==3:\n","                     # return args,hospital,train_data,table_hospital\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGp1CWqeZuFy","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2d56ba27-d2ec-4b3b-fa13-ca1a1e609a22","executionInfo":{"status":"error","timestamp":1690720435591,"user_tz":-60,"elapsed":1980762,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05345583 0.05097473]\n"," [0.49095011 0.49543648]\n"," [0.06533224 0.06207772]\n"," [0.06643013 0.06219492]]\n","\n","Average MAE Loss:\n","[0.05221528 0.49319329 0.06370498 0.06431252]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.05415963 0.05153574]\n"," [0.49095011 0.49543648]\n"," [0.05382052 0.05131982]\n"," [0.05640134 0.05298167]]\n","\n","Average MAE Loss:\n","[0.05284768 0.49319329 0.05257017 0.05469151]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.05318925 0.05082954]\n"," [0.49095011 0.49543648]\n"," [0.05265465 0.05084252]\n"," [0.05392184 0.05089458]]\n","\n","Average MAE Loss:\n","[0.05200939 0.49319329 0.05174859 0.05240821]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.05275841 0.05062888]\n"," [0.49095011 0.49543648]\n"," [0.05175738 0.05029481]\n"," [0.05362499 0.05067039]]\n","\n","Average MAE Loss:\n","[0.05169365 0.49319329 0.0510261  0.05214769]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0524671  0.05054544]\n"," [0.49095011 0.49543648]\n"," [0.05149432 0.0503583 ]\n"," [0.05343515 0.05055086]]\n","\n","Average MAE Loss:\n","[0.05150627 0.49319329 0.05092631 0.051993  ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.05206618 0.0504548 ]\n"," [0.49095011 0.49543648]\n"," [0.05139219 0.050652  ]\n"," [0.05348063 0.05053636]]\n","\n","Average MAE Loss:\n","[0.05126049 0.49319329 0.05102209 0.05200849]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.05162657 0.05047863]\n"," [0.49095011 0.49543648]\n"," [0.05095196 0.05059573]\n"," [0.05359182 0.05056644]]\n","\n","Average MAE Loss:\n","[0.0510526  0.49319329 0.05077384 0.05207913]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08564283 0.08234258]\n"," [0.05359182 0.05056644]\n"," [0.05054531 0.05011644]\n"," [0.05195894 0.05156049]]\n","\n","Average MAE Loss:\n","[0.0839927  0.05207913 0.05033087 0.05175971]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0854552  0.08214737]\n"," [0.05359182 0.05056643]\n"," [0.04823323 0.04922233]\n"," [0.05088428 0.05071777]]\n","\n","Average MAE Loss:\n","[0.08380128 0.05207913 0.04872778 0.05080102]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07180884 0.0672083 ]\n"," [0.05359182 0.05056643]\n"," [0.04635588 0.04838319]\n"," [0.05103267 0.05087609]]\n","\n","Average MAE Loss:\n","[0.06950857 0.05207913 0.04736954 0.05095438]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05605161 0.05309691]\n"," [0.05359182 0.05056643]\n"," [0.05666353 0.08427129]\n"," [0.05074587 0.05096145]]\n","\n","Average MAE Loss:\n","[0.05457426 0.05207913 0.07046741 0.05085366]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05354869 0.05090956]\n"," [0.05359182 0.05056643]\n"," [0.04873531 0.05544519]\n"," [0.05040836 0.0507419 ]]\n","\n","Average MAE Loss:\n","[0.05222913 0.05207913 0.05209025 0.05057513]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05316485 0.050742  ]\n"," [0.05359182 0.05056643]\n"," [0.04972367 0.06244404]\n"," [0.05027784 0.05100922]]\n","\n","Average MAE Loss:\n","[0.05195342 0.05207913 0.05608386 0.05064353]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05301319 0.05067793]\n"," [0.05359182 0.05056644]\n"," [0.05093429 0.0682519 ]\n"," [0.05021165 0.05137109]]\n","\n","Average MAE Loss:\n","[0.05184556 0.05207913 0.05959309 0.05079137]\n","\n","Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.32945077 0.3304486 ]\n"," [0.35994687 0.36270542]\n"," [0.32077143 0.32092717]\n"," [0.05355403 0.05083499]]\n","\n","Average MAE Loss:\n","[0.32994969 0.36132615 0.3208493  0.05219451]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.23200197 0.22982459]\n"," [0.35994687 0.36270542]\n"," [0.05779408 0.05455123]\n"," [0.08744595 0.08433442]]\n","\n","Average MAE Loss:\n","[0.23091328 0.36132615 0.05617265 0.08589018]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05387362 0.05119655]\n"," [0.35994687 0.36270542]\n"," [0.06858785 0.06504   ]\n"," [0.08748133 0.08437824]]\n","\n","Average MAE Loss:\n","[0.05253509 0.36132615 0.06681393 0.08592978]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05703682 0.05396248]\n"," [0.35994687 0.36270542]\n"," [0.0576421  0.05483001]\n"," [0.0874744  0.08436908]]\n","\n","Average MAE Loss:\n","[0.05549965 0.36132615 0.05623606 0.08592174]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0540674  0.05165126]\n"," [0.35994687 0.36270542]\n"," [0.05341641 0.05101783]\n"," [0.08745735 0.08434703]]\n","\n","Average MAE Loss:\n","[0.05285933 0.36132615 0.05221712 0.08590219]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.0536849  0.05132942]\n"," [0.35994687 0.36270542]\n"," [0.05342355 0.05104173]\n"," [0.08742971 0.08431156]]\n","\n","Average MAE Loss:\n","[0.05250716 0.36132615 0.05223264 0.08587064]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.05334033 0.05100313]\n"," [0.35994687 0.36270542]\n"," [0.05365161 0.05127723]\n"," [0.08740855 0.08428457]]\n","\n","Average MAE Loss:\n","[0.05217173 0.36132615 0.05246442 0.08584656]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.35563214 0.35806646]\n"," [0.08740855 0.08428457]\n"," [0.05352076 0.05119831]\n"," [0.05327699 0.05087342]]\n","\n","Average MAE Loss:\n","[0.3568493  0.08584656 0.05235954 0.0520752 ]\n","\n","Epoch 00022: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.34816546 0.35002823]\n"," [0.08740855 0.08428457]\n"," [0.05347692 0.0511638 ]\n"," [0.05331314 0.05093106]]\n","\n","Average MAE Loss:\n","[0.34909685 0.08584656 0.05232036 0.0521221 ]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.34311721 0.34457955]\n"," [0.08740855 0.08428457]\n"," [0.05346941 0.05116332]\n"," [0.05330037 0.05091364]]\n","\n","Average MAE Loss:\n","[0.34384838 0.08584656 0.05231636 0.052107  ]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.33635483 0.33729606]\n"," [0.08740855 0.08428457]\n"," [0.05346791 0.05116901]\n"," [0.05332887 0.05095032]]\n","\n","Average MAE Loss:\n","[0.33682544 0.08584656 0.05231846 0.0521396 ]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.32689819 0.32716944]\n"," [0.08740855 0.08428457]\n"," [0.05345311 0.05116404]\n"," [0.05335099 0.05097613]]\n","\n","Average MAE Loss:\n","[0.32703382 0.08584656 0.05230857 0.05216356]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.3131613  0.31260397]\n"," [0.08740855 0.08428457]\n"," [0.05344802 0.0511651 ]\n"," [0.05333952 0.05096253]]\n","\n","Average MAE Loss:\n","[0.31288264 0.08584656 0.05230656 0.05215102]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.30376434 0.30271854]\n"," [0.08740855 0.08428457]\n"," [0.05344043 0.05116472]\n"," [0.05333794 0.05096048]]\n","\n","Average MAE Loss:\n","[0.30324144 0.08584656 0.05230258 0.05214921]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.0554014  0.05264375]\n"," [0.05879426 0.05574265]\n"," [0.05525467 0.05251663]\n"," [0.05432883 0.05174568]]\n","\n","Average MAE Loss:\n","[0.05402257 0.05726846 0.05388565 0.05303725]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05424623 0.0516783 ]\n"," [0.05879426 0.05574265]\n"," [0.05369907 0.05122048]\n"," [0.053369   0.05088126]]\n","\n","Average MAE Loss:\n","[0.05296227 0.05726846 0.05245978 0.05212513]\n","\n","Epoch 00030: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05382528 0.05133058]\n"," [0.05879426 0.05574265]\n"," [0.05359617 0.05113037]\n"," [0.05336719 0.05087808]]\n","\n","Average MAE Loss:\n","[0.05257793 0.05726846 0.05236327 0.05212263]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05373123 0.05125031]\n"," [0.05879426 0.05574265]\n"," [0.05359724 0.05113389]\n"," [0.05342549 0.05094898]]\n","\n","Average MAE Loss:\n","[0.05249077 0.05726846 0.05236556 0.05218724]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05367293 0.05119898]\n"," [0.05879426 0.05574265]\n"," [0.05361229 0.05114982]\n"," [0.05344419 0.05097   ]]\n","\n","Average MAE Loss:\n","[0.05243595 0.05726846 0.05238105 0.05220709]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05363074 0.05116216]\n"," [0.05879426 0.05574265]\n"," [0.05362024 0.05115952]\n"," [0.05344914 0.05097548]]\n","\n","Average MAE Loss:\n","[0.05239645 0.05726846 0.05238988 0.05221231]\n","\n","Epoch 00034: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05359893 0.05113523]\n"," [0.05879426 0.05574265]\n"," [0.05362098 0.05116153]\n"," [0.05344803 0.05097407]]\n","\n","Average MAE Loss:\n","[0.05236708 0.05726846 0.05239125 0.05221105]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05817002 0.0551675 ]\n"," [0.05344803 0.05097407]\n"," [0.05360423 0.05114123]\n"," [0.05354325 0.05109015]]\n","\n","Average MAE Loss:\n","[0.05666876 0.05221105 0.05237273 0.0523167 ]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00036: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05732971 0.0543988 ]\n"," [0.05344803 0.05097407]\n"," [0.05361069 0.05114826]\n"," [0.05350446 0.05105082]]\n","\n","Average MAE Loss:\n","[0.05586426 0.05221105 0.05237948 0.05227764]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05661775 0.05373726]\n"," [0.05344803 0.05097407]\n"," [0.05361556 0.05115387]\n"," [0.05347942 0.05102471]]\n","\n","Average MAE Loss:\n","[0.05517751 0.05221105 0.05238471 0.05225206]\n","\n","Epoch 00038: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05604566 0.05321376]\n"," [0.05344803 0.05097407]\n"," [0.05361698 0.05115578]\n"," [0.05346502 0.05100871]]\n","\n","Average MAE Loss:\n","[0.05462971 0.05221105 0.05238638 0.05223687]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05580859 0.05300047]\n"," [0.05344803 0.05097407]\n"," [0.053618   0.05115735]\n"," [0.05345584 0.05099859]]\n","\n","Average MAE Loss:\n","[0.05440453 0.05221105 0.05238767 0.05222722]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.0555992  0.05281627]\n"," [0.05344803 0.05097407]\n"," [0.05361864 0.05115861]\n"," [0.05345293 0.05099533]]\n","\n","Average MAE Loss:\n","[0.05420773 0.05221105 0.05238862 0.05222413]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05541019 0.0526527 ]\n"," [0.05344803 0.05097407]\n"," [0.0536191  0.05115972]\n"," [0.05345083 0.05099298]]\n","\n","Average MAE Loss:\n","[0.05403145 0.05221105 0.05238941 0.0522219 ]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05371947 0.05124368]\n"," [0.05376098 0.05128012]\n"," [0.0537549  0.05127516]\n"," [0.05371866 0.0512427 ]]\n","\n","Average MAE Loss:\n","[0.05248157 0.05252055 0.05251503 0.05248068]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05371025 0.05123562]\n"," [0.05376098 0.05128012]\n"," [0.05374517 0.05126699]\n"," [0.05366247 0.05119203]]\n","\n","Average MAE Loss:\n","[0.05247293 0.05252055 0.05250608 0.05242725]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05370293 0.05122924]\n"," [0.05376098 0.05128012]\n"," [0.05373548 0.05125874]\n"," [0.05363837 0.05117028]]\n","\n","Average MAE Loss:\n","[0.05246609 0.05252055 0.05249711 0.05240433]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05369602 0.05122324]\n"," [0.05376098 0.05128012]\n"," [0.05372632 0.0512509 ]\n"," [0.05361708 0.05115126]]\n","\n","Average MAE Loss:\n","[0.05245963 0.05252055 0.05248861 0.05238417]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05368935 0.05121741]\n"," [0.05376098 0.05128012]\n"," [0.05372204 0.05124723]\n"," [0.05359805 0.05113446]]\n","\n","Average MAE Loss:\n","[0.05245338 0.05252055 0.05248464 0.05236625]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05368611 0.05121455]\n"," [0.05376098 0.05128012]\n"," [0.0537179  0.05124371]\n"," [0.05358125 0.05111931]]\n","\n","Average MAE Loss:\n","[0.05245033 0.05252055 0.0524808  0.05235028]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05368289 0.0512117 ]\n"," [0.05376098 0.05128012]\n"," [0.05371389 0.05124031]\n"," [0.05357387 0.05111246]]\n","\n","Average MAE Loss:\n","[0.0524473  0.05252055 0.0524771  0.05234316]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05375664 0.05127643]\n"," [0.05357387 0.05111246]\n"," [0.05367979 0.05120904]\n"," [0.05370114 0.05122893]]\n","\n","Average MAE Loss:\n","[0.05251653 0.05234317 0.05244442 0.05246503]\n","\n","Epoch 00050: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05375192 0.05127239]\n"," [0.05357387 0.05111246]\n"," [0.0536785  0.05120796]\n"," [0.05368643 0.05121573]]\n","\n","Average MAE Loss:\n","[0.05251216 0.05234317 0.05244323 0.05245108]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05374954 0.05127037]\n"," [0.05357387 0.05111246]\n"," [0.05367727 0.05120692]\n"," [0.05367247 0.05120297]]\n","\n","Average MAE Loss:\n","[0.05250995 0.05234317 0.05244209 0.05243772]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05374718 0.05126835]\n"," [0.05357387 0.05111246]\n"," [0.05367606 0.0512059 ]\n"," [0.05366597 0.05119705]]\n","\n","Average MAE Loss:\n","[0.05250777 0.05234316 0.05244098 0.05243151]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05374485 0.05126634]\n"," [0.05357387 0.05111246]\n"," [0.05367489 0.05120492]\n"," [0.05365965 0.05119134]]\n","\n","Average MAE Loss:\n","[0.05250559 0.05234317 0.0524399  0.05242549]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05374253 0.05126433]\n"," [0.05357387 0.05111246]\n"," [0.0536743  0.05120444]\n"," [0.0536535  0.05118577]]\n","\n","Average MAE Loss:\n","[0.05250343 0.05234317 0.05243937 0.05241964]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05374137 0.05126331]\n"," [0.05357387 0.05111246]\n"," [0.05367374 0.05120397]\n"," [0.05364745 0.05118033]]\n","\n","Average MAE Loss:\n","[0.05250234 0.05234317 0.05243886 0.05241389]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05365605 0.05118723]\n"," [0.05365691 0.05118798]\n"," [0.05365648 0.05118764]\n"," [0.05365385 0.05118523]]\n","\n","Average MAE Loss:\n","[0.05242164 0.05242244 0.05242206 0.05241954]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.0536553  0.05118657]\n"," [0.05365691 0.05118798]\n"," [0.05365612 0.05118737]\n"," [0.05365082 0.0511825 ]]\n","\n","Average MAE Loss:\n","[0.05242094 0.05242244 0.05242174 0.05241666]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05365456 0.05118594]\n"," [0.05365691 0.05118798]\n"," [0.05365595 0.05118724]\n"," [0.05364783 0.05117979]]\n","\n","Average MAE Loss:\n","[0.05242025 0.05242244 0.05242159 0.05241381]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05365419 0.05118561]\n"," [0.05365691 0.05118798]\n"," [0.05365579 0.05118712]\n"," [0.05364487 0.05117711]]\n","\n","Average MAE Loss:\n","[0.0524199  0.05242244 0.05242145 0.05241099]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","\n","epochs finished with time:1680.8302085399628\n","\n","[[0.05365419 0.05118561]\n"," [0.05365691 0.05118798]\n"," [0.05365579 0.05118712]\n"," [0.05364487 0.05117711]]\n","00:28:28.952446769000062\n","------------------------------------Fold [2/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07889262 0.07875302]\n"," [0.46320858 0.47067857]\n"," [0.0968108  0.0949769 ]\n"," [0.092621   0.089985  ]]\n","\n","Average MAE Loss:\n","[0.07882282 0.46694357 0.09589385 0.091303  ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08007262 0.07976595]\n"," [0.46320858 0.47067857]\n"," [0.07742679 0.07748413]\n"," [0.07630675 0.0761118 ]]\n","\n","Average MAE Loss:\n","[0.07991928 0.46694357 0.07745546 0.07620927]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07776559 0.07778976]\n"," [0.46320858 0.47067857]\n"," [0.07485065 0.07552447]\n"," [0.07710516 0.07645527]]\n","\n","Average MAE Loss:\n","[0.07777767 0.46694357 0.07518756 0.07678021]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07666738 0.07691305]\n"," [0.46320858 0.47067857]\n"," [0.07488843 0.07550378]\n"," [0.07727067 0.07654685]]\n","\n","Average MAE Loss:\n","[0.07679022 0.46694357 0.07519611 0.07690876]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07625697 0.0766706 ]\n"," [0.46320858 0.47067857]\n"," [0.07475541 0.07565165]\n"," [0.07671055 0.07612194]]\n","\n","Average MAE Loss:\n","[0.07646379 0.46694357 0.07520353 0.07641624]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07556364 0.07628424]\n"," [0.46320858 0.47067857]\n"," [0.07370925 0.07536277]\n"," [0.07696525 0.07627895]]\n","\n","Average MAE Loss:\n","[0.07592394 0.46694357 0.07453601 0.0766221 ]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07428023 0.07554678]\n"," [0.46320858 0.47067857]\n"," [0.07091025 0.07329083]\n"," [0.07649581 0.07592587]]\n","\n","Average MAE Loss:\n","[0.0749135  0.46694357 0.07210054 0.07621084]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.11641684 0.11434875]\n"," [0.07649581 0.07592587]\n"," [0.07330388 0.07520355]\n"," [0.07530977 0.0790067 ]]\n","\n","Average MAE Loss:\n","[0.11538279 0.07621084 0.07425371 0.07715823]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.11691505 0.11490499]\n"," [0.07649581 0.07592588]\n"," [0.07031782 0.07454652]\n"," [0.07561563 0.07866396]]\n","\n","Average MAE Loss:\n","[0.11591002 0.07621085 0.07243217 0.07713979]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.09978347 0.09752146]\n"," [0.07649581 0.07592588]\n"," [0.07021741 0.07128984]\n"," [0.07671152 0.0797133 ]]\n","\n","Average MAE Loss:\n","[0.09865246 0.07621085 0.07075362 0.07821241]\n","\n","Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08202704 0.08129608]\n"," [0.07649581 0.07592588]\n"," [0.06954878 0.07034352]\n"," [0.07512876 0.07768393]]\n","\n","Average MAE Loss:\n","[0.08166156 0.07621085 0.06994615 0.07640635]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07675874 0.0768577 ]\n"," [0.07649581 0.07592588]\n"," [0.06974633 0.07055987]\n"," [0.07271895 0.07487352]]\n","\n","Average MAE Loss:\n","[0.07680822 0.07621085 0.0701531  0.07379624]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07625124 0.07644334]\n"," [0.07649581 0.07592588]\n"," [0.06973364 0.07063003]\n"," [0.07344556 0.07556109]]\n","\n","Average MAE Loss:\n","[0.07634729 0.07621085 0.07018184 0.07450333]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07676786 0.07685229]\n"," [0.07649581 0.07592588]\n"," [0.07001985 0.07116943]\n"," [0.07365323 0.07578275]]\n","\n","Average MAE Loss:\n","[0.07681007 0.07621085 0.07059464 0.07471799]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.30515349 0.31050906]\n"," [0.3367758  0.34338908]\n"," [0.15413432 0.15751781]\n"," [0.31940749 0.32433801]]\n","\n","Average MAE Loss:\n","[0.30783128 0.34008244 0.15582607 0.32187275]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.20083857 0.20531213]\n"," [0.3367758  0.34338908]\n"," [0.10042886 0.09808208]\n"," [0.23198822 0.23288887]]\n","\n","Average MAE Loss:\n","[0.20307535 0.34008244 0.09925547 0.23243854]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.08314871 0.08210401]\n"," [0.3367758  0.34338908]\n"," [0.07567368 0.07628974]\n"," [0.11375185 0.11507437]]\n","\n","Average MAE Loss:\n","[0.08262636 0.34008244 0.07598171 0.11441311]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.08288904 0.0818878 ]\n"," [0.3367758  0.34338908]\n"," [0.07744319 0.07726671]\n"," [0.07707044 0.07686898]]\n","\n","Average MAE Loss:\n","[0.08238842 0.34008244 0.07735495 0.07696971]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07561367 0.07601836]\n"," [0.3367758  0.34338908]\n"," [0.07582509 0.07610947]\n"," [0.08017537 0.07934688]]\n","\n","Average MAE Loss:\n","[0.07581601 0.34008244 0.07596728 0.07976112]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00019: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07594225 0.07616705]\n"," [0.3367758  0.34338908]\n"," [0.07616584 0.07632259]\n"," [0.07708579 0.07686945]]\n","\n","Average MAE Loss:\n","[0.07605465 0.34008244 0.07624421 0.07697762]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07641414 0.07649047]\n"," [0.3367758  0.34338908]\n"," [0.07613169 0.07630734]\n"," [0.07651917 0.07644518]]\n","\n","Average MAE Loss:\n","[0.07645231 0.34008244 0.07621951 0.07648217]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.33242214 0.3387558 ]\n"," [0.07651917 0.07644518]\n"," [0.07602739 0.07622942]\n"," [0.07608578 0.07627683]]\n","\n","Average MAE Loss:\n","[0.33558897 0.07648217 0.07612841 0.0761813 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.32475121 0.33061175]\n"," [0.07651917 0.07644518]\n"," [0.07612281 0.07629705]\n"," [0.07614173 0.07631358]]\n","\n","Average MAE Loss:\n","[0.32768148 0.07648217 0.07620993 0.07622765]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00023: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.31943545 0.32497268]\n"," [0.07651917 0.07644518]\n"," [0.07608842 0.07627689]\n"," [0.07618922 0.0763454 ]]\n","\n","Average MAE Loss:\n","[0.32220407 0.07648217 0.07618266 0.07626731]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.31218629 0.31732421]\n"," [0.07651917 0.07644518]\n"," [0.07605591 0.07625692]\n"," [0.07620351 0.07635493]]\n","\n","Average MAE Loss:\n","[0.31475525 0.07648217 0.07615642 0.07627922]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.30190668 0.30658965]\n"," [0.07651917 0.07644519]\n"," [0.07604671 0.07624969]\n"," [0.07621366 0.07636175]]\n","\n","Average MAE Loss:\n","[0.30424817 0.07648218 0.0761482  0.0762877 ]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.28686371 0.29099278]\n"," [0.07651917 0.07644518]\n"," [0.07602746 0.07623566]\n"," [0.07622043 0.07636623]]\n","\n","Average MAE Loss:\n","[0.28892825 0.07648217 0.07613156 0.07629333]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.27652457 0.28035419]\n"," [0.07651917 0.07644518]\n"," [0.07602384 0.07623332]\n"," [0.07622405 0.07636857]]\n","\n","Average MAE Loss:\n","[0.27843938 0.07648217 0.07612858 0.07629631]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.08725508 0.08872906]\n"," [0.10236943 0.10492572]\n"," [0.0801616  0.08100501]\n"," [0.09618872 0.09824975]]\n","\n","Average MAE Loss:\n","[0.08799207 0.10364757 0.0805833  0.09721923]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.07869874 0.07947412]\n"," [0.10236943 0.10492572]\n"," [0.07574372 0.07604882]\n"," [0.0886647  0.09025145]]\n","\n","Average MAE Loss:\n","[0.07908643 0.10364757 0.07589627 0.08945807]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.07601967 0.07665815]\n"," [0.10236943 0.10492572]\n"," [0.07665441 0.07664205]\n"," [0.08339603 0.08455576]]\n","\n","Average MAE Loss:\n","[0.07633891 0.10364757 0.07664823 0.08397589]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00031: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.07564382 0.07625436]\n"," [0.10236943 0.10492572]\n"," [0.07656174 0.0765741 ]\n"," [0.08004286 0.08088215]]\n","\n","Average MAE Loss:\n","[0.07594909 0.10364757 0.07656792 0.0804625 ]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07554284 0.07609791]\n"," [0.10236943 0.10492572]\n"," [0.07638773 0.07644801]\n"," [0.07892345 0.07971219]]\n","\n","Average MAE Loss:\n","[0.07582037 0.10364757 0.07641787 0.07931782]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07555768 0.0760349 ]\n"," [0.10236943 0.10492572]\n"," [0.07627093 0.07636689]\n"," [0.07808086 0.07884429]]\n","\n","Average MAE Loss:\n","[0.07579629 0.10364757 0.07631891 0.07846257]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.07561996 0.07601895]\n"," [0.10236943 0.10492572]\n"," [0.07620648 0.07632415]\n"," [0.07745129 0.0781767 ]]\n","\n","Average MAE Loss:\n","[0.07581945 0.10364757 0.07626531 0.07781399]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00035: reducing learning rate of group 0 to 1.5625e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.09917816 0.10147756]\n"," [0.07745129 0.0781767 ]\n"," [0.07566094 0.0760237 ]\n"," [0.07635148 0.07642448]]\n","\n","Average MAE Loss:\n","[0.10032786 0.07781399 0.07584232 0.07638798]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.09505612 0.09703017]\n"," [0.07745129 0.0781767 ]\n"," [0.07573751 0.07604549]\n"," [0.07636181 0.07643192]]\n","\n","Average MAE Loss:\n","[0.09604314 0.07781399 0.0758915  0.07639686]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.09141023 0.09314958]\n"," [0.07745129 0.0781767 ]\n"," [0.07582028 0.07608398]\n"," [0.07636037 0.07643086]]\n","\n","Average MAE Loss:\n","[0.09227991 0.07781399 0.07595213 0.07639562]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.08837301 0.08993105]\n"," [0.07745129 0.0781767 ]\n"," [0.07589648 0.07612488]\n"," [0.07635699 0.0764284 ]]\n","\n","Average MAE Loss:\n","[0.08915203 0.07781399 0.07601068 0.0763927 ]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00039: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.08709371 0.08854746]\n"," [0.07745129 0.0781767 ]\n"," [0.07592828 0.07614262]\n"," [0.07635334 0.07642574]]\n","\n","Average MAE Loss:\n","[0.08782058 0.07781399 0.07603545 0.07638954]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.08594856 0.08730876]\n"," [0.07745129 0.0781767 ]\n"," [0.07595716 0.07615982]\n"," [0.07635151 0.07642441]]\n","\n","Average MAE Loss:\n","[0.08662866 0.07781399 0.07605849 0.07638796]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.08491101 0.08619096]\n"," [0.07745129 0.0781767 ]\n"," [0.07598314 0.07617604]\n"," [0.07634967 0.07642308]]\n","\n","Average MAE Loss:\n","[0.08555098 0.07781399 0.07607959 0.07638637]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.07589075 0.07652915]\n"," [0.07602616 0.0766729 ]\n"," [0.07585542 0.07649151]\n"," [0.07599927 0.07664434]]\n","\n","Average MAE Loss:\n","[0.07620995 0.07634953 0.07617347 0.0763218 ]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00043: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.0758519  0.07648715]\n"," [0.07602616 0.0766729 ]\n"," [0.0757529  0.07638325]\n"," [0.07595602 0.07659812]]\n","\n","Average MAE Loss:\n","[0.07616952 0.07634953 0.07606808 0.07627707]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.07581797 0.07645054]\n"," [0.07602616 0.0766729 ]\n"," [0.07567072 0.07629354]\n"," [0.07593375 0.07657453]]\n","\n","Average MAE Loss:\n","[0.07613426 0.07634953 0.07598213 0.07625414]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.07578581 0.07641678]\n"," [0.07602616 0.0766729 ]\n"," [0.07561264 0.07622356]\n"," [0.07591168 0.07655136]]\n","\n","Average MAE Loss:\n","[0.0761013  0.07634953 0.0759181  0.07623152]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.07575523 0.07638516]\n"," [0.07602616 0.0766729 ]\n"," [0.07557304 0.07616879]\n"," [0.07589035 0.07652889]]\n","\n","Average MAE Loss:\n","[0.0760702  0.07634953 0.07587091 0.07620962]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.07574079 0.07636974]\n"," [0.07602616 0.0766729 ]\n"," [0.07555947 0.07614744]\n"," [0.07586994 0.07650696]]\n","\n","Average MAE Loss:\n","[0.07605527 0.07634953 0.07585346 0.07618845]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.07572694 0.0763548 ]\n"," [0.07602616 0.0766729 ]\n"," [0.07554906 0.07612831]\n"," [0.07586002 0.07649619]]\n","\n","Average MAE Loss:\n","[0.07604087 0.07634953 0.07583869 0.07617811]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.07600425 0.07664966]\n"," [0.07586002 0.07649619]\n"," [0.07569038 0.07631502]\n"," [0.07554737 0.07612467]]\n","\n","Average MAE Loss:\n","[0.07632696 0.07617811 0.0760027  0.07583602]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.07598138 0.07662526]\n"," [0.07586002 0.07649619]\n"," [0.07565624 0.07627639]\n"," [0.07554597 0.07612157]]\n","\n","Average MAE Loss:\n","[0.07630332 0.07617811 0.07596631 0.07583377]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.07597013 0.07661321]\n"," [0.07586002 0.07649619]\n"," [0.07564117 0.07625864]\n"," [0.07554473 0.07611862]]\n","\n","Average MAE Loss:\n","[0.07629167 0.07617811 0.0759499  0.07583168]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.07595903 0.07660132]\n"," [0.07586002 0.07649619]\n"," [0.07562747 0.07624177]\n"," [0.07554414 0.07611716]]\n","\n","Average MAE Loss:\n","[0.07628018 0.07617811 0.07593462 0.07583065]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.07594796 0.07658954]\n"," [0.07586002 0.07649619]\n"," [0.07561486 0.07622587]\n"," [0.07554358 0.07611571]]\n","\n","Average MAE Loss:\n","[0.07626875 0.07617811 0.07592037 0.07582964]\n","\n","Epoch 00054: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.07593693 0.07657787]\n"," [0.07586002 0.07649619]\n"," [0.07560338 0.07621077]\n"," [0.07554302 0.07611428]]\n","\n","Average MAE Loss:\n","[0.0762574  0.07617811 0.07590708 0.07582865]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.07593141 0.07657208]\n"," [0.07586002 0.07649619]\n"," [0.075598   0.07620358]\n"," [0.07554247 0.07611287]]\n","\n","Average MAE Loss:\n","[0.07625175 0.07617811 0.07590079 0.07582767]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.07567877 0.07630235]\n"," [0.07568192 0.0763059 ]\n"," [0.0756735  0.07629644]\n"," [0.07568047 0.07630427]]\n","\n","Average MAE Loss:\n","[0.07599056 0.07599391 0.07598497 0.07599237]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.0756758  0.07629902]\n"," [0.07568192 0.0763059 ]\n"," [0.07566497 0.07628677]\n"," [0.07567887 0.07630245]]\n","\n","Average MAE Loss:\n","[0.07598741 0.07599391 0.07597587 0.07599066]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.07567288 0.07629572]\n"," [0.07568192 0.07630591]\n"," [0.07565683 0.07627738]\n"," [0.07567723 0.07630062]]\n","\n","Average MAE Loss:\n","[0.0759843  0.07599392 0.0759671  0.07598892]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.07567141 0.07629408]\n"," [0.07568192 0.0763059 ]\n"," [0.07565288 0.07627281]\n"," [0.07567558 0.07629876]]\n","\n","Average MAE Loss:\n","[0.07598274 0.07599391 0.07596284 0.07598717]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","\n","epochs finished with time:1678.5104179382324\n","\n","[[0.07567141 0.07629408]\n"," [0.07568192 0.07630591]\n"," [0.07565288 0.07627281]\n"," [0.07567558 0.07629876]]\n","00:28:27.57848067399982\n","------------------------------------Fold [3/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.06341103 0.05611828]\n"," [0.47698912 0.48960996]\n"," [0.0795275  0.06913822]\n"," [0.07642589 0.06540529]]\n","\n","Average MAE Loss:\n","[0.05976466 0.48329954 0.07433286 0.07091559]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06365068 0.05621445]\n"," [0.47698912 0.48960996]\n"," [0.06168854 0.05514868]\n"," [0.06175546 0.05671275]]\n","\n","Average MAE Loss:\n","[0.05993257 0.48329954 0.05841861 0.05923411]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06233083 0.05544038]\n"," [0.47698912 0.48960996]\n"," [0.06025923 0.05702335]\n"," [0.06164463 0.055464  ]]\n","\n","Average MAE Loss:\n","[0.05888561 0.48329954 0.05864129 0.05855431]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.06168843 0.05516157]\n"," [0.47698912 0.48960996]\n"," [0.05979998 0.0554733 ]\n"," [0.06204423 0.05509959]]\n","\n","Average MAE Loss:\n","[0.058425   0.48329954 0.05763664 0.05857191]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.06148009 0.05506913]\n"," [0.47698912 0.48960996]\n"," [0.05942488 0.0550226 ]\n"," [0.06184927 0.05502174]]\n","\n","Average MAE Loss:\n","[0.05827461 0.48329954 0.05722374 0.0584355 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.0611208  0.05495196]\n"," [0.47698912 0.48960996]\n"," [0.05870044 0.05457964]\n"," [0.06179712 0.05495778]]\n","\n","Average MAE Loss:\n","[0.05803638 0.48329954 0.05664004 0.05837745]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.0600053  0.05467224]\n"," [0.47698912 0.48960996]\n"," [0.05704825 0.05667042]\n"," [0.06192024 0.05488928]]\n","\n","Average MAE Loss:\n","[0.05733877 0.48329954 0.05685933 0.05840476]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.09755009 0.08598505]\n"," [0.06192024 0.05488928]\n"," [0.05966838 0.05453864]\n"," [0.05776227 0.05577227]]\n","\n","Average MAE Loss:\n","[0.09176757 0.05840476 0.05710351 0.05676727]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06158736 0.05561722]\n"," [0.06192024 0.05488928]\n"," [0.05714727 0.05418212]\n"," [0.05800836 0.05430252]]\n","\n","Average MAE Loss:\n","[0.05860229 0.05840476 0.0556647  0.05615544]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0618644  0.05540482]\n"," [0.06192024 0.05488928]\n"," [0.05498859 0.05503882]\n"," [0.05759926 0.05452385]]\n","\n","Average MAE Loss:\n","[0.05863461 0.05840476 0.0550137  0.05606155]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06248739 0.05548979]\n"," [0.06192024 0.05488928]\n"," [0.05809675 0.08082531]\n"," [0.05714703 0.05462534]]\n","\n","Average MAE Loss:\n","[0.05898859 0.05840476 0.06946103 0.05588619]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.06101031 0.05528563]\n"," [0.06192024 0.05488928]\n"," [0.05403934 0.05734443]\n"," [0.05721463 0.05376774]]\n","\n","Average MAE Loss:\n","[0.05814797 0.05840476 0.05569188 0.05549118]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06110353 0.05514354]\n"," [0.06192025 0.05488928]\n"," [0.05490979 0.06754442]\n"," [0.0571636  0.05390148]]\n","\n","Average MAE Loss:\n","[0.05812353 0.05840476 0.0612271  0.05553254]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06100834 0.05503812]\n"," [0.06192024 0.05488928]\n"," [0.05483674 0.06801645]\n"," [0.0568981  0.05418253]]\n","\n","Average MAE Loss:\n","[0.05802323 0.05840476 0.06142659 0.05554032]\n","\n","Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.31549782 0.32471642]\n"," [0.34619835 0.35706258]\n"," [0.31002375 0.31849617]\n"," [0.0634787  0.05586632]]\n","\n","Average MAE Loss:\n","[0.32010712 0.35163046 0.31425996 0.05967251]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.21667479 0.22413616]\n"," [0.34619835 0.35706258]\n"," [0.06361377 0.05604152]\n"," [0.10148447 0.09025542]]\n","\n","Average MAE Loss:\n","[0.22040547 0.35163046 0.05982765 0.09586995]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06353784 0.0560292 ]\n"," [0.34619835 0.35706258]\n"," [0.07896459 0.06828551]\n"," [0.10149892 0.0902741 ]]\n","\n","Average MAE Loss:\n","[0.05978352 0.35163046 0.07362505 0.09588651]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06783719 0.05916461]\n"," [0.34619835 0.35706258]\n"," [0.06264358 0.06094013]\n"," [0.10149971 0.09027513]]\n","\n","Average MAE Loss:\n","[0.0635009  0.35163046 0.06179186 0.09588742]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06090739 0.05617823]\n"," [0.34619835 0.35706258]\n"," [0.06112229 0.05554858]\n"," [0.1014991  0.09027427]]\n","\n","Average MAE Loss:\n","[0.05854281 0.35163046 0.05833543 0.09588668]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06098381 0.05570442]\n"," [0.34619835 0.35706258]\n"," [0.06109078 0.05557428]\n"," [0.10149816 0.09027302]]\n","\n","Average MAE Loss:\n","[0.05834412 0.35163046 0.05833253 0.09588559]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06127445 0.05539218]\n"," [0.34619835 0.35706258]\n"," [0.0609212  0.05595649]\n"," [0.10149762 0.0902723 ]]\n","\n","Average MAE Loss:\n","[0.05833332 0.35163046 0.05843884 0.09588496]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.34193143 0.35248291]\n"," [0.10149762 0.0902723 ]\n"," [0.06088883 0.05599322]\n"," [0.06135536 0.05537397]]\n","\n","Average MAE Loss:\n","[0.34720717 0.09588496 0.05844102 0.05836467]\n","\n","Epoch 00022: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.33457303 0.34459582]\n"," [0.10149762 0.0902723 ]\n"," [0.06088969 0.0558493 ]\n"," [0.06111438 0.05552854]]\n","\n","Average MAE Loss:\n","[0.33958443 0.09588496 0.05836949 0.05832146]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.32960495 0.33926997]\n"," [0.10149762 0.0902723 ]\n"," [0.0609022  0.05573943]\n"," [0.06121985 0.05543521]]\n","\n","Average MAE Loss:\n","[0.33443746 0.09588496 0.05832082 0.05832753]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.32296354 0.33215263]\n"," [0.10149762 0.0902723 ]\n"," [0.06088843 0.05574265]\n"," [0.0611573  0.055483  ]]\n","\n","Average MAE Loss:\n","[0.32755809 0.09588496 0.05831554 0.05832015]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.31368366 0.32225889]\n"," [0.10149762 0.0902723 ]\n"," [0.06087432 0.0557554 ]\n"," [0.06115314 0.05548625]]\n","\n","Average MAE Loss:\n","[0.31797127 0.09588496 0.05831486 0.0583197 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.30022886 0.30804622]\n"," [0.10149762 0.0902723 ]\n"," [0.06087139 0.05575095]\n"," [0.06115907 0.05548006]]\n","\n","Average MAE Loss:\n","[0.30413754 0.09588496 0.05831117 0.05831956]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.29102483 0.29841539]\n"," [0.10149762 0.0902723 ]\n"," [0.06086764 0.05574959]\n"," [0.06115526 0.05548295]]\n","\n","Average MAE Loss:\n","[0.29472011 0.09588496 0.05830862 0.0583191 ]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06098221 0.05613107]\n"," [0.06137918 0.05800241]\n"," [0.06101821 0.05664586]\n"," [0.06099942 0.05590533]]\n","\n","Average MAE Loss:\n","[0.05855664 0.0596908  0.05883204 0.05845238]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0610721  0.05567546]\n"," [0.06137918 0.05800241]\n"," [0.06100814 0.05584251]\n"," [0.06135453 0.05538131]]\n","\n","Average MAE Loss:\n","[0.05837378 0.0596908  0.05842532 0.05836792]\n","\n","Epoch 00030: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06115853 0.05553779]\n"," [0.06137918 0.05800241]\n"," [0.06102858 0.05576362]\n"," [0.06128754 0.05541797]]\n","\n","Average MAE Loss:\n","[0.05834816 0.0596908  0.0583961  0.05835276]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06118418 0.05550686]\n"," [0.06137918 0.05800241]\n"," [0.0610314  0.05574979]\n"," [0.06121021 0.05548127]]\n","\n","Average MAE Loss:\n","[0.05834552 0.0596908  0.0583906  0.05834574]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06120205 0.05548694]\n"," [0.06137918 0.05800241]\n"," [0.06103002 0.05574715]\n"," [0.06120918 0.05548197]]\n","\n","Average MAE Loss:\n","[0.05834449 0.0596908  0.05838858 0.05834558]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06121537 0.05547278]\n"," [0.06137918 0.05800241]\n"," [0.06102762 0.055747  ]\n"," [0.06121472 0.05547623]]\n","\n","Average MAE Loss:\n","[0.05834407 0.0596908  0.05838731 0.05834547]\n","\n","Epoch 00034: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06122582 0.05546197]\n"," [0.06137918 0.05800241]\n"," [0.06102636 0.05574705]\n"," [0.0612177  0.05547304]]\n","\n","Average MAE Loss:\n","[0.0583439  0.0596908  0.05838671 0.05834537]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06128161 0.05772413]\n"," [0.0612177  0.05547304]\n"," [0.06119252 0.05549287]\n"," [0.0610866  0.05562324]]\n","\n","Average MAE Loss:\n","[0.05950287 0.05834537 0.0583427  0.05835492]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06116081 0.05732952]\n"," [0.0612177  0.05547304]\n"," [0.06114772 0.05554219]\n"," [0.06112636 0.055563  ]]\n","\n","Average MAE Loss:\n","[0.05924517 0.05834537 0.05834495 0.05834468]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06108012 0.05698688]\n"," [0.0612177  0.05547304]\n"," [0.06111191 0.05558919]\n"," [0.0611568  0.05552461]]\n","\n","Average MAE Loss:\n","[0.0590335  0.05834537 0.05835055 0.0583407 ]\n","\n","Epoch 00038: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06102853 0.05670711]\n"," [0.0612177  0.05547304]\n"," [0.06109868 0.05560868]\n"," [0.06117751 0.0555012 ]]\n","\n","Average MAE Loss:\n","[0.05886782 0.05834537 0.05835368 0.05833935]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06101155 0.05658786]\n"," [0.0612177  0.05547304]\n"," [0.06108762 0.05562572]\n"," [0.06118986 0.05548813]]\n","\n","Average MAE Loss:\n","[0.0587997  0.05834537 0.05835667 0.058339  ]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06099857 0.05648272]\n"," [0.0612177  0.05547304]\n"," [0.06107843 0.0556404 ]\n"," [0.06119351 0.05548438]]\n","\n","Average MAE Loss:\n","[0.05874065 0.05834537 0.05835941 0.05833894]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.0609888  0.05638714]\n"," [0.0612177  0.05547304]\n"," [0.06107043 0.05565374]\n"," [0.06119597 0.05548184]]\n","\n","Average MAE Loss:\n","[0.05868797 0.05834537 0.05836208 0.0583389 ]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06108088 0.0556445 ]\n"," [0.0610652  0.05567438]\n"," [0.06106229 0.05567948]\n"," [0.06107977 0.05564689]]\n","\n","Average MAE Loss:\n","[0.05836269 0.05836979 0.05837089 0.05836333]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06108574 0.05563565]\n"," [0.0610652  0.05567438]\n"," [0.06105994 0.05568358]\n"," [0.06110096 0.05561067]]\n","\n","Average MAE Loss:\n","[0.0583607  0.05836979 0.05837176 0.05835582]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06109008 0.05562795]\n"," [0.0610652  0.05567438]\n"," [0.06105782 0.05568727]\n"," [0.061111   0.05559473]]\n","\n","Average MAE Loss:\n","[0.05835902 0.05836979 0.05837255 0.05835287]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06109429 0.05562066]\n"," [0.0610652  0.05567438]\n"," [0.06105589 0.05569062]\n"," [0.06112039 0.0555807 ]]\n","\n","Average MAE Loss:\n","[0.05835747 0.05836979 0.05837325 0.05835054]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.0610984  0.05561367]\n"," [0.0610652  0.05567438]\n"," [0.06105499 0.05569218]\n"," [0.06112913 0.05556823]]\n","\n","Average MAE Loss:\n","[0.05835604 0.05836979 0.05837358 0.05834868]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0611004  0.05561027]\n"," [0.0610652  0.05567438]\n"," [0.06105411 0.05569366]\n"," [0.061137   0.05555741]]\n","\n","Average MAE Loss:\n","[0.05835534 0.05836979 0.05837389 0.0583472 ]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06110241 0.0556069 ]\n"," [0.0610652  0.05567438]\n"," [0.06105327 0.05569509]\n"," [0.06114072 0.05555251]]\n","\n","Average MAE Loss:\n","[0.05835466 0.05836979 0.05837418 0.05834661]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06106734 0.0556701 ]\n"," [0.06114072 0.05555251]\n"," [0.06110034 0.05560996]\n"," [0.06105826 0.0556846 ]]\n","\n","Average MAE Loss:\n","[0.05836872 0.05834661 0.05835515 0.05837143]\n","\n","Epoch 00050: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0610697  0.05566546]\n"," [0.06114071 0.05555251]\n"," [0.06109895 0.05561208]\n"," [0.06106437 0.05567241]]\n","\n","Average MAE Loss:\n","[0.05836758 0.05834661 0.05835551 0.05836839]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0610709  0.05566311]\n"," [0.06114071 0.05555251]\n"," [0.0610975  0.05561431]\n"," [0.06107063 0.05566049]]\n","\n","Average MAE Loss:\n","[0.058367   0.05834661 0.0583559  0.05836556]\n","\n","Epoch 00052: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0610721  0.05566077]\n"," [0.06114071 0.05555251]\n"," [0.06109605 0.05561653]\n"," [0.06107371 0.05565476]]\n","\n","Average MAE Loss:\n","[0.05836644 0.05834661 0.05835629 0.05836423]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06107333 0.05565843]\n"," [0.06114071 0.05555251]\n"," [0.06109463 0.05561872]\n"," [0.06107669 0.05564923]]\n","\n","Average MAE Loss:\n","[0.05836588 0.05834661 0.05835667 0.05836296]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06107454 0.05565609]\n"," [0.06114072 0.05555251]\n"," [0.06109392 0.05561979]\n"," [0.06107961 0.05564387]]\n","\n","Average MAE Loss:\n","[0.05836532 0.05834661 0.05835686 0.05836174]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06107515 0.05565492]\n"," [0.06114072 0.05555251]\n"," [0.06109323 0.05562086]\n"," [0.06108253 0.05563866]]\n","\n","Average MAE Loss:\n","[0.05836504 0.05834661 0.05835704 0.05836059]\n","\n","Epoch 00056: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06109777 0.05561436]\n"," [0.06109717 0.05561537]\n"," [0.06109646 0.05561648]\n"," [0.06109859 0.05561305]]\n","\n","Average MAE Loss:\n","[0.05835607 0.05835627 0.05835647 0.05835582]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06109833 0.05561342]\n"," [0.06109717 0.05561537]\n"," [0.06109573 0.05561759]\n"," [0.06109994 0.05561085]]\n","\n","Average MAE Loss:\n","[0.05835588 0.05835627 0.05835666 0.05835539]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06109888 0.05561248]\n"," [0.06109717 0.05561537]\n"," [0.06109537 0.05561814]\n"," [0.06110126 0.05560869]]\n","\n","Average MAE Loss:\n","[0.05835568 0.05835627 0.05835676 0.05835498]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06109916 0.05561201]\n"," [0.06109717 0.05561537]\n"," [0.06109502 0.0556187 ]\n"," [0.06110259 0.05560657]]\n","\n","Average MAE Loss:\n","[0.05835559 0.05835627 0.05835686 0.05835458]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","\n","epochs finished with time:1657.414543390274\n","\n","[[0.06109916 0.05561201]\n"," [0.06109717 0.05561537]\n"," [0.06109502 0.0556187 ]\n"," [0.06110259 0.05560657]]\n","00:28:1.0087540600002285\n","------------------------------------Fold [4/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05038958 0.06679778]\n"," [0.49439925 0.47389853]\n"," [0.06046459 0.08182992]\n"," [0.05846939 0.0784977 ]]\n","\n","Average MAE Loss:\n","[0.05859368 0.48414889 0.07114725 0.06848354]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.05005836 0.06765569]\n"," [0.49439925 0.47389853]\n"," [0.0505324  0.0694095 ]\n"," [0.05121048 0.06658414]]\n","\n","Average MAE Loss:\n","[0.05885702 0.48414889 0.05997095 0.05889731]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04990462 0.06767469]\n"," [0.49439925 0.47389853]\n"," [0.04945537 0.06593006]\n"," [0.05090311 0.06629895]]\n","\n","Average MAE Loss:\n","[0.05878966 0.48414889 0.05769272 0.05860103]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0496258  0.0673959 ]\n"," [0.49439925 0.47389853]\n"," [0.04893032 0.06562588]\n"," [0.05040731 0.06684901]]\n","\n","Average MAE Loss:\n","[0.05851085 0.48414889 0.0572781  0.05862816]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04927368 0.06718893]\n"," [0.49439925 0.47389853]\n"," [0.04797336 0.06555836]\n"," [0.05033441 0.06658221]]\n","\n","Average MAE Loss:\n","[0.0582313  0.48414889 0.05676586 0.05845831]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04871364 0.06664641]\n"," [0.49439925 0.47389853]\n"," [0.0469672  0.06453902]\n"," [0.05024933 0.06667832]]\n","\n","Average MAE Loss:\n","[0.05768002 0.48414889 0.05575311 0.05846383]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04808463 0.06531572]\n"," [0.49439925 0.47389853]\n"," [0.04600231 0.06341885]\n"," [0.05019692 0.0667813 ]]\n","\n","Average MAE Loss:\n","[0.05670017 0.48414889 0.05471058 0.05848911]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07925966 0.10130205]\n"," [0.05019693 0.0667813 ]\n"," [0.04815068 0.06514193]\n"," [0.04693589 0.0642501 ]]\n","\n","Average MAE Loss:\n","[0.09028085 0.05848912 0.05664631 0.055593  ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05536401 0.06785452]\n"," [0.05019692 0.0667813 ]\n"," [0.044297   0.06286148]\n"," [0.04600517 0.06408203]]\n","\n","Average MAE Loss:\n","[0.06160927 0.05848911 0.05357924 0.0550436 ]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05076414 0.06637017]\n"," [0.05019693 0.0667813 ]\n"," [0.04435771 0.06728842]\n"," [0.04595052 0.0643281 ]]\n","\n","Average MAE Loss:\n","[0.05856716 0.05848911 0.05582307 0.05513931]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04993149 0.06671509]\n"," [0.05019692 0.0667813 ]\n"," [0.04989136 0.07162566]\n"," [0.04576229 0.06382397]]\n","\n","Average MAE Loss:\n","[0.05832329 0.05848911 0.06075851 0.05479313]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04989889 0.06641443]\n"," [0.05019692 0.0667813 ]\n"," [0.04484949 0.06247767]\n"," [0.04562302 0.06383823]]\n","\n","Average MAE Loss:\n","[0.05815666 0.05848911 0.05366358 0.05473063]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04975386 0.06621917]\n"," [0.05019693 0.0667813 ]\n"," [0.04634526 0.06552193]\n"," [0.04549209 0.06368995]]\n","\n","Average MAE Loss:\n","[0.05798651 0.05848911 0.0559336  0.05459102]\n","\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04953132 0.0661917 ]\n"," [0.05019692 0.0667813 ]\n"," [0.04669333 0.06640069]\n"," [0.04549205 0.06368498]]\n","\n","Average MAE Loss:\n","[0.05786151 0.05848911 0.05654701 0.05458851]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.32269892 0.30233943]\n"," [0.363352   0.34354836]\n"," [0.32088706 0.30008447]\n"," [0.050567   0.06804503]]\n","\n","Average MAE Loss:\n","[0.31251918 0.35345018 0.31048577 0.05930602]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.15676492 0.14709109]\n"," [0.363352   0.34354836]\n"," [0.05855186 0.07921945]\n"," [0.08406854 0.10648765]]\n","\n","Average MAE Loss:\n","[0.15192801 0.35345018 0.06888565 0.0952781 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05738163 0.07797428]\n"," [0.363352   0.34354836]\n"," [0.05660229 0.07694624]\n"," [0.08409759 0.10652447]]\n","\n","Average MAE Loss:\n","[0.06767796 0.35345018 0.06677427 0.09531103]\n","\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00017: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05027188 0.06726003]\n"," [0.363352   0.34354836]\n"," [0.05186845 0.06646061]\n"," [0.08409826 0.10652534]]\n","\n","Average MAE Loss:\n","[0.05876596 0.35345018 0.05916453 0.0953118 ]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.05130767 0.06636355]\n"," [0.363352   0.34354836]\n"," [0.05074284 0.06649376]\n"," [0.08409795 0.10652493]]\n","\n","Average MAE Loss:\n","[0.05883561 0.35345018 0.0586183  0.09531144]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.05052339 0.06659551]\n"," [0.363352   0.34354836]\n"," [0.05048371 0.06667528]\n"," [0.08409746 0.10652427]]\n","\n","Average MAE Loss:\n","[0.05855945 0.35345018 0.0585795  0.09531087]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.05047697 0.0666313 ]\n"," [0.363352   0.34354836]\n"," [0.05066018 0.0665193 ]\n"," [0.08409688 0.1065235 ]]\n","\n","Average MAE Loss:\n","[0.05855414 0.35345018 0.05858974 0.09531019]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.35853311 0.33857542]\n"," [0.08409688 0.1065235 ]\n"," [0.0505611  0.06654529]\n"," [0.05045208 0.06669237]]\n","\n","Average MAE Loss:\n","[0.34855427 0.09531019 0.05855319 0.05857223]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.34999016 0.32974893]\n"," [0.08409688 0.1065235 ]\n"," [0.05056017 0.06652994]\n"," [0.05048998 0.06664887]]\n","\n","Average MAE Loss:\n","[0.33986954 0.09531019 0.05854506 0.05856943]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.34384882 0.3234036 ]\n"," [0.08409688 0.1065235 ]\n"," [0.05052943 0.06653386]\n"," [0.05050286 0.06663423]]\n","\n","Average MAE Loss:\n","[0.33362621 0.09531019 0.05853165 0.05856854]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.33514291 0.31444436]\n"," [0.08409688 0.1065235 ]\n"," [0.0505164  0.06652635]\n"," [0.0504784  0.06665904]]\n","\n","Average MAE Loss:\n","[0.32479364 0.09531019 0.05852137 0.05856872]\n","\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.32224095 0.301287  ]\n"," [0.08409688 0.1065235 ]\n"," [0.05050667 0.06652576]\n"," [0.05050887 0.06662633]]\n","\n","Average MAE Loss:\n","[0.31176397 0.09531019 0.05851622 0.0585676 ]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.30245087 0.28143588]\n"," [0.08409688 0.1065235 ]\n"," [0.05049755 0.06652535]\n"," [0.05053036 0.06660505]]\n","\n","Average MAE Loss:\n","[0.29194337 0.09531019 0.05851145 0.0585677 ]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.28841925 0.26754922]\n"," [0.08409688 0.1065235 ]\n"," [0.05048969 0.0665245 ]\n"," [0.05052213 0.06661206]]\n","\n","Average MAE Loss:\n","[0.27798423 0.09531019 0.0585071  0.0585671 ]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.0504956  0.06670526]\n"," [0.05135102 0.06635541]\n"," [0.05095095 0.06640613]\n"," [0.05057421 0.06661262]]\n","\n","Average MAE Loss:\n","[0.05860043 0.05885321 0.05867854 0.05859341]\n","\n","Epoch 00029: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05047032 0.06673668]\n"," [0.05135102 0.06635541]\n"," [0.05079202 0.06646004]\n"," [0.05053578 0.06665238]]\n","\n","Average MAE Loss:\n","[0.0586035  0.05885321 0.05862603 0.05859408]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05051659 0.0666701 ]\n"," [0.05135102 0.06635541]\n"," [0.05070936 0.06649777]\n"," [0.05059353 0.06659219]]\n","\n","Average MAE Loss:\n","[0.05859335 0.05885321 0.05860356 0.05859286]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05054044 0.0666409 ]\n"," [0.05135102 0.06635541]\n"," [0.0506658  0.0665203 ]\n"," [0.05061444 0.0665722 ]]\n","\n","Average MAE Loss:\n","[0.05859067 0.05885321 0.05859305 0.05859332]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05055976 0.06661853]\n"," [0.05135102 0.06635541]\n"," [0.05064563 0.06652937]\n"," [0.0506117  0.06657357]]\n","\n","Average MAE Loss:\n","[0.05858915 0.05885321 0.0585875  0.05859263]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05057583 0.06660067]\n"," [0.05135102 0.06635541]\n"," [0.05064039 0.06653072]\n"," [0.0506098  0.06657472]]\n","\n","Average MAE Loss:\n","[0.05858825 0.05885321 0.05858555 0.05859226]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05058889 0.06658639]\n"," [0.05135102 0.06635541]\n"," [0.05063623 0.06653132]\n"," [0.05060993 0.0665741 ]]\n","\n","Average MAE Loss:\n","[0.05858764 0.05885321 0.05858378 0.05859201]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05128967 0.06635615]\n"," [0.05060993 0.0665741 ]\n"," [0.05059764 0.06657597]\n"," [0.0506181  0.06654394]]\n","\n","Average MAE Loss:\n","[0.05882291 0.05859201 0.0585868  0.05858102]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05119438 0.06636356]\n"," [0.05060992 0.0665741 ]\n"," [0.05060931 0.06656384]\n"," [0.05059855 0.0665587 ]]\n","\n","Average MAE Loss:\n","[0.05877897 0.05859201 0.05858658 0.05857863]\n","\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00037: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05110805 0.06637508]\n"," [0.05060993 0.0665741 ]\n"," [0.05061379 0.06655907]\n"," [0.050593   0.06656295]]\n","\n","Average MAE Loss:\n","[0.05874156 0.05859201 0.05858643 0.05857797]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05103541 0.06638803]\n"," [0.05060993 0.0665741 ]\n"," [0.0506174  0.06655511]\n"," [0.05059052 0.06656468]]\n","\n","Average MAE Loss:\n","[0.05871172 0.05859201 0.05858625 0.0585776 ]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.051004   0.06639484]\n"," [0.05060993 0.0665741 ]\n"," [0.05062    0.066552  ]\n"," [0.05058935 0.06656531]]\n","\n","Average MAE Loss:\n","[0.05869942 0.05859201 0.058586   0.05857733]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05097494 0.06640168]\n"," [0.05060993 0.0665741 ]\n"," [0.05062182 0.06654951]\n"," [0.05058841 0.06656577]]\n","\n","Average MAE Loss:\n","[0.05868831 0.05859201 0.05858566 0.05857709]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00041: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05094815 0.06640847]\n"," [0.05060993 0.0665741 ]\n"," [0.05062246 0.06654846]\n"," [0.05058827 0.06656572]]\n","\n","Average MAE Loss:\n","[0.05867831 0.05859201 0.05858546 0.058577  ]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05066984 0.0665203 ]\n"," [0.0506793  0.06651469]\n"," [0.05067766 0.06651523]\n"," [0.05067066 0.06652   ]]\n","\n","Average MAE Loss:\n","[0.05859507 0.05859699 0.05859645 0.05859533]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05066798 0.06652131]\n"," [0.0506793  0.06651469]\n"," [0.05067472 0.06651659]\n"," [0.05065859 0.0665277 ]]\n","\n","Average MAE Loss:\n","[0.05859464 0.05859699 0.05859566 0.05859315]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05066656 0.06652202]\n"," [0.0506793  0.06651469]\n"," [0.05067177 0.06651798]\n"," [0.05064851 0.06653441]]\n","\n","Average MAE Loss:\n","[0.05859429 0.05859699 0.05859487 0.05859146]\n","\n","Epoch 00045: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00045: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05066523 0.06652267]\n"," [0.0506793  0.06651469]\n"," [0.05067038 0.06651862]\n"," [0.05064432 0.06653733]]\n","\n","Average MAE Loss:\n","[0.05859395 0.05859699 0.0585945  0.05859082]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05066393 0.0665233 ]\n"," [0.0506793  0.06651469]\n"," [0.05066906 0.06651922]\n"," [0.05064069 0.06653986]]\n","\n","Average MAE Loss:\n","[0.05859361 0.05859699 0.05859414 0.05859028]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0506633  0.0665236 ]\n"," [0.0506793  0.06651469]\n"," [0.05066777 0.0665198 ]\n"," [0.05063736 0.06654221]]\n","\n","Average MAE Loss:\n","[0.05859345 0.05859699 0.05859378 0.05858978]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05066269 0.06652389]\n"," [0.0506793  0.06651469]\n"," [0.05066651 0.06652036]\n"," [0.05063427 0.0665444 ]]\n","\n","Average MAE Loss:\n","[0.05859329 0.05859699 0.05859344 0.05858934]\n","\n","Epoch 00049: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00049: reducing learning rate of group 0 to 1.9531e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05067853 0.06651506]\n"," [0.05063427 0.0665444 ]\n"," [0.05066212 0.06652413]\n"," [0.05066404 0.06652191]]\n","\n","Average MAE Loss:\n","[0.0585968  0.05858934 0.05859313 0.05859298]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05067767 0.06651549]\n"," [0.05063427 0.0665444 ]\n"," [0.05066158 0.06652436]\n"," [0.05066106 0.0665238 ]]\n","\n","Average MAE Loss:\n","[0.05859658 0.05858934 0.05859297 0.05859243]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05067723 0.06651571]\n"," [0.05063427 0.0665444 ]\n"," [0.05066103 0.0665246 ]\n"," [0.05065819 0.06652564]]\n","\n","Average MAE Loss:\n","[0.05859647 0.05858934 0.05859281 0.05859191]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05067679 0.06651593]\n"," [0.05063427 0.0665444 ]\n"," [0.05066051 0.06652481]\n"," [0.05065544 0.06652741]]\n","\n","Average MAE Loss:\n","[0.05859636 0.05858934 0.05859266 0.05859143]\n","\n","Epoch 00053: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05067636 0.06651615]\n"," [0.05063427 0.0665444 ]\n"," [0.05066025 0.06652492]\n"," [0.05065415 0.06652825]]\n","\n","Average MAE Loss:\n","[0.05859626 0.05858934 0.05859259 0.0585912 ]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05067591 0.06651638]\n"," [0.05063427 0.0665444 ]\n"," [0.05065999 0.06652502]\n"," [0.05065288 0.06652908]]\n","\n","Average MAE Loss:\n","[0.05859615 0.05858934 0.0585925  0.05859098]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05067569 0.06651649]\n"," [0.05063427 0.0665444 ]\n"," [0.05065972 0.06652513]\n"," [0.05065165 0.06652988]]\n","\n","Average MAE Loss:\n","[0.05859609 0.05858934 0.05859243 0.05859077]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05065494 0.06652889]\n"," [0.05065509 0.06652881]\n"," [0.05065487 0.06652889]\n"," [0.05065384 0.06652963]]\n","\n","Average MAE Loss:\n","[0.05859192 0.05859195 0.05859188 0.05859173]\n","\n","Epoch 00057: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05065482 0.06652893]\n"," [0.05065509 0.0665288 ]\n"," [0.05065477 0.06652892]\n"," [0.05065321 0.06653003]]\n","\n","Average MAE Loss:\n","[0.05859188 0.05859195 0.05859185 0.05859162]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05065471 0.06652898]\n"," [0.05065509 0.06652881]\n"," [0.05065467 0.06652896]\n"," [0.0506526  0.06653044]]\n","\n","Average MAE Loss:\n","[0.05859184 0.05859195 0.05859181 0.05859152]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05065466 0.066529  ]\n"," [0.0506551  0.06652881]\n"," [0.05065456 0.066529  ]\n"," [0.050652   0.06653084]]\n","\n","Average MAE Loss:\n","[0.05859183 0.05859195 0.05859178 0.05859142]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","\n","epochs finished with time:1711.1082210540771\n","\n","[[0.05065466 0.066529  ]\n"," [0.05065509 0.06652881]\n"," [0.05065456 0.066529  ]\n"," [0.050652   0.06653084]]\n","00:28:54.0182896489996\n","------------------------------------Fold [5/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.06211826 0.06631997]\n"," [0.47447544 0.47155899]\n"," [0.08108941 0.08522234]\n"," [0.07802549 0.08065291]]\n","\n","Average MAE Loss:\n","[0.06421912 0.47301722 0.08315588 0.0793392 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06389604 0.06819174]\n"," [0.47447541 0.47155899]\n"," [0.06405827 0.06848391]\n"," [0.06224061 0.06566508]]\n","\n","Average MAE Loss:\n","[0.06604389 0.4730172  0.06627109 0.06395284]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06401996 0.06849476]\n"," [0.47447541 0.47155899]\n"," [0.06007738 0.06492054]\n"," [0.06236004 0.06584124]]\n","\n","Average MAE Loss:\n","[0.06625736 0.4730172  0.06249896 0.06410064]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.06339253 0.06813604]\n"," [0.47447544 0.47155899]\n"," [0.05948347 0.06496292]\n"," [0.06316914 0.0665807 ]]\n","\n","Average MAE Loss:\n","[0.06576428 0.47301722 0.06222319 0.06487492]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.06298908 0.0681091 ]\n"," [0.47447541 0.47155899]\n"," [0.05885373 0.06522028]\n"," [0.0627563  0.06618761]]\n","\n","Average MAE Loss:\n","[0.06554909 0.4730172  0.06203701 0.06447195]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.06094907 0.0662828 ]\n"," [0.47447541 0.47155899]\n"," [0.05706935 0.06418591]\n"," [0.06279983 0.06618491]]\n","\n","Average MAE Loss:\n","[0.06361594 0.4730172  0.06062763 0.06449237]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.06022364 0.06583227]\n"," [0.47447541 0.47155899]\n"," [0.05553553 0.06243549]\n"," [0.06213596 0.06559568]]\n","\n","Average MAE Loss:\n","[0.06302796 0.4730172  0.05898551 0.06386582]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08654974 0.08979321]\n"," [0.06213596 0.06559568]\n"," [0.05914644 0.06550129]\n"," [0.05532772 0.06253439]]\n","\n","Average MAE Loss:\n","[0.08817148 0.06386582 0.06232386 0.05893106]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0740333  0.07748023]\n"," [0.06213596 0.06559568]\n"," [0.05640921 0.06358972]\n"," [0.0559928  0.06473799]]\n","\n","Average MAE Loss:\n","[0.07575677 0.06386582 0.05999947 0.06036539]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06229988 0.06602471]\n"," [0.06213596 0.06559568]\n"," [0.05370722 0.06245195]\n"," [0.05469463 0.06214744]]\n","\n","Average MAE Loss:\n","[0.0641623  0.06386582 0.05807958 0.05842103]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06214115 0.06598274]\n"," [0.06213596 0.06559568]\n"," [0.05277536 0.06017627]\n"," [0.05441769 0.06243458]]\n","\n","Average MAE Loss:\n","[0.06406194 0.06386582 0.05647582 0.05842613]\n","\n","Epoch 00011: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.06211384 0.06601828]\n"," [0.06213596 0.06559568]\n"," [0.0572484  0.06722184]\n"," [0.05383661 0.06169957]]\n","\n","Average MAE Loss:\n","[0.06406606 0.06386582 0.06223512 0.05776809]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06217175 0.06614745]\n"," [0.06213596 0.06559568]\n"," [0.05571539 0.06515741]\n"," [0.05333582 0.06115117]]\n","\n","Average MAE Loss:\n","[0.0641596  0.06386582 0.0604364  0.05724349]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06198803 0.06600822]\n"," [0.06213596 0.06559568]\n"," [0.05665123 0.07053792]\n"," [0.0531093  0.06134104]]\n","\n","Average MAE Loss:\n","[0.06399812 0.06386582 0.06359457 0.05722517]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.33277062 0.3276678 ]\n"," [0.34460148 0.34013826]\n"," [0.13461956 0.12910664]\n"," [0.27737716 0.26738524]]\n","\n","Average MAE Loss:\n","[0.33021921 0.34236987 0.1318631  0.2723812 ]\n","\n","Epoch 00015: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.31711146 0.31124607]\n"," [0.34460148 0.34013826]\n"," [0.08338484 0.08694958]\n"," [0.09299108 0.09606426]]\n","\n","Average MAE Loss:\n","[0.31417876 0.34236987 0.08516721 0.09452767]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.2760835  0.26889154]\n"," [0.34460148 0.34013826]\n"," [0.06201761 0.06520822]\n"," [0.07439914 0.07714258]]\n","\n","Average MAE Loss:\n","[0.27248752 0.34236987 0.06361292 0.07577086]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.17002186 0.16355416]\n"," [0.34460148 0.34013826]\n"," [0.06281333 0.06685013]\n"," [0.06178933 0.06506782]]\n","\n","Average MAE Loss:\n","[0.16678801 0.34236987 0.06483173 0.06342857]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06239671 0.06546004]\n"," [0.34460148 0.34013826]\n"," [0.06138324 0.0654051 ]\n"," [0.06376332 0.06703629]]\n","\n","Average MAE Loss:\n","[0.06392838 0.34236987 0.06339417 0.0653998 ]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00019: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.0623904  0.06646948]\n"," [0.34460148 0.34013826]\n"," [0.06173621 0.06584034]\n"," [0.0626756  0.06604822]]\n","\n","Average MAE Loss:\n","[0.06442994 0.34236987 0.06378828 0.06436191]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0628745  0.06693947]\n"," [0.34460148 0.34013826]\n"," [0.0615696  0.06568073]\n"," [0.06265683 0.06603083]]\n","\n","Average MAE Loss:\n","[0.06490698 0.34236987 0.06362516 0.06434383]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.34284204 0.33826032]\n"," [0.06265683 0.06603083]\n"," [0.06138884 0.06541356]\n"," [0.06221355 0.06636011]]\n","\n","Average MAE Loss:\n","[0.34055118 0.06434383 0.0634012  0.06428683]\n","\n","Epoch 00022: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.3400282  0.33525699]\n"," [0.06265683 0.06603083]\n"," [0.06175562 0.06587713]\n"," [0.06212304 0.0662678 ]]\n","\n","Average MAE Loss:\n","[0.3376426  0.06434383 0.06381638 0.06419542]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00023: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.33850715 0.33363128]\n"," [0.06265683 0.06603083]\n"," [0.06160365 0.06572251]\n"," [0.0620097  0.06615371]]\n","\n","Average MAE Loss:\n","[0.33606921 0.06434383 0.06366308 0.0640817 ]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.33689237 0.33190283]\n"," [0.06265683 0.06603083]\n"," [0.06159988 0.06573519]\n"," [0.06201496 0.06615905]]\n","\n","Average MAE Loss:\n","[0.3343976  0.06434383 0.06366753 0.064087  ]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.33514795 0.33003336]\n"," [0.06265683 0.06603083]\n"," [0.06158652 0.06574539]\n"," [0.06203593 0.06618017]]\n","\n","Average MAE Loss:\n","[0.33259065 0.06434383 0.06366596 0.06410805]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.33322227 0.32796681]\n"," [0.06265683 0.06603083]\n"," [0.06152537 0.06571012]\n"," [0.06203391 0.06617815]]\n","\n","Average MAE Loss:\n","[0.33059454 0.06434383 0.06361775 0.06410603]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.33216187 0.32682809]\n"," [0.06265683 0.06603083]\n"," [0.06151225 0.0657119 ]\n"," [0.06202311 0.06616732]]\n","\n","Average MAE Loss:\n","[0.32949498 0.06434383 0.06361208 0.06409522]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.10318595 0.09977208]\n"," [0.11362619 0.10936149]\n"," [0.07427268 0.074545  ]\n"," [0.07281049 0.07333215]]\n","\n","Average MAE Loss:\n","[0.10147902 0.11149384 0.07440884 0.07307132]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.09109774 0.0888777 ]\n"," [0.11362619 0.10936149]\n"," [0.06132712 0.06496001]\n"," [0.06137317 0.0651672 ]]\n","\n","Average MAE Loss:\n","[0.08998772 0.11149384 0.06314357 0.06327019]\n","\n","Epoch 00030: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.0815663  0.08070225]\n"," [0.11362619 0.10936149]\n"," [0.06213095 0.06606356]\n"," [0.06195299 0.06587605]]\n","\n","Average MAE Loss:\n","[0.08113427 0.11149384 0.06409726 0.06391452]\n","\n","Epoch 00031: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00031: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.0779524  0.07764237]\n"," [0.11362619 0.10936149]\n"," [0.06214146 0.06607775]\n"," [0.06218965 0.06611951]]\n","\n","Average MAE Loss:\n","[0.07779739 0.11149384 0.0641096  0.06415458]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07498424 0.07513238]\n"," [0.11362619 0.10936149]\n"," [0.06200553 0.06594118]\n"," [0.06223491 0.06616404]]\n","\n","Average MAE Loss:\n","[0.07505831 0.11149384 0.06397336 0.06419947]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07248627 0.07306994]\n"," [0.11362619 0.10936149]\n"," [0.06190461 0.06583948]\n"," [0.06223349 0.06616256]]\n","\n","Average MAE Loss:\n","[0.07277811 0.11149384 0.06387205 0.06419803]\n","\n","Epoch 00034: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.07039028 0.07137209]\n"," [0.11362619 0.10936149]\n"," [0.06184135 0.06577732]\n"," [0.06222954 0.06615865]]\n","\n","Average MAE Loss:\n","[0.07088119 0.11149384 0.06380934 0.0641941 ]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00035: reducing learning rate of group 0 to 1.5625e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.11108709 0.10703351]\n"," [0.06222954 0.06615865]\n"," [0.06735251 0.06899001]\n"," [0.06187974 0.06581759]]\n","\n","Average MAE Loss:\n","[0.1090603  0.0641941  0.06817126 0.06384867]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.10832095 0.10448848]\n"," [0.06222954 0.06615865]\n"," [0.06428719 0.06665493]\n"," [0.06194051 0.06588104]]\n","\n","Average MAE Loss:\n","[0.10640471 0.0641941  0.06547106 0.06391078]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.10567474 0.10205309]\n"," [0.06222954 0.06615865]\n"," [0.06259742 0.06549156]\n"," [0.06199548 0.06593896]]\n","\n","Average MAE Loss:\n","[0.10386391 0.0641941  0.06404449 0.06396722]\n","\n","Epoch 00038: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.10317294 0.09975502]\n"," [0.06222954 0.06615865]\n"," [0.06178445 0.06502655]\n"," [0.06201794 0.06596281]]\n","\n","Average MAE Loss:\n","[0.10146398 0.0641941  0.0634055  0.06399038]\n","\n","Epoch 00039: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00039: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.10198073 0.09866567]\n"," [0.06222954 0.06615865]\n"," [0.06157713 0.06495062]\n"," [0.06203759 0.0659835 ]]\n","\n","Average MAE Loss:\n","[0.1003232  0.0641941  0.06326387 0.06401055]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.10083471 0.09761796]\n"," [0.06222954 0.06615865]\n"," [0.06144428 0.06492408]\n"," [0.06205526 0.06600197]]\n","\n","Average MAE Loss:\n","[0.09922634 0.0641941  0.06318418 0.06402862]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.09972653 0.09661051]\n"," [0.06222954 0.06615865]\n"," [0.06137051 0.06492434]\n"," [0.06207167 0.06601896]]\n","\n","Average MAE Loss:\n","[0.09816852 0.0641941  0.06314742 0.06404531]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06242897 0.06539019]\n"," [0.06255533 0.06546968]\n"," [0.06218603 0.06524322]\n"," [0.06236639 0.06535114]]\n","\n","Average MAE Loss:\n","[0.06390958 0.06401251 0.06371463 0.06385877]\n","\n","Epoch 00043: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00043: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06239189 0.06536686]\n"," [0.06255533 0.06546968]\n"," [0.06201153 0.0651435 ]\n"," [0.06210868 0.06519857]]\n","\n","Average MAE Loss:\n","[0.06387938 0.06401251 0.06357752 0.06365363]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06235868 0.0653463 ]\n"," [0.06255533 0.06546968]\n"," [0.06186211 0.06506545]\n"," [0.06189559 0.06508173]]\n","\n","Average MAE Loss:\n","[0.06385249 0.06401251 0.06346378 0.06348866]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0623261  0.06532647]\n"," [0.06255533 0.06546968]\n"," [0.06173894 0.06500991]\n"," [0.06172736 0.06500522]]\n","\n","Average MAE Loss:\n","[0.06382628 0.06401251 0.06337443 0.06336629]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06229381 0.06530705]\n"," [0.06255533 0.06546968]\n"," [0.06163719 0.06497186]\n"," [0.06165898 0.06497957]]\n","\n","Average MAE Loss:\n","[0.06380043 0.06401251 0.06330452 0.06331928]\n","\n","Epoch 00047: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06227796 0.06529744]\n"," [0.06255533 0.06546968]\n"," [0.06159215 0.06495702]\n"," [0.06159792 0.06495867]]\n","\n","Average MAE Loss:\n","[0.0637877  0.06401251 0.06327458 0.0632783 ]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06226213 0.06528778]\n"," [0.06255533 0.06546968]\n"," [0.06155002 0.06494511]\n"," [0.06154187 0.06494291]]\n","\n","Average MAE Loss:\n","[0.06377495 0.06401251 0.06324757 0.06324239]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06253657 0.06545786]\n"," [0.06154187 0.06494291]\n"," [0.06217233 0.06523525]\n"," [0.06150104 0.06493411]]\n","\n","Average MAE Loss:\n","[0.06399721 0.06324239 0.06370379 0.06321757]\n","\n","Epoch 00050: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06251745 0.0654458 ]\n"," [0.06154187 0.06494291]\n"," [0.06208131 0.0651826 ]\n"," [0.06148006 0.06493006]]\n","\n","Average MAE Loss:\n","[0.06398162 0.06324239 0.06363195 0.06320506]\n","\n","Epoch 00051: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06250784 0.06543972]\n"," [0.06154187 0.06494291]\n"," [0.06203841 0.06515845]\n"," [0.06146117 0.06492686]]\n","\n","Average MAE Loss:\n","[0.06397378 0.06324239 0.06359843 0.06319401]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06249817 0.06543363]\n"," [0.06154187 0.06494291]\n"," [0.06199742 0.0651355 ]\n"," [0.06144362 0.06492459]]\n","\n","Average MAE Loss:\n","[0.0639659  0.06324239 0.06356646 0.06318411]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06248844 0.06542754]\n"," [0.06154187 0.06494291]\n"," [0.06195812 0.065114  ]\n"," [0.06142706 0.06492295]]\n","\n","Average MAE Loss:\n","[0.06395799 0.06324239 0.06353606 0.06317501]\n","\n","Epoch 00054: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06247868 0.06542145]\n"," [0.06154187 0.06494291]\n"," [0.06192022 0.06509409]\n"," [0.06141936 0.06492224]]\n","\n","Average MAE Loss:\n","[0.06395007 0.06324239 0.06350716 0.0631708 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06247378 0.06541838]\n"," [0.06154187 0.06494291]\n"," [0.06190177 0.06508466]\n"," [0.06141205 0.06492165]]\n","\n","Average MAE Loss:\n","[0.06394608 0.06324239 0.06349322 0.06316685]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06174864 0.06501393]\n"," [0.06175118 0.06501503]\n"," [0.06173688 0.06500903]\n"," [0.06173468 0.06500813]]\n","\n","Average MAE Loss:\n","[0.06338128 0.0633831  0.06337295 0.0633714 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06174632 0.06501293]\n"," [0.06175118 0.06501503]\n"," [0.06172316 0.06500352]\n"," [0.0617171  0.06500111]]\n","\n","Average MAE Loss:\n","[0.06337962 0.0633831  0.06336334 0.0633591 ]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.061744   0.06501196]\n"," [0.06175118 0.06501503]\n"," [0.06170966 0.06499824]\n"," [0.06170838 0.06499773]]\n","\n","Average MAE Loss:\n","[0.06337798 0.0633831  0.06335395 0.06335306]\n","\n","Epoch 00059: reducing learning rate of group 0 to 6.1035e-08.\n","Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06174283 0.06501147]\n"," [0.06175118 0.06501503]\n"," [0.06170297 0.06499568]\n"," [0.06169975 0.06499443]]\n","\n","Average MAE Loss:\n","[0.06337715 0.0633831  0.06334933 0.06334709]\n","\n","\n","epochs finished with time:1745.4864583015442\n","\n","[[0.06174283 0.06501147]\n"," [0.06175118 0.06501503]\n"," [0.06170297 0.06499568]\n"," [0.06169975 0.06499443]]\n","00:29:29.17859688000044\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-3900dce8d1e3>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Meta_4D-FED-GNN++'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_hospital\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gnns_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbosity_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"]}],"source":["# seed = 10 table = 4/8 fixed Meta 2 layers x\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Meta_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaeeFzWheodt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"21b6f1aa-9069-436e-cfef-05ea9b17424f","executionInfo":{"status":"ok","timestamp":1690727794415,"user_tz":-60,"elapsed":4289072,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.25\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.22951756 0.22984366]\n"," [0.49095011 0.49543648]\n"," [0.06533224 0.06207772]\n"," [0.06643012 0.06219492]]\n","\n","Average MAE Loss:\n","[0.22968061 0.49319329 0.06370498 0.06431252]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.22951756 0.22984366]\n"," [0.49095011 0.49543648]\n"," [0.05382052 0.05131982]\n"," [0.05640135 0.05298167]]\n","\n","Average MAE Loss:\n","[0.22968061 0.49319329 0.05257017 0.05469151]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.22951756 0.22984366]\n"," [0.49095011 0.49543648]\n"," [0.05265465 0.05084252]\n"," [0.05392184 0.05089458]]\n","\n","Average MAE Loss:\n","[0.22968061 0.49319329 0.05174859 0.05240821]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.22951756 0.22984366]\n"," [0.49095011 0.49543648]\n"," [0.05175738 0.05029481]\n"," [0.05362499 0.05067039]]\n","\n","Average MAE Loss:\n","[0.22968061 0.49319329 0.0510261  0.05214769]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.22951756 0.22984366]\n"," [0.49095011 0.49543648]\n"," [0.05149432 0.0503583 ]\n"," [0.05343514 0.05055086]]\n","\n","Average MAE Loss:\n","[0.22968061 0.49319329 0.05092631 0.051993  ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.22951756 0.22984366]\n"," [0.49095011 0.49543648]\n"," [0.0513922  0.050652  ]\n"," [0.05348063 0.05053636]]\n","\n","Average MAE Loss:\n","[0.22968061 0.49319329 0.0510221  0.05200849]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.22951756 0.22984366]\n"," [0.49095011 0.49543648]\n"," [0.05095196 0.05059573]\n"," [0.05359182 0.05056643]]\n","\n","Average MAE Loss:\n","[0.22968061 0.49319329 0.05077384 0.05207913]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.49095011 0.49543648]\n"," [0.05359182 0.05056643]\n"," [0.05517014 0.05269517]\n"," [0.05195894 0.05156049]]\n","\n","Average MAE Loss:\n","[0.49319329 0.05207913 0.05393265 0.05175972]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.49095011 0.49543648]\n"," [0.05359182 0.05056643]\n"," [0.05324026 0.05101902]\n"," [0.05088428 0.05071777]]\n","\n","Average MAE Loss:\n","[0.49319329 0.05207913 0.05212964 0.05080102]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.49095011 0.49543648]\n"," [0.05359182 0.05056643]\n"," [0.05312757 0.0508842 ]\n"," [0.05103267 0.05087609]]\n","\n","Average MAE Loss:\n","[0.49319329 0.05207913 0.05200588 0.05095438]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.49095011 0.49543648]\n"," [0.05359182 0.05056643]\n"," [0.05343717 0.05131342]\n"," [0.05074587 0.05096145]]\n","\n","Average MAE Loss:\n","[0.49319329 0.05207913 0.0523753  0.05085366]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.49095011 0.49543648]\n"," [0.05359182 0.05056643]\n"," [0.05231178 0.0506401 ]\n"," [0.05040836 0.0507419 ]]\n","\n","Average MAE Loss:\n","[0.49319329 0.05207913 0.05147594 0.05057513]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.49095011 0.49543648]\n"," [0.05359182 0.05056643]\n"," [0.05222194 0.05075035]\n"," [0.05027784 0.05100923]]\n","\n","Average MAE Loss:\n","[0.49319329 0.05207913 0.05148615 0.05064353]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.49095011 0.49543648]\n"," [0.05359182 0.05056643]\n"," [0.05181753 0.05051163]\n"," [0.05021166 0.0513711 ]]\n","\n","Average MAE Loss:\n","[0.49319329 0.05207913 0.05116458 0.05079138]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.3638133  0.36704929]\n"," [0.34249506 0.34425144]\n"," [0.11253268 0.10647141]]\n","\n","Average MAE Loss:\n","[0.3654313  0.3654313  0.34337325 0.10950204]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.3638133  0.36704929]\n"," [0.28673892 0.28498461]\n"," [0.08702062 0.083834  ]]\n","\n","Average MAE Loss:\n","[0.3654313  0.3654313  0.28586177 0.08542731]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.3638133  0.36704929]\n"," [0.07002635 0.06620029]\n"," [0.08698518 0.08378664]]\n","\n","Average MAE Loss:\n","[0.3654313  0.3654313  0.06811332 0.08538591]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.3638133  0.36704929]\n"," [0.05967205 0.05633144]\n"," [0.08638613 0.08308785]]\n","\n","Average MAE Loss:\n","[0.3654313  0.3654313  0.05800175 0.08473699]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.3638133  0.36704929]\n"," [0.0535714  0.05115266]\n"," [0.08318313 0.07948712]]\n","\n","Average MAE Loss:\n","[0.3654313  0.3654313  0.05236203 0.08133513]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.3638133  0.36704929]\n"," [0.05409408 0.05161428]\n"," [0.05561274 0.05215979]]\n","\n","Average MAE Loss:\n","[0.3654313  0.3654313  0.05285418 0.05388626]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.3638133  0.36704929]\n"," [0.05363779 0.05122546]\n"," [0.05517087 0.05213451]]\n","\n","Average MAE Loss:\n","[0.3654313  0.3654313  0.05243163 0.05365269]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.05517087 0.05213451]\n"," [0.36033132 0.36331303]\n"," [0.05332772 0.05088127]]\n","\n","Average MAE Loss:\n","[0.3654313  0.05365269 0.36182218 0.05210449]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.05517087 0.05213451]\n"," [0.35483072 0.35741138]\n"," [0.05338264 0.05096276]]\n","\n","Average MAE Loss:\n","[0.3654313  0.05365269 0.35612105 0.0521727 ]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.05517087 0.05213451]\n"," [0.35151048 0.35383725]\n"," [0.05334193 0.05090934]]\n","\n","Average MAE Loss:\n","[0.3654313  0.05365269 0.35267386 0.05212563]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.05517087 0.05213451]\n"," [0.34742272 0.34941154]\n"," [0.05338533 0.05096495]]\n","\n","Average MAE Loss:\n","[0.3654313  0.05365269 0.34841713 0.05217514]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.05517087 0.05213451]\n"," [0.34206562 0.34360256]\n"," [0.05340338 0.05098521]]\n","\n","Average MAE Loss:\n","[0.3654313  0.05365269 0.34283409 0.05219429]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.05517087 0.05213451]\n"," [0.33460784 0.33552599]\n"," [0.05338985 0.05096972]]\n","\n","Average MAE Loss:\n","[0.3654313  0.05365269 0.33506691 0.05217978]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.3638133  0.36704929]\n"," [0.05517087 0.05213451]\n"," [0.32955372 0.3300693 ]\n"," [0.0533886  0.05096797]]\n","\n","Average MAE Loss:\n","[0.3654313  0.05365269 0.32981151 0.05217828]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.21869707 0.2128396 ]\n"," [0.19396852 0.18770909]\n"," [0.16341444 0.15684919]]\n","\n","Average MAE Loss:\n","[0.21576834 0.21576834 0.19083881 0.16013182]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.21869707 0.2128396 ]\n"," [0.16184342 0.15550796]\n"," [0.08920076 0.08388365]]\n","\n","Average MAE Loss:\n","[0.21576834 0.21576834 0.15867569 0.0865422 ]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.21869707 0.2128396 ]\n"," [0.13015046 0.12398161]\n"," [0.05742381 0.05431093]]\n","\n","Average MAE Loss:\n","[0.21576834 0.21576834 0.12706603 0.05586737]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.21869707 0.2128396 ]\n"," [0.11599271 0.11005468]\n"," [0.05360521 0.05103273]]\n","\n","Average MAE Loss:\n","[0.21576834 0.21576834 0.1130237  0.05231897]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.21869707 0.2128396 ]\n"," [0.10369809 0.09802397]\n"," [0.05350577 0.05092451]]\n","\n","Average MAE Loss:\n","[0.21576834 0.21576834 0.10086103 0.05221514]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.21869707 0.2128396 ]\n"," [0.09319757 0.08783206]\n"," [0.05350429 0.05092264]]\n","\n","Average MAE Loss:\n","[0.21576834 0.21576834 0.09051481 0.05221347]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.21869707 0.2128396 ]\n"," [0.08443328 0.07950159]\n"," [0.05351627 0.05093608]]\n","\n","Average MAE Loss:\n","[0.21576834 0.21576834 0.08196744 0.05222618]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.05351627 0.05093608]\n"," [0.21384821 0.20787809]\n"," [0.07357352 0.06916459]]\n","\n","Average MAE Loss:\n","[0.21576834 0.05222618 0.21086315 0.07136905]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00036: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.05351627 0.05093608]\n"," [0.20867944 0.20260961]\n"," [0.06733226 0.06328683]]\n","\n","Average MAE Loss:\n","[0.21576834 0.05222618 0.20564452 0.06530955]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.21869705 0.2128396 ]\n"," [0.05351627 0.05093607]\n"," [0.20348086 0.19731647]\n"," [0.06283887 0.05921329]]\n","\n","Average MAE Loss:\n","[0.21576833 0.05222617 0.20039867 0.06102608]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.05351627 0.05093608]\n"," [0.19828699 0.19204032]\n"," [0.05985848 0.05653082]]\n","\n","Average MAE Loss:\n","[0.21576834 0.05222618 0.19516366 0.05819465]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.05351627 0.05093608]\n"," [0.19570434 0.18941749]\n"," [0.05791089 0.05477676]]\n","\n","Average MAE Loss:\n","[0.21576834 0.05222618 0.19256092 0.05634383]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.05351627 0.05093608]\n"," [0.1931487  0.18682945]\n"," [0.05722729 0.05415389]]\n","\n","Average MAE Loss:\n","[0.21576834 0.05222618 0.18998907 0.05569059]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.21869707 0.2128396 ]\n"," [0.05351627 0.05093608]\n"," [0.19061487 0.1842715 ]\n"," [0.05667559 0.05365392]]\n","\n","Average MAE Loss:\n","[0.21576834 0.05222618 0.18744318 0.05516475]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.1122822  0.10631437]\n"," [0.10984342 0.10392327]\n"," [0.10705637 0.10120033]]\n","\n","Average MAE Loss:\n","[0.10929829 0.10929829 0.10688335 0.10412835]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.1122822  0.10631437]\n"," [0.10866622 0.10277052]\n"," [0.1003148  0.09464893]]\n","\n","Average MAE Loss:\n","[0.10929829 0.10929829 0.10571837 0.09748186]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.1122822  0.10631437]\n"," [0.10751803 0.10165   ]\n"," [0.0971809  0.091608  ]]\n","\n","Average MAE Loss:\n","[0.10929829 0.10929829 0.10458401 0.09439445]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.1122822  0.10631437]\n"," [0.10639583 0.10055742]\n"," [0.0943089  0.08882648]]\n","\n","Average MAE Loss:\n","[0.10929829 0.10929829 0.10347663 0.09156769]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.1122822  0.10631437]\n"," [0.1052977  0.09948944]\n"," [0.0916661  0.08629449]]\n","\n","Average MAE Loss:\n","[0.10929829 0.10929829 0.10239357 0.08898029]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.1122822  0.10631437]\n"," [0.10475892 0.09896526]\n"," [0.08922129 0.08396039]]\n","\n","Average MAE Loss:\n","[0.10929829 0.10929829 0.10186209 0.08659084]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.1122822  0.10631437]\n"," [0.10422797 0.09844874]\n"," [0.0880793  0.08287548]]\n","\n","Average MAE Loss:\n","[0.10929829 0.10929829 0.10133835 0.08547739]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.0880793  0.08287548]\n"," [0.11173109 0.10577354]\n"," [0.10289052 0.09715013]]\n","\n","Average MAE Loss:\n","[0.10929829 0.08547739 0.10875232 0.10002032]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.0880793  0.08287548]\n"," [0.11118035 0.1052331 ]\n"," [0.10154099 0.09583828]]\n","\n","Average MAE Loss:\n","[0.10929829 0.08547739 0.10820673 0.09868964]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.0880793  0.08287548]\n"," [0.11090779 0.10496591]\n"," [0.10023669 0.09457223]]\n","\n","Average MAE Loss:\n","[0.10929829 0.08547739 0.10793685 0.09740446]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.0880793  0.08287548]\n"," [0.11063817 0.10470127]\n"," [0.09960718 0.0939621 ]]\n","\n","Average MAE Loss:\n","[0.10929829 0.08547739 0.10766972 0.09678464]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.0880793  0.08287548]\n"," [0.11037091 0.104439  ]\n"," [0.09899613 0.09336985]]\n","\n","Average MAE Loss:\n","[0.10929829 0.08547739 0.10740495 0.09618299]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.0880793  0.08287548]\n"," [0.11010605 0.10417914]\n"," [0.09840256 0.09279405]]\n","\n","Average MAE Loss:\n","[0.10929829 0.08547739 0.1071426  0.0955983 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.1122822  0.10631437]\n"," [0.0880793  0.08287548]\n"," [0.10997471 0.10405033]\n"," [0.09782492 0.09223276]]\n","\n","Average MAE Loss:\n","[0.10929829 0.08547739 0.10701252 0.09502884]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.10174631 0.09603835]\n"," [0.10174631 0.09603835]\n"," [0.10162339 0.09591882]\n"," [0.10145082 0.09575103]]\n","\n","Average MAE Loss:\n","[0.09889233 0.09889233 0.0987711  0.09860093]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.10174631 0.09603835]\n"," [0.10174631 0.09603835]\n"," [0.10150313 0.09580185]\n"," [0.101158   0.0954667 ]]\n","\n","Average MAE Loss:\n","[0.09889233 0.09889233 0.09865249 0.09831235]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.10174631 0.09603835]\n"," [0.10174631 0.09603835]\n"," [0.10138383 0.09568592]\n"," [0.10086989 0.09518709]]\n","\n","Average MAE Loss:\n","[0.09889233 0.09889233 0.09853487 0.09802849]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.10174631 0.09603835]\n"," [0.10174631 0.09603835]\n"," [0.10132451 0.0956283 ]\n"," [0.10058684 0.09491237]]\n","\n","Average MAE Loss:\n","[0.09889233 0.09889233 0.09847641 0.09774961]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","\n","epochs finished with time:1573.7495353221893\n","\n","[[0.10174631 0.09603835]\n"," [0.10174631 0.09603835]\n"," [0.10132451 0.0956283 ]\n"," [0.10058684 0.09491237]]\n","00:26:50.294308503998764\n","------------------------------------Fold [2/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.21430998 0.22112401]\n"," [0.46320858 0.47067857]\n"," [0.0968108  0.0949769 ]\n"," [0.092621   0.089985  ]]\n","\n","Average MAE Loss:\n","[0.21771699 0.46694357 0.09589385 0.091303  ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.21430998 0.22112401]\n"," [0.46320858 0.47067857]\n"," [0.07742678 0.07748413]\n"," [0.07630675 0.0761118 ]]\n","\n","Average MAE Loss:\n","[0.21771699 0.46694357 0.07745546 0.07620927]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.21430998 0.22112401]\n"," [0.46320858 0.47067857]\n"," [0.07485066 0.07552447]\n"," [0.07710516 0.07645527]]\n","\n","Average MAE Loss:\n","[0.21771699 0.46694357 0.07518756 0.07678021]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.21430998 0.22112401]\n"," [0.46320858 0.47067857]\n"," [0.07488843 0.07550378]\n"," [0.07727067 0.07654685]]\n","\n","Average MAE Loss:\n","[0.21771699 0.46694357 0.07519611 0.07690876]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.21430998 0.22112401]\n"," [0.46320858 0.47067857]\n"," [0.07475541 0.07565164]\n"," [0.07671055 0.07612194]]\n","\n","Average MAE Loss:\n","[0.21771699 0.46694357 0.07520352 0.07641624]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.21430998 0.22112401]\n"," [0.46320858 0.47067857]\n"," [0.07370925 0.07536277]\n"," [0.07696525 0.07627894]]\n","\n","Average MAE Loss:\n","[0.21771699 0.46694357 0.07453601 0.0766221 ]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.21430998 0.22112401]\n"," [0.46320858 0.47067857]\n"," [0.07091025 0.07329082]\n"," [0.07649581 0.07592587]]\n","\n","Average MAE Loss:\n","[0.21771699 0.46694357 0.07210054 0.07621084]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.46320858 0.47067857]\n"," [0.07649581 0.07592587]\n"," [0.08977162 0.08847033]\n"," [0.07530977 0.0790067 ]]\n","\n","Average MAE Loss:\n","[0.46694357 0.07621084 0.08912098 0.07715823]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.46320858 0.47067857]\n"," [0.07649581 0.07592587]\n"," [0.07686537 0.07698786]\n"," [0.07561563 0.07866396]]\n","\n","Average MAE Loss:\n","[0.46694357 0.07621084 0.07692661 0.07713979]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.46320858 0.47067857]\n"," [0.07649581 0.07592588]\n"," [0.07647983 0.07668896]\n"," [0.07671151 0.0797133 ]]\n","\n","Average MAE Loss:\n","[0.46694357 0.07621085 0.07658439 0.07821241]\n","\n","Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.46320858 0.47067857]\n"," [0.07649581 0.07592588]\n"," [0.07622278 0.07657039]\n"," [0.07512875 0.07768393]]\n","\n","Average MAE Loss:\n","[0.46694357 0.07621085 0.07639658 0.07640634]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.46320858 0.47067857]\n"," [0.07649581 0.07592587]\n"," [0.07563358 0.07616633]\n"," [0.07271894 0.07487352]]\n","\n","Average MAE Loss:\n","[0.46694357 0.07621084 0.07589996 0.07379623]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.46320858 0.47067857]\n"," [0.07649581 0.07592588]\n"," [0.07524643 0.07595899]\n"," [0.07344556 0.07556109]]\n","\n","Average MAE Loss:\n","[0.46694357 0.07621084 0.07560271 0.07450333]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.46320858 0.47067857]\n"," [0.07649581 0.07592587]\n"," [0.07504482 0.07590997]\n"," [0.07365323 0.07578275]]\n","\n","Average MAE Loss:\n","[0.46694357 0.07621084 0.0754774  0.07471799]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.34054096 0.34761901]\n"," [0.31950712 0.32535183]\n"," [0.3278697  0.33376975]]\n","\n","Average MAE Loss:\n","[0.34407998 0.34407998 0.32242948 0.33081973]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.34054096 0.34761901]\n"," [0.26393893 0.26743016]\n"," [0.27703376 0.27959502]]\n","\n","Average MAE Loss:\n","[0.34407998 0.34407998 0.26568455 0.27831439]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.34054096 0.34761901]\n"," [0.08023248 0.08091687]\n"," [0.18774918 0.18888233]]\n","\n","Average MAE Loss:\n","[0.34407998 0.34407998 0.08057467 0.18831576]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.34054096 0.34761901]\n"," [0.08874398 0.08704913]\n"," [0.08225314 0.0827398 ]]\n","\n","Average MAE Loss:\n","[0.34407998 0.34407998 0.08789656 0.08249647]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.34054096 0.34761901]\n"," [0.07631081 0.07639306]\n"," [0.07945623 0.07875204]]\n","\n","Average MAE Loss:\n","[0.34407998 0.34407998 0.07635193 0.07910413]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.34054096 0.34761901]\n"," [0.07579507 0.07608277]\n"," [0.07888632 0.07827904]]\n","\n","Average MAE Loss:\n","[0.34407998 0.34407998 0.07593892 0.07858268]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.34054096 0.34761901]\n"," [0.07612535 0.07627254]\n"," [0.07748281 0.07717014]]\n","\n","Average MAE Loss:\n","[0.34407998 0.34407998 0.07619894 0.07732647]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.07748281 0.07717014]\n"," [0.33713272 0.34397912]\n"," [0.07598957 0.07618603]]\n","\n","Average MAE Loss:\n","[0.34407998 0.07732648 0.34055592 0.0760878 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.07748281 0.07717014]\n"," [0.33170106 0.33819509]\n"," [0.07610286 0.07625721]]\n","\n","Average MAE Loss:\n","[0.34407998 0.07732647 0.33494807 0.07618004]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.07748281 0.07717014]\n"," [0.32838817 0.3346643 ]\n"," [0.07622388 0.0763386 ]]\n","\n","Average MAE Loss:\n","[0.34407998 0.07732647 0.33152623 0.07628124]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.07748281 0.07717014]\n"," [0.32425573 0.33025396]\n"," [0.0762574  0.07636138]]\n","\n","Average MAE Loss:\n","[0.34407998 0.07732647 0.32725484 0.07630939]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.07748281 0.07717014]\n"," [0.31878383 0.32441173]\n"," [0.07627445 0.07637287]]\n","\n","Average MAE Loss:\n","[0.34407998 0.07732647 0.32159778 0.07632366]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.07748281 0.07717014]\n"," [0.31109567 0.31624565]\n"," [0.07628424 0.07637942]]\n","\n","Average MAE Loss:\n","[0.34407998 0.07732647 0.31367066 0.07633183]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.34054096 0.34761901]\n"," [0.07748281 0.07717014]\n"," [0.30585663 0.31072121]\n"," [0.07628988 0.07638319]]\n","\n","Average MAE Loss:\n","[0.34407998 0.07732647 0.30828892 0.07633653]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.21516979 0.21729295]\n"," [0.18894738 0.19110137]\n"," [0.20579088 0.20785196]]\n","\n","Average MAE Loss:\n","[0.21623137 0.21623137 0.19002438 0.20682142]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.21516979 0.21729295]\n"," [0.15508976 0.15747497]\n"," [0.19124575 0.19326386]]\n","\n","Average MAE Loss:\n","[0.21623137 0.21623137 0.15628237 0.19225481]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.21516979 0.21729295]\n"," [0.12333456 0.12586062]\n"," [0.17634668 0.17837248]]\n","\n","Average MAE Loss:\n","[0.21623137 0.21623137 0.12459759 0.17735958]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.21516979 0.21729295]\n"," [0.11049109 0.11254642]\n"," [0.16188848 0.16396869]]\n","\n","Average MAE Loss:\n","[0.21623137 0.21623137 0.11151875 0.16292859]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.21516979 0.21729295]\n"," [0.1003374  0.10189281]\n"," [0.15495509 0.1571314 ]]\n","\n","Average MAE Loss:\n","[0.21623137 0.21623137 0.1011151  0.15604324]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.21516979 0.21729295]\n"," [0.09263734 0.09381301]\n"," [0.14840539 0.15067441]]\n","\n","Average MAE Loss:\n","[0.21623137 0.21623137 0.09322517 0.1495399 ]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.21516979 0.21729295]\n"," [0.08696238 0.0879282 ]\n"," [0.14221351 0.14458346]]\n","\n","Average MAE Loss:\n","[0.21623137 0.21623137 0.08744529 0.14339849]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.14221351 0.14458346]\n"," [0.21039944 0.21250019]\n"," [0.08463812 0.08549213]]\n","\n","Average MAE Loss:\n","[0.21623137 0.14339849 0.21144981 0.08506512]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.14221351 0.14458346]\n"," [0.20509269 0.20718846]\n"," [0.08379631 0.0845989 ]]\n","\n","Average MAE Loss:\n","[0.21623137 0.14339849 0.20614058 0.08419761]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.14221351 0.14458346]\n"," [0.19970067 0.20179813]\n"," [0.08305944 0.08380853]]\n","\n","Average MAE Loss:\n","[0.21623137 0.14339849 0.2007494  0.08343398]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.14221351 0.14458346]\n"," [0.19428564 0.19638938]\n"," [0.08238501 0.08308899]]\n","\n","Average MAE Loss:\n","[0.21623137 0.14339849 0.19533751 0.082737  ]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.14221351 0.14458346]\n"," [0.19158747 0.19369128]\n"," [0.08176427 0.08242953]]\n","\n","Average MAE Loss:\n","[0.21623137 0.14339849 0.19263937 0.0820969 ]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.14221351 0.14458346]\n"," [0.18891734 0.19102345]\n"," [0.08147352 0.082122  ]]\n","\n","Average MAE Loss:\n","[0.21623137 0.14339849 0.18997039 0.08179776]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.21516979 0.21729295]\n"," [0.14221351 0.14458346]\n"," [0.18627504 0.1883936 ]\n"," [0.08119649 0.08182886]]\n","\n","Average MAE Loss:\n","[0.21623137 0.14339849 0.18733432 0.08151268]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.15169073 0.15401447]\n"," [0.14893397 0.15129811]\n"," [0.15051681 0.15285673]]\n","\n","Average MAE Loss:\n","[0.1528526  0.1528526  0.15011604 0.15168677]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.15169073 0.15401447]\n"," [0.14756044 0.14994559]\n"," [0.14910107 0.15145957]]\n","\n","Average MAE Loss:\n","[0.1528526  0.1528526  0.14875302 0.15028032]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.15169073 0.15401447]\n"," [0.14621372 0.14862044]\n"," [0.14839129 0.15076002]]\n","\n","Average MAE Loss:\n","[0.1528526  0.1528526  0.14741708 0.14957566]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.15169073 0.15401447]\n"," [0.14489555 0.14732335]\n"," [0.14769738 0.15007555]]\n","\n","Average MAE Loss:\n","[0.1528526  0.1528526  0.14610945 0.14888647]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.15169073 0.15401447]\n"," [0.14360358 0.14605143]\n"," [0.14701922 0.14940742]]\n","\n","Average MAE Loss:\n","[0.1528526  0.1528526  0.1448275  0.14821332]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.15169073 0.15401447]\n"," [0.14296806 0.14542632]\n"," [0.14635642 0.14875453]]\n","\n","Average MAE Loss:\n","[0.1528526  0.1528526  0.14419719 0.14755547]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.15169073 0.15401447]\n"," [0.14234227 0.14481099]\n"," [0.14603164 0.14843491]]\n","\n","Average MAE Loss:\n","[0.1528526  0.1528526  0.14357663 0.14723328]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.14603167 0.14843491]\n"," [0.15106505 0.15339801]\n"," [0.14202578 0.14449905]]\n","\n","Average MAE Loss:\n","[0.1528526  0.14723329 0.15223153 0.14326242]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.14603164 0.14843491]\n"," [0.15044637 0.15278778]\n"," [0.14171516 0.14419255]]\n","\n","Average MAE Loss:\n","[0.1528526  0.14723328 0.15161707 0.14295385]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.14603167 0.14843491]\n"," [0.15014045 0.15248631]\n"," [0.14140944 0.14389091]]\n","\n","Average MAE Loss:\n","[0.1528526  0.14723329 0.15131338 0.14265018]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.14603167 0.14843491]\n"," [0.1498378  0.15218787]\n"," [0.14125875 0.14374223]]\n","\n","Average MAE Loss:\n","[0.1528526  0.14723329 0.15101284 0.14250049]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.14603164 0.14843491]\n"," [0.14953792 0.15189228]\n"," [0.14111004 0.14359555]]\n","\n","Average MAE Loss:\n","[0.1528526  0.14723328 0.1507151  0.1423528 ]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.14603167 0.14843491]\n"," [0.14924089 0.15159956]\n"," [0.14096313 0.14345068]]\n","\n","Average MAE Loss:\n","[0.1528526  0.14723329 0.15042022 0.1422069 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.15169073 0.15401447]\n"," [0.14603164 0.14843491]\n"," [0.14909369 0.15145445]\n"," [0.14081793 0.1433075 ]]\n","\n","Average MAE Loss:\n","[0.1528526  0.14723328 0.15027407 0.14206271]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.1468953  0.14928823]\n"," [0.14689531 0.14928823]\n"," [0.14675003 0.14914525]\n"," [0.14682251 0.14921653]]\n","\n","Average MAE Loss:\n","[0.14809176 0.14809177 0.14794764 0.14801952]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.1468953  0.14928823]\n"," [0.1468953  0.14928823]\n"," [0.14660606 0.14900353]\n"," [0.1467503  0.14914534]]\n","\n","Average MAE Loss:\n","[0.14809176 0.14809176 0.1478048  0.14794782]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.1468953  0.14928823]\n"," [0.1468953  0.14928823]\n"," [0.14646313 0.14886293]\n"," [0.14667871 0.14907479]]\n","\n","Average MAE Loss:\n","[0.14809176 0.14809176 0.14766303 0.14787675]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.1468953  0.14928823]\n"," [0.14689531 0.14928823]\n"," [0.14639219 0.14879316]\n"," [0.14660776 0.14900489]]\n","\n","Average MAE Loss:\n","[0.14809176 0.14809177 0.14759268 0.14780632]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","\n","epochs finished with time:1492.1572060585022\n","\n","[[0.1468953  0.14928823]\n"," [0.14689531 0.14928823]\n"," [0.14639219 0.14879316]\n"," [0.14660776 0.14900489]]\n","00:25:21.636320445999445\n","------------------------------------Fold [3/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.21653169 0.22640839]\n"," [0.47698912 0.48960996]\n"," [0.0795275  0.06913822]\n"," [0.07642589 0.06540529]]\n","\n","Average MAE Loss:\n","[0.22147004 0.48329954 0.07433286 0.07091559]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.21653169 0.22640839]\n"," [0.47698912 0.48960996]\n"," [0.06168854 0.05514868]\n"," [0.06175546 0.05671275]]\n","\n","Average MAE Loss:\n","[0.22147004 0.48329954 0.05841861 0.05923411]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.21653169 0.22640839]\n"," [0.47698912 0.48960996]\n"," [0.06025923 0.05702335]\n"," [0.06164463 0.055464  ]]\n","\n","Average MAE Loss:\n","[0.22147004 0.48329954 0.05864129 0.05855431]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.21653169 0.22640839]\n"," [0.47698912 0.48960996]\n"," [0.05979998 0.05547329]\n"," [0.06204423 0.0550996 ]]\n","\n","Average MAE Loss:\n","[0.22147004 0.48329954 0.05763663 0.05857191]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.21653169 0.22640839]\n"," [0.47698912 0.48960996]\n"," [0.05942488 0.05502259]\n"," [0.06184927 0.05502174]]\n","\n","Average MAE Loss:\n","[0.22147004 0.48329954 0.05722374 0.0584355 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.21653169 0.22640839]\n"," [0.47698912 0.48960996]\n"," [0.05870044 0.05457964]\n"," [0.06179712 0.05495778]]\n","\n","Average MAE Loss:\n","[0.22147004 0.48329954 0.05664004 0.05837745]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.21653169 0.22640839]\n"," [0.47698912 0.48960996]\n"," [0.05704825 0.05667042]\n"," [0.06192024 0.05488928]]\n","\n","Average MAE Loss:\n","[0.22147004 0.48329954 0.05685933 0.05840476]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.47698912 0.48960996]\n"," [0.06192024 0.05488927]\n"," [0.06121424 0.05577832]\n"," [0.05776227 0.05577227]]\n","\n","Average MAE Loss:\n","[0.48329954 0.05840476 0.05849628 0.05676727]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.47698912 0.48960996]\n"," [0.06192024 0.05488928]\n"," [0.06079756 0.05584206]\n"," [0.05800836 0.05430252]]\n","\n","Average MAE Loss:\n","[0.48329954 0.05840476 0.05831981 0.05615544]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.47698912 0.48960996]\n"," [0.06192024 0.05488928]\n"," [0.06083679 0.05532669]\n"," [0.05759926 0.05452385]]\n","\n","Average MAE Loss:\n","[0.48329954 0.05840476 0.05808174 0.05606155]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.47698912 0.48960996]\n"," [0.06192024 0.05488927]\n"," [0.06042077 0.05570066]\n"," [0.05714703 0.05462535]]\n","\n","Average MAE Loss:\n","[0.48329954 0.05840476 0.05806071 0.05588619]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.47698912 0.48960996]\n"," [0.06192024 0.05488927]\n"," [0.06025405 0.05571367]\n"," [0.05721463 0.05376774]]\n","\n","Average MAE Loss:\n","[0.48329954 0.05840476 0.05798386 0.05549118]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.47698912 0.48960996]\n"," [0.06192024 0.05488927]\n"," [0.06012303 0.0555422 ]\n"," [0.0571636  0.05390149]]\n","\n","Average MAE Loss:\n","[0.48329954 0.05840476 0.05783261 0.05553254]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.47698912 0.48960996]\n"," [0.06192024 0.05488927]\n"," [0.05992472 0.05547116]\n"," [0.0568981  0.05418253]]\n","\n","Average MAE Loss:\n","[0.48329954 0.05840476 0.05769794 0.05554032]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.35001943 0.3613376 ]\n"," [0.34085682 0.35149443]\n"," [0.08738615 0.09081484]]\n","\n","Average MAE Loss:\n","[0.35567851 0.35567851 0.34617563 0.08910049]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.35001943 0.3613376 ]\n"," [0.31687766 0.32565546]\n"," [0.10128396 0.09001209]]\n","\n","Average MAE Loss:\n","[0.35567851 0.35567851 0.32126656 0.09564802]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00016: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.35001943 0.3613376 ]\n"," [0.19986776 0.20547788]\n"," [0.1013357  0.09007242]]\n","\n","Average MAE Loss:\n","[0.35567851 0.35567851 0.20267282 0.09570406]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.35001943 0.3613376 ]\n"," [0.06585056 0.05758372]\n"," [0.10127912 0.09000321]]\n","\n","Average MAE Loss:\n","[0.35567851 0.35567851 0.06171714 0.09564117]\n","\n","Epoch 00018: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.35001943 0.3613376 ]\n"," [0.06551943 0.05734465]\n"," [0.10114864 0.08984597]]\n","\n","Average MAE Loss:\n","[0.35567851 0.35567851 0.06143204 0.09549731]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.35001943 0.3613376 ]\n"," [0.06124281 0.05548558]\n"," [0.10082014 0.08945556]]\n","\n","Average MAE Loss:\n","[0.35567851 0.35567851 0.05836419 0.09513785]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.35001943 0.3613376 ]\n"," [0.06097386 0.0562905 ]\n"," [0.10038015 0.08894087]]\n","\n","Average MAE Loss:\n","[0.35567851 0.35567851 0.05863218 0.09466051]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.10038015 0.08894087]\n"," [0.34678042 0.35783589]\n"," [0.06156897 0.05536064]]\n","\n","Average MAE Loss:\n","[0.35567851 0.09466051 0.35230815 0.05846481]\n","\n","Epoch 00022: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.10038015 0.08894087]\n"," [0.34421277 0.35506284]\n"," [0.0611405  0.05558535]]\n","\n","Average MAE Loss:\n","[0.35567851 0.09466051 0.34963781 0.05836292]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.10038015 0.08894087]\n"," [0.34135565 0.35196921]\n"," [0.06131997 0.05542661]]\n","\n","Average MAE Loss:\n","[0.35567851 0.09466051 0.34666243 0.05837329]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.10038015 0.08894087]\n"," [0.33806431 0.34839576]\n"," [0.06120647 0.05550778]]\n","\n","Average MAE Loss:\n","[0.35567851 0.09466051 0.34323004 0.05835712]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.10038015 0.08894087]\n"," [0.33399266 0.34398189]\n"," [0.06122392 0.0554914 ]]\n","\n","Average MAE Loss:\n","[0.35567851 0.09466051 0.33898728 0.05835766]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.10038015 0.08894087]\n"," [0.33145493 0.34122813]\n"," [0.06122239 0.05549216]]\n","\n","Average MAE Loss:\n","[0.35567851 0.09466051 0.33634153 0.05835727]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.35001943 0.3613376 ]\n"," [0.10038015 0.08894087]\n"," [0.32842636 0.33794314]\n"," [0.06122183 0.05549209]]\n","\n","Average MAE Loss:\n","[0.35567851 0.09466051 0.33318475 0.05835696]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.16720623 0.17153892]\n"," [0.13913345 0.14390004]\n"," [0.10177915 0.10626034]]\n","\n","Average MAE Loss:\n","[0.16937257 0.16937257 0.14151675 0.10401974]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.16720623 0.17153892]\n"," [0.10652637 0.11129538]\n"," [0.06278767 0.06071881]]\n","\n","Average MAE Loss:\n","[0.16937257 0.16937257 0.10891088 0.06175324]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.16720623 0.17153892]\n"," [0.09393413 0.09805123]\n"," [0.06143809 0.05538993]]\n","\n","Average MAE Loss:\n","[0.16937257 0.16937257 0.09599268 0.05841401]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.16720623 0.17153892]\n"," [0.0844676  0.08773783]\n"," [0.06172793 0.05533075]]\n","\n","Average MAE Loss:\n","[0.16937257 0.16937257 0.08610272 0.05852934]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00032: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.16720623 0.17153892]\n"," [0.07757669 0.07996571]\n"," [0.06159136 0.05534416]]\n","\n","Average MAE Loss:\n","[0.16937257 0.16937257 0.0787712  0.05846776]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.16720623 0.17153892]\n"," [0.07258087 0.07416452]\n"," [0.06146489 0.05537743]]\n","\n","Average MAE Loss:\n","[0.16937257 0.16937257 0.0733727  0.05842116]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.16720623 0.17153892]\n"," [0.07070152 0.07189175]\n"," [0.06138606 0.05541779]]\n","\n","Average MAE Loss:\n","[0.16937257 0.16937257 0.07129664 0.05840192]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.06138606 0.05541779]\n"," [0.16164549 0.16605884]\n"," [0.06559172 0.06511576]]\n","\n","Average MAE Loss:\n","[0.16937257 0.05840192 0.16385216 0.06535374]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00036: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.06138606 0.05541779]\n"," [0.15511414 0.15963949]\n"," [0.06331683 0.06163687]]\n","\n","Average MAE Loss:\n","[0.16937257 0.05840192 0.15737682 0.06247685]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.06138606 0.05541779]\n"," [0.14867805 0.1532954 ]\n"," [0.06209346 0.05940904]]\n","\n","Average MAE Loss:\n","[0.16937257 0.05840193 0.15098672 0.06075125]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.06138606 0.05541779]\n"," [0.1455752  0.15022792]\n"," [0.06149983 0.05805303]]\n","\n","Average MAE Loss:\n","[0.16937257 0.05840192 0.14790156 0.05977643]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.06138606 0.05541779]\n"," [0.14257589 0.14726435]\n"," [0.06122223 0.05720674]]\n","\n","Average MAE Loss:\n","[0.16937257 0.05840192 0.14492012 0.05921449]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.06138606 0.05541779]\n"," [0.13967204 0.14439353]\n"," [0.0611523  0.05690921]]\n","\n","Average MAE Loss:\n","[0.16937257 0.05840193 0.14203279 0.05903076]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.16720623 0.17153892]\n"," [0.06138606 0.05541779]\n"," [0.13685763 0.14161421]\n"," [0.06110865 0.05667121]]\n","\n","Average MAE Loss:\n","[0.16937257 0.05840192 0.13923592 0.05888993]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.9531e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.08576684 0.08916382]\n"," [0.08494695 0.08825022]\n"," [0.08193653 0.08486886]]\n","\n","Average MAE Loss:\n","[0.08746533 0.08746533 0.08659858 0.08340269]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.08576684 0.08916382]\n"," [0.08423223 0.08745147]\n"," [0.0773045  0.07963959]]\n","\n","Average MAE Loss:\n","[0.08746533 0.08746533 0.08584185 0.07847204]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.08576684 0.08916382]\n"," [0.08354995 0.08668778]\n"," [0.07536519 0.07741671]]\n","\n","Average MAE Loss:\n","[0.08746533 0.08746533 0.08511886 0.07639095]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.08576684 0.08916382]\n"," [0.08289132 0.08594677]\n"," [0.07368937 0.0754644 ]]\n","\n","Average MAE Loss:\n","[0.08746533 0.08746533 0.08441904 0.07457688]\n","\n","Epoch 00046: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.08576684 0.08916382]\n"," [0.08257097 0.08558401]\n"," [0.07222746 0.07373158]]\n","\n","Average MAE Loss:\n","[0.08746533 0.08746533 0.08407749 0.07297952]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.08576684 0.08916382]\n"," [0.08225628 0.08522771]\n"," [0.07095638 0.0721947 ]]\n","\n","Average MAE Loss:\n","[0.08746533 0.08746533 0.083742   0.07157554]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.08576684 0.08916382]\n"," [0.08194578 0.08487745]\n"," [0.07038625 0.07149542]]\n","\n","Average MAE Loss:\n","[0.08746533 0.08746533 0.08341162 0.07094083]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.07038625 0.07149542]\n"," [0.08542413 0.08878248]\n"," [0.08100311 0.08381551]]\n","\n","Average MAE Loss:\n","[0.08746533 0.07094083 0.0871033  0.08240931]\n","\n","Epoch 00050: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.07038625 0.07149542]\n"," [0.08525126 0.0885897 ]\n"," [0.08002294 0.08271035]]\n","\n","Average MAE Loss:\n","[0.08746533 0.07094083 0.08692048 0.08136664]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.07038625 0.07149542]\n"," [0.08507971 0.08839825]\n"," [0.07909018 0.08166266]]\n","\n","Average MAE Loss:\n","[0.08746533 0.07094083 0.08673898 0.08037642]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00052: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.07038625 0.07149542]\n"," [0.08490964 0.08820827]\n"," [0.07864925 0.08116712]]\n","\n","Average MAE Loss:\n","[0.08746533 0.07094083 0.08655896 0.07990818]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.07038625 0.07149542]\n"," [0.08474119 0.08802012]\n"," [0.07823027 0.08069223]]\n","\n","Average MAE Loss:\n","[0.08746533 0.07094083 0.08638066 0.07946125]\n","\n","Epoch 00054: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.07038625 0.07149542]\n"," [0.08465756 0.08792682]\n"," [0.07782902 0.08023433]]\n","\n","Average MAE Loss:\n","[0.08746533 0.07094083 0.08629219 0.07903168]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.08576684 0.08916382]\n"," [0.07038625 0.07149542]\n"," [0.08457445 0.08783399]\n"," [0.07744161 0.07979293]]\n","\n","Average MAE Loss:\n","[0.08746533 0.07094083 0.08620422 0.07861727]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch 00056: reducing learning rate of group 0 to 4.8828e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.07899632 0.08155894]\n"," [0.07899631 0.08155894]\n"," [0.07892689 0.08148091]\n"," [0.07879541 0.08133312]]\n","\n","Average MAE Loss:\n","[0.08027763 0.08027762 0.0802039  0.08006427]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.07899631 0.08155894]\n"," [0.07899631 0.08155894]\n"," [0.07886042 0.08140618]\n"," [0.0785971  0.08110986]]\n","\n","Average MAE Loss:\n","[0.08027762 0.08027762 0.0801333  0.07985348]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.07899631 0.08155894]\n"," [0.07899631 0.08155894]\n"," [0.07882743 0.08136911]\n"," [0.07840396 0.08089133]]\n","\n","Average MAE Loss:\n","[0.08027762 0.08027762 0.08009827 0.07964765]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.07899632 0.08155894]\n"," [0.07899632 0.08155894]\n"," [0.07879451 0.0813321 ]\n"," [0.07821557 0.08067703]]\n","\n","Average MAE Loss:\n","[0.08027763 0.08027763 0.08006331 0.0794463 ]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","Epoch 00060: reducing learning rate of group 0 to 2.4414e-07.\n","\n","epochs finished with time:1201.6959743499756\n","\n","[[0.07899632 0.08155894]\n"," [0.07899631 0.08155894]\n"," [0.07879451 0.0813321 ]\n"," [0.07821557 0.08067703]]\n","00:20:25.20184041399989\n","------------------------------------Fold [4/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.23157874 0.21650687]\n"," [0.49439925 0.47389853]\n"," [0.06046459 0.08182992]\n"," [0.05846939 0.0784977 ]]\n","\n","Average MAE Loss:\n","[0.2240428  0.48414889 0.07114726 0.06848354]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.23157874 0.21650687]\n"," [0.49439925 0.47389853]\n"," [0.0505324  0.0694095 ]\n"," [0.05121048 0.06658414]]\n","\n","Average MAE Loss:\n","[0.2240428  0.48414889 0.05997095 0.05889731]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.23157874 0.21650687]\n"," [0.49439925 0.47389853]\n"," [0.04945537 0.06593005]\n"," [0.05090311 0.06629895]]\n","\n","Average MAE Loss:\n","[0.2240428  0.48414889 0.05769271 0.05860103]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.23157874 0.21650687]\n"," [0.49439925 0.47389853]\n"," [0.04893032 0.06562588]\n"," [0.05040731 0.06684901]]\n","\n","Average MAE Loss:\n","[0.2240428  0.48414889 0.0572781  0.05862816]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.23157874 0.21650687]\n"," [0.49439925 0.47389853]\n"," [0.04797336 0.06555836]\n"," [0.05033441 0.06658221]]\n","\n","Average MAE Loss:\n","[0.2240428  0.48414889 0.05676586 0.05845831]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.23157874 0.21650687]\n"," [0.49439925 0.47389853]\n"," [0.0469672  0.06453902]\n"," [0.05024933 0.06667832]]\n","\n","Average MAE Loss:\n","[0.2240428  0.48414889 0.05575311 0.05846382]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.23157874 0.21650687]\n"," [0.49439925 0.47389853]\n"," [0.04600231 0.06341885]\n"," [0.05019693 0.0667813 ]]\n","\n","Average MAE Loss:\n","[0.2240428  0.48414889 0.05471058 0.05848911]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.49439925 0.47389853]\n"," [0.05019692 0.0667813 ]\n"," [0.05042977 0.0670272 ]\n"," [0.04693588 0.06425011]]\n","\n","Average MAE Loss:\n","[0.48414889 0.05848911 0.05872849 0.055593  ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.49439925 0.47389853]\n"," [0.05019692 0.0667813 ]\n"," [0.0516246  0.06652277]\n"," [0.04600517 0.06408203]]\n","\n","Average MAE Loss:\n","[0.48414889 0.05848911 0.05907368 0.0550436 ]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.49439925 0.47389853]\n"," [0.05019692 0.0667813 ]\n"," [0.04995607 0.06643969]\n"," [0.04595052 0.0643281 ]]\n","\n","Average MAE Loss:\n","[0.48414889 0.05848911 0.05819788 0.05513931]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.49439925 0.47389853]\n"," [0.05019692 0.0667813 ]\n"," [0.04942058 0.06664767]\n"," [0.04576229 0.06382398]]\n","\n","Average MAE Loss:\n","[0.48414889 0.05848911 0.05803412 0.05479313]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.49439925 0.47389853]\n"," [0.05019692 0.0667813 ]\n"," [0.04937378 0.06624144]\n"," [0.04562302 0.06383824]]\n","\n","Average MAE Loss:\n","[0.48414889 0.05848911 0.05780761 0.05473063]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.49439925 0.47389853]\n"," [0.05019692 0.0667813 ]\n"," [0.04914735 0.06610426]\n"," [0.04549208 0.06368995]]\n","\n","Average MAE Loss:\n","[0.48414889 0.05848911 0.0576258  0.05459102]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.49439925 0.47389853]\n"," [0.05019692 0.0667813 ]\n"," [0.04872421 0.06602665]\n"," [0.04549205 0.06368499]]\n","\n","Average MAE Loss:\n","[0.48414889 0.05848911 0.05737543 0.05458852]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.36707962 0.34751832]\n"," [0.34249753 0.32210022]\n"," [0.0965421  0.09599691]]\n","\n","Average MAE Loss:\n","[0.35729897 0.35729897 0.33229887 0.0962695 ]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.36707962 0.34751832]\n"," [0.26537234 0.24471211]\n"," [0.08392812 0.10631977]]\n","\n","Average MAE Loss:\n","[0.35729897 0.35729897 0.25504223 0.09512395]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.36707962 0.34751832]\n"," [0.05046643 0.06684342]\n"," [0.08402871 0.10643867]]\n","\n","Average MAE Loss:\n","[0.35729897 0.35729897 0.05865492 0.09523369]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00017: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.36707962 0.34751832]\n"," [0.05418272 0.07392093]\n"," [0.08401991 0.10642765]]\n","\n","Average MAE Loss:\n","[0.35729897 0.35729897 0.06405182 0.09522378]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.36707962 0.34751832]\n"," [0.05168227 0.06642279]\n"," [0.08400003 0.10640311]]\n","\n","Average MAE Loss:\n","[0.35729897 0.35729897 0.05905253 0.09520157]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.36707962 0.34751832]\n"," [0.05110303 0.06641221]\n"," [0.08396837 0.10636431]]\n","\n","Average MAE Loss:\n","[0.35729897 0.35729897 0.05875762 0.09516634]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.36707962 0.34751832]\n"," [0.05059877 0.06661454]\n"," [0.08391522 0.1062995 ]]\n","\n","Average MAE Loss:\n","[0.35729897 0.35729897 0.05860665 0.09510736]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.08391522 0.1062995 ]\n"," [0.36329168 0.34358701]\n"," [0.05054555 0.06666601]]\n","\n","Average MAE Loss:\n","[0.35729897 0.09510736 0.35343935 0.05860578]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.08391522 0.1062995 ]\n"," [0.35723424 0.33730185]\n"," [0.0505413  0.06666987]]\n","\n","Average MAE Loss:\n","[0.35729897 0.09510736 0.34726804 0.05860559]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.08391522 0.1062995 ]\n"," [0.35345271 0.33336022]\n"," [0.05055765 0.0666516 ]]\n","\n","Average MAE Loss:\n","[0.35729897 0.09510736 0.34340647 0.05860462]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.08391522 0.1062995 ]\n"," [0.34860304 0.32828838]\n"," [0.05054179 0.06666768]]\n","\n","Average MAE Loss:\n","[0.35729897 0.09510736 0.33844571 0.05860474]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.08391522 0.1062995 ]\n"," [0.34196422 0.321363  ]\n"," [0.05057026 0.06663732]]\n","\n","Average MAE Loss:\n","[0.35729897 0.09510736 0.33166361 0.05860379]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.08391522 0.1062995 ]\n"," [0.33228004 0.31130779]\n"," [0.05059096 0.0666177 ]]\n","\n","Average MAE Loss:\n","[0.35729897 0.09510736 0.32179391 0.05860433]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.36707962 0.34751832]\n"," [0.08391522 0.1062995 ]\n"," [0.32548249 0.30429772]\n"," [0.05058345 0.06662387]]\n","\n","Average MAE Loss:\n","[0.35729897 0.09510736 0.3148901  0.05860366]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.1516034  0.14039414]\n"," [0.11806352 0.11312535]\n"," [0.05345866 0.06687133]]\n","\n","Average MAE Loss:\n","[0.14599877 0.14599877 0.11559444 0.06016499]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.1516034  0.14039414]\n"," [0.08593921 0.08822915]\n"," [0.05042472 0.06756263]]\n","\n","Average MAE Loss:\n","[0.14599877 0.14599877 0.08708418 0.05899367]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.1516034  0.14039414]\n"," [0.06664219 0.07463109]\n"," [0.05041029 0.06725873]]\n","\n","Average MAE Loss:\n","[0.14599877 0.14599877 0.07063664 0.05883451]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.1516034  0.14039414]\n"," [0.06144376 0.07131388]\n"," [0.05056306 0.06673213]]\n","\n","Average MAE Loss:\n","[0.14599877 0.14599877 0.06637882 0.0586476 ]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.1516034  0.14039414]\n"," [0.05809253 0.06931777]\n"," [0.05070685 0.06657769]]\n","\n","Average MAE Loss:\n","[0.14599877 0.14599877 0.06370515 0.05864227]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.1516034  0.14039414]\n"," [0.05592795 0.06806215]\n"," [0.05071823 0.06656846]]\n","\n","Average MAE Loss:\n","[0.14599877 0.14599877 0.06199505 0.05864334]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.1516034  0.14039414]\n"," [0.0544896  0.06732103]\n"," [0.05071472 0.06657095]]\n","\n","Average MAE Loss:\n","[0.14599877 0.14599877 0.06090531 0.05864283]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.05071472 0.06657095]\n"," [0.14538704 0.13526602]\n"," [0.05283398 0.06665204]]\n","\n","Average MAE Loss:\n","[0.14599877 0.05864283 0.14032653 0.05974301]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.05071472 0.06657095]\n"," [0.13726801 0.12864131]\n"," [0.05146611 0.06637339]]\n","\n","Average MAE Loss:\n","[0.14599877 0.05864283 0.13295466 0.05891975]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00037: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.05071472 0.06657095]\n"," [0.12945153 0.12226748]\n"," [0.05117127 0.06639677]]\n","\n","Average MAE Loss:\n","[0.14599877 0.05864283 0.1258595  0.05878402]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.05071472 0.06657095]\n"," [0.12217838 0.11639579]\n"," [0.05100926 0.06643462]]\n","\n","Average MAE Loss:\n","[0.14599877 0.05864283 0.11928708 0.05872194]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.05071472 0.06657095]\n"," [0.11880282 0.1136837 ]\n"," [0.0509059  0.0664705 ]]\n","\n","Average MAE Loss:\n","[0.14599877 0.05864283 0.11624326 0.0586882 ]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.05071472 0.06657095]\n"," [0.11560529 0.11113386]\n"," [0.05083793 0.06650181]]\n","\n","Average MAE Loss:\n","[0.14599877 0.05864283 0.11336958 0.05866987]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.1516034  0.14039414]\n"," [0.05071472 0.06657095]\n"," [0.11256176 0.10873418]\n"," [0.05081468 0.06651393]]\n","\n","Average MAE Loss:\n","[0.14599877 0.05864283 0.11064797 0.05866431]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.07506636 0.08042435]\n"," [0.07335328 0.0792421 ]\n"," [0.07111079 0.07768799]]\n","\n","Average MAE Loss:\n","[0.07774535 0.07774535 0.07629769 0.07439939]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.07506637 0.08042435]\n"," [0.07265062 0.07875656]\n"," [0.06630376 0.07440051]]\n","\n","Average MAE Loss:\n","[0.07774535 0.07774536 0.07570359 0.07035214]\n","\n","Epoch 00044: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.07506636 0.08042435]\n"," [0.07198753 0.07829725]\n"," [0.06257308 0.07200491]]\n","\n","Average MAE Loss:\n","[0.07774535 0.07774535 0.07514239 0.06728899]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.07506637 0.08042435]\n"," [0.07506636 0.08042435]\n"," [0.07134925 0.07785226]\n"," [0.0611282  0.07112003]]\n","\n","Average MAE Loss:\n","[0.07774536 0.07774535 0.07460076 0.06612412]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.07506636 0.08042435]\n"," [0.07073037 0.07742172]\n"," [0.05991667 0.0703913 ]]\n","\n","Average MAE Loss:\n","[0.07774535 0.07774535 0.07407605 0.06515398]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.07506636 0.08042435]\n"," [0.07042877 0.07721262]\n"," [0.05888139 0.06977864]]\n","\n","Average MAE Loss:\n","[0.07774535 0.07774535 0.07382069 0.06433002]\n","\n","Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.07506636 0.08042435]\n"," [0.07013173 0.07700703]\n"," [0.05799102 0.06925576]]\n","\n","Average MAE Loss:\n","[0.07774535 0.07774535 0.07356938 0.06362339]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.9531e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.05799102 0.06925576]\n"," [0.07471757 0.08018339]\n"," [0.06922771 0.0763794 ]]\n","\n","Average MAE Loss:\n","[0.07774535 0.06362339 0.07745048 0.07280356]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.05799102 0.06925576]\n"," [0.07436007 0.0799368 ]\n"," [0.06824128 0.07570091]]\n","\n","Average MAE Loss:\n","[0.07774535 0.06362339 0.07714843 0.0719711 ]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.05799102 0.06925576]\n"," [0.0741825  0.07981437]\n"," [0.06731211 0.0750697 ]]\n","\n","Average MAE Loss:\n","[0.07774535 0.06362339 0.07699843 0.0711909 ]\n","\n","Epoch 00052: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.05799102 0.06925576]\n"," [0.0740068  0.07969277]\n"," [0.06644904 0.07449447]]\n","\n","Average MAE Loss:\n","[0.07774535 0.06362339 0.07684978 0.07047175]\n","\n","Epoch 00053: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.05799102 0.06925576]\n"," [0.07383273 0.07957239]\n"," [0.06604487 0.07422701]]\n","\n","Average MAE Loss:\n","[0.07774535 0.06362339 0.07670256 0.07013594]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.05799102 0.06925576]\n"," [0.07366005 0.07945327]\n"," [0.06566067 0.07397372]]\n","\n","Average MAE Loss:\n","[0.07774535 0.06362339 0.07655666 0.06981719]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.07506636 0.08042435]\n"," [0.05799102 0.06925576]\n"," [0.07357436 0.07939421]\n"," [0.06529255 0.07373332]]\n","\n","Average MAE Loss:\n","[0.07774535 0.06362339 0.07648429 0.06951294]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2207e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06724675 0.07502717]\n"," [0.06724676 0.07502717]\n"," [0.06718034 0.07498254]\n"," [0.06685679 0.07476635]]\n","\n","Average MAE Loss:\n","[0.07113696 0.07113696 0.07108144 0.07081157]\n","\n","Epoch 00057: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06724676 0.07502717]\n"," [0.06724675 0.07502717]\n"," [0.06711796 0.07494076]\n"," [0.06666353 0.07463807]]\n","\n","Average MAE Loss:\n","[0.07113696 0.07113696 0.07102936 0.0706508 ]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06724675 0.07502717]\n"," [0.06724675 0.07502717]\n"," [0.06705615 0.07489942]\n"," [0.06647515 0.07451314]]\n","\n","Average MAE Loss:\n","[0.07113696 0.07113696 0.07097778 0.07049415]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06724675 0.07502717]\n"," [0.06724675 0.07502717]\n"," [0.06702526 0.07487877]\n"," [0.06629134 0.07439126]]\n","\n","Average MAE Loss:\n","[0.07113696 0.07113696 0.07095201 0.0703413 ]\n","\n","Epoch 00060: reducing learning rate of group 0 to 6.1035e-08.\n","\n","epochs finished with time:1197.1983649730682\n","\n","[[0.06724675 0.07502717]\n"," [0.06724675 0.07502717]\n"," [0.06702526 0.07487877]\n"," [0.06629134 0.07439126]]\n","00:20:19.103349458000594\n","------------------------------------Fold [5/5]-----------------------------------------\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Meta 2 X\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.21275094 0.20781346]\n"," [0.47447547 0.47155899]\n"," [0.08108941 0.08522234]\n"," [0.07802549 0.08065291]]\n","\n","Average MAE Loss:\n","[0.2102822  0.47301723 0.08315588 0.0793392 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.21275094 0.20781347]\n"," [0.47447544 0.47155899]\n"," [0.06405827 0.06848391]\n"," [0.0622406  0.06566508]]\n","\n","Average MAE Loss:\n","[0.21028221 0.47301722 0.06627109 0.06395284]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.21275094 0.20781346]\n"," [0.47447544 0.47155899]\n"," [0.06007738 0.06492054]\n"," [0.06236005 0.06584124]]\n","\n","Average MAE Loss:\n","[0.2102822  0.47301722 0.06249896 0.06410064]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.21275096 0.20781344]\n"," [0.47447541 0.47155899]\n"," [0.05948347 0.06496292]\n"," [0.06316914 0.0665807 ]]\n","\n","Average MAE Loss:\n","[0.2102822  0.4730172  0.06222319 0.06487492]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.21275096 0.20781346]\n"," [0.47447541 0.47155899]\n"," [0.05885374 0.06522028]\n"," [0.0627563  0.06618761]]\n","\n","Average MAE Loss:\n","[0.21028221 0.4730172  0.06203701 0.06447195]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.21275094 0.20781344]\n"," [0.47447541 0.47155899]\n"," [0.05706934 0.06418591]\n"," [0.06279983 0.06618492]]\n","\n","Average MAE Loss:\n","[0.21028219 0.4730172  0.06062763 0.06449237]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.21275094 0.20781346]\n"," [0.47447541 0.47155899]\n"," [0.05553553 0.06243549]\n"," [0.06213596 0.06559568]]\n","\n","Average MAE Loss:\n","[0.2102822  0.4730172  0.05898551 0.06386582]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.47447544 0.47155899]\n"," [0.06213596 0.06559568]\n"," [0.0624228  0.06557395]\n"," [0.05532772 0.06253439]]\n","\n","Average MAE Loss:\n","[0.47301722 0.06386582 0.06399838 0.05893105]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.47447541 0.47155899]\n"," [0.06213596 0.06559568]\n"," [0.06091999 0.06502049]\n"," [0.0559928  0.06473799]]\n","\n","Average MAE Loss:\n","[0.4730172  0.06386582 0.06297024 0.06036539]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.47447541 0.47155899]\n"," [0.06213596 0.06559569]\n"," [0.06115901 0.0657928 ]\n"," [0.05469463 0.06214744]]\n","\n","Average MAE Loss:\n","[0.4730172  0.06386582 0.0634759  0.05842103]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.47447544 0.47155899]\n"," [0.06213596 0.06559568]\n"," [0.06114108 0.06624904]\n"," [0.05441769 0.06243457]]\n","\n","Average MAE Loss:\n","[0.47301722 0.06386582 0.06369506 0.05842613]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.47447541 0.47155899]\n"," [0.06213596 0.06559568]\n"," [0.06035932 0.06563614]\n"," [0.05383661 0.06169957]]\n","\n","Average MAE Loss:\n","[0.4730172  0.06386582 0.06299773 0.05776809]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.47447544 0.47155899]\n"," [0.06213596 0.06559568]\n"," [0.05982373 0.06537247]\n"," [0.05333582 0.06115115]]\n","\n","Average MAE Loss:\n","[0.47301722 0.06386582 0.0625981  0.05724349]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.47447541 0.47155899]\n"," [0.06213596 0.06559568]\n"," [0.05933306 0.06532061]\n"," [0.0531093  0.06134103]]\n","\n","Average MAE Loss:\n","[0.4730172  0.06386582 0.06232684 0.05722516]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.34741804 0.34329697]\n"," [0.32165006 0.31601238]\n"," [0.29927006 0.29100466]]\n","\n","Average MAE Loss:\n","[0.34535751 0.34535751 0.31883122 0.29513736]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.34741804 0.34329697]\n"," [0.23901618 0.2301807 ]\n"," [0.08873596 0.09179088]]\n","\n","Average MAE Loss:\n","[0.34535751 0.34535751 0.23459844 0.09026342]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.34741804 0.34329697]\n"," [0.06371697 0.06757496]\n"," [0.08851543 0.09133871]]\n","\n","Average MAE Loss:\n","[0.34535751 0.34535751 0.06564597 0.08992707]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.34741804 0.34329697]\n"," [0.06959154 0.07331713]\n"," [0.06198924 0.06496095]]\n","\n","Average MAE Loss:\n","[0.34535751 0.34535751 0.07145434 0.0634751 ]\n","\n","Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.34741804 0.34329697]\n"," [0.06131385 0.06505954]\n"," [0.06281856 0.06635096]]\n","\n","Average MAE Loss:\n","[0.34535751 0.34535751 0.0631867  0.06458476]\n","\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.34741804 0.34329697]\n"," [0.06150053 0.06541624]\n"," [0.0631118  0.06662038]]\n","\n","Average MAE Loss:\n","[0.34535751 0.34535751 0.06345838 0.06486609]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.34741804 0.34329697]\n"," [0.06188445 0.06586766]\n"," [0.06240902 0.06595628]]\n","\n","Average MAE Loss:\n","[0.34535751 0.34535751 0.06387606 0.06418265]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.06240902 0.06595628]\n"," [0.34344399 0.33906686]\n"," [0.06231626 0.06631047]]\n","\n","Average MAE Loss:\n","[0.34535751 0.06418265 0.34125543 0.06431337]\n","\n","Epoch 00022: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.06240902 0.06595628]\n"," [0.33711845 0.33234206]\n"," [0.06226649 0.06626008]]\n","\n","Average MAE Loss:\n","[0.34535751 0.06418265 0.33473025 0.06426329]\n","\n","Epoch 00023: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.06240902 0.06595628]\n"," [0.3331508  0.32811058]\n"," [0.0621712  0.06616483]]\n","\n","Average MAE Loss:\n","[0.34535751 0.06418265 0.33063069 0.06416801]\n","\n","Epoch 00024: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.06240902 0.06595628]\n"," [0.32801831 0.32262409]\n"," [0.06217355 0.06616696]]\n","\n","Average MAE Loss:\n","[0.34535751 0.06418265 0.3253212  0.06417025]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.06240902 0.06595628]\n"," [0.32092303 0.3150385 ]\n"," [0.0621881  0.06618137]]\n","\n","Average MAE Loss:\n","[0.34535751 0.06418265 0.31798077 0.06418473]\n","\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.06240902 0.06595628]\n"," [0.31045344 0.30387399]\n"," [0.06218537 0.06617851]]\n","\n","Average MAE Loss:\n","[0.34535751 0.06418265 0.30716372 0.06418194]\n","\n","Epoch 00027: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.34741804 0.34329697]\n"," [0.06240902 0.06595628]\n"," [0.30303863 0.29600033]\n"," [0.06218123 0.06617424]]\n","\n","Average MAE Loss:\n","[0.34535751 0.06418265 0.29951948 0.06417773]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.20275146 0.19279699]\n"," [0.17320728 0.16418664]\n"," [0.14467734 0.13691932]]\n","\n","Average MAE Loss:\n","[0.19777423 0.19777422 0.16869696 0.14079833]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.20275146 0.19279701]\n"," [0.13692766 0.12993854]\n"," [0.07731797 0.07670467]]\n","\n","Average MAE Loss:\n","[0.19777423 0.19777423 0.1334331  0.07701132]\n","\n","Epoch 00030: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.20275146 0.19279701]\n"," [0.1049211  0.10059711]\n"," [0.06500536 0.0670328 ]]\n","\n","Average MAE Loss:\n","[0.19777423 0.19777423 0.1027591  0.06601908]\n","\n","Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.20275146 0.19279701]\n"," [0.09256122 0.08958694]\n"," [0.06186996 0.06500115]]\n","\n","Average MAE Loss:\n","[0.19777423 0.19777423 0.09107408 0.06343555]\n","\n","Epoch 00032: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.20275146 0.19279699]\n"," [0.08299822 0.08147894]\n"," [0.06144394 0.06500822]]\n","\n","Average MAE Loss:\n","[0.19777423 0.19777422 0.08223858 0.06322608]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.20275146 0.19279701]\n"," [0.0760188  0.07568012]\n"," [0.06158166 0.06530125]]\n","\n","Average MAE Loss:\n","[0.19777423 0.19777423 0.07584946 0.06344146]\n","\n","Epoch 00034: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.20275146 0.19279701]\n"," [0.07100209 0.07162598]\n"," [0.06168286 0.06543086]]\n","\n","Average MAE Loss:\n","[0.19777423 0.19777423 0.07131403 0.06355686]\n","\n","Epoch 00035: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.06168286 0.06543086]\n"," [0.19753724 0.18768811]\n"," [0.067904   0.06922743]]\n","\n","Average MAE Loss:\n","[0.19777423 0.06355686 0.19261268 0.06856571]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279699]\n"," [0.06168286 0.06543086]\n"," [0.1916417  0.18193011]\n"," [0.06484012 0.06693225]]\n","\n","Average MAE Loss:\n","[0.19777422 0.06355686 0.18678591 0.06588618]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.06168286 0.06543086]\n"," [0.18566956 0.1761377 ]\n"," [0.06303282 0.06568108]]\n","\n","Average MAE Loss:\n","[0.19777423 0.06355686 0.18090363 0.06435695]\n","\n","Epoch 00038: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.06168286 0.06543086]\n"," [0.17969204 0.17037058]\n"," [0.06249659 0.06534609]]\n","\n","Average MAE Loss:\n","[0.19777423 0.06355686 0.17503131 0.06392134]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.06168286 0.06543086]\n"," [0.17671779 0.16751282]\n"," [0.06212257 0.06513243]]\n","\n","Average MAE Loss:\n","[0.19777423 0.06355686 0.1721153  0.0636275 ]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.06168286 0.06543086]\n"," [0.17378126 0.16469616]\n"," [0.06186032 0.06500889]]\n","\n","Average MAE Loss:\n","[0.19777423 0.06355686 0.16923871 0.06343461]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.20275146 0.19279701]\n"," [0.06168286 0.06543086]\n"," [0.17088258 0.16192268]\n"," [0.06167392 0.06494871]]\n","\n","Average MAE Loss:\n","[0.19777423 0.06355686 0.16640263 0.06331132]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.1015525  0.09750175]\n"," [0.09919286 0.09539585]\n"," [0.09898669 0.09521208]]\n","\n","Average MAE Loss:\n","[0.09952713 0.09952713 0.09729435 0.09709939]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.1015525  0.09750175]\n"," [0.09807562 0.0944024 ]\n"," [0.09570791 0.09230791]]\n","\n","Average MAE Loss:\n","[0.09952713 0.09952713 0.09623901 0.09400791]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.1015525  0.09750175]\n"," [0.09699643 0.09344652]\n"," [0.09263917 0.08960885]]\n","\n","Average MAE Loss:\n","[0.09952713 0.09952713 0.09522147 0.09112401]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.1015525  0.09750175]\n"," [0.0959482  0.09252035]\n"," [0.08986652 0.08720263]]\n","\n","Average MAE Loss:\n","[0.09952713 0.09952713 0.09423428 0.08853457]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.10155249 0.09750175]\n"," [0.09492914 0.09162054]\n"," [0.08859701 0.08611626]]\n","\n","Average MAE Loss:\n","[0.09952713 0.09952712 0.09327484 0.08735663]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.1015525  0.09750175]\n"," [0.09443214 0.09118067]\n"," [0.08740365 0.08510674]]\n","\n","Average MAE Loss:\n","[0.09952713 0.09952713 0.09280641 0.08625519]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.1015525  0.09750175]\n"," [0.09394456 0.09075053]\n"," [0.08627311 0.08416218]]\n","\n","Average MAE Loss:\n","[0.09952713 0.09952713 0.09234755 0.08521765]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.08627311 0.08416218]\n"," [0.10102554 0.09702979]\n"," [0.0927143  0.08967473]]\n","\n","Average MAE Loss:\n","[0.09952713 0.08521765 0.09902766 0.09119452]\n","\n","Epoch 00050: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.08627311 0.08416218]\n"," [0.10049847 0.09655854]\n"," [0.09211007 0.08914821]]\n","\n","Average MAE Loss:\n","[0.09952713 0.08521765 0.0985285  0.09062914]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.08627311 0.08416218]\n"," [0.10023838 0.09632608]\n"," [0.09152609 0.0886402 ]]\n","\n","Average MAE Loss:\n","[0.09952713 0.08521765 0.09828223 0.09008314]\n","\n","Epoch 00052: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.08627311 0.08416218]\n"," [0.09998122 0.09609669]\n"," [0.09096204 0.08815107]]\n","\n","Average MAE Loss:\n","[0.09952713 0.08521765 0.09803895 0.08955655]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.08627311 0.08416218]\n"," [0.09972699 0.0958699 ]\n"," [0.09041763 0.08767983]]\n","\n","Average MAE Loss:\n","[0.09952713 0.08521765 0.09779844 0.08904873]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.08627311 0.08416218]\n"," [0.09947556 0.09564602]\n"," [0.0901527  0.08745036]]\n","\n","Average MAE Loss:\n","[0.09952713 0.08521765 0.09756079 0.08880153]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.1015525  0.09750175]\n"," [0.08627311 0.08416218]\n"," [0.09935091 0.09553536]\n"," [0.0898937  0.08722638]]\n","\n","Average MAE Loss:\n","[0.09952713 0.08521765 0.09744313 0.08856004]\n","\n","Epoch 00056: reducing learning rate of group 0 to 2.4414e-07.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.09404823 0.09084149]\n"," [0.09404823 0.09084149]\n"," [0.09393335 0.09074037]\n"," [0.09377523 0.09060136]]\n","\n","Average MAE Loss:\n","[0.09244486 0.09244486 0.09233686 0.09218829]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.09404823 0.09084149]\n"," [0.09404823 0.09084149]\n"," [0.09382065 0.09064125]\n"," [0.09350397 0.09036373]]\n","\n","Average MAE Loss:\n","[0.09244486 0.09244486 0.09223095 0.09193385]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.09404823 0.09084149]\n"," [0.09404823 0.09084149]\n"," [0.09370887 0.09054313]\n"," [0.09337037 0.090247  ]]\n","\n","Average MAE Loss:\n","[0.09244486 0.09244486 0.092126   0.09180868]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.09404823 0.09084149]\n"," [0.09404823 0.09084149]\n"," [0.09365331 0.09049442]\n"," [0.09323876 0.090132  ]]\n","\n","Average MAE Loss:\n","[0.09244486 0.09244486 0.09207387 0.09168538]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.2207e-07.\n","\n","epochs finished with time:1181.3340549468994\n","\n","[[0.09404823 0.09084149]\n"," [0.09404823 0.09084149]\n"," [0.09365331 0.09049442]\n"," [0.09323876 0.090132  ]]\n","00:20:2.598950543002502\n"]}],"source":["# seed = 10 table = 2/8 fixed Meta 2 layers x\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='Meta_4D-FED-GNN++'\n","train_gnns_final(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"fcXSL4SIvnZ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VoaPdwAI_3OO"},"source":["## CNNs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBcvkSE_ACp_"},"outputs":[],"source":["def train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = False\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(True)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_gnns(args,device))\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cBsklrSsPUtm"},"source":["### CNN Simple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W6_2ZEr8PY_b"},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","class ConvolutionalNN_1(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNN_1, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Flatten()  # Flatten the 2D outputs from the conv layer\n","        )\n","\n","        self.fc = nn.Linear(128*17*17, 35*35)  # 35*35, the size of output image\n","\n","    def forward(self, data):\n","\n","        data = data.view(1,1,data.shape[0],data.shape[1])\n","        data = self.conv(data)\n","\n","        out = F.relu(self.fc(data))\n","        return out.view(35, 35)  # Reshape back into image format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iL3xYCC7aCeb"},"outputs":[],"source":["class ConvolutionalNN_1(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNN_1, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Flatten()  # Flatten the 2D outputs from the conv layer\n","        )\n","\n","        self.fc = nn.Linear(128*17*17, 35*35)  # 35*35, the size of output image\n","\n","    def forward(self, data):\n","\n","        data = data.view(1,1,data.shape[0],data.shape[1])\n","        data = self.conv(data)\n","\n","        out = F.relu(self.fc(data))\n","        return out.view(35, 35)  # Reshape back into image format"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1690022111427,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"ps3HBgHLKsAu","outputId":"125f4e4e-9118-4b8a-9890-7ea92c0771b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 45390921\n","Trainable parameters: 45390921\n"]}],"source":["model = ConvolutionalNN_1()\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1aPeaN0azIc"},"outputs":[],"source":["class Hospital_cnns_1():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = ConvolutionalNN_1().to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQR68uLiad-6"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class ConvolutionalNN_2(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNN_2, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(1024, 2048, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","\n","\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Flatten()  # Flatten the 2D outputs from the conv layer\n","        )\n","\n","        # Calculate the size of the flattened layer\n","        flattened_size = 2048 * 4 * 4  # update the size based on the conv layers\n","        self.fc = nn.Linear(flattened_size, 35*35)  # 35*35, the size of output image\n","\n","    def forward(self, data):\n","        data = data.view(1,1,data.shape[0],data.shape[1])\n","        data = self.conv(data)\n","        out = F.relu(self.fc(data))\n","        return out.view(35, 35)  # Reshape back into image format\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9zsFy17rDVS"},"outputs":[],"source":["class Hospital_cnns_2():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = ConvolutionalNN_2().to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpeccPZLdfXz"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class ConvolutionalNN_2_dropout(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNN_2_dropout, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.2),  # Dropout after first layer\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.2),  # Dropout after second layer\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.3),  # Increase dropout rate as network goes deeper\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.3),  # Increase dropout rate as network goes deeper\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.4),  # Increase dropout rate as network goes deeper\n","            nn.Conv2d(1024, 2048, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.4),  # Increase dropout rate as network goes deeper\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Flatten()  # Flatten the 2D outputs from the conv layer\n","        )\n","\n","        # Calculate the size of the flattened layer\n","        flattened_size = 2048 * 4 * 4  # update the size based on the conv layers\n","        self.fc = nn.Linear(flattened_size, 35*35)  # 35*35, the size of output image\n","\n","    def forward(self, data):\n","        data = data.view(1,1,data.shape[0],data.shape[1])\n","        data = self.conv(data)\n","        out = F.relu(self.fc(data))\n","        return out.view(35, 35)  # Reshape back into image format\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ux9JI0Hhdten"},"outputs":[],"source":["class Hospital_cnns_2_dropout():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = ConvolutionalNN_2_dropout().to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1690366103036,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"8zWEtOPkHQxQ","outputId":"70f2a05c-9534-40ab-af5d-d5c8663927c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 65287881\n","Trainable parameters: 65287881\n"]}],"source":["model = ConvolutionalNN_2_dropout()\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXcpJYHiXN8O"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ConvolutionalNN_3(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNN_3, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.2),\n","\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.2),\n","\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.3),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.3),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.4),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","\n","            nn.Flatten()  # Flatten the 2D outputs from the conv layer\n","        )\n","\n","        flattened_size = 512 * 2 * 2  # update the size based on the conv layers\n","        self.fc = nn.Linear(flattened_size, 35*35)  # 35*35, the size of output image\n","\n","    def forward(self, data):\n","        data = data.view(1,1,data.shape[0],data.shape[1])\n","        data = self.conv(data)\n","        out = F.relu(self.fc(data))\n","        return out.view(35, 35)  # Reshape back into image format\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1690029726562,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"o15KVxZDbhkP","outputId":"c4143d04-8537-4f00-da5a-6f87d0f07ce6"},"outputs":[{"data":{"text/plain":["torch.Size([35, 35])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["model(dataset[0][0]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1690029725194,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"a0mAbszGZkTE","outputId":"50877951-48a8-48ac-b4ea-bdfa2f2f827b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 11895113\n","Trainable parameters: 11895113\n"]}],"source":["model = ConvolutionalNN_3()\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HslY-n6ZkaK"},"outputs":[],"source":["class Hospital_cnns_3():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = ConvolutionalNN_3().to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=3,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVchG-piTyUA"},"outputs":[],"source":["def train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = True\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_vgg(args,device))\n","                  print('VGG')\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns(hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals, ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0836hy1FJd5"},"outputs":[],"source":["def train_cnns_custom(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1):\n","        \"\"\"\n","            Arguments:\n","            args: arguments\n","            dataset: the whole dataset (train and test set)\n","            table: [num_hospitals, num_timepoints], holds timepoint-wise availability of hospitals\n","\n","        This function performs training and testing reporting Mean Absolute Error (MAE) of the testing brain graphs.\n","        \"\"\"\n","\n","        # Create the results folders\n","        print('Train NEW')\n","        os.makedirs(args.save_path, exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/real_and_predicted_graphs', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/tp_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/train_losses/total_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/test_mae_losses', exist_ok=True)\n","        os.makedirs(args.save_path+f'{args.save_name}/trained_models', exist_ok=True)\n","\n","\n","        # Create the results folders\n","        manualSeed = seed\n","\n","        np.random.seed(manualSeed)\n","        random.seed(manualSeed)\n","        torch.manual_seed(manualSeed)\n","\n","        # show the fixed seed\n","        print(f'Fixed seed:{seed}')\n","        print()\n","\n","        # create the table that shows the data availability by  timepoint\n","        table = np.zeros((args.num_folds - 1, args.num_timepoints))\n","        table = random_table(args, ratio)\n","        print(f'Table:')\n","        print(table)\n","        print(f'Ratio:{ratio}')\n","        print()\n","\n","        if torch.cuda.is_available():\n","            device = torch.device('cuda:0')\n","            print('running on GPU')\n","            # if you are using GPU\n","            torch.cuda.manual_seed(manualSeed)\n","            torch.cuda.manual_seed_all(manualSeed)\n","\n","            torch.backends.cudnn.enabled = False\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = True\n","            os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # !!! necessary for the below line\n","            torch.use_deterministic_algorithms(False)\n","            print('TRAIN deterministic algorithms')\n","\n","        else:\n","            device = torch.device(\"cpu\")\n","            print('running on CPU')\n","\n","        # change the save path\n","        args.save_path = args.save_path+f'{args.save_name}/'\n","\n","        # Choosing the right get_order function\n","        if args.mode == 'weighted_weight_exchange':\n","            get_order =  get_order_weighted\n","        else:\n","            get_order =  get_order_gnns\n","\n","\n","        # Getting our fold dict\n","        fold_dict,X = mf.create_fold_dict_new(dataset,num_hospitals=4,num_folds=5)\n","\n","        # Perform the 5-fold Cross-Validation\n","        num_hospitals = args.num_folds - 1\n","        for f in range(args.num_folds):\n","\n","            # fix the seeds\n","            np.random.seed(manualSeed)\n","            random.seed(manualSeed)\n","            torch.manual_seed(manualSeed)\n","\n","            tic0 = timeit.default_timer()\n","\n","            print(\n","                f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n","\n","                        # Create hospitals\n","            hospitals = []\n","            for i in range(num_hospitals):\n","                  hospitals.append(Hospital_vgg(args,device))\n","\n","\n","            # Train data for each hospital\n","            train_data_list = []\n","\n","            # Current test data\n","            test_data = X[-1][f].to(device)\n","\n","            for i in range(num_hospitals):\n","\n","                # Append the fold related to Hospital i - fold f. Note, here we append the real data, not the indices\n","                train_data_list.append(X[i][fold_dict[f'Hospital_{i}'][f'fold_{f}'][0]].to(device))\n","\n","\n","            # Start measuring the epochs time\n","            epochs_start = time.time()\n","\n","            # Initiate Training\n","            for epoch in range(args.num_epochs):\n","\n","                  epoch +=1\n","\n","                  # order the hospitals based on the data availability\n","                  ordered_hospitals = get_order_gnns(table)\n","\n","                  for h_i,hospital in enumerate(hospitals):\n","\n","                    # get the train data for the hospital\n","                    train_data = train_data_list[h_i]\n","\n","                    # get the table for th current hospital\n","                    table_hospital = table[h_i]\n","\n","                    # Train the current hospital at the current timepoint for 1 epoch\n","                    mae_loss,hospital = train_one_epoch_gnns(args,hospital,train_data,table_hospital)\n","\n","                    # Updating the hospital\n","                    hospitals[h_i] = hospital\n","\n","                    if verbose:\n","                        print(f'Epoch:{epoch} Hospital:{h_i}, Train MAE Loss:{mae_loss}')\n","\n","\n","                  # Perform validation during training\n","                  if train_validate_verbose and (epoch%train_validate_verbosity_epochs==0 or epoch==1):\n","\n","                      val_loss,val_mean_loss = validate_during_training_gnns(args, hospitals, test_data)\n","                      print(f\"Epoch:{epoch},val_loss:\")\n","                      print(f'Total MAE Loss')\n","                      print(val_loss)\n","                      print()\n","                      print(f'Average MAE Loss:')\n","                      print(val_mean_loss)\n","                      print()\n","\n","                      for h_i,l in enumerate(val_mean_loss):\n","\n","                          hospitals[h_i].scheduler.step(l)\n","\n","\n","                  if epoch != args.num_epochs - 1 or epoch != 0:\n","                      if epoch % args.C == 0 and args.mode != \"4D-GNN\":\n","                          print('Central Aggregation')\n","                          hospitals = update_main_by_average_gnns_new(hospitals,ordered_hospitals,table)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN+\":\n","                          hospitals = exchange_models(hospitals, t)\n","                      elif epoch % args.D == 0 and args.mode == \"4D-FED-GNN++\":\n","                          print('4D-FED-GNN++')\n","                          hospitals = exchange_models_based_on_order_gnns(hospitals,ordered_hospitals)\n","                      elif epoch % args.D == 0 and args.mode == \"weighted_weight_exchange\":\n","                          print('weighted_weight_exchange')\n","                          hospitals = exchange_models_weights_pairs_extreme(hospitals, t, ordered_hospitals)\n","\n","\n","\n","            epochs_end = time.time()-epochs_start\n","            print()\n","            print(f'epochs finished with time:{epochs_end}')\n","            print()\n","            validate_gnns(args, hospitals, test_data, f)\n","            tic1 = timeit.default_timer()\n","            timer(tic0,tic1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzyfCyPOedRC"},"outputs":[],"source":["with torch.no_grad():\n","    torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"ke3Z16hUbTSE"},"source":["#### CNN 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5,"status":"error","timestamp":1689942519848,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"hqQBg42ebFmS","outputId":"673ff646-2cd0-4d97-877e-482307f75b52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 1.]\n"," [1. 0. 1.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.0\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-94-0460980aa6da>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Simple_CNN_4D-FED-GNN++'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_cnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbosity_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-93-a6f4db574cd7>\u001b[0m in \u001b[0;36mtrain_cnns\u001b[0;34m(args, dataset, seed, ratio, verbose, train_validate_verbose, train_validate_verbosity_epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mhospitals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hospitals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                   \u001b[0mhospitals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHospital_cnns_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Hospital_cnns_2' is not defined"]}],"source":["# table = 4/8 CNN1\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Simple_CNN_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=0/4,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4I5YBN6TGPcY"},"outputs":[],"source":["torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{"id":"E77zvOJPlTul"},"source":["#### CNN 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290697,"status":"ok","timestamp":1690366535783,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"eijAZPJerbPh","outputId":"9ae7b9e2-f905-4b70-9666-53191eeb7536"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07516651 0.07205517]\n"," [0.0858584  0.08280423]\n"," [0.06787438 0.06496757]\n"," [0.07490906 0.07166309]]\n","\n","Average MAE Loss:\n","[0.07361084 0.08433132 0.06642098 0.07328607]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07498428 0.07173545]\n"," [0.0858584  0.08280423]\n"," [0.06639742 0.0638234 ]\n"," [0.06206009 0.05967656]]\n","\n","Average MAE Loss:\n","[0.07335987 0.08433132 0.06511041 0.06086833]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07476131 0.07152765]\n"," [0.0858584  0.08280423]\n"," [0.06700019 0.06495788]\n"," [0.05992341 0.05798169]]\n","\n","Average MAE Loss:\n","[0.07314448 0.08433132 0.06597904 0.05895255]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07395066 0.07067663]\n"," [0.0858584  0.08280423]\n"," [0.04834321 0.04577595]\n"," [0.05947026 0.05753409]]\n","\n","Average MAE Loss:\n","[0.07231365 0.08433132 0.04705958 0.05850218]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07460122 0.07132901]\n"," [0.0858584  0.08280423]\n"," [0.04714756 0.04592667]\n"," [0.06018343 0.05849025]]\n","\n","Average MAE Loss:\n","[0.07296512 0.08433132 0.04653711 0.05933684]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07384793 0.07049378]\n"," [0.0858584  0.08280423]\n"," [0.04532447 0.04352521]\n"," [0.0675809  0.06688603]]\n","\n","Average MAE Loss:\n","[0.07217086 0.08433132 0.04442484 0.06723346]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07391277 0.07046885]\n"," [0.0858584  0.08280423]\n"," [0.04532189 0.04321736]\n"," [0.05837941 0.05671083]]\n","\n","Average MAE Loss:\n","[0.07219081 0.08433132 0.04426962 0.05754512]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04830213 0.04547687]\n"," [0.05837941 0.05671083]\n"," [0.0751286  0.0715753 ]\n"," [0.04666681 0.04496093]]\n","\n","Average MAE Loss:\n","[0.0468895  0.05754512 0.07335195 0.04581387]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04494185 0.04200202]\n"," [0.05837941 0.05671083]\n"," [0.07420354 0.07114234]\n"," [0.04660594 0.04491795]]\n","\n","Average MAE Loss:\n","[0.04347194 0.05754512 0.07267294 0.04576195]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04505244 0.04245394]\n"," [0.05837941 0.05671083]\n"," [0.07469345 0.07175657]\n"," [0.04826055 0.0457844 ]]\n","\n","Average MAE Loss:\n","[0.04375319 0.05754512 0.07322501 0.04702248]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04264438 0.04004217]\n"," [0.05837941 0.05671083]\n"," [0.07347522 0.07033774]\n"," [0.04582438 0.04369801]]\n","\n","Average MAE Loss:\n","[0.04134328 0.05754512 0.07190648 0.0447612 ]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04399645 0.04144671]\n"," [0.05837941 0.05671083]\n"," [0.07100365 0.0678174 ]\n"," [0.04429479 0.04238231]]\n","\n","Average MAE Loss:\n","[0.04272158 0.05754512 0.06941052 0.04333855]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04157186 0.03909111]\n"," [0.05837941 0.05671083]\n"," [0.06973163 0.06682174]\n"," [0.04303205 0.04074728]]\n","\n","Average MAE Loss:\n","[0.04033148 0.05754512 0.06827669 0.04188966]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04125367 0.03853981]\n"," [0.05837941 0.05671083]\n"," [0.05102256 0.04890125]\n"," [0.04316255 0.0411353 ]]\n","\n","Average MAE Loss:\n","[0.03989674 0.05754512 0.04996191 0.04214893]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04382676 0.04206167]\n"," [0.07616464 0.07294137]\n"," [0.04642892 0.04481206]\n"," [0.04369219 0.04101683]]\n","\n","Average MAE Loss:\n","[0.04294422 0.074553   0.04562049 0.04235451]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04130729 0.0388972 ]\n"," [0.07616464 0.07294137]\n"," [0.04103764 0.03930812]\n"," [0.04167795 0.04020517]]\n","\n","Average MAE Loss:\n","[0.04010225 0.074553   0.04017288 0.04094156]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04177777 0.03920857]\n"," [0.07616464 0.07294137]\n"," [0.04030917 0.03857422]\n"," [0.04116317 0.03933284]]\n","\n","Average MAE Loss:\n","[0.04049317 0.074553   0.0394417  0.040248  ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04061313 0.03817243]\n"," [0.07616464 0.07294137]\n"," [0.04031992 0.03896026]\n"," [0.04051955 0.03885437]]\n","\n","Average MAE Loss:\n","[0.03939278 0.074553   0.03964009 0.03968696]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04141225 0.03901224]\n"," [0.07616464 0.07294137]\n"," [0.03998773 0.03851881]\n"," [0.04047993 0.03922594]]\n","\n","Average MAE Loss:\n","[0.04021225 0.074553   0.03925327 0.03985294]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04042909 0.03782355]\n"," [0.07616464 0.07294137]\n"," [0.04014922 0.03884295]\n"," [0.04010801 0.0387513 ]]\n","\n","Average MAE Loss:\n","[0.03912632 0.074553   0.03949609 0.03942966]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04039524 0.03795418]\n"," [0.07616464 0.07294137]\n"," [0.04003419 0.03840742]\n"," [0.04317287 0.04113829]]\n","\n","Average MAE Loss:\n","[0.03917471 0.074553   0.0392208  0.04215558]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04641275 0.04421149]\n"," [0.04317287 0.04113829]\n"," [0.04040912 0.03853886]\n"," [0.04024005 0.03888285]]\n","\n","Average MAE Loss:\n","[0.04531212 0.04215558 0.03947399 0.03956145]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04199584 0.03926459]\n"," [0.04317287 0.04113829]\n"," [0.04039084 0.03829086]\n"," [0.03996943 0.03854285]]\n","\n","Average MAE Loss:\n","[0.04063022 0.04215558 0.03934085 0.03925614]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04095518 0.03850963]\n"," [0.04317287 0.04113829]\n"," [0.04048392 0.03852428]\n"," [0.04154674 0.0396907 ]]\n","\n","Average MAE Loss:\n","[0.0397324  0.04215558 0.0395041  0.04061872]\n","\n","Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04023424 0.03772679]\n"," [0.04317287 0.04113829]\n"," [0.04044749 0.03851542]\n"," [0.04039872 0.03869767]]\n","\n","Average MAE Loss:\n","[0.03898052 0.04215558 0.03948146 0.0395482 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04006846 0.03775724]\n"," [0.04317287 0.04113829]\n"," [0.04047534 0.03850728]\n"," [0.03986367 0.03865145]]\n","\n","Average MAE Loss:\n","[0.03891285 0.04215558 0.03949131 0.03925756]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03988063 0.03747434]\n"," [0.04317287 0.04113829]\n"," [0.04065748 0.03871379]\n"," [0.04032477 0.03834654]]\n","\n","Average MAE Loss:\n","[0.03867749 0.04215558 0.03968563 0.03933565]\n","\n","Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03995985 0.03730487]\n"," [0.04317287 0.04113829]\n"," [0.04066901 0.03876307]\n"," [0.03976265 0.03820753]]\n","\n","Average MAE Loss:\n","[0.03863236 0.04215558 0.03971604 0.03898509]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03939277 0.03700657]\n"," [0.04130782 0.03886164]\n"," [0.03955282 0.03716592]\n"," [0.03948132 0.03780232]]\n","\n","Average MAE Loss:\n","[0.03819967 0.04008473 0.03835937 0.03864182]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03904059 0.03661587]\n"," [0.04130782 0.03886164]\n"," [0.03885174 0.03691061]\n"," [0.0389573  0.03739265]]\n","\n","Average MAE Loss:\n","[0.03782823 0.04008473 0.03788118 0.03817498]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03915888 0.03671457]\n"," [0.04130782 0.03886164]\n"," [0.03910358 0.03729808]\n"," [0.03897314 0.0375032 ]]\n","\n","Average MAE Loss:\n","[0.03793672 0.04008473 0.03820083 0.03823817]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03916886 0.03660619]\n"," [0.04130782 0.03886164]\n"," [0.0391376  0.0373397 ]\n"," [0.03883247 0.03727661]]\n","\n","Average MAE Loss:\n","[0.03788752 0.04008473 0.03823865 0.03805454]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03912632 0.0364857 ]\n"," [0.04130782 0.03886164]\n"," [0.03915994 0.03724619]\n"," [0.03875314 0.03708022]]\n","\n","Average MAE Loss:\n","[0.03780601 0.04008473 0.03820307 0.03791668]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03904792 0.03649668]\n"," [0.04130782 0.03886164]\n"," [0.03939565 0.03744571]\n"," [0.03879786 0.03712647]]\n","\n","Average MAE Loss:\n","[0.0377723  0.04008473 0.03842068 0.03796216]\n","\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0389516  0.03641741]\n"," [0.04130782 0.03886164]\n"," [0.03949656 0.03757155]\n"," [0.03863533 0.03705263]]\n","\n","Average MAE Loss:\n","[0.0376845  0.04008473 0.03853405 0.03784398]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03928818 0.03686072]\n"," [0.03863533 0.03705263]\n"," [0.03891784 0.03647151]\n"," [0.03839395 0.03683457]]\n","\n","Average MAE Loss:\n","[0.03807445 0.03784398 0.03769467 0.03761426]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03929581 0.03663156]\n"," [0.03863533 0.03705263]\n"," [0.038857   0.03655362]\n"," [0.0384699  0.03692805]]\n","\n","Average MAE Loss:\n","[0.03796369 0.03784398 0.03770531 0.03769897]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03896335 0.03658173]\n"," [0.03863533 0.03705263]\n"," [0.03881835 0.03666427]\n"," [0.03855777 0.03704825]]\n","\n","Average MAE Loss:\n","[0.03777254 0.03784398 0.03774131 0.03780301]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03931976 0.03698725]\n"," [0.03863533 0.03705263]\n"," [0.0388519  0.0367676 ]\n"," [0.03847498 0.03704176]]\n","\n","Average MAE Loss:\n","[0.03815351 0.03784398 0.03780975 0.03775837]\n","\n","Epoch 00039: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03894466 0.03656467]\n"," [0.03863533 0.03705263]\n"," [0.03890411 0.03684917]\n"," [0.03871084 0.03723798]]\n","\n","Average MAE Loss:\n","[0.03775466 0.03784398 0.03787664 0.03797441]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00040: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03891017 0.0365062 ]\n"," [0.03863533 0.03705263]\n"," [0.03893789 0.03689173]\n"," [0.03877088 0.03732628]]\n","\n","Average MAE Loss:\n","[0.03770819 0.03784398 0.03791481 0.03804858]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03897388 0.03647002]\n"," [0.03863533 0.03705263]\n"," [0.03894398 0.03690524]\n"," [0.03864793 0.03715078]]\n","\n","Average MAE Loss:\n","[0.03772195 0.03784398 0.03792461 0.03789935]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0384377  0.03627855]\n"," [0.03832403 0.03642327]\n"," [0.03833114 0.03642753]\n"," [0.0382869  0.03645264]]\n","\n","Average MAE Loss:\n","[0.03735812 0.03737365 0.03737933 0.03736977]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03876382 0.03645907]\n"," [0.03832403 0.03642327]\n"," [0.03835381 0.03646125]\n"," [0.03837241 0.03671738]]\n","\n","Average MAE Loss:\n","[0.03761144 0.03737365 0.03740753 0.03754489]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03880843 0.03632888]\n"," [0.03832403 0.03642327]\n"," [0.03838578 0.0364917 ]\n"," [0.03848394 0.03692049]]\n","\n","Average MAE Loss:\n","[0.03756865 0.03737365 0.03743874 0.03770222]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03889889 0.03640636]\n"," [0.03832403 0.03642327]\n"," [0.03842091 0.03652064]\n"," [0.03846128 0.0370315 ]]\n","\n","Average MAE Loss:\n","[0.03765262 0.03737365 0.03747078 0.03774639]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03895749 0.0365111 ]\n"," [0.03832403 0.03642327]\n"," [0.0384863  0.03660205]\n"," [0.03846301 0.03700519]]\n","\n","Average MAE Loss:\n","[0.0377343  0.03737365 0.03754417 0.0377341 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03892348 0.0364978 ]\n"," [0.03832403 0.03642327]\n"," [0.03851538 0.03663326]\n"," [0.03842687 0.03694612]]\n","\n","Average MAE Loss:\n","[0.03771064 0.03737365 0.03757432 0.0376865 ]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03890309 0.03645734]\n"," [0.03832403 0.03642327]\n"," [0.03853917 0.03665269]\n"," [0.038454   0.0369631 ]]\n","\n","Average MAE Loss:\n","[0.03768022 0.03737365 0.03759593 0.03770855]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0384049  0.0363618 ]\n"," [0.038454   0.0369631 ]\n"," [0.03888849 0.03645972]\n"," [0.03845224 0.03669484]]\n","\n","Average MAE Loss:\n","[0.03738335 0.03770855 0.03767411 0.03757354]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03857328 0.03639917]\n"," [0.038454   0.0369631 ]\n"," [0.03886321 0.03646774]\n"," [0.03836085 0.03670022]]\n","\n","Average MAE Loss:\n","[0.03748623 0.03770855 0.03766548 0.03753054]\n","\n","Epoch 00051: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00051: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03866236 0.03641592]\n"," [0.038454   0.0369631 ]\n"," [0.03885421 0.03647379]\n"," [0.03839178 0.03675936]]\n","\n","Average MAE Loss:\n","[0.03753914 0.03770855 0.037664   0.03757557]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03871762 0.03642018]\n"," [0.038454   0.0369631 ]\n"," [0.03884674 0.03647836]\n"," [0.03841464 0.03680547]]\n","\n","Average MAE Loss:\n","[0.0375689  0.03770855 0.03766255 0.03761006]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0387425  0.03644594]\n"," [0.038454   0.0369631 ]\n"," [0.03883699 0.03648044]\n"," [0.03846589 0.03688718]]\n","\n","Average MAE Loss:\n","[0.03759422 0.03770855 0.03765872 0.03767653]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03876622 0.03647623]\n"," [0.038454   0.0369631 ]\n"," [0.03882411 0.03647233]\n"," [0.03838118 0.03679328]]\n","\n","Average MAE Loss:\n","[0.03762123 0.03770855 0.03764822 0.03758723]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00055: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03872895 0.0364261 ]\n"," [0.038454   0.0369631 ]\n"," [0.03881917 0.03647269]\n"," [0.03837869 0.03678971]]\n","\n","Average MAE Loss:\n","[0.03757752 0.03770855 0.03764593 0.0375842 ]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03832954 0.03634787]\n"," [0.03833002 0.03639354]\n"," [0.03833082 0.03639592]\n"," [0.03831485 0.03639646]]\n","\n","Average MAE Loss:\n","[0.03733871 0.03736178 0.03736337 0.03735565]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03838124 0.03635407]\n"," [0.03833002 0.03639354]\n"," [0.03833288 0.03639751]\n"," [0.03830339 0.03642457]]\n","\n","Average MAE Loss:\n","[0.03736766 0.03736178 0.0373652  0.03736398]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03843612 0.03636683]\n"," [0.03833002 0.03639354]\n"," [0.0383359  0.03639838]\n"," [0.03829891 0.03644048]]\n","\n","Average MAE Loss:\n","[0.03740148 0.03736178 0.03736714 0.0373697 ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03847736 0.03638728]\n"," [0.03833002 0.03639354]\n"," [0.03834011 0.03640136]\n"," [0.03827724 0.03644087]]\n","\n","Average MAE Loss:\n","[0.03743232 0.03736178 0.03737073 0.03735905]\n","\n","\n","epochs finished with time:43.027395486831665\n","\n","[[0.03847736 0.03638728]\n"," [0.03833002 0.03639354]\n"," [0.03834011 0.03640136]\n"," [0.03827724 0.03644087]]\n","00:01:1.7743713900001694\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10510032 0.10465345]\n"," [0.11635957 0.1145391 ]\n"," [0.09953049 0.09869741]\n"," [0.10200627 0.10030578]]\n","\n","Average MAE Loss:\n","[0.10487689 0.11544933 0.09911395 0.10115603]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10282139 0.10169646]\n"," [0.11635957 0.1145391 ]\n"," [0.09678903 0.09675982]\n"," [0.09335021 0.09085104]]\n","\n","Average MAE Loss:\n","[0.10225892 0.11544933 0.09677443 0.09210063]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10189503 0.10143955]\n"," [0.11635957 0.1145391 ]\n"," [0.0963984  0.09519034]\n"," [0.09062812 0.08862266]]\n","\n","Average MAE Loss:\n","[0.10166729 0.11544933 0.09579437 0.08962539]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10146573 0.10078686]\n"," [0.11635957 0.1145391 ]\n"," [0.09527292 0.09480906]\n"," [0.09234017 0.08993156]]\n","\n","Average MAE Loss:\n","[0.1011263  0.11544933 0.09504099 0.09113586]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10191199 0.10081086]\n"," [0.11635957 0.1145391 ]\n"," [0.09648607 0.09612367]\n"," [0.0891773  0.08788525]]\n","\n","Average MAE Loss:\n","[0.10136142 0.11544933 0.09630487 0.08853128]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10089303 0.10017546]\n"," [0.11635957 0.1145391 ]\n"," [0.0946812  0.0945891 ]\n"," [0.09094366 0.08937688]]\n","\n","Average MAE Loss:\n","[0.10053425 0.11544933 0.09463515 0.09016027]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10218892 0.10101864]\n"," [0.11635957 0.1145391 ]\n"," [0.09522852 0.0949191 ]\n"," [0.09515497 0.09271277]]\n","\n","Average MAE Loss:\n","[0.10160378 0.11544933 0.09507381 0.09393387]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07467083 0.07451853]\n"," [0.09515497 0.09271277]\n"," [0.10200489 0.10114928]\n"," [0.10138748 0.09986295]]\n","\n","Average MAE Loss:\n","[0.07459468 0.09393387 0.10157709 0.10062521]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06992674 0.07053012]\n"," [0.09515497 0.09271277]\n"," [0.10282867 0.10172184]\n"," [0.09672837 0.09524617]]\n","\n","Average MAE Loss:\n","[0.07022843 0.09393387 0.10227525 0.09598727]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06765438 0.06907552]\n"," [0.09515497 0.09271277]\n"," [0.10270858 0.10174299]\n"," [0.09553538 0.09409731]]\n","\n","Average MAE Loss:\n","[0.06836495 0.09393387 0.10222579 0.09481634]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07024558 0.07035668]\n"," [0.09515497 0.09271277]\n"," [0.10113494 0.10069313]\n"," [0.09474797 0.09349049]]\n","\n","Average MAE Loss:\n","[0.07030113 0.09393387 0.10091404 0.09411923]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0691143  0.06914317]\n"," [0.09515497 0.09271277]\n"," [0.10142605 0.10058779]\n"," [0.09454229 0.09337714]]\n","\n","Average MAE Loss:\n","[0.06912873 0.09393387 0.10100692 0.09395971]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06759102 0.06772021]\n"," [0.09515497 0.09271277]\n"," [0.10090337 0.10030745]\n"," [0.09472159 0.09355721]]\n","\n","Average MAE Loss:\n","[0.06765561 0.09393387 0.10060541 0.0941394 ]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06620704 0.06702821]\n"," [0.09515497 0.09271277]\n"," [0.1013817  0.10044687]\n"," [0.09420024 0.09322854]]\n","\n","Average MAE Loss:\n","[0.06661762 0.09393387 0.10091428 0.09371439]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06721607 0.06879017]\n"," [0.11263821 0.11078798]\n"," [0.09297123 0.09154356]\n"," [0.09502968 0.09367782]]\n","\n","Average MAE Loss:\n","[0.06800312 0.11171309 0.0922574  0.09435375]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06789638 0.06818358]\n"," [0.11263821 0.11078798]\n"," [0.06950147 0.07076178]\n"," [0.06932827 0.06956885]]\n","\n","Average MAE Loss:\n","[0.06803998 0.11171309 0.07013162 0.06944856]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06833284 0.06785747]\n"," [0.11263821 0.11078798]\n"," [0.07007617 0.07003406]\n"," [0.06833627 0.06779398]]\n","\n","Average MAE Loss:\n","[0.06809515 0.11171309 0.07005512 0.06806512]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06651009 0.06698211]\n"," [0.11263821 0.11078798]\n"," [0.06758297 0.06813811]\n"," [0.06740546 0.06709267]]\n","\n","Average MAE Loss:\n","[0.0667461  0.11171309 0.06786054 0.06724906]\n","\n","Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06458389 0.06570647]\n"," [0.11263821 0.11078798]\n"," [0.06682912 0.06795542]\n"," [0.06798053 0.06745345]]\n","\n","Average MAE Loss:\n","[0.06514518 0.11171309 0.06739227 0.06771699]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06576293 0.06617086]\n"," [0.11263821 0.11078798]\n"," [0.0669924  0.06773108]\n"," [0.06663688 0.0667417 ]]\n","\n","Average MAE Loss:\n","[0.06596689 0.11171309 0.06736174 0.06668929]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06523188 0.06590732]\n"," [0.11263821 0.11078798]\n"," [0.06706979 0.0677834 ]\n"," [0.06706246 0.06675526]]\n","\n","Average MAE Loss:\n","[0.0655696  0.11171309 0.0674266  0.06690886]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.10443518 0.10269597]\n"," [0.06706246 0.06675526]\n"," [0.06537735 0.06631985]\n"," [0.06773523 0.06777037]]\n","\n","Average MAE Loss:\n","[0.10356557 0.06690886 0.0658486  0.0677528 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07179515 0.07111533]\n"," [0.06706246 0.06675526]\n"," [0.06468806 0.06663382]\n"," [0.06755525 0.06742191]]\n","\n","Average MAE Loss:\n","[0.07145524 0.06690886 0.06566094 0.06748858]\n","\n","Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0659025  0.06613403]\n"," [0.06706246 0.06675526]\n"," [0.06593955 0.06719747]\n"," [0.06716997 0.06684922]]\n","\n","Average MAE Loss:\n","[0.06601827 0.06690886 0.06656851 0.0670096 ]\n","\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.06509213 0.06572654]\n"," [0.06706246 0.06675526]\n"," [0.06553882 0.06649307]\n"," [0.06737402 0.06691247]]\n","\n","Average MAE Loss:\n","[0.06540934 0.06690886 0.06601594 0.06714324]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06607192 0.06635764]\n"," [0.06706246 0.06675526]\n"," [0.06558113 0.06648591]\n"," [0.06635376 0.06637145]]\n","\n","Average MAE Loss:\n","[0.06621478 0.06690886 0.06603352 0.06636261]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06520783 0.06590853]\n"," [0.06706246 0.06675526]\n"," [0.06578588 0.06664761]\n"," [0.0665895  0.06647491]]\n","\n","Average MAE Loss:\n","[0.06555818 0.06690886 0.06621675 0.06653221]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00027: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06531115 0.06587931]\n"," [0.06706246 0.06675526]\n"," [0.06569273 0.06663717]\n"," [0.06712053 0.06668295]]\n","\n","Average MAE Loss:\n","[0.06559523 0.06690886 0.06616495 0.06690174]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06725855 0.06686816]\n"," [0.07031246 0.06944386]\n"," [0.06706569 0.06682115]\n"," [0.06691399 0.06654413]]\n","\n","Average MAE Loss:\n","[0.06706336 0.06987816 0.06694342 0.06672906]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06466569 0.06489518]\n"," [0.07031246 0.06944386]\n"," [0.06478784 0.06528476]\n"," [0.06482239 0.06491759]]\n","\n","Average MAE Loss:\n","[0.06478043 0.06987816 0.0650363  0.06486999]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06409737 0.06452283]\n"," [0.07031246 0.06944386]\n"," [0.06422545 0.06529436]\n"," [0.0646598  0.06482517]]\n","\n","Average MAE Loss:\n","[0.0643101  0.06987816 0.06475991 0.06474248]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06396336 0.06460711]\n"," [0.07031246 0.06944386]\n"," [0.06433908 0.06553165]\n"," [0.06472024 0.06479799]]\n","\n","Average MAE Loss:\n","[0.06428523 0.06987816 0.06493537 0.06475912]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06424805 0.06481263]\n"," [0.07031246 0.06944386]\n"," [0.06468759 0.06565936]\n"," [0.06460561 0.064671  ]]\n","\n","Average MAE Loss:\n","[0.06453034 0.06987816 0.06517348 0.06463831]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06442462 0.06495263]\n"," [0.07031246 0.06944386]\n"," [0.06485783 0.06574954]\n"," [0.06461365 0.06465608]]\n","\n","Average MAE Loss:\n","[0.06468863 0.06987816 0.06530368 0.06463487]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0639249  0.06470169]\n"," [0.07031246 0.06944386]\n"," [0.06503745 0.06585057]\n"," [0.06475875 0.06465296]]\n","\n","Average MAE Loss:\n","[0.06431329 0.06987816 0.06544401 0.06470585]\n","\n","Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06708164 0.06685585]\n"," [0.06475875 0.06465296]\n"," [0.06379158 0.06460917]\n"," [0.06509344 0.06559175]]\n","\n","Average MAE Loss:\n","[0.06696874 0.06470585 0.06420038 0.06534259]\n","\n","Epoch 00036: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06552075 0.06567179]\n"," [0.06475875 0.06465296]\n"," [0.06395996 0.06476904]\n"," [0.06486195 0.06524774]]\n","\n","Average MAE Loss:\n","[0.06559627 0.06470585 0.0643645  0.06505485]\n","\n","Epoch 00037: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06478803 0.06512908]\n"," [0.06475875 0.06465296]\n"," [0.0640192  0.06490425]\n"," [0.06461447 0.06503906]]\n","\n","Average MAE Loss:\n","[0.06495855 0.06470585 0.06446173 0.06482677]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.0642415  0.06477498]\n"," [0.06475875 0.06465296]\n"," [0.06404449 0.06503178]\n"," [0.06436214 0.06485943]]\n","\n","Average MAE Loss:\n","[0.06450824 0.06470585 0.06453814 0.06461079]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06401564 0.06466027]\n"," [0.06475875 0.06465296]\n"," [0.06427798 0.06523448]\n"," [0.06445844 0.06486324]]\n","\n","Average MAE Loss:\n","[0.06433796 0.06470585 0.06475623 0.06466084]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06397411 0.06462322]\n"," [0.06475875 0.06465296]\n"," [0.06437405 0.06532252]\n"," [0.06441381 0.06478294]]\n","\n","Average MAE Loss:\n","[0.06429866 0.06470585 0.06484829 0.06459838]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06387708 0.06458381]\n"," [0.06475875 0.06465296]\n"," [0.06435924 0.06532232]\n"," [0.06464788 0.06482342]]\n","\n","Average MAE Loss:\n","[0.06423045 0.06470585 0.06484078 0.06473565]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06407582 0.06452279]\n"," [0.06419945 0.06461209]\n"," [0.06408319 0.06459368]\n"," [0.06435894 0.06464843]]\n","\n","Average MAE Loss:\n","[0.06429931 0.06440577 0.06433844 0.06450368]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06405936 0.0644797 ]\n"," [0.06419945 0.06461209]\n"," [0.06399993 0.06460402]\n"," [0.06443453 0.06466925]]\n","\n","Average MAE Loss:\n","[0.06426953 0.06440577 0.06430197 0.06455189]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06406781 0.06448269]\n"," [0.06419945 0.06461209]\n"," [0.06402504 0.06464356]\n"," [0.06443123 0.06460016]]\n","\n","Average MAE Loss:\n","[0.06427525 0.06440577 0.0643343  0.0645157 ]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0641793  0.06453869]\n"," [0.06419945 0.06461209]\n"," [0.06403326 0.06467588]\n"," [0.0645094  0.06458975]]\n","\n","Average MAE Loss:\n","[0.064359   0.06440577 0.06435457 0.06454957]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06418167 0.06454523]\n"," [0.06419945 0.06461209]\n"," [0.06401078 0.06468662]\n"," [0.06475166 0.0646638 ]]\n","\n","Average MAE Loss:\n","[0.06436345 0.06440577 0.0643487  0.06470773]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06416804 0.06454297]\n"," [0.06419945 0.06461209]\n"," [0.06400312 0.06471896]\n"," [0.06480481 0.06468608]]\n","\n","Average MAE Loss:\n","[0.0643555  0.06440577 0.06436104 0.06474544]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.0641579  0.06453431]\n"," [0.06419945 0.06461209]\n"," [0.0639983  0.06473858]\n"," [0.06480093 0.06469126]]\n","\n","Average MAE Loss:\n","[0.0643461  0.06440577 0.06436844 0.0647461 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06419532 0.06458723]\n"," [0.06480093 0.06469126]\n"," [0.06410954 0.06451939]\n"," [0.06402521 0.06471505]]\n","\n","Average MAE Loss:\n","[0.06439128 0.0647461  0.06431447 0.06437013]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06420137 0.06457478]\n"," [0.06480093 0.06469126]\n"," [0.06407685 0.06451941]\n"," [0.06412673 0.06472154]]\n","\n","Average MAE Loss:\n","[0.06438808 0.0647461  0.06429813 0.06442414]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0641871  0.06455967]\n"," [0.06480093 0.06469126]\n"," [0.06403541 0.06451455]\n"," [0.06418992 0.06471123]]\n","\n","Average MAE Loss:\n","[0.06437339 0.0647461  0.06427498 0.06445058]\n","\n","Epoch 00052: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06418293 0.06455157]\n"," [0.06480093 0.06469126]\n"," [0.06402459 0.06451553]\n"," [0.06415277 0.06466799]]\n","\n","Average MAE Loss:\n","[0.06436725 0.0647461  0.06427006 0.06441038]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06412936 0.06452133]\n"," [0.06480093 0.06469126]\n"," [0.06401355 0.06451583]\n"," [0.06417508 0.06465411]]\n","\n","Average MAE Loss:\n","[0.06432535 0.0647461  0.06426469 0.0644146 ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00054: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06410455 0.06450577]\n"," [0.06480093 0.06469126]\n"," [0.06399317 0.06451116]\n"," [0.0642002  0.06466076]]\n","\n","Average MAE Loss:\n","[0.06430516 0.0647461  0.06425216 0.06443048]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06410109 0.06450051]\n"," [0.06480093 0.06469126]\n"," [0.0639907  0.06451786]\n"," [0.06422522 0.06466135]]\n","\n","Average MAE Loss:\n","[0.0643008  0.0647461  0.06425428 0.06444328]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.9531e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06419782 0.06450002]\n"," [0.06421599 0.0645107 ]\n"," [0.06420763 0.06451043]\n"," [0.06425315 0.06452245]]\n","\n","Average MAE Loss:\n","[0.06434892 0.06436334 0.06435903 0.0643878 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06418384 0.06449284]\n"," [0.06421599 0.0645107 ]\n"," [0.06419939 0.06450916]\n"," [0.06426482 0.06452703]]\n","\n","Average MAE Loss:\n","[0.06433834 0.06436334 0.06435427 0.06439593]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00058: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06417487 0.06448867]\n"," [0.06421599 0.0645107 ]\n"," [0.06419677 0.06451287]\n"," [0.06426848 0.06452413]]\n","\n","Average MAE Loss:\n","[0.06433177 0.06436334 0.06435482 0.0643963 ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06416202 0.06448186]\n"," [0.06421599 0.0645107 ]\n"," [0.06420096 0.06451828]\n"," [0.0642678  0.06452122]]\n","\n","Average MAE Loss:\n","[0.06432194 0.06436334 0.06435962 0.06439451]\n","\n","Epoch 00060: reducing learning rate of group 0 to 9.7656e-07.\n","\n","epochs finished with time:39.569435358047485\n","\n","[[0.06416202 0.06448186]\n"," [0.06421599 0.0645107 ]\n"," [0.06420096 0.06451828]\n"," [0.0642678  0.06452122]]\n","00:00:58.81767392100005\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08825204 0.07975547]\n"," [0.09980381 0.08862089]\n"," [0.0861783  0.07643912]\n"," [0.07069796 0.06302464]]\n","\n","Average MAE Loss:\n","[0.08400375 0.09421235 0.08130871 0.0668613 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08631101 0.07762063]\n"," [0.09980381 0.08862089]\n"," [0.06118828 0.05300362]\n"," [0.08172977 0.07783348]]\n","\n","Average MAE Loss:\n","[0.08196582 0.09421235 0.05709595 0.07978163]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08677182 0.07701027]\n"," [0.09980381 0.08862089]\n"," [0.05701384 0.05153537]\n"," [0.06979001 0.06058442]]\n","\n","Average MAE Loss:\n","[0.08189104 0.09421235 0.0542746  0.06518721]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08584036 0.07643961]\n"," [0.09980381 0.08862089]\n"," [0.05986859 0.05141927]\n"," [0.06853527 0.06015126]]\n","\n","Average MAE Loss:\n","[0.08113999 0.09421235 0.05564393 0.06434327]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08677844 0.07658666]\n"," [0.09980381 0.08862089]\n"," [0.05674252 0.04998876]\n"," [0.0659872  0.05792901]]\n","\n","Average MAE Loss:\n","[0.08168255 0.09421235 0.05336564 0.0619581 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.0859831  0.07641423]\n"," [0.09980381 0.08862089]\n"," [0.0629312  0.05300169]\n"," [0.06511058 0.05673812]]\n","\n","Average MAE Loss:\n","[0.08119866 0.09421235 0.05796645 0.06092435]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.0856747  0.07637946]\n"," [0.09980381 0.08862089]\n"," [0.05510673 0.04953185]\n"," [0.06425548 0.05551306]]\n","\n","Average MAE Loss:\n","[0.08102708 0.09421235 0.05231929 0.05988427]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06262937 0.05365084]\n"," [0.06425548 0.05551306]\n"," [0.08953498 0.07866926]\n"," [0.05780734 0.0498413 ]]\n","\n","Average MAE Loss:\n","[0.05814011 0.05988427 0.08410212 0.05382432]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05300539 0.04895032]\n"," [0.06425548 0.05551306]\n"," [0.08709856 0.07682366]\n"," [0.05866156 0.05153028]]\n","\n","Average MAE Loss:\n","[0.05097785 0.05988427 0.08196111 0.05509592]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05288283 0.04597397]\n"," [0.06425548 0.05551306]\n"," [0.08691743 0.07620303]\n"," [0.05512771 0.05043618]]\n","\n","Average MAE Loss:\n","[0.0494284  0.05988427 0.08156023 0.05278194]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05317763 0.04633067]\n"," [0.06425548 0.05551306]\n"," [0.08853907 0.07788265]\n"," [0.05561312 0.04775548]]\n","\n","Average MAE Loss:\n","[0.04975415 0.05988427 0.08321086 0.0516843 ]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05382321 0.04590755]\n"," [0.06425548 0.05551306]\n"," [0.08546251 0.07563673]\n"," [0.05642368 0.04724537]]\n","\n","Average MAE Loss:\n","[0.04986538 0.05988427 0.08054962 0.05183452]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0514836  0.04424959]\n"," [0.06425548 0.05551306]\n"," [0.0854946  0.07545545]\n"," [0.05575454 0.0463858 ]]\n","\n","Average MAE Loss:\n","[0.0478666  0.05988427 0.08047502 0.05107017]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05090545 0.04386154]\n"," [0.06425548 0.05551306]\n"," [0.08639286 0.07592069]\n"," [0.05397249 0.0455002 ]]\n","\n","Average MAE Loss:\n","[0.04738349 0.05988427 0.08115677 0.04973635]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05006844 0.04703899]\n"," [0.09231821 0.08109967]\n"," [0.05958758 0.05070559]\n"," [0.05301648 0.04506503]]\n","\n","Average MAE Loss:\n","[0.04855372 0.08670894 0.05514659 0.04904075]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05046519 0.04359797]\n"," [0.09231821 0.08109967]\n"," [0.05152084 0.04613348]\n"," [0.05039306 0.04322771]]\n","\n","Average MAE Loss:\n","[0.04703158 0.08670894 0.04882716 0.04681038]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04885489 0.04313082]\n"," [0.09231821 0.08109967]\n"," [0.05050372 0.04268829]\n"," [0.04958331 0.0429507 ]]\n","\n","Average MAE Loss:\n","[0.04599285 0.08670894 0.04659601 0.046267  ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0480985  0.04223736]\n"," [0.09231821 0.08109967]\n"," [0.04942954 0.0425467 ]\n"," [0.04932681 0.0427853 ]]\n","\n","Average MAE Loss:\n","[0.04516793 0.08670894 0.04598812 0.04605606]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04909053 0.04254133]\n"," [0.09231821 0.08109967]\n"," [0.04904431 0.04229308]\n"," [0.05096708 0.04313958]]\n","\n","Average MAE Loss:\n","[0.04581593 0.08670894 0.0456687  0.04705333]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04794876 0.0424484 ]\n"," [0.09231821 0.08109967]\n"," [0.04915841 0.04276344]\n"," [0.04892118 0.04302612]]\n","\n","Average MAE Loss:\n","[0.04519858 0.08670894 0.04596092 0.04597365]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04862422 0.04192667]\n"," [0.09231821 0.08109967]\n"," [0.04929937 0.0423191 ]\n"," [0.04897391 0.04215271]]\n","\n","Average MAE Loss:\n","[0.04527545 0.08670894 0.04580924 0.04556331]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06503353 0.05491041]\n"," [0.04897391 0.04215271]\n"," [0.04781115 0.0420572 ]\n"," [0.05061112 0.04320812]]\n","\n","Average MAE Loss:\n","[0.05997197 0.04556331 0.04493418 0.04690962]\n","\n","Epoch 00022: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04815    0.04388477]\n"," [0.04897391 0.04215271]\n"," [0.04831864 0.04199826]\n"," [0.0517173  0.04348008]]\n","\n","Average MAE Loss:\n","[0.04601739 0.04556331 0.04515845 0.04759869]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04953355 0.04270836]\n"," [0.04897391 0.04215271]\n"," [0.04870128 0.04200567]\n"," [0.04989293 0.04240472]]\n","\n","Average MAE Loss:\n","[0.04612095 0.04556331 0.04535347 0.04614883]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04720593 0.04205431]\n"," [0.04897391 0.04215271]\n"," [0.04871485 0.0418101 ]\n"," [0.05043098 0.04248955]]\n","\n","Average MAE Loss:\n","[0.04463012 0.04556331 0.04526247 0.04646027]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04744678 0.04202747]\n"," [0.04897391 0.04215271]\n"," [0.04855286 0.04187291]\n"," [0.04919282 0.04212675]]\n","\n","Average MAE Loss:\n","[0.04473713 0.04556331 0.04521289 0.04565979]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04750969 0.0419778 ]\n"," [0.04897391 0.04215271]\n"," [0.04845013 0.04184486]\n"," [0.04923519 0.04193146]]\n","\n","Average MAE Loss:\n","[0.04474374 0.04556331 0.0451475  0.04558332]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04738422 0.04194362]\n"," [0.04897391 0.04215271]\n"," [0.04825741 0.04194975]\n"," [0.04929056 0.04184797]]\n","\n","Average MAE Loss:\n","[0.04466392 0.04556331 0.04510358 0.04556927]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04639247 0.04133195]\n"," [0.04994096 0.04200588]\n"," [0.04817085 0.04100274]\n"," [0.04772031 0.04156723]]\n","\n","Average MAE Loss:\n","[0.04386221 0.04597342 0.0445868  0.04464377]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0457427  0.04166194]\n"," [0.04994096 0.04200588]\n"," [0.04731443 0.04092013]\n"," [0.04803514 0.04132694]]\n","\n","Average MAE Loss:\n","[0.04370232 0.04597342 0.04411728 0.04468104]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04666395 0.0409758 ]\n"," [0.04994096 0.04200588]\n"," [0.04720221 0.0413527 ]\n"," [0.04816269 0.04140108]]\n","\n","Average MAE Loss:\n","[0.04381987 0.04597342 0.04427746 0.04478189]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04673982 0.0412266 ]\n"," [0.04994096 0.04200588]\n"," [0.04729545 0.04140989]\n"," [0.0479887  0.04236874]]\n","\n","Average MAE Loss:\n","[0.04398321 0.04597342 0.04435267 0.04517872]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04772146 0.0414351 ]\n"," [0.04994096 0.04200588]\n"," [0.04745246 0.04136414]\n"," [0.04795152 0.04136883]]\n","\n","Average MAE Loss:\n","[0.04457828 0.04597342 0.0444083  0.04466017]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04608354 0.04155024]\n"," [0.04994096 0.04200588]\n"," [0.04740366 0.04133791]\n"," [0.04791419 0.04114625]]\n","\n","Average MAE Loss:\n","[0.04381689 0.04597342 0.04437079 0.04453022]\n","\n","Epoch 00034: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04621256 0.04117825]\n"," [0.04994096 0.04200588]\n"," [0.04731002 0.04125717]\n"," [0.04772338 0.04105721]]\n","\n","Average MAE Loss:\n","[0.04369541 0.04597342 0.04428359 0.0443903 ]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04765815 0.04092482]\n"," [0.04772338 0.04105721]\n"," [0.04608409 0.04108668]\n"," [0.04770318 0.04074041]]\n","\n","Average MAE Loss:\n","[0.04429149 0.0443903  0.04358539 0.04422179]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04669547 0.0407056 ]\n"," [0.04772338 0.04105721]\n"," [0.04594716 0.04107243]\n"," [0.04787777 0.04084603]]\n","\n","Average MAE Loss:\n","[0.04370053 0.0443903  0.04350979 0.0443619 ]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04631283 0.0407727 ]\n"," [0.04772338 0.04105721]\n"," [0.04593568 0.04106604]\n"," [0.04744877 0.04091845]]\n","\n","Average MAE Loss:\n","[0.04354276 0.0443903  0.04350086 0.04418361]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04612329 0.04085215]\n"," [0.04772338 0.04105721]\n"," [0.04605493 0.04094574]\n"," [0.04725121 0.04120462]]\n","\n","Average MAE Loss:\n","[0.04348772 0.0443903  0.04350033 0.04422792]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04625531 0.04082873]\n"," [0.04772338 0.04105721]\n"," [0.0461305  0.04100283]\n"," [0.04751986 0.0408811 ]]\n","\n","Average MAE Loss:\n","[0.04354202 0.0443903  0.04356666 0.04420048]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04584999 0.04108408]\n"," [0.04772338 0.04105721]\n"," [0.04623434 0.04101572]\n"," [0.04755061 0.04091741]]\n","\n","Average MAE Loss:\n","[0.04346703 0.0443903  0.04362503 0.04423401]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04613055 0.04094551]\n"," [0.04772338 0.04105721]\n"," [0.04638334 0.04095199]\n"," [0.04731967 0.0410339 ]]\n","\n","Average MAE Loss:\n","[0.04353803 0.0443903  0.04366766 0.04417679]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0464659  0.04059408]\n"," [0.04662386 0.04051387]\n"," [0.04657945 0.0405476 ]\n"," [0.04664956 0.04056985]]\n","\n","Average MAE Loss:\n","[0.04352999 0.04356886 0.04356352 0.04360971]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04659361 0.04075369]\n"," [0.04662386 0.04051387]\n"," [0.04655756 0.04060096]\n"," [0.04702346 0.04071243]]\n","\n","Average MAE Loss:\n","[0.04367365 0.04356886 0.04357926 0.04386795]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04656745 0.04088699]\n"," [0.04662386 0.04051387]\n"," [0.04658715 0.0406169 ]\n"," [0.04760414 0.04077711]]\n","\n","Average MAE Loss:\n","[0.04372722 0.04356886 0.04360203 0.04419062]\n","\n","Epoch 00045: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04663661 0.04086895]\n"," [0.04662386 0.04051387]\n"," [0.04657413 0.04069275]\n"," [0.04746997 0.04087827]]\n","\n","Average MAE Loss:\n","[0.04375278 0.04356886 0.04363344 0.04417412]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04637647 0.04086736]\n"," [0.04662386 0.04051387]\n"," [0.04656487 0.04074727]\n"," [0.04746362 0.04094049]]\n","\n","Average MAE Loss:\n","[0.04362192 0.04356886 0.04365607 0.04420206]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04627742 0.04090176]\n"," [0.04662386 0.04051387]\n"," [0.04656884 0.04076567]\n"," [0.04734941 0.04103931]]\n","\n","Average MAE Loss:\n","[0.04358959 0.04356886 0.04366726 0.04419436]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04614325 0.04092102]\n"," [0.04662386 0.04051387]\n"," [0.04656058 0.04079715]\n"," [0.04732516 0.04103402]]\n","\n","Average MAE Loss:\n","[0.04353213 0.04356886 0.04367886 0.04417959]\n","\n","Epoch 00049: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04656576 0.0404937 ]\n"," [0.04732516 0.04103402]\n"," [0.04611815 0.04089369]\n"," [0.04676658 0.040575  ]]\n","\n","Average MAE Loss:\n","[0.04352973 0.04417959 0.04350592 0.04367079]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04642492 0.0405191 ]\n"," [0.04732516 0.04103402]\n"," [0.04610262 0.04087446]\n"," [0.04679687 0.04062793]]\n","\n","Average MAE Loss:\n","[0.04347201 0.04417959 0.04348854 0.0437124 ]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04632839 0.0405267 ]\n"," [0.04732516 0.04103402]\n"," [0.04610599 0.0408549 ]\n"," [0.04685076 0.04062161]]\n","\n","Average MAE Loss:\n","[0.04342754 0.04417959 0.04348044 0.04373618]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04625561 0.04054908]\n"," [0.04732516 0.04103402]\n"," [0.04608116 0.04084889]\n"," [0.04707365 0.04058824]]\n","\n","Average MAE Loss:\n","[0.04340235 0.04417959 0.04346503 0.04383095]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04614596 0.04057089]\n"," [0.04732516 0.04103402]\n"," [0.04606009 0.04084232]\n"," [0.04721702 0.04055704]]\n","\n","Average MAE Loss:\n","[0.04335842 0.04417959 0.0434512  0.04388703]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0461269  0.04057552]\n"," [0.04732516 0.04103402]\n"," [0.04605277 0.04083592]\n"," [0.04725899 0.04058147]]\n","\n","Average MAE Loss:\n","[0.04335121 0.04417959 0.04344435 0.04392023]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.046086   0.04063218]\n"," [0.04732516 0.04103402]\n"," [0.04602414 0.04083852]\n"," [0.04722875 0.0405976 ]]\n","\n","Average MAE Loss:\n","[0.04335909 0.04417959 0.04343133 0.04391317]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04646228 0.04044918]\n"," [0.04647136 0.04048005]\n"," [0.0464505  0.04049278]\n"," [0.0464262  0.04050338]]\n","\n","Average MAE Loss:\n","[0.04345573 0.04347571 0.04347164 0.04346479]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04637753 0.04044984]\n"," [0.04647136 0.04048005]\n"," [0.04644096 0.04050222]\n"," [0.04646182 0.04049791]]\n","\n","Average MAE Loss:\n","[0.04341369 0.04347571 0.04347159 0.04347987]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04631149 0.04047745]\n"," [0.04647136 0.04048005]\n"," [0.04643909 0.04050269]\n"," [0.04650394 0.04051497]]\n","\n","Average MAE Loss:\n","[0.04339447 0.04347571 0.04347089 0.04350946]\n","\n","Epoch 00059: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04634796 0.04048282]\n"," [0.04647136 0.04048005]\n"," [0.04643141 0.04050912]\n"," [0.04649682 0.04054958]]\n","\n","Average MAE Loss:\n","[0.04341539 0.04347571 0.04347026 0.0435232 ]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:39.68986940383911\n","\n","[[0.04634796 0.04048282]\n"," [0.04647136 0.04048005]\n"," [0.04643141 0.04050912]\n"," [0.04649682 0.04054958]]\n","00:00:55.134612597000114\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.0730218  0.09427813]\n"," [0.08241    0.10488542]\n"," [0.06100626 0.08343536]\n"," [0.06754824 0.09074706]]\n","\n","Average MAE Loss:\n","[0.08364996 0.09364771 0.07222081 0.07914765]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07162531 0.09374291]\n"," [0.08241    0.10488542]\n"," [0.05085458 0.07186476]\n"," [0.05135656 0.07207036]]\n","\n","Average MAE Loss:\n","[0.08268411 0.09364771 0.06135967 0.06171346]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07132348 0.0930851 ]\n"," [0.08241    0.10488542]\n"," [0.06153886 0.0842347 ]\n"," [0.05125635 0.07038772]]\n","\n","Average MAE Loss:\n","[0.08220429 0.09364771 0.07288678 0.06082203]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07084286 0.09240121]\n"," [0.08241    0.10488542]\n"," [0.05101027 0.07272241]\n"," [0.05041784 0.07139302]]\n","\n","Average MAE Loss:\n","[0.08162203 0.09364771 0.06186634 0.06090543]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07083213 0.09220071]\n"," [0.08241    0.10488542]\n"," [0.04746146 0.06810886]\n"," [0.04909232 0.06956748]]\n","\n","Average MAE Loss:\n","[0.08151642 0.09364771 0.05778516 0.0593299 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07167067 0.09357994]\n"," [0.08241    0.10488542]\n"," [0.04764234 0.06742841]\n"," [0.05338421 0.07461511]]\n","\n","Average MAE Loss:\n","[0.0826253  0.09364771 0.05753538 0.06399966]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07079654 0.09224741]\n"," [0.08241    0.10488542]\n"," [0.0459341  0.06647848]\n"," [0.0496302  0.07035328]]\n","\n","Average MAE Loss:\n","[0.08152198 0.09364771 0.05620629 0.05999174]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05394465 0.06959221]\n"," [0.0496302  0.07035328]\n"," [0.07189753 0.09369417]\n"," [0.05180048 0.07308491]]\n","\n","Average MAE Loss:\n","[0.06176843 0.05999174 0.08279585 0.0624427 ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05357517 0.06953809]\n"," [0.0496302  0.07035328]\n"," [0.07097198 0.09266157]\n"," [0.04859573 0.06793619]]\n","\n","Average MAE Loss:\n","[0.06155663 0.05999174 0.08181678 0.05826596]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04319866 0.06220048]\n"," [0.0496302  0.07035328]\n"," [0.07057315 0.09163681]\n"," [0.04644454 0.06721806]]\n","\n","Average MAE Loss:\n","[0.05269957 0.05999174 0.08110498 0.0568313 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0424593  0.06157852]\n"," [0.0496302  0.07035328]\n"," [0.07052618 0.09122452]\n"," [0.04708458 0.06717372]]\n","\n","Average MAE Loss:\n","[0.05201891 0.05999174 0.08087535 0.05712915]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04221776 0.06155371]\n"," [0.0496302  0.07035328]\n"," [0.06953231 0.09034437]\n"," [0.04803959 0.06955158]]\n","\n","Average MAE Loss:\n","[0.05188574 0.05999174 0.07993834 0.05879559]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04214102 0.06157511]\n"," [0.0496302  0.07035328]\n"," [0.06814533 0.0894945 ]\n"," [0.04796029 0.06918248]]\n","\n","Average MAE Loss:\n","[0.05185807 0.05999174 0.07881992 0.05857138]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04229709 0.06193799]\n"," [0.0496302  0.07035328]\n"," [0.06701767 0.08816495]\n"," [0.04708098 0.06839782]]\n","\n","Average MAE Loss:\n","[0.05211754 0.05999174 0.07759131 0.0577394 ]\n","\n","Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04239755 0.06240942]\n"," [0.07513803 0.09756675]\n"," [0.04515848 0.06443211]\n"," [0.045653   0.06584512]]\n","\n","Average MAE Loss:\n","[0.05240349 0.08635239 0.05479529 0.05574906]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04206723 0.06065373]\n"," [0.07513803 0.09756675]\n"," [0.04173625 0.06286502]\n"," [0.04225574 0.06163668]]\n","\n","Average MAE Loss:\n","[0.05136048 0.08635239 0.05230064 0.05194621]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0422197  0.06085348]\n"," [0.07513803 0.09756675]\n"," [0.04077062 0.06148697]\n"," [0.04346596 0.06155322]]\n","\n","Average MAE Loss:\n","[0.05153659 0.08635239 0.0511288  0.05250959]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04099433 0.05999105]\n"," [0.07513803 0.09756675]\n"," [0.04096728 0.06164408]\n"," [0.04057051 0.06045324]]\n","\n","Average MAE Loss:\n","[0.05049269 0.08635239 0.05130568 0.05051188]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04257057 0.06073077]\n"," [0.07513803 0.09756675]\n"," [0.04197407 0.06183697]\n"," [0.04061894 0.06050712]]\n","\n","Average MAE Loss:\n","[0.05165067 0.08635239 0.05190552 0.05056303]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04095831 0.06010051]\n"," [0.07513803 0.09756675]\n"," [0.04094299 0.06140958]\n"," [0.04057995 0.06026306]]\n","\n","Average MAE Loss:\n","[0.05052941 0.08635239 0.05117629 0.05042151]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04138346 0.06092466]\n"," [0.07513803 0.09756675]\n"," [0.04118004 0.06114145]\n"," [0.04080182 0.06039235]]\n","\n","Average MAE Loss:\n","[0.05115406 0.08635239 0.05116075 0.05059708]\n","\n","Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04312407 0.06135504]\n"," [0.04080182 0.06039235]\n"," [0.04090394 0.05994819]\n"," [0.0412046  0.06155949]]\n","\n","Average MAE Loss:\n","[0.05223955 0.05059708 0.05042606 0.05138204]\n","\n","Epoch 00022: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04142109 0.06096616]\n"," [0.04080182 0.06039235]\n"," [0.04052667 0.06051069]\n"," [0.04112199 0.06078862]]\n","\n","Average MAE Loss:\n","[0.05119362 0.05059708 0.05051868 0.0509553 ]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.041065   0.05941261]\n"," [0.04080182 0.06039235]\n"," [0.04081191 0.06062845]\n"," [0.04074493 0.06042412]]\n","\n","Average MAE Loss:\n","[0.0502388  0.05059708 0.05072018 0.05058453]\n","\n","Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04052967 0.05923145]\n"," [0.04080182 0.06039235]\n"," [0.04080933 0.0606727 ]\n"," [0.04062354 0.06033712]]\n","\n","Average MAE Loss:\n","[0.04988056 0.05059708 0.05074102 0.05048033]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04019884 0.05934433]\n"," [0.04080182 0.06039235]\n"," [0.04101358 0.0608005 ]\n"," [0.04069443 0.06046394]]\n","\n","Average MAE Loss:\n","[0.04977158 0.05059708 0.05090704 0.05057919]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04034488 0.05908248]\n"," [0.04080182 0.06039235]\n"," [0.04096651 0.0608117 ]\n"," [0.04083416 0.06035342]]\n","\n","Average MAE Loss:\n","[0.04971368 0.05059708 0.05088911 0.05059379]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04092555 0.05921272]\n"," [0.04080182 0.06039235]\n"," [0.04089106 0.06083306]\n"," [0.04090155 0.06042744]]\n","\n","Average MAE Loss:\n","[0.05006913 0.05059708 0.05086206 0.05066449]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04022633 0.05853928]\n"," [0.04070869 0.06098919]\n"," [0.03942572 0.05929976]\n"," [0.03977035 0.05969625]]\n","\n","Average MAE Loss:\n","[0.0493828  0.05084894 0.04936274 0.0497333 ]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03978786 0.0586623 ]\n"," [0.04070869 0.06098919]\n"," [0.03904342 0.05884324]\n"," [0.03942198 0.05900663]]\n","\n","Average MAE Loss:\n","[0.04922508 0.05084894 0.04894333 0.0492143 ]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.039584   0.05818857]\n"," [0.04070869 0.06098919]\n"," [0.03902648 0.05891762]\n"," [0.0394708  0.05890483]]\n","\n","Average MAE Loss:\n","[0.04888628 0.05084894 0.04897205 0.04918781]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.0395644  0.05815535]\n"," [0.04070869 0.06098919]\n"," [0.03891635 0.05904771]\n"," [0.03964275 0.05894362]]\n","\n","Average MAE Loss:\n","[0.04885988 0.05084894 0.04898203 0.04929318]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03960413 0.05815829]\n"," [0.04070869 0.06098919]\n"," [0.03909093 0.05917185]\n"," [0.03962424 0.05894045]]\n","\n","Average MAE Loss:\n","[0.04888121 0.05084894 0.04913139 0.04928235]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03974218 0.05827074]\n"," [0.04070869 0.06098919]\n"," [0.03942179 0.05927293]\n"," [0.03959124 0.05898079]]\n","\n","Average MAE Loss:\n","[0.04900646 0.05084894 0.04934736 0.04928601]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0399296  0.05836602]\n"," [0.04070869 0.06098919]\n"," [0.03940815 0.05926544]\n"," [0.03956638 0.05901651]]\n","\n","Average MAE Loss:\n","[0.04914781 0.05084894 0.0493368  0.04929144]\n","\n","Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03971767 0.0581726 ]\n"," [0.03956638 0.05901651]\n"," [0.03971786 0.05835294]\n"," [0.03938798 0.0591718 ]]\n","\n","Average MAE Loss:\n","[0.04894514 0.04929144 0.0490354  0.04927989]\n","\n","Epoch 00036: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03988099 0.05826651]\n"," [0.03956638 0.05901651]\n"," [0.03956339 0.05839409]\n"," [0.03930911 0.05903839]]\n","\n","Average MAE Loss:\n","[0.04907375 0.04929144 0.04897874 0.04917375]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03959811 0.05829922]\n"," [0.03956638 0.05901651]\n"," [0.03942322 0.05846118]\n"," [0.03928063 0.05893596]]\n","\n","Average MAE Loss:\n","[0.04894867 0.04929144 0.0489422  0.04910829]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.0398302  0.05828484]\n"," [0.03956638 0.05901651]\n"," [0.03936819 0.05850185]\n"," [0.03933441 0.05892012]]\n","\n","Average MAE Loss:\n","[0.04905752 0.04929144 0.04893502 0.04912727]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03977037 0.05826774]\n"," [0.03956638 0.05901651]\n"," [0.0393483  0.05855831]\n"," [0.03933959 0.05894344]]\n","\n","Average MAE Loss:\n","[0.04901906 0.04929144 0.0489533  0.04914151]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03965225 0.05823883]\n"," [0.03956638 0.05901651]\n"," [0.039354   0.05861123]\n"," [0.03934336 0.05894031]]\n","\n","Average MAE Loss:\n","[0.04894554 0.04929144 0.04898262 0.04914184]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03971737 0.05819437]\n"," [0.03956638 0.05901651]\n"," [0.03940073 0.0586765 ]\n"," [0.03933891 0.05894622]]\n","\n","Average MAE Loss:\n","[0.04895587 0.04929144 0.04903862 0.04914257]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03917517 0.0582381 ]\n"," [0.03918587 0.05844375]\n"," [0.03916389 0.05847526]\n"," [0.03920444 0.05845779]]\n","\n","Average MAE Loss:\n","[0.04870664 0.04881481 0.04881958 0.04883112]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03931066 0.05813387]\n"," [0.03918587 0.05844375]\n"," [0.03913444 0.0585027 ]\n"," [0.03924721 0.0584898 ]]\n","\n","Average MAE Loss:\n","[0.04872227 0.04881481 0.04881857 0.0488685 ]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03934362 0.05809133]\n"," [0.03918587 0.05844375]\n"," [0.03910344 0.05855293]\n"," [0.03928767 0.05852141]]\n","\n","Average MAE Loss:\n","[0.04871747 0.04881481 0.04882818 0.04890454]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03936101 0.05811244]\n"," [0.03918587 0.05844375]\n"," [0.03906246 0.05860075]\n"," [0.03934414 0.05856999]]\n","\n","Average MAE Loss:\n","[0.04873673 0.04881481 0.04883161 0.04895706]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03957111 0.05813913]\n"," [0.03918587 0.05844375]\n"," [0.03903604 0.05865368]\n"," [0.03936175 0.05860417]]\n","\n","Average MAE Loss:\n","[0.04885512 0.04881481 0.04884486 0.04898296]\n","\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03969009 0.05816955]\n"," [0.03918587 0.05844375]\n"," [0.03902503 0.05867151]\n"," [0.03937917 0.05861853]]\n","\n","Average MAE Loss:\n","[0.04892982 0.04881481 0.04884827 0.04899885]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03971923 0.05819141]\n"," [0.03918587 0.05844375]\n"," [0.03902174 0.05869644]\n"," [0.03940288 0.05863963]]\n","\n","Average MAE Loss:\n","[0.04895532 0.04881481 0.04885909 0.04902125]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03917    0.0583741 ]\n"," [0.03940288 0.05863963]\n"," [0.039694   0.05820207]\n"," [0.03901824 0.05869082]]\n","\n","Average MAE Loss:\n","[0.04877205 0.04902125 0.04894804 0.04885453]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03921523 0.05830867]\n"," [0.03940288 0.05863963]\n"," [0.03964863 0.05821089]\n"," [0.03902468 0.05868436]]\n","\n","Average MAE Loss:\n","[0.04876195 0.04902125 0.04892976 0.04885452]\n","\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03925066 0.05828991]\n"," [0.03940288 0.05863963]\n"," [0.03961048 0.05821398]\n"," [0.03903012 0.05868122]]\n","\n","Average MAE Loss:\n","[0.04877028 0.04902125 0.04891223 0.04885567]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03928666 0.05826419]\n"," [0.03940288 0.05863963]\n"," [0.0395698  0.05821786]\n"," [0.0390358  0.05867777]]\n","\n","Average MAE Loss:\n","[0.04877542 0.04902125 0.04889383 0.04885679]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0393341  0.05826431]\n"," [0.03940288 0.05863963]\n"," [0.03954583 0.05822124]\n"," [0.03903659 0.05867499]]\n","\n","Average MAE Loss:\n","[0.0487992  0.04902125 0.04888354 0.04885579]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03942665 0.05826647]\n"," [0.03940288 0.05863963]\n"," [0.03952461 0.05822785]\n"," [0.03904631 0.05867187]]\n","\n","Average MAE Loss:\n","[0.04884656 0.04902125 0.04887623 0.04885909]\n","\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03944033 0.05826544]\n"," [0.03940288 0.05863963]\n"," [0.03951107 0.05822948]\n"," [0.03905059 0.05867019]]\n","\n","Average MAE Loss:\n","[0.04885289 0.04902125 0.04887027 0.04886039]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03926491 0.0583417 ]\n"," [0.03924614 0.0583506 ]\n"," [0.03923689 0.05835671]\n"," [0.03924833 0.05835344]]\n","\n","Average MAE Loss:\n","[0.04880331 0.04879837 0.0487968  0.04880089]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03927419 0.05832236]\n"," [0.03924614 0.0583506 ]\n"," [0.03923081 0.05836135]\n"," [0.03924805 0.05835642]]\n","\n","Average MAE Loss:\n","[0.04879828 0.04879837 0.04879608 0.04880224]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03926127 0.05830129]\n"," [0.03924614 0.0583506 ]\n"," [0.03922437 0.05836465]\n"," [0.0392514  0.05835991]]\n","\n","Average MAE Loss:\n","[0.04878128 0.04879837 0.04879451 0.04880565]\n","\n","Epoch 00059: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.0392573  0.05829091]\n"," [0.03924614 0.0583506 ]\n"," [0.03921574 0.05836802]\n"," [0.03925657 0.05836334]]\n","\n","Average MAE Loss:\n","[0.04877411 0.04879837 0.04879188 0.04880995]\n","\n","\n","epochs finished with time:41.31748700141907\n","\n","[[0.0392573  0.05829091]\n"," [0.03924614 0.0583506 ]\n"," [0.03921574 0.05836802]\n"," [0.03925657 0.05836334]]\n","00:00:56.873425347999955\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08914629 0.09305587]\n"," [0.10227572 0.10665325]\n"," [0.08885211 0.09008958]\n"," [0.0950296  0.09654912]]\n","\n","Average MAE Loss:\n","[0.09110108 0.10446449 0.08947085 0.09578936]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08750453 0.09145403]\n"," [0.10227572 0.10665325]\n"," [0.09103232 0.09385743]\n"," [0.09391421 0.09696159]]\n","\n","Average MAE Loss:\n","[0.08947928 0.10446449 0.09244487 0.0954379 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08700891 0.09078287]\n"," [0.10227572 0.10665325]\n"," [0.06922374 0.07354248]\n"," [0.09065993 0.09394596]]\n","\n","Average MAE Loss:\n","[0.08889589 0.10446449 0.07138311 0.09230294]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08782569 0.09127279]\n"," [0.10227572 0.10665325]\n"," [0.0808344  0.0850439 ]\n"," [0.06478146 0.066129  ]]\n","\n","Average MAE Loss:\n","[0.08954924 0.10446449 0.08293915 0.06545523]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08760498 0.09160049]\n"," [0.10227572 0.10665325]\n"," [0.06295933 0.0627016 ]\n"," [0.06859524 0.0717482 ]]\n","\n","Average MAE Loss:\n","[0.08960273 0.10446449 0.06283046 0.07017172]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08758349 0.09151269]\n"," [0.10227572 0.10665325]\n"," [0.06295669 0.06636869]\n"," [0.0603412  0.06249051]]\n","\n","Average MAE Loss:\n","[0.08954809 0.10446449 0.06466269 0.06141585]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08628779 0.08938123]\n"," [0.10227572 0.10665325]\n"," [0.0636971  0.06755647]\n"," [0.0653519  0.06671276]]\n","\n","Average MAE Loss:\n","[0.08783451 0.10446449 0.06562679 0.06603233]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05484353 0.0547857 ]\n"," [0.0653519  0.06671276]\n"," [0.09032749 0.09396558]\n"," [0.06284684 0.06567892]]\n","\n","Average MAE Loss:\n","[0.05481462 0.06603233 0.09214653 0.06426288]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05185515 0.05456811]\n"," [0.0653519  0.06671276]\n"," [0.08824425 0.09243491]\n"," [0.06487343 0.06766303]]\n","\n","Average MAE Loss:\n","[0.05321163 0.06603233 0.09033958 0.06626823]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04810026 0.05063539]\n"," [0.0653519  0.06671276]\n"," [0.08663792 0.09023583]\n"," [0.0637916  0.06688419]]\n","\n","Average MAE Loss:\n","[0.04936783 0.06603233 0.08843688 0.06533789]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05258047 0.05627247]\n"," [0.0653519  0.06671276]\n"," [0.08659123 0.08981516]\n"," [0.05842365 0.06073263]]\n","\n","Average MAE Loss:\n","[0.05442647 0.06603233 0.0882032  0.05957814]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05050885 0.05248363]\n"," [0.0653519  0.06671276]\n"," [0.08623764 0.08960631]\n"," [0.05733164 0.05920186]]\n","\n","Average MAE Loss:\n","[0.05149624 0.06603233 0.08792197 0.05826675]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04793818 0.04966577]\n"," [0.0653519  0.06671276]\n"," [0.08568823 0.08939648]\n"," [0.05624727 0.05684115]]\n","\n","Average MAE Loss:\n","[0.04880198 0.06603233 0.08754236 0.05654421]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04845867 0.05116434]\n"," [0.0653519  0.06671276]\n"," [0.08468062 0.08796448]\n"," [0.05752495 0.05963724]]\n","\n","Average MAE Loss:\n","[0.04981151 0.06603233 0.08632255 0.0585811 ]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05390263 0.05774294]\n"," [0.09547956 0.09987726]\n"," [0.07615958 0.08030571]\n"," [0.06424543 0.067715  ]]\n","\n","Average MAE Loss:\n","[0.05582279 0.09767841 0.07823265 0.06598021]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04803623 0.05010823]\n"," [0.09547956 0.09987726]\n"," [0.04970746 0.05074392]\n"," [0.05202058 0.05393375]]\n","\n","Average MAE Loss:\n","[0.04907223 0.09767841 0.05022569 0.05297717]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05024824 0.05298887]\n"," [0.09547956 0.09987726]\n"," [0.05003746 0.05137342]\n"," [0.04968847 0.05042684]]\n","\n","Average MAE Loss:\n","[0.05161856 0.09767841 0.05070544 0.05005766]\n","\n","Epoch 00017: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05028022 0.05274573]\n"," [0.09547956 0.09987726]\n"," [0.05057338 0.05216493]\n"," [0.05369458 0.05576527]]\n","\n","Average MAE Loss:\n","[0.05151298 0.09767841 0.05136916 0.05472993]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04954143 0.05175391]\n"," [0.09547956 0.09987726]\n"," [0.05043911 0.05254584]\n"," [0.05086268 0.05257781]]\n","\n","Average MAE Loss:\n","[0.05064767 0.09767841 0.05149247 0.05172024]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04884031 0.05074444]\n"," [0.09547956 0.09987726]\n"," [0.04778437 0.04902682]\n"," [0.0502934  0.05210007]]\n","\n","Average MAE Loss:\n","[0.04979238 0.09767841 0.04840559 0.05119673]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04787211 0.04944419]\n"," [0.09547956 0.09987726]\n"," [0.04902057 0.05085175]\n"," [0.0508353  0.05290481]]\n","\n","Average MAE Loss:\n","[0.04865815 0.09767841 0.04993616 0.05187006]\n","\n","Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.08670833 0.09116051]\n"," [0.0508353  0.05290481]\n"," [0.04748428 0.04890452]\n"," [0.04949842 0.05124468]]\n","\n","Average MAE Loss:\n","[0.08893442 0.05187006 0.0481944  0.05037155]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.05219651 0.05555037]\n"," [0.0508353  0.05290481]\n"," [0.0475552  0.04845034]\n"," [0.05205873 0.05420716]]\n","\n","Average MAE Loss:\n","[0.05387344 0.05187006 0.04800277 0.05313294]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04813024 0.04871182]\n"," [0.0508353  0.05290481]\n"," [0.04836643 0.04977844]\n"," [0.05127269 0.05313478]]\n","\n","Average MAE Loss:\n","[0.04842103 0.05187006 0.04907243 0.05220373]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05200614 0.05471329]\n"," [0.0508353  0.05290481]\n"," [0.04859361 0.04986573]\n"," [0.05023056 0.05147965]]\n","\n","Average MAE Loss:\n","[0.05335972 0.05187006 0.04922967 0.0508551 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04741581 0.04832963]\n"," [0.0508353  0.05290481]\n"," [0.04799631 0.04902571]\n"," [0.04955336 0.05040696]]\n","\n","Average MAE Loss:\n","[0.04787272 0.05187006 0.04851101 0.04998016]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04924037 0.05121943]\n"," [0.0508353  0.05290481]\n"," [0.04763539 0.04868364]\n"," [0.04990103 0.05103004]]\n","\n","Average MAE Loss:\n","[0.0502299  0.05187006 0.04815952 0.05046553]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04682842 0.04820356]\n"," [0.0508353  0.05290481]\n"," [0.047947   0.04919165]\n"," [0.04983166 0.05106269]]\n","\n","Average MAE Loss:\n","[0.04751599 0.05187006 0.04856933 0.05044717]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04616666 0.04747666]\n"," [0.05707732 0.06034887]\n"," [0.05168166 0.05420005]\n"," [0.05242422 0.05504003]]\n","\n","Average MAE Loss:\n","[0.04682166 0.0587131  0.05294086 0.05373213]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04710059 0.04890837]\n"," [0.05707732 0.06034887]\n"," [0.0472245  0.04835475]\n"," [0.04863354 0.0501393 ]]\n","\n","Average MAE Loss:\n","[0.04800448 0.0587131  0.04778963 0.04938642]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04677646 0.04838434]\n"," [0.05707732 0.06034887]\n"," [0.04643557 0.04722401]\n"," [0.04785806 0.04873228]]\n","\n","Average MAE Loss:\n","[0.0475804  0.0587131  0.04682979 0.04829517]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04641375 0.04781966]\n"," [0.05707732 0.06034887]\n"," [0.04677828 0.04791883]\n"," [0.04815464 0.04891945]]\n","\n","Average MAE Loss:\n","[0.0471167  0.0587131  0.04734856 0.04853705]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.046496   0.04743054]\n"," [0.05707732 0.06034887]\n"," [0.04717882 0.04850671]\n"," [0.04804192 0.04873984]]\n","\n","Average MAE Loss:\n","[0.04696327 0.0587131  0.04784276 0.04839088]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.0458801  0.04626639]\n"," [0.05707732 0.06034887]\n"," [0.04702884 0.04852999]\n"," [0.04772096 0.04815939]]\n","\n","Average MAE Loss:\n","[0.04607325 0.0587131  0.04777941 0.04794018]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0470537  0.0482731 ]\n"," [0.05707732 0.06034887]\n"," [0.04697435 0.04831781]\n"," [0.0477377  0.04812239]]\n","\n","Average MAE Loss:\n","[0.0476634  0.0587131  0.04764608 0.04793004]\n","\n","Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04941397 0.05161087]\n"," [0.0477377  0.04812239]\n"," [0.04673271 0.04779466]\n"," [0.04758447 0.04915449]]\n","\n","Average MAE Loss:\n","[0.05051242 0.04793004 0.04726368 0.04836948]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04578786 0.046315  ]\n"," [0.0477377  0.04812239]\n"," [0.04660787 0.04759365]\n"," [0.04828582 0.04985525]]\n","\n","Average MAE Loss:\n","[0.04605143 0.04793004 0.04710076 0.04907053]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04743299 0.04891966]\n"," [0.0477377  0.04812239]\n"," [0.04624333 0.04704861]\n"," [0.04808107 0.04932231]]\n","\n","Average MAE Loss:\n","[0.04817633 0.04793004 0.04664597 0.04870169]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04762988 0.04925537]\n"," [0.0477377  0.04812239]\n"," [0.04643194 0.04741019]\n"," [0.04805492 0.04920797]]\n","\n","Average MAE Loss:\n","[0.04844263 0.04793004 0.04692107 0.04863144]\n","\n","Epoch 00039: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04682731 0.04802196]\n"," [0.0477377  0.04812239]\n"," [0.04713248 0.04848805]\n"," [0.04786289 0.04898764]]\n","\n","Average MAE Loss:\n","[0.04742463 0.04793004 0.04781027 0.04842526]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04589705 0.04654398]\n"," [0.0477377  0.04812239]\n"," [0.04692144 0.04818214]\n"," [0.04792025 0.04910192]]\n","\n","Average MAE Loss:\n","[0.04622051 0.04793004 0.04755179 0.04851108]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04624179 0.04719214]\n"," [0.0477377  0.04812239]\n"," [0.04652402 0.0475556 ]\n"," [0.04802079 0.04921223]]\n","\n","Average MAE Loss:\n","[0.04671696 0.04793004 0.04703981 0.04861651]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0470522  0.04825576]\n"," [0.04704875 0.0480974 ]\n"," [0.0468742  0.04789357]\n"," [0.0472631  0.04838914]]\n","\n","Average MAE Loss:\n","[0.04765398 0.04757307 0.04738389 0.04782612]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04676255 0.04803586]\n"," [0.04704875 0.0480974 ]\n"," [0.04669448 0.0477453 ]\n"," [0.04728657 0.0483639 ]]\n","\n","Average MAE Loss:\n","[0.0473992  0.04757307 0.04721989 0.04782523]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04657837 0.04782373]\n"," [0.04704875 0.0480974 ]\n"," [0.04669618 0.04783926]\n"," [0.04743041 0.04850473]]\n","\n","Average MAE Loss:\n","[0.04720105 0.04757307 0.04726772 0.04796757]\n","\n","Epoch 00045: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04671386 0.04800223]\n"," [0.04704875 0.0480974 ]\n"," [0.04667498 0.04784752]\n"," [0.04768048 0.04875698]]\n","\n","Average MAE Loss:\n","[0.04735804 0.04757307 0.04726125 0.04821873]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04655088 0.04776023]\n"," [0.04704875 0.0480974 ]\n"," [0.04673145 0.04795809]\n"," [0.04784982 0.04888701]]\n","\n","Average MAE Loss:\n","[0.04715556 0.04757307 0.04734477 0.04836842]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04635333 0.04744241]\n"," [0.04704875 0.0480974 ]\n"," [0.04670292 0.04794904]\n"," [0.0477658  0.0487283 ]]\n","\n","Average MAE Loss:\n","[0.04689787 0.04757307 0.04732598 0.04824705]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04625812 0.04725567]\n"," [0.04704875 0.0480974 ]\n"," [0.04670983 0.04798413]\n"," [0.04767736 0.04855705]]\n","\n","Average MAE Loss:\n","[0.0467569  0.04757307 0.04734698 0.0481172 ]\n","\n","Epoch 00049: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04682855 0.0477867 ]\n"," [0.04767736 0.04855705]\n"," [0.0462173  0.04720202]\n"," [0.04665971 0.04785892]]\n","\n","Average MAE Loss:\n","[0.04730763 0.0481172  0.04670966 0.04725932]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04674878 0.04771624]\n"," [0.04767736 0.04855705]\n"," [0.04618967 0.04716073]\n"," [0.04669623 0.04783735]]\n","\n","Average MAE Loss:\n","[0.04723251 0.0481172  0.0466752  0.04726679]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04671161 0.04772539]\n"," [0.04767736 0.04855705]\n"," [0.04617058 0.0471356 ]\n"," [0.04672916 0.04781983]]\n","\n","Average MAE Loss:\n","[0.0472185  0.0481172  0.04665309 0.0472745 ]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0466916  0.04775892]\n"," [0.04767736 0.04855705]\n"," [0.0461673  0.04713115]\n"," [0.04679431 0.0478791 ]]\n","\n","Average MAE Loss:\n","[0.04722526 0.0481172  0.04664922 0.04733671]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04666207 0.04773322]\n"," [0.04767736 0.04855705]\n"," [0.04612784 0.04708179]\n"," [0.04683883 0.04786495]]\n","\n","Average MAE Loss:\n","[0.04719765 0.0481172  0.04660482 0.04735189]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04661175 0.04765949]\n"," [0.04767736 0.04855705]\n"," [0.04612374 0.04707713]\n"," [0.04686188 0.04787867]]\n","\n","Average MAE Loss:\n","[0.04713562 0.0481172  0.04660044 0.04737028]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04657843 0.04762701]\n"," [0.04767736 0.04855705]\n"," [0.04611947 0.04707166]\n"," [0.04688023 0.04787908]]\n","\n","Average MAE Loss:\n","[0.04710272 0.0481172  0.04659557 0.04737966]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04663152 0.04758625]\n"," [0.0467033  0.04767349]\n"," [0.04668298 0.04764992]\n"," [0.04668681 0.04761064]]\n","\n","Average MAE Loss:\n","[0.04710888 0.04718839 0.04716645 0.04714873]\n","\n","Epoch 00057: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04666273 0.04764876]\n"," [0.0467033  0.04767349]\n"," [0.04662839 0.04757752]\n"," [0.04674427 0.04765886]]\n","\n","Average MAE Loss:\n","[0.04715575 0.04718839 0.04710295 0.04720157]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04668495 0.04769246]\n"," [0.0467033  0.04767349]\n"," [0.04661226 0.04757417]\n"," [0.04678123 0.0476675 ]]\n","\n","Average MAE Loss:\n","[0.0471887  0.04718839 0.04709321 0.04722436]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04670063 0.04772612]\n"," [0.0467033  0.04767349]\n"," [0.04658981 0.04755513]\n"," [0.04688035 0.04780221]]\n","\n","Average MAE Loss:\n","[0.04721338 0.04718839 0.04707247 0.04734128]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:40.81206130981445\n","\n","[[0.04670063 0.04772612]\n"," [0.0467033  0.04767349]\n"," [0.04658981 0.04755513]\n"," [0.04688035 0.04780221]]\n","00:00:56.41718245699985\n"]}],"source":["# seed = 10 table = 4/8 fixed folds CNN_2 dropout\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='CNN2_dropout_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221908,"status":"ok","timestamp":1690366757633,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"YvuS6aJdrmGZ","outputId":"33a0dd13-a98a-4e7b-b59d-4ab158a1d974"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.25\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.07732824 0.07402257]\n"," [0.05657713 0.05247714]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.0756754  0.05452714]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05827196 0.05617904]\n"," [0.06396585 0.06359772]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.0572255  0.06378178]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.0561692  0.05395243]\n"," [0.05768136 0.05621753]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.05506081 0.05694945]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05027616 0.04800574]\n"," [0.06010064 0.05850105]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.04914095 0.05930085]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.04951288 0.04693143]\n"," [0.05088991 0.04937106]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.04822215 0.05013048]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05054837 0.04782192]\n"," [0.05098377 0.04933642]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.04918515 0.0501601 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05351669 0.05138484]\n"," [0.05155104 0.05048664]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.05245076 0.05101884]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05155104 0.05048664]\n"," [0.13993831 0.16269287]\n"," [0.0539866  0.05141538]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05101884 0.15131559 0.05270099]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05155104 0.05048664]\n"," [0.06012923 0.057297  ]\n"," [0.05003563 0.04897309]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05101884 0.05871311 0.04950436]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05155104 0.05048664]\n"," [0.0592682  0.05633548]\n"," [0.04783556 0.04541999]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05101884 0.05780184 0.04662778]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05155104 0.05048664]\n"," [0.05145833 0.04917963]\n"," [0.04934295 0.04707712]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05101884 0.05031898 0.04821004]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05155104 0.05048664]\n"," [0.04859202 0.04580808]\n"," [0.05124354 0.0495235 ]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05101884 0.04720005 0.05038352]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05155104 0.05048664]\n"," [0.04763276 0.04536662]\n"," [0.04805639 0.04667556]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05101884 0.04649969 0.04736598]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05155104 0.05048664]\n"," [0.0473467  0.04513082]\n"," [0.04752639 0.04578907]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05101884 0.04623876 0.04665773]\n","\n","Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.07815808 0.07498668]\n"," [0.04378263 0.04181244]\n"," [0.044879   0.04294468]]\n","\n","Average MAE Loss:\n","[0.07657238 0.07657238 0.04279754 0.04391184]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.07815808 0.07498668]\n"," [0.04448568 0.04257974]\n"," [0.04340124 0.04206499]]\n","\n","Average MAE Loss:\n","[0.07657238 0.07657238 0.04353271 0.04273311]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.07815808 0.07498668]\n"," [0.0429421  0.0411844 ]\n"," [0.04157196 0.04013426]]\n","\n","Average MAE Loss:\n","[0.07657238 0.07657238 0.04206325 0.04085311]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.07815808 0.07498668]\n"," [0.04297006 0.04080627]\n"," [0.04070678 0.03924843]]\n","\n","Average MAE Loss:\n","[0.07657238 0.07657238 0.04188817 0.0399776 ]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.07815808 0.07498668]\n"," [0.0446509  0.0434069 ]\n"," [0.04057681 0.03939874]]\n","\n","Average MAE Loss:\n","[0.07657238 0.07657238 0.0440289  0.03998777]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.07815808 0.07498668]\n"," [0.04183708 0.04009597]\n"," [0.04069903 0.03947006]]\n","\n","Average MAE Loss:\n","[0.07657238 0.07657238 0.04096653 0.04008455]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.07815808 0.07498668]\n"," [0.04187375 0.04015177]\n"," [0.04076006 0.03962045]]\n","\n","Average MAE Loss:\n","[0.07657238 0.07657238 0.04101276 0.04019026]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.04076006 0.03962045]\n"," [0.05203621 0.04996562]\n"," [0.04126773 0.0400591 ]]\n","\n","Average MAE Loss:\n","[0.07657238 0.04019026 0.05100092 0.04066342]\n","\n","Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.04076006 0.03962045]\n"," [0.04477877 0.04300427]\n"," [0.04059227 0.03935708]]\n","\n","Average MAE Loss:\n","[0.07657238 0.04019026 0.04389152 0.03997468]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.04076006 0.03962045]\n"," [0.04264573 0.04106413]\n"," [0.04063486 0.0393865 ]]\n","\n","Average MAE Loss:\n","[0.07657238 0.04019026 0.04185493 0.04001068]\n","\n","Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.04076006 0.03962045]\n"," [0.04216051 0.04055049]\n"," [0.04036982 0.03912815]]\n","\n","Average MAE Loss:\n","[0.07657238 0.04019026 0.0413555  0.03974898]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.04076006 0.03962045]\n"," [0.04194541 0.04016377]\n"," [0.04022674 0.03894599]]\n","\n","Average MAE Loss:\n","[0.07657238 0.04019026 0.04105459 0.03958637]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.04076006 0.03962045]\n"," [0.04185268 0.04004002]\n"," [0.04036022 0.03900673]]\n","\n","Average MAE Loss:\n","[0.07657238 0.04019026 0.04094635 0.03968347]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07815808 0.07498668]\n"," [0.04076006 0.03962045]\n"," [0.04200384 0.04035006]\n"," [0.04024946 0.03898592]]\n","\n","Average MAE Loss:\n","[0.07657238 0.04019026 0.04117695 0.03961769]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.04948868 0.04737023]\n"," [0.04019294 0.03827061]\n"," [0.03999624 0.03825189]]\n","\n","Average MAE Loss:\n","[0.04842946 0.04842946 0.03923177 0.03912407]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.04948868 0.04737023]\n"," [0.04137255 0.03966146]\n"," [0.04021696 0.03883568]]\n","\n","Average MAE Loss:\n","[0.04842946 0.04842946 0.04051701 0.03952632]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.04948868 0.04737023]\n"," [0.04101985 0.0392225 ]\n"," [0.039924   0.03868252]]\n","\n","Average MAE Loss:\n","[0.04842946 0.04842946 0.04012118 0.03930326]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.04948868 0.04737023]\n"," [0.04080918 0.03919314]\n"," [0.03980936 0.03871939]]\n","\n","Average MAE Loss:\n","[0.04842946 0.04842946 0.04000116 0.03926437]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.04948868 0.04737023]\n"," [0.04043254 0.03903265]\n"," [0.03976143 0.03843312]]\n","\n","Average MAE Loss:\n","[0.04842946 0.04842946 0.03973259 0.03909728]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.04948868 0.04737023]\n"," [0.04033383 0.0389065 ]\n"," [0.03966685 0.03838886]]\n","\n","Average MAE Loss:\n","[0.04842946 0.04842946 0.03962016 0.03902785]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.04948868 0.04737023]\n"," [0.04057809 0.0391486 ]\n"," [0.03981342 0.038611  ]]\n","\n","Average MAE Loss:\n","[0.04842946 0.04842946 0.03986335 0.03921221]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.03981342 0.038611  ]\n"," [0.04270445 0.0406837 ]\n"," [0.03941724 0.03808751]]\n","\n","Average MAE Loss:\n","[0.04842946 0.03921221 0.04169407 0.03875237]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.03981342 0.038611  ]\n"," [0.03998496 0.03846086]\n"," [0.03968942 0.03857017]]\n","\n","Average MAE Loss:\n","[0.04842946 0.03921221 0.03922291 0.0391298 ]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.03981342 0.038611  ]\n"," [0.04033481 0.03878067]\n"," [0.03982209 0.03857242]]\n","\n","Average MAE Loss:\n","[0.04842946 0.03921221 0.03955774 0.03919726]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.03981342 0.038611  ]\n"," [0.0408152  0.03933645]\n"," [0.0398577  0.03836861]]\n","\n","Average MAE Loss:\n","[0.04842946 0.03921221 0.04007583 0.03911316]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.03981342 0.038611  ]\n"," [0.04040745 0.03880275]\n"," [0.03964003 0.0383491 ]]\n","\n","Average MAE Loss:\n","[0.04842946 0.03921221 0.0396051  0.03899457]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.03981342 0.038611  ]\n"," [0.04046195 0.03891799]\n"," [0.0396302  0.03832783]]\n","\n","Average MAE Loss:\n","[0.04842946 0.03921221 0.03968997 0.03897902]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04948868 0.04737023]\n"," [0.03981342 0.038611  ]\n"," [0.04046834 0.03897946]\n"," [0.03972394 0.03835794]]\n","\n","Average MAE Loss:\n","[0.04842946 0.03921221 0.0397239  0.03904094]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.04030917 0.03855391]\n"," [0.03975985 0.03804718]\n"," [0.03946792 0.03795272]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03943154 0.03890351 0.03871032]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.04030917 0.03855391]\n"," [0.0395294  0.03798003]\n"," [0.0395084  0.03814941]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03943154 0.03875472 0.0388289 ]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.04030917 0.03855391]\n"," [0.03985346 0.03833674]\n"," [0.0395944  0.03833897]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03943154 0.0390951  0.03896669]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.04030917 0.03855391]\n"," [0.04005035 0.03851933]\n"," [0.03978554 0.0385634 ]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03943154 0.03928484 0.03917447]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.04030917 0.03855391]\n"," [0.04016299 0.03858651]\n"," [0.03954453 0.03832195]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03943154 0.03937475 0.03893324]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.04030917 0.03855391]\n"," [0.04021779 0.03862326]\n"," [0.03952428 0.03828815]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03943154 0.03942052 0.03890622]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.04030917 0.03855391]\n"," [0.04021696 0.03860582]\n"," [0.03951899 0.03826227]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03943154 0.03941139 0.03889063]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.03951899 0.03826227]\n"," [0.03999901 0.03828101]\n"," [0.03988043 0.0383184 ]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03889063 0.03914001 0.03909942]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.03951899 0.03826227]\n"," [0.03966804 0.03801209]\n"," [0.03951752 0.03801581]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03889063 0.03884007 0.03876666]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.03951899 0.03826227]\n"," [0.0395575  0.0379639 ]\n"," [0.03941541 0.03797277]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03889063 0.0387607  0.03869409]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.03951899 0.03826227]\n"," [0.03954777 0.03799113]\n"," [0.03937576 0.03799809]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03889063 0.03876945 0.03868692]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.03951899 0.03826227]\n"," [0.03955157 0.03802447]\n"," [0.0393789  0.03803202]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03889063 0.03878802 0.03870546]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.03951899 0.03826227]\n"," [0.03957385 0.03805805]\n"," [0.03937581 0.03806151]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03889063 0.03881595 0.03871866]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04030917 0.03855391]\n"," [0.03951899 0.03826227]\n"," [0.03961509 0.03810867]\n"," [0.03937692 0.03807192]]\n","\n","Average MAE Loss:\n","[0.03943154 0.03889063 0.03886188 0.03872442]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03941578 0.03792533]\n"," [0.03941578 0.03792533]\n"," [0.03939888 0.03791405]\n"," [0.03938166 0.03794703]]\n","\n","Average MAE Loss:\n","[0.03867055 0.03867055 0.03865647 0.03866435]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03941578 0.03792533]\n"," [0.03941578 0.03792533]\n"," [0.03938107 0.03791641]\n"," [0.03938968 0.03800174]]\n","\n","Average MAE Loss:\n","[0.03867055 0.03867055 0.03864874 0.03869571]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03941578 0.03792533]\n"," [0.03941578 0.03792533]\n"," [0.03937146 0.03792312]\n"," [0.03939452 0.0380466 ]]\n","\n","Average MAE Loss:\n","[0.03867055 0.03867055 0.03864729 0.03872056]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03941578 0.03792533]\n"," [0.03941578 0.03792533]\n"," [0.03936855 0.03793487]\n"," [0.03939853 0.03807575]]\n","\n","Average MAE Loss:\n","[0.03867055 0.03867055 0.03865171 0.03873714]\n","\n","\n","epochs finished with time:27.290541648864746\n","\n","[[0.03941578 0.03792533]\n"," [0.03941578 0.03792533]\n"," [0.03936855 0.03793487]\n"," [0.03939853 0.03807575]]\n","00:00:45.16250255\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.11082814 0.10938141]\n"," [0.08550396 0.08378996]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.11010478 0.08464696]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.10429312 0.1033813 ]\n"," [0.08103558 0.07953643]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.10383721 0.08028601]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.1023169  0.10115089]\n"," [0.09469487 0.09219368]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.1017339  0.09344428]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.10032145 0.09947515]\n"," [0.08377748 0.081872  ]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.0998983  0.08282474]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.0999503  0.09906585]\n"," [0.07654466 0.07780226]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.09950807 0.07717346]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.09879317 0.09843554]\n"," [0.07780179 0.07649725]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.09861436 0.07714952]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.09921693 0.09852337]\n"," [0.07789578 0.07684183]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.09887015 0.0773688 ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07789578 0.07684183]\n"," [0.10792391 0.10602636]\n"," [0.09933901 0.09817592]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0773688  0.10697514 0.09875747]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07789578 0.07684183]\n"," [0.09635717 0.09729397]\n"," [0.10029383 0.09906422]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0773688  0.09682557 0.09967902]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07789578 0.07684183]\n"," [0.08398241 0.08533955]\n"," [0.09929467 0.09813105]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0773688  0.08466098 0.09871286]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07789578 0.07684183]\n"," [0.07872078 0.07754284]\n"," [0.09853113 0.09741101]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0773688  0.07813181 0.09797107]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07789578 0.07684183]\n"," [0.07404095 0.07251944]\n"," [0.09935558 0.09801731]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0773688  0.07328019 0.09868644]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07789578 0.07684183]\n"," [0.07275202 0.07164661]\n"," [0.09759785 0.09661077]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0773688  0.07219932 0.09710431]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07789578 0.07684183]\n"," [0.07440535 0.07399827]\n"," [0.09771912 0.0965633 ]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0773688  0.07420181 0.09714121]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.11245712 0.11059353]\n"," [0.0806891  0.07959403]\n"," [0.08757986 0.08625542]]\n","\n","Average MAE Loss:\n","[0.11152533 0.11152533 0.08014156 0.08691764]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.11245712 0.11059353]\n"," [0.07081752 0.07124023]\n"," [0.07478371 0.07321019]]\n","\n","Average MAE Loss:\n","[0.11152533 0.11152533 0.07102888 0.07399695]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.11245712 0.11059353]\n"," [0.06975976 0.07080192]\n"," [0.06861014 0.06827641]]\n","\n","Average MAE Loss:\n","[0.11152533 0.11152533 0.07028084 0.06844328]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.11245712 0.11059353]\n"," [0.06870925 0.06999542]\n"," [0.06906728 0.06805837]]\n","\n","Average MAE Loss:\n","[0.11152533 0.11152533 0.06935233 0.06856282]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.11245712 0.11059353]\n"," [0.06919951 0.0695772 ]\n"," [0.06749859 0.06791196]]\n","\n","Average MAE Loss:\n","[0.11152533 0.11152533 0.06938836 0.06770528]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.11245712 0.11059353]\n"," [0.06925622 0.06954434]\n"," [0.06876075 0.06829091]]\n","\n","Average MAE Loss:\n","[0.11152533 0.11152533 0.06940028 0.06852583]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.11245712 0.11059353]\n"," [0.06876491 0.06943567]\n"," [0.06723296 0.06725432]]\n","\n","Average MAE Loss:\n","[0.11152533 0.11152533 0.06910029 0.06724364]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.06723296 0.06725432]\n"," [0.08100227 0.08208839]\n"," [0.06898292 0.06890893]]\n","\n","Average MAE Loss:\n","[0.11152533 0.06724364 0.08154533 0.06894593]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.06723296 0.06725432]\n"," [0.07275388 0.07358353]\n"," [0.06854071 0.06840179]]\n","\n","Average MAE Loss:\n","[0.11152533 0.06724364 0.0731687  0.06847125]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.06723296 0.06725432]\n"," [0.06971233 0.0703903 ]\n"," [0.0681067  0.06811082]]\n","\n","Average MAE Loss:\n","[0.11152533 0.06724364 0.07005131 0.06810876]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.06723296 0.06725432]\n"," [0.06974925 0.07040601]\n"," [0.06933867 0.06864167]]\n","\n","Average MAE Loss:\n","[0.11152533 0.06724364 0.07007763 0.06899017]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.06723296 0.06725432]\n"," [0.07024481 0.07102062]\n"," [0.06944512 0.06857347]]\n","\n","Average MAE Loss:\n","[0.11152533 0.06724364 0.07063271 0.06900929]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.06723296 0.06725432]\n"," [0.06897593 0.06969979]\n"," [0.06884857 0.06808507]]\n","\n","Average MAE Loss:\n","[0.11152533 0.06724364 0.06933786 0.06846682]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.11245712 0.11059353]\n"," [0.06723296 0.06725432]\n"," [0.06937157 0.06961826]\n"," [0.06866856 0.06794115]]\n","\n","Average MAE Loss:\n","[0.11152533 0.06724364 0.06949492 0.06830486]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.08762134 0.08573291]\n"," [0.06648747 0.06715666]\n"," [0.08127141 0.07935524]]\n","\n","Average MAE Loss:\n","[0.08667713 0.08667713 0.06682207 0.08031332]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.08762134 0.08573291]\n"," [0.0664816  0.06779119]\n"," [0.07575405 0.0740504 ]]\n","\n","Average MAE Loss:\n","[0.08667713 0.08667713 0.0671364  0.07490223]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.08762134 0.08573291]\n"," [0.06734766 0.06799314]\n"," [0.07123468 0.06988788]]\n","\n","Average MAE Loss:\n","[0.08667713 0.08667713 0.0676704  0.07056128]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.08762134 0.08573291]\n"," [0.06645961 0.06730169]\n"," [0.06868519 0.06779194]]\n","\n","Average MAE Loss:\n","[0.08667713 0.08667713 0.06688065 0.06823857]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.08762134 0.08573291]\n"," [0.06619889 0.06732898]\n"," [0.06738002 0.06675214]]\n","\n","Average MAE Loss:\n","[0.08667713 0.08667713 0.06676393 0.06706608]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.08762134 0.08573291]\n"," [0.06699733 0.0677186 ]\n"," [0.06692555 0.06642019]]\n","\n","Average MAE Loss:\n","[0.08667713 0.08667713 0.06735796 0.06667287]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.08762134 0.08573291]\n"," [0.06617378 0.06730277]\n"," [0.06683339 0.06636263]]\n","\n","Average MAE Loss:\n","[0.08667713 0.08667713 0.06673827 0.06659801]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.06683339 0.06636263]\n"," [0.0668387  0.06747356]\n"," [0.06599191 0.06691385]]\n","\n","Average MAE Loss:\n","[0.08667713 0.06659801 0.06715613 0.06645288]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.06683339 0.06636263]\n"," [0.06717055 0.06810206]\n"," [0.06598054 0.06666405]]\n","\n","Average MAE Loss:\n","[0.08667713 0.06659801 0.0676363  0.0663223 ]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.06683339 0.06636263]\n"," [0.06648601 0.06755979]\n"," [0.06615533 0.06661716]]\n","\n","Average MAE Loss:\n","[0.08667713 0.06659801 0.0670229  0.06638624]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.06683339 0.06636263]\n"," [0.06666968 0.06773879]\n"," [0.0662137  0.06658617]]\n","\n","Average MAE Loss:\n","[0.08667713 0.06659801 0.06720423 0.06639993]\n","\n","Epoch 00039: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.06683339 0.06636263]\n"," [0.06657489 0.06771504]\n"," [0.0663085  0.06657151]]\n","\n","Average MAE Loss:\n","[0.08667713 0.06659801 0.06714497 0.06644001]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.06683339 0.06636263]\n"," [0.06652444 0.06754633]\n"," [0.06613434 0.06641925]]\n","\n","Average MAE Loss:\n","[0.08667713 0.06659801 0.06703538 0.06627679]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.08762134 0.08573291]\n"," [0.06683339 0.06636263]\n"," [0.06627489 0.0672977 ]\n"," [0.06583123 0.06620957]]\n","\n","Average MAE Loss:\n","[0.08667713 0.06659801 0.06678629 0.0660204 ]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.07043525 0.06961513]\n"," [0.06640922 0.06664821]\n"," [0.06888486 0.06827554]]\n","\n","Average MAE Loss:\n","[0.07002519 0.07002519 0.06652871 0.0685802 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.07043525 0.06961513]\n"," [0.06595285 0.06684362]\n"," [0.06738225 0.06697988]]\n","\n","Average MAE Loss:\n","[0.07002519 0.07002519 0.06639823 0.06718107]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.07043525 0.06961513]\n"," [0.06600322 0.066996  ]\n"," [0.06657814 0.06633008]]\n","\n","Average MAE Loss:\n","[0.07002519 0.07002519 0.06649961 0.06645411]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.07043525 0.06961513]\n"," [0.06683638 0.06758385]\n"," [0.0662453  0.06607987]]\n","\n","Average MAE Loss:\n","[0.07002519 0.07002519 0.06721012 0.06616258]\n","\n","Epoch 00046: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.07043525 0.06961513]\n"," [0.06609234 0.06713063]\n"," [0.0662708  0.06607516]]\n","\n","Average MAE Loss:\n","[0.07002519 0.07002519 0.06661148 0.06617298]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.07043525 0.06961513]\n"," [0.06597224 0.06699696]\n"," [0.06628827 0.06607988]]\n","\n","Average MAE Loss:\n","[0.07002519 0.07002519 0.0664846  0.06618407]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.07043525 0.06961513]\n"," [0.06571589 0.06685144]\n"," [0.06631263 0.0660884 ]]\n","\n","Average MAE Loss:\n","[0.07002519 0.07002519 0.06628367 0.06620051]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.06631263 0.0660884 ]\n"," [0.06781516 0.0675972 ]\n"," [0.06566699 0.06675019]]\n","\n","Average MAE Loss:\n","[0.07002519 0.06620051 0.06770618 0.06620859]\n","\n","Epoch 00050: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.06631263 0.0660884 ]\n"," [0.06617087 0.06655703]\n"," [0.0656258  0.06666918]]\n","\n","Average MAE Loss:\n","[0.07002519 0.06620051 0.06636395 0.06614749]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.06631263 0.0660884 ]\n"," [0.06592776 0.0666222 ]\n"," [0.06558893 0.06659599]]\n","\n","Average MAE Loss:\n","[0.07002519 0.06620051 0.06627498 0.06609246]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.06631263 0.0660884 ]\n"," [0.06587074 0.06664706]\n"," [0.06555563 0.0665188 ]]\n","\n","Average MAE Loss:\n","[0.07002519 0.06620051 0.0662589  0.06603721]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.06631263 0.0660884 ]\n"," [0.06596755 0.06677688]\n"," [0.06551505 0.06644844]]\n","\n","Average MAE Loss:\n","[0.07002519 0.06620051 0.06637221 0.06598174]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.06631263 0.0660884 ]\n"," [0.06618702 0.06696187]\n"," [0.06548476 0.06638346]]\n","\n","Average MAE Loss:\n","[0.07002519 0.06620051 0.06657445 0.06593411]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.07043525 0.06961513]\n"," [0.06631263 0.0660884 ]\n"," [0.06620191 0.06702526]\n"," [0.0654853  0.06633968]]\n","\n","Average MAE Loss:\n","[0.07002519 0.06620051 0.06661359 0.06591249]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06670912 0.06686405]\n"," [0.06670912 0.06686405]\n"," [0.06596915 0.06645977]\n"," [0.0666773  0.06681646]]\n","\n","Average MAE Loss:\n","[0.06678658 0.06678658 0.06621446 0.06674688]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06670912 0.06686405]\n"," [0.06670912 0.06686405]\n"," [0.06555666 0.0664822 ]\n"," [0.0664685  0.06663558]]\n","\n","Average MAE Loss:\n","[0.06678658 0.06678658 0.06601943 0.06655204]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06670912 0.06686405]\n"," [0.06670912 0.06686405]\n"," [0.06547422 0.06656883]\n"," [0.06633742 0.06651953]]\n","\n","Average MAE Loss:\n","[0.06678658 0.06678658 0.06602152 0.06642848]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06670912 0.06686405]\n"," [0.06670912 0.06686405]\n"," [0.06566769 0.06665328]\n"," [0.06627541 0.06645907]]\n","\n","Average MAE Loss:\n","[0.06678658 0.06678658 0.06616049 0.06636724]\n","\n","Epoch 00060: reducing learning rate of group 0 to 7.8125e-06.\n","\n","epochs finished with time:27.293667793273926\n","\n","[[0.06670912 0.06686405]\n"," [0.06670912 0.06686405]\n"," [0.06566769 0.06665328]\n"," [0.06627541 0.06645907]]\n","00:00:47.866457861999834\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.07307686 0.06470358]\n"," [0.08405714 0.07250556]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.06889022 0.07828135]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.08900966 0.07872766]\n"," [0.06661287 0.06013416]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.08386866 0.06337351]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.07136732 0.06291519]\n"," [0.06762308 0.06138353]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.06714125 0.0645033 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.07024199 0.06095615]\n"," [0.0689151  0.0652513 ]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.06559907 0.0670832 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06321749 0.05601685]\n"," [0.06992719 0.05890507]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05961717 0.06441613]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06385712 0.05607356]\n"," [0.06202191 0.0534477 ]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05996534 0.0577348 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06466393 0.05622055]\n"," [0.06428778 0.05439294]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.06044224 0.05934036]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.06428778 0.05439294]\n"," [0.08953608 0.07906765]\n"," [0.06187661 0.05478065]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05934036 0.08430186 0.05832863]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.06428778 0.05439294]\n"," [0.0790883  0.06928557]\n"," [0.06092633 0.05263663]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05934036 0.07418694 0.05678148]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.06428778 0.05439294]\n"," [0.07238767 0.06356306]\n"," [0.05961557 0.05234163]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05934036 0.06797537 0.0559786 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.06428778 0.05439294]\n"," [0.06872304 0.06018898]\n"," [0.06076848 0.05648336]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05934036 0.06445601 0.05862592]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.06428778 0.05439294]\n"," [0.0689263  0.06064404]\n"," [0.05602665 0.04916666]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05934036 0.06478517 0.05259665]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.06428778 0.05439294]\n"," [0.06977915 0.06027476]\n"," [0.05686241 0.04909303]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05934036 0.06502696 0.05297772]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.06428778 0.05439294]\n"," [0.065457   0.05742053]\n"," [0.05822214 0.04983004]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05934036 0.06143877 0.05402609]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.0945886  0.08338038]\n"," [0.0713372  0.06069699]\n"," [0.05895649 0.04898905]]\n","\n","Average MAE Loss:\n","[0.08898449 0.08898449 0.06601709 0.05397277]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.0945886  0.08338038]\n"," [0.05340842 0.04700888]\n"," [0.05557251 0.04941807]]\n","\n","Average MAE Loss:\n","[0.08898449 0.08898449 0.05020865 0.05249529]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.0945886  0.08338038]\n"," [0.05139413 0.04562221]\n"," [0.05433413 0.04572765]]\n","\n","Average MAE Loss:\n","[0.08898449 0.08898449 0.04850817 0.05003089]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.0945886  0.08338038]\n"," [0.05211003 0.04529528]\n"," [0.05376057 0.04510773]]\n","\n","Average MAE Loss:\n","[0.08898449 0.08898449 0.04870266 0.04943415]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.0945886  0.08338038]\n"," [0.05181988 0.04502091]\n"," [0.05237409 0.04525342]]\n","\n","Average MAE Loss:\n","[0.08898449 0.08898449 0.04842039 0.04881376]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.0945886  0.08338038]\n"," [0.05212082 0.04432607]\n"," [0.05265877 0.04489398]]\n","\n","Average MAE Loss:\n","[0.08898449 0.08898449 0.04822345 0.04877638]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.0945886  0.08338038]\n"," [0.0507416  0.04497646]\n"," [0.05309623 0.04483064]]\n","\n","Average MAE Loss:\n","[0.08898449 0.08898449 0.04785903 0.04896343]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.05309623 0.04483064]\n"," [0.08877335 0.07764533]\n"," [0.054584   0.04643391]]\n","\n","Average MAE Loss:\n","[0.08898449 0.04896343 0.08320934 0.05050896]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.05309623 0.04483064]\n"," [0.06965902 0.05936091]\n"," [0.05210542 0.04448783]]\n","\n","Average MAE Loss:\n","[0.08898449 0.04896343 0.06450997 0.04829663]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.05309623 0.04483064]\n"," [0.05393695 0.04639395]\n"," [0.05323267 0.04496421]]\n","\n","Average MAE Loss:\n","[0.08898449 0.04896343 0.05016545 0.04909844]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.05309623 0.04483064]\n"," [0.05304497 0.0458556 ]\n"," [0.05153758 0.04457239]]\n","\n","Average MAE Loss:\n","[0.08898449 0.04896343 0.04945029 0.04805498]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.05309623 0.04483064]\n"," [0.05208463 0.04559725]\n"," [0.05121814 0.04432254]]\n","\n","Average MAE Loss:\n","[0.08898449 0.04896343 0.04884094 0.04777034]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.05309623 0.04483064]\n"," [0.05198579 0.04586785]\n"," [0.05179939 0.04486394]]\n","\n","Average MAE Loss:\n","[0.08898449 0.04896343 0.04892682 0.04833166]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0945886  0.08338038]\n"," [0.05309623 0.04483064]\n"," [0.05203508 0.04489341]\n"," [0.05186647 0.0438736 ]]\n","\n","Average MAE Loss:\n","[0.08898449 0.04896343 0.04846424 0.04787004]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.06545175 0.05481493]\n"," [0.05794166 0.04823156]\n"," [0.0530226  0.04467242]]\n","\n","Average MAE Loss:\n","[0.06013334 0.06013334 0.05308661 0.04884751]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.06545175 0.05481493]\n"," [0.05376988 0.04496381]\n"," [0.05083138 0.04358302]]\n","\n","Average MAE Loss:\n","[0.06013334 0.06013334 0.04936684 0.0472072 ]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.06545175 0.05481493]\n"," [0.05165302 0.04353459]\n"," [0.05102746 0.04493793]]\n","\n","Average MAE Loss:\n","[0.06013334 0.06013334 0.0475938  0.04798269]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.06545175 0.05481493]\n"," [0.05084645 0.04338002]\n"," [0.05179726 0.04420745]]\n","\n","Average MAE Loss:\n","[0.06013334 0.06013334 0.04711323 0.04800236]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.06545175 0.05481493]\n"," [0.05054407 0.04342718]\n"," [0.05179807 0.04420871]]\n","\n","Average MAE Loss:\n","[0.06013334 0.06013334 0.04698562 0.04800339]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.06545175 0.05481493]\n"," [0.05038841 0.04341904]\n"," [0.05186655 0.04396062]]\n","\n","Average MAE Loss:\n","[0.06013334 0.06013334 0.04690372 0.04791358]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.06545175 0.05481493]\n"," [0.05030058 0.04340153]\n"," [0.05098091 0.043385  ]]\n","\n","Average MAE Loss:\n","[0.06013334 0.06013334 0.04685106 0.04718295]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.05098091 0.043385  ]\n"," [0.06214186 0.05179126]\n"," [0.05072625 0.04373765]]\n","\n","Average MAE Loss:\n","[0.06013334 0.04718295 0.05696656 0.04723195]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.05098091 0.043385  ]\n"," [0.0572871  0.04762986]\n"," [0.05065274 0.04360968]]\n","\n","Average MAE Loss:\n","[0.06013334 0.04718295 0.05245848 0.04713121]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.05098091 0.043385  ]\n"," [0.0537661  0.04487409]\n"," [0.05068833 0.04363661]]\n","\n","Average MAE Loss:\n","[0.06013334 0.04718295 0.0493201  0.04716247]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.05098091 0.043385  ]\n"," [0.0519753  0.04362633]\n"," [0.05110259 0.04354659]]\n","\n","Average MAE Loss:\n","[0.06013334 0.04718295 0.04780081 0.04732459]\n","\n","Epoch 00039: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.05098091 0.043385  ]\n"," [0.05138683 0.04333002]\n"," [0.05041076 0.04365064]]\n","\n","Average MAE Loss:\n","[0.06013334 0.04718295 0.04735842 0.0470307 ]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.05098091 0.043385  ]\n"," [0.05095831 0.04323762]\n"," [0.05029437 0.04368003]]\n","\n","Average MAE Loss:\n","[0.06013334 0.04718295 0.04709797 0.0469872 ]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06545175 0.05481493]\n"," [0.05098091 0.043385  ]\n"," [0.05073864 0.04328879]\n"," [0.05062709 0.0433892 ]]\n","\n","Average MAE Loss:\n","[0.06013334 0.04718295 0.04701372 0.04700815]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05352394 0.04477319]\n"," [0.05265802 0.04412879]\n"," [0.05077033 0.04447871]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04914856 0.0483934  0.04762452]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05352394 0.04477319]\n"," [0.05218301 0.04376862]\n"," [0.05061415 0.04331005]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04914856 0.04797581 0.0469621 ]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05352394 0.04477319]\n"," [0.0517913  0.04349599]\n"," [0.05005278 0.04386304]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04914856 0.04764365 0.04695791]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05352394 0.04477319]\n"," [0.05145435 0.04330422]\n"," [0.05025964 0.04326414]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04914856 0.04737928 0.04676189]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05352394 0.04477319]\n"," [0.05116375 0.04315517]\n"," [0.05032753 0.04359262]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04914856 0.04715946 0.04696008]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05352394 0.04477319]\n"," [0.05103349 0.04309523]\n"," [0.05037142 0.0434717 ]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04914856 0.04706436 0.04692156]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05352394 0.04477319]\n"," [0.05093359 0.04304976]\n"," [0.05017027 0.04344983]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04914856 0.04699167 0.04681005]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05017027 0.04344983]\n"," [0.05329608 0.04460325]\n"," [0.05029014 0.04432448]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04681005 0.04894966 0.04730731]\n","\n","Epoch 00050: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05017027 0.04344983]\n"," [0.05301894 0.04439534]\n"," [0.05008614 0.04362683]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04681005 0.04870714 0.04685649]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05017027 0.04344983]\n"," [0.05288997 0.04429711]\n"," [0.05027307 0.04332125]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04681005 0.04859354 0.04679716]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05017027 0.04344983]\n"," [0.05275633 0.04419767]\n"," [0.05002948 0.04355126]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04681005 0.048477   0.04679037]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05017027 0.04344983]\n"," [0.05262683 0.04410389]\n"," [0.0499759  0.04328978]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04681005 0.04836536 0.04663284]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05017027 0.04344983]\n"," [0.05250777 0.04401639]\n"," [0.04997768 0.04311432]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04681005 0.04826208 0.046546  ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05352394 0.04477319]\n"," [0.05017027 0.04344983]\n"," [0.05244926 0.04397285]\n"," [0.04999773 0.04330159]]\n","\n","Average MAE Loss:\n","[0.04914856 0.04681005 0.04821106 0.04664966]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05116128 0.0432226 ]\n"," [0.05116128 0.0432226 ]\n"," [0.05112329 0.04320039]\n"," [0.05026719 0.04318319]]\n","\n","Average MAE Loss:\n","[0.04719194 0.04719194 0.04716184 0.04672519]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05116128 0.0432226 ]\n"," [0.05116128 0.0432226 ]\n"," [0.05109143 0.04318178]\n"," [0.05013013 0.04353467]]\n","\n","Average MAE Loss:\n","[0.04719194 0.04719194 0.0471366  0.0468324 ]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05116128 0.0432226 ]\n"," [0.05116128 0.0432226 ]\n"," [0.05106    0.04316493]\n"," [0.05003478 0.04346683]]\n","\n","Average MAE Loss:\n","[0.04719194 0.04719194 0.04711246 0.04675081]\n","\n","Epoch 00059: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00059: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05116128 0.0432226 ]\n"," [0.05116128 0.0432226 ]\n"," [0.05104526 0.04315749]\n"," [0.05010016 0.04314735]]\n","\n","Average MAE Loss:\n","[0.04719194 0.04719194 0.04710138 0.04662375]\n","\n","\n","epochs finished with time:27.012192964553833\n","\n","[[0.05116128 0.0432226 ]\n"," [0.05116128 0.0432226 ]\n"," [0.05104526 0.04315749]\n"," [0.05010016 0.04314735]]\n","00:00:42.948911487999794\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.07230424 0.09443357]\n"," [0.07443574 0.09702218]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.0833689  0.08572896]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.06748961 0.08916372]\n"," [0.07320452 0.09530996]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.07832666 0.08425724]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.06718899 0.08939409]\n"," [0.07438166 0.09665944]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.07829154 0.08552055]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.05052768 0.07236553]\n"," [0.07264878 0.0950558 ]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.0614466  0.08385229]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.04924394 0.06846274]\n"," [0.07277073 0.09510306]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.05885334 0.0839369 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.04521121 0.06530186]\n"," [0.07154937 0.09357308]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.05525653 0.08256122]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.04934148 0.07054056]\n"," [0.07235738 0.09378423]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.05994102 0.0830708 ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.07235738 0.09378423]\n"," [0.0750033  0.09558962]\n"," [0.04834378 0.06715114]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0830708  0.08529646 0.05774746]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.07235738 0.09378423]\n"," [0.0936925  0.10907941]\n"," [0.04706151 0.06598599]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0830708  0.10138595 0.05652375]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.07235738 0.09378423]\n"," [0.05116109 0.07255305]\n"," [0.04348225 0.06407502]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0830708  0.06185707 0.05377863]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.07235738 0.09378423]\n"," [0.04481027 0.064684  ]\n"," [0.04348485 0.06362264]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0830708  0.05474714 0.05355375]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.07235738 0.09378423]\n"," [0.04286987 0.06339719]\n"," [0.04301775 0.06357971]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0830708  0.05313353 0.05329873]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.07235738 0.09378423]\n"," [0.0512701  0.07307388]\n"," [0.04234076 0.06292237]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0830708  0.06217199 0.05263156]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.07235738 0.09378423]\n"," [0.04234868 0.06293525]\n"," [0.04271809 0.06242694]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0830708  0.05264196 0.05257251]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.07625397 0.0987263 ]\n"," [0.04467653 0.06402227]\n"," [0.0481123  0.0694427 ]]\n","\n","Average MAE Loss:\n","[0.08749013 0.08749013 0.0543494  0.0587775 ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.07625397 0.0987263 ]\n"," [0.04036906 0.06116544]\n"," [0.04587572 0.06379569]]\n","\n","Average MAE Loss:\n","[0.08749013 0.08749013 0.05076725 0.0548357 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.07625397 0.0987263 ]\n"," [0.04014048 0.06083293]\n"," [0.04146078 0.06154798]]\n","\n","Average MAE Loss:\n","[0.08749013 0.08749013 0.05048671 0.05150438]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.07625397 0.0987263 ]\n"," [0.03968702 0.0606495 ]\n"," [0.04465695 0.06577031]]\n","\n","Average MAE Loss:\n","[0.08749013 0.08749013 0.05016826 0.05521363]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.07625397 0.0987263 ]\n"," [0.03968999 0.06031505]\n"," [0.04071558 0.06025796]]\n","\n","Average MAE Loss:\n","[0.08749013 0.08749013 0.05000252 0.05048677]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.07625397 0.0987263 ]\n"," [0.04028713 0.06005869]\n"," [0.04255073 0.0610146 ]]\n","\n","Average MAE Loss:\n","[0.08749013 0.08749013 0.05017291 0.05178266]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.07625397 0.0987263 ]\n"," [0.03974091 0.06000289]\n"," [0.0407727  0.06093369]]\n","\n","Average MAE Loss:\n","[0.08749013 0.08749013 0.0498719  0.05085319]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.0407727  0.06093369]\n"," [0.04793789 0.06945312]\n"," [0.04158795 0.06121664]]\n","\n","Average MAE Loss:\n","[0.08749013 0.05085319 0.05869551 0.05140229]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.0407727  0.06093369]\n"," [0.04500539 0.06680606]\n"," [0.04100621 0.06111467]]\n","\n","Average MAE Loss:\n","[0.08749013 0.05085319 0.05590572 0.05106044]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.0407727  0.06093369]\n"," [0.04252042 0.06248939]\n"," [0.03981696 0.05977057]]\n","\n","Average MAE Loss:\n","[0.08749013 0.05085319 0.0525049  0.04979376]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.0407727  0.06093369]\n"," [0.0427564  0.06394358]\n"," [0.03957731 0.05952578]]\n","\n","Average MAE Loss:\n","[0.08749013 0.05085319 0.05334999 0.04955154]\n","\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.0407727  0.06093369]\n"," [0.04116298 0.06198701]\n"," [0.03984592 0.05939781]]\n","\n","Average MAE Loss:\n","[0.08749013 0.05085319 0.05157499 0.04962187]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.0407727  0.06093369]\n"," [0.04030819 0.06081139]\n"," [0.03990871 0.05954017]]\n","\n","Average MAE Loss:\n","[0.08749013 0.05085319 0.05055979 0.04972444]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07625397 0.0987263 ]\n"," [0.0407727  0.06093369]\n"," [0.04017176 0.06031791]\n"," [0.04047742 0.05981304]]\n","\n","Average MAE Loss:\n","[0.08749013 0.05085319 0.05024484 0.05014523]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.05211174 0.07397063]\n"," [0.04085539 0.06160651]\n"," [0.0461654  0.0634279 ]]\n","\n","Average MAE Loss:\n","[0.06304118 0.06304118 0.05123095 0.05479665]\n","\n","Epoch 00029: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00029: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.05211174 0.07397063]\n"," [0.03859288 0.05871022]\n"," [0.0393625  0.05905348]]\n","\n","Average MAE Loss:\n","[0.06304118 0.06304118 0.04865155 0.04920799]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.05211174 0.07397063]\n"," [0.0390475  0.05873194]\n"," [0.03919106 0.05894766]]\n","\n","Average MAE Loss:\n","[0.06304118 0.06304118 0.04888972 0.04906936]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.05211174 0.07397063]\n"," [0.03855191 0.05859918]\n"," [0.0391614  0.05897038]]\n","\n","Average MAE Loss:\n","[0.06304118 0.06304118 0.04857554 0.04906589]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.05211174 0.07397063]\n"," [0.03836894 0.05868293]\n"," [0.03926678 0.05889292]]\n","\n","Average MAE Loss:\n","[0.06304118 0.06304118 0.04852594 0.04907985]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.05211174 0.07397063]\n"," [0.0383893  0.05857639]\n"," [0.03929532 0.05905531]]\n","\n","Average MAE Loss:\n","[0.06304118 0.06304118 0.04848285 0.04917531]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.05211174 0.07397063]\n"," [0.03852517 0.05865075]\n"," [0.03925474 0.05904532]]\n","\n","Average MAE Loss:\n","[0.06304118 0.06304118 0.04858796 0.04915003]\n","\n","Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.03925474 0.05904532]\n"," [0.04667512 0.06822555]\n"," [0.0385277  0.05830512]]\n","\n","Average MAE Loss:\n","[0.06304118 0.04915003 0.05745034 0.04841641]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.03925474 0.05904532]\n"," [0.0403399  0.06108785]\n"," [0.03871488 0.05830479]]\n","\n","Average MAE Loss:\n","[0.06304118 0.04915003 0.05071387 0.04850983]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.03925474 0.05904532]\n"," [0.03854564 0.05877002]\n"," [0.03890838 0.05838612]]\n","\n","Average MAE Loss:\n","[0.06304118 0.04915003 0.04865783 0.04864725]\n","\n","Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.03925474 0.05904532]\n"," [0.03851866 0.05860119]\n"," [0.03895108 0.05860384]]\n","\n","Average MAE Loss:\n","[0.06304118 0.04915003 0.04855993 0.04877746]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.03925474 0.05904532]\n"," [0.03862226 0.05860629]\n"," [0.03905131 0.05866278]]\n","\n","Average MAE Loss:\n","[0.06304118 0.04915003 0.04861428 0.04885704]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.03925474 0.05904532]\n"," [0.03873672 0.05868704]\n"," [0.03894324 0.05864139]]\n","\n","Average MAE Loss:\n","[0.06304118 0.04915003 0.04871188 0.04879232]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05211174 0.07397063]\n"," [0.03925474 0.05904532]\n"," [0.0386052  0.05869073]\n"," [0.03890767 0.05860215]]\n","\n","Average MAE Loss:\n","[0.06304118 0.04915003 0.04864796 0.04875491]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03962135 0.06016324]\n"," [0.03922219 0.05973168]\n"," [0.03909828 0.05952965]]\n","\n","Average MAE Loss:\n","[0.0498923  0.0498923  0.04947694 0.04931396]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03962135 0.06016324]\n"," [0.03871395 0.05916014]\n"," [0.03864617 0.05882639]]\n","\n","Average MAE Loss:\n","[0.0498923  0.0498923  0.04893705 0.04873628]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03962135 0.06016324]\n"," [0.03840984 0.05877912]\n"," [0.03859293 0.05863328]]\n","\n","Average MAE Loss:\n","[0.0498923  0.0498923  0.04859448 0.0486131 ]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03962135 0.06016324]\n"," [0.0382372  0.05858064]\n"," [0.03861374 0.05855202]]\n","\n","Average MAE Loss:\n","[0.0498923  0.0498923  0.04840892 0.04858288]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03962135 0.06016324]\n"," [0.03817581 0.05849903]\n"," [0.0386464  0.05851501]]\n","\n","Average MAE Loss:\n","[0.0498923  0.0498923  0.04833742 0.0485807 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03962135 0.06016324]\n"," [0.03814366 0.05846117]\n"," [0.03868091 0.05851873]]\n","\n","Average MAE Loss:\n","[0.0498923  0.0498923  0.04830241 0.04859982]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03962135 0.06016324]\n"," [0.03815015 0.05845863]\n"," [0.03871041 0.05852523]]\n","\n","Average MAE Loss:\n","[0.0498923  0.0498923  0.04830439 0.04861782]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03871041 0.05852523]\n"," [0.03914786 0.0596567 ]\n"," [0.03818561 0.05843304]]\n","\n","Average MAE Loss:\n","[0.0498923  0.04861782 0.04940228 0.04830932]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03871041 0.05852523]\n"," [0.03868853 0.05914059]\n"," [0.03821851 0.0584151 ]]\n","\n","Average MAE Loss:\n","[0.0498923  0.04861782 0.04891456 0.04831681]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03871041 0.05852523]\n"," [0.03839362 0.05879534]\n"," [0.03825381 0.05840278]]\n","\n","Average MAE Loss:\n","[0.0498923  0.04861782 0.04859448 0.0483283 ]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03871041 0.05852523]\n"," [0.03830386 0.05868991]\n"," [0.03829177 0.05840009]]\n","\n","Average MAE Loss:\n","[0.0498923  0.04861782 0.04849689 0.04834593]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03871041 0.05852523]\n"," [0.03823067 0.05860852]\n"," [0.03833325 0.0583807 ]]\n","\n","Average MAE Loss:\n","[0.0498923  0.04861782 0.04841959 0.04835698]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00054: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03871041 0.05852523]\n"," [0.03817669 0.05854142]\n"," [0.03834017 0.0583722 ]]\n","\n","Average MAE Loss:\n","[0.0498923  0.04861782 0.04835906 0.04835618]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03962135 0.06016324]\n"," [0.03871041 0.05852523]\n"," [0.03814209 0.0584912 ]\n"," [0.03835087 0.05836657]]\n","\n","Average MAE Loss:\n","[0.0498923  0.04861782 0.04831665 0.04835872]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03836645 0.05859983]\n"," [0.03836645 0.05859983]\n"," [0.03833069 0.05856563]\n"," [0.03836614 0.05858011]]\n","\n","Average MAE Loss:\n","[0.04848314 0.04848314 0.04844816 0.04847312]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03836645 0.05859983]\n"," [0.03836645 0.05859983]\n"," [0.038298   0.05853077]\n"," [0.0383601  0.05855361]]\n","\n","Average MAE Loss:\n","[0.04848314 0.04848314 0.04841438 0.04845686]\n","\n","Epoch 00058: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03836645 0.05859983]\n"," [0.03836645 0.05859983]\n"," [0.03827218 0.05850518]\n"," [0.03835989 0.0585455 ]]\n","\n","Average MAE Loss:\n","[0.04848314 0.04848314 0.04838868 0.04845269]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03836645 0.05859983]\n"," [0.03836645 0.05859983]\n"," [0.03825111 0.05848172]\n"," [0.03836369 0.05853873]]\n","\n","Average MAE Loss:\n","[0.04848314 0.04848314 0.04836642 0.04845121]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:27.53857946395874\n","\n","[[0.03836645 0.05859983]\n"," [0.03836645 0.05859983]\n"," [0.03825111 0.05848172]\n"," [0.03836369 0.05853873]]\n","00:00:43.12770479000028\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.07017043 0.07162912]\n"," [0.09358852 0.0989831 ]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.07089978 0.09628581]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.07558739 0.0790659 ]\n"," [0.09187753 0.09614538]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.07732664 0.09401145]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.07152866 0.07444357]\n"," [0.09186269 0.09528947]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.07298611 0.09357608]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06435433 0.06716468]\n"," [0.08675958 0.09063363]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06575951 0.08869661]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06599767 0.06685705]\n"," [0.08735266 0.09066997]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06642736 0.08901132]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06802205 0.07199164]\n"," [0.07165746 0.0755233 ]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.07000685 0.07359038]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06032878 0.06244601]\n"," [0.06530564 0.06832358]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.0613874  0.06681461]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06530564 0.06832358]\n"," [0.08093251 0.08050733]\n"," [0.06668694 0.06972693]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06681461 0.08071992 0.06820693]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06530564 0.06832358]\n"," [0.06795033 0.07048839]\n"," [0.06444033 0.06768268]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06681461 0.06921936 0.06606151]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06530564 0.06832358]\n"," [0.0672805  0.07108825]\n"," [0.06019895 0.06310675]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06681461 0.06918437 0.06165285]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06530564 0.06832358]\n"," [0.06187287 0.06479219]\n"," [0.06341224 0.06711133]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06681461 0.06333253 0.06526178]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06530564 0.06832358]\n"," [0.06207282 0.06499807]\n"," [0.05710111 0.05880783]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06681461 0.06353544 0.05795447]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06530564 0.06832358]\n"," [0.05483386 0.05637801]\n"," [0.05622175 0.0585258 ]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06681461 0.05560594 0.05737377]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06530564 0.06832358]\n"," [0.05406044 0.0559548 ]\n"," [0.05628934 0.05887282]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06681461 0.05500762 0.05758108]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.09341415 0.09782052]\n"," [0.05335782 0.05097412]\n"," [0.06433475 0.06776703]]\n","\n","Average MAE Loss:\n","[0.09561734 0.09561734 0.05216597 0.06605089]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.09341415 0.09782052]\n"," [0.05162401 0.05322132]\n"," [0.05425932 0.0536615 ]]\n","\n","Average MAE Loss:\n","[0.09561734 0.09561734 0.05242266 0.05396041]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.09341415 0.09782052]\n"," [0.05281666 0.05455271]\n"," [0.05657517 0.05967985]]\n","\n","Average MAE Loss:\n","[0.09561734 0.09561734 0.05368469 0.05812751]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.09341415 0.09782052]\n"," [0.04892876 0.04872677]\n"," [0.06589499 0.07009293]]\n","\n","Average MAE Loss:\n","[0.09561734 0.09561734 0.04882776 0.06799396]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.09341415 0.09782052]\n"," [0.04978802 0.05179206]\n"," [0.05436114 0.05636098]]\n","\n","Average MAE Loss:\n","[0.09561734 0.09561734 0.05079004 0.05536106]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.09341415 0.09782052]\n"," [0.04853696 0.04985511]\n"," [0.05042903 0.05144144]]\n","\n","Average MAE Loss:\n","[0.09561734 0.09561734 0.04919604 0.05093523]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.09341415 0.09782052]\n"," [0.04837292 0.04901621]\n"," [0.05087885 0.05190625]]\n","\n","Average MAE Loss:\n","[0.09561734 0.09561734 0.04869456 0.05139255]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.05087885 0.05190625]\n"," [0.05605705 0.0581175 ]\n"," [0.05314271 0.05454058]]\n","\n","Average MAE Loss:\n","[0.09561734 0.05139255 0.05708728 0.05384164]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.05087885 0.05190625]\n"," [0.05293047 0.05488815]\n"," [0.05257701 0.0541501 ]]\n","\n","Average MAE Loss:\n","[0.09561734 0.05139255 0.05390931 0.05336355]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.05087885 0.05190625]\n"," [0.04995109 0.05101041]\n"," [0.04987797 0.05101594]]\n","\n","Average MAE Loss:\n","[0.09561734 0.05139255 0.05048075 0.05044696]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.05087885 0.05190625]\n"," [0.0485888  0.04945619]\n"," [0.05145041 0.05275579]]\n","\n","Average MAE Loss:\n","[0.09561734 0.05139255 0.0490225  0.0521031 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.05087885 0.05190625]\n"," [0.05012345 0.0520439 ]\n"," [0.05002575 0.0514364 ]]\n","\n","Average MAE Loss:\n","[0.09561734 0.05139255 0.05108367 0.05073108]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.05087885 0.05190625]\n"," [0.04810225 0.04880831]\n"," [0.04996357 0.05126448]]\n","\n","Average MAE Loss:\n","[0.09561734 0.05139255 0.04845528 0.05061403]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.09341415 0.09782052]\n"," [0.05087885 0.05190625]\n"," [0.0491565  0.05038899]\n"," [0.05314279 0.05535848]]\n","\n","Average MAE Loss:\n","[0.09561734 0.05139255 0.04977274 0.05425064]\n","\n","Epoch 00028: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.06724462 0.07116275]\n"," [0.05213453 0.05420149]\n"," [0.04817363 0.04683163]]\n","\n","Average MAE Loss:\n","[0.06920369 0.06920369 0.05316801 0.04750263]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.06724462 0.07116275]\n"," [0.04751093 0.04834687]\n"," [0.05045754 0.05190424]]\n","\n","Average MAE Loss:\n","[0.06920369 0.06920369 0.0479289  0.05118089]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.06724462 0.07116275]\n"," [0.04730072 0.04848419]\n"," [0.04893638 0.04948521]]\n","\n","Average MAE Loss:\n","[0.06920369 0.06920369 0.04789245 0.0492108 ]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.06724462 0.07116275]\n"," [0.04833931 0.05016929]\n"," [0.04970712 0.05087886]]\n","\n","Average MAE Loss:\n","[0.06920369 0.06920369 0.0492543  0.05029299]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.06724462 0.07116275]\n"," [0.04731431 0.04847047]\n"," [0.04920158 0.05009166]]\n","\n","Average MAE Loss:\n","[0.06920369 0.06920369 0.04789239 0.04964662]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.06724462 0.07116275]\n"," [0.04787493 0.04944687]\n"," [0.04822186 0.04885857]]\n","\n","Average MAE Loss:\n","[0.06920369 0.06920369 0.0486609  0.04854022]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.06724462 0.07116275]\n"," [0.04689041 0.04815453]\n"," [0.04843059 0.04946421]]\n","\n","Average MAE Loss:\n","[0.06920369 0.06920369 0.04752247 0.0489474 ]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.04843059 0.04946421]\n"," [0.05412478 0.05647156]\n"," [0.04681499 0.04785089]]\n","\n","Average MAE Loss:\n","[0.06920369 0.0489474  0.05529817 0.04733294]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.04843059 0.04946421]\n"," [0.04752364 0.04851806]\n"," [0.04921439 0.05085061]]\n","\n","Average MAE Loss:\n","[0.06920369 0.0489474  0.04802085 0.0500325 ]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.04843059 0.04946421]\n"," [0.0474353  0.0488515 ]\n"," [0.04858384 0.04966054]]\n","\n","Average MAE Loss:\n","[0.06920369 0.0489474  0.0481434  0.04912219]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.04843059 0.04946421]\n"," [0.04771332 0.04924592]\n"," [0.04965325 0.05144807]]\n","\n","Average MAE Loss:\n","[0.06920369 0.0489474  0.04847962 0.05055066]\n","\n","Epoch 00039: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.04843059 0.04946421]\n"," [0.04697912 0.04820838]\n"," [0.04879149 0.05003967]]\n","\n","Average MAE Loss:\n","[0.06920369 0.0489474  0.04759375 0.04941558]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.04843059 0.04946421]\n"," [0.04733104 0.04866151]\n"," [0.04837699 0.04927082]]\n","\n","Average MAE Loss:\n","[0.06920369 0.0489474  0.04799628 0.0488239 ]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06724462 0.07116275]\n"," [0.04843059 0.04946421]\n"," [0.04701975 0.04801577]\n"," [0.04845827 0.04963269]]\n","\n","Average MAE Loss:\n","[0.06920369 0.0489474  0.04751776 0.04904548]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.05166974 0.05383715]\n"," [0.04886054 0.05038552]\n"," [0.0499199  0.05177622]]\n","\n","Average MAE Loss:\n","[0.05275345 0.05275345 0.04962303 0.05084806]\n","\n","Epoch 00043: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.05166974 0.05383715]\n"," [0.04797378 0.04933148]\n"," [0.04864982 0.04999601]]\n","\n","Average MAE Loss:\n","[0.05275345 0.05275345 0.04865263 0.04932292]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.05166974 0.05383715]\n"," [0.04736511 0.04858658]\n"," [0.04823611 0.04920624]]\n","\n","Average MAE Loss:\n","[0.05275345 0.05275345 0.04797584 0.04872117]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.05166974 0.05383715]\n"," [0.04722167 0.0484689 ]\n"," [0.04812729 0.04899442]]\n","\n","Average MAE Loss:\n","[0.05275345 0.05275345 0.04784529 0.04856086]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.05166974 0.05383715]\n"," [0.04713375 0.04845757]\n"," [0.04812993 0.0490487 ]]\n","\n","Average MAE Loss:\n","[0.05275345 0.05275345 0.04779566 0.04858932]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.05166974 0.05383715]\n"," [0.04703007 0.04832006]\n"," [0.04817502 0.04911153]]\n","\n","Average MAE Loss:\n","[0.05275345 0.05275345 0.04767506 0.04864327]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.05166974 0.05383715]\n"," [0.04699487 0.04826792]\n"," [0.04824628 0.04921412]]\n","\n","Average MAE Loss:\n","[0.05275345 0.05275345 0.0476314  0.0487302 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.04824628 0.04921412]\n"," [0.05078144 0.05278231]\n"," [0.0470577  0.04829435]]\n","\n","Average MAE Loss:\n","[0.05275345 0.0487302  0.05178188 0.04767602]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.04824628 0.04921412]\n"," [0.04974774 0.05157078]\n"," [0.04719746 0.04850826]]\n","\n","Average MAE Loss:\n","[0.05275345 0.0487302  0.05065926 0.04785286]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00051: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.04824628 0.04921412]\n"," [0.049371   0.05110668]\n"," [0.04742984 0.0489034 ]]\n","\n","Average MAE Loss:\n","[0.05275345 0.0487302  0.05023884 0.04816662]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.04824628 0.04921412]\n"," [0.04907226 0.05074229]\n"," [0.04754594 0.04903447]]\n","\n","Average MAE Loss:\n","[0.05275345 0.0487302  0.04990727 0.04829021]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.04824628 0.04921412]\n"," [0.04880714 0.05042705]\n"," [0.04753276 0.0489563 ]]\n","\n","Average MAE Loss:\n","[0.05275345 0.0487302  0.04961709 0.04824453]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.04824628 0.04921412]\n"," [0.04853394 0.05007537]\n"," [0.04758905 0.0489761 ]]\n","\n","Average MAE Loss:\n","[0.05275345 0.0487302  0.04930465 0.04828257]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05166974 0.05383715]\n"," [0.04824628 0.04921412]\n"," [0.04841742 0.04991894]\n"," [0.04768085 0.04908283]]\n","\n","Average MAE Loss:\n","[0.05275345 0.0487302  0.04916818 0.04838184]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04879298 0.05034446]\n"," [0.04879298 0.05034446]\n"," [0.04864334 0.05015898]\n"," [0.04877472 0.05031366]]\n","\n","Average MAE Loss:\n","[0.04956872 0.04956872 0.04940116 0.04954419]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04879298 0.05034446]\n"," [0.04879298 0.05034446]\n"," [0.04852012 0.05001338]\n"," [0.04870048 0.05020174]]\n","\n","Average MAE Loss:\n","[0.04956872 0.04956872 0.04926675 0.04945111]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04879298 0.05034446]\n"," [0.04879298 0.05034446]\n"," [0.04841882 0.04989866]\n"," [0.04863246 0.0501068 ]]\n","\n","Average MAE Loss:\n","[0.04956872 0.04956872 0.04915874 0.04936963]\n","\n","Epoch 00059: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04879298 0.05034446]\n"," [0.04879298 0.05034446]\n"," [0.04837512 0.04985176]\n"," [0.04861151 0.05007643]]\n","\n","Average MAE Loss:\n","[0.04956872 0.04956872 0.04911344 0.04934397]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:27.62583041191101\n","\n","[[0.04879298 0.05034446]\n"," [0.04879298 0.05034446]\n"," [0.04837512 0.04985176]\n"," [0.04861151 0.05007643]]\n","00:00:42.280428136999944\n"]}],"source":["# seed = 10 table = 2/8 fixed folds CNN_2 dropout\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='CNN2_dropout_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280332,"status":"ok","timestamp":1690368308902,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"wfogZnuZrmJp","outputId":"5b9e9044-95ef-4a61-c407-f23e535d6a63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07953594 0.07624164]\n"," [0.0858584  0.08280423]\n"," [0.05980194 0.05644431]\n"," [0.05575079 0.0533058 ]]\n","\n","Average MAE Loss:\n","[0.07788879 0.08433132 0.05812313 0.0545283 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07526286 0.07202254]\n"," [0.0858584  0.08280423]\n"," [0.051752   0.05004802]\n"," [0.05443482 0.05243137]]\n","\n","Average MAE Loss:\n","[0.0736427  0.08433132 0.05090001 0.0534331 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07593101 0.07298071]\n"," [0.0858584  0.08280423]\n"," [0.05155187 0.04938125]\n"," [0.05168367 0.04962032]]\n","\n","Average MAE Loss:\n","[0.07445586 0.08433132 0.05046656 0.050652  ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07575615 0.0727097 ]\n"," [0.0858584  0.08280423]\n"," [0.05093988 0.04871966]\n"," [0.05181687 0.05005123]]\n","\n","Average MAE Loss:\n","[0.07423293 0.08433132 0.04982977 0.05093405]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07508878 0.07196832]\n"," [0.0858584  0.08280423]\n"," [0.05140737 0.04912954]\n"," [0.05031849 0.04852774]]\n","\n","Average MAE Loss:\n","[0.07352855 0.08433132 0.05026846 0.04942311]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07548441 0.07252302]\n"," [0.0858584  0.08280423]\n"," [0.05081621 0.04879475]\n"," [0.04984302 0.04795679]]\n","\n","Average MAE Loss:\n","[0.07400372 0.08433132 0.04980548 0.04889991]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07500023 0.07189748]\n"," [0.0858584  0.08280423]\n"," [0.05126519 0.04883164]\n"," [0.05033181 0.04833591]]\n","\n","Average MAE Loss:\n","[0.07344885 0.08433132 0.05004841 0.04933386]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05497375 0.05106822]\n"," [0.05033181 0.04833591]\n"," [0.07512148 0.07239046]\n"," [0.05195426 0.04996295]]\n","\n","Average MAE Loss:\n","[0.05302098 0.04933386 0.07375597 0.0509586 ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05052563 0.04732632]\n"," [0.05033181 0.04833591]\n"," [0.07405855 0.07098314]\n"," [0.05096031 0.04906904]]\n","\n","Average MAE Loss:\n","[0.04892597 0.04933386 0.07252085 0.05001467]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05000094 0.0469696 ]\n"," [0.05033181 0.04833591]\n"," [0.07401142 0.0709887 ]\n"," [0.05098611 0.04896863]]\n","\n","Average MAE Loss:\n","[0.04848527 0.04933386 0.07250006 0.04997737]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04961223 0.04655439]\n"," [0.05033181 0.04833591]\n"," [0.07373028 0.07058777]\n"," [0.04967861 0.04756207]]\n","\n","Average MAE Loss:\n","[0.04808331 0.04933386 0.07215902 0.04862034]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04962525 0.04663923]\n"," [0.05033181 0.04833591]\n"," [0.07376651 0.07059058]\n"," [0.05008778 0.0478222 ]]\n","\n","Average MAE Loss:\n","[0.04813224 0.04933386 0.07217854 0.04895499]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04946927 0.0464415 ]\n"," [0.05033181 0.04833591]\n"," [0.07369105 0.07047451]\n"," [0.04965491 0.0475872 ]]\n","\n","Average MAE Loss:\n","[0.04795539 0.04933386 0.07208278 0.04862105]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04941279 0.0463704 ]\n"," [0.05033181 0.04833591]\n"," [0.0735656  0.07035877]\n"," [0.04936051 0.04750475]]\n","\n","Average MAE Loss:\n","[0.0478916  0.04933386 0.07196218 0.04843263]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04650773 0.04405822]\n"," [0.08196179 0.07885014]\n"," [0.0536813  0.05124326]\n"," [0.04800935 0.04557456]]\n","\n","Average MAE Loss:\n","[0.04528297 0.08040597 0.05246228 0.04679196]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04339476 0.04080428]\n"," [0.08196179 0.07885014]\n"," [0.04318632 0.04121666]\n"," [0.04341605 0.04159524]]\n","\n","Average MAE Loss:\n","[0.04209952 0.08040597 0.04220149 0.04250564]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04305585 0.04029981]\n"," [0.08196179 0.07885014]\n"," [0.04322411 0.04141075]\n"," [0.04261833 0.04089932]]\n","\n","Average MAE Loss:\n","[0.04167783 0.08040597 0.04231743 0.04175882]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04290546 0.0401176 ]\n"," [0.08196179 0.07885014]\n"," [0.04279801 0.04071859]\n"," [0.04201301 0.04019693]]\n","\n","Average MAE Loss:\n","[0.04151153 0.08040597 0.0417583  0.04110497]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04310037 0.04019514]\n"," [0.08196179 0.07885014]\n"," [0.04286258 0.04077304]\n"," [0.04199922 0.04020564]]\n","\n","Average MAE Loss:\n","[0.04164776 0.08040597 0.04181781 0.04110243]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04316725 0.04021292]\n"," [0.08196179 0.07885014]\n"," [0.04275948 0.04064289]\n"," [0.04185354 0.04019801]]\n","\n","Average MAE Loss:\n","[0.04169008 0.08040597 0.04170119 0.04102578]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04290898 0.04008357]\n"," [0.08196179 0.07885014]\n"," [0.04279888 0.04069213]\n"," [0.04194944 0.04022001]]\n","\n","Average MAE Loss:\n","[0.04149628 0.08040597 0.0417455  0.04108472]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0576215  0.0552636 ]\n"," [0.04194944 0.04022001]\n"," [0.04275593 0.04045613]\n"," [0.04184012 0.04010262]]\n","\n","Average MAE Loss:\n","[0.05644255 0.04108472 0.04160603 0.04097137]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0445975  0.0422273 ]\n"," [0.04194944 0.04022001]\n"," [0.04285835 0.04054668]\n"," [0.04200075 0.04019224]]\n","\n","Average MAE Loss:\n","[0.0434124  0.04108472 0.04170251 0.0410965 ]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04338917 0.04074337]\n"," [0.04194944 0.04022001]\n"," [0.04292938 0.04058461]\n"," [0.04191067 0.04010914]]\n","\n","Average MAE Loss:\n","[0.04206627 0.04108472 0.041757   0.0410099 ]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04341323 0.04054683]\n"," [0.04194944 0.04022001]\n"," [0.04290337 0.04060078]\n"," [0.04189691 0.04011212]]\n","\n","Average MAE Loss:\n","[0.04198003 0.04108472 0.04175208 0.04100452]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04273884 0.04020386]\n"," [0.04194944 0.04022001]\n"," [0.0429096  0.04061481]\n"," [0.04188535 0.04008132]]\n","\n","Average MAE Loss:\n","[0.04147135 0.04108472 0.04176221 0.04098334]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04263751 0.04006226]\n"," [0.04194944 0.04022001]\n"," [0.04296705 0.04068651]\n"," [0.04182854 0.04009202]]\n","\n","Average MAE Loss:\n","[0.04134988 0.04108472 0.04182678 0.04096028]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0426531  0.0400538 ]\n"," [0.04194944 0.04022001]\n"," [0.04292404 0.04064728]\n"," [0.04177946 0.04002457]]\n","\n","Average MAE Loss:\n","[0.04135345 0.04108472 0.04178566 0.04090201]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04213372 0.03953223]\n"," [0.04312941 0.04068884]\n"," [0.04153004 0.03935598]\n"," [0.04180911 0.03982779]]\n","\n","Average MAE Loss:\n","[0.04083298 0.04190913 0.04044301 0.04081845]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.041947   0.03917992]\n"," [0.04312941 0.04068884]\n"," [0.04201198 0.03995045]\n"," [0.04147056 0.03946607]]\n","\n","Average MAE Loss:\n","[0.04056346 0.04190913 0.04098121 0.04046831]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04190879 0.03921129]\n"," [0.04312941 0.04068884]\n"," [0.04196185 0.03980951]\n"," [0.04155112 0.03959303]]\n","\n","Average MAE Loss:\n","[0.04056004 0.04190913 0.04088568 0.04057208]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04188904 0.03915244]\n"," [0.04312941 0.04068884]\n"," [0.04211359 0.03994132]\n"," [0.04154098 0.0395864 ]]\n","\n","Average MAE Loss:\n","[0.04052074 0.04190913 0.04102746 0.04056369]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04186767 0.03916853]\n"," [0.04312941 0.04068884]\n"," [0.04219261 0.03999385]\n"," [0.04156466 0.03964881]]\n","\n","Average MAE Loss:\n","[0.0405181  0.04190913 0.04109323 0.04060674]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04184833 0.03913661]\n"," [0.04312941 0.04068884]\n"," [0.04221673 0.04001549]\n"," [0.04153691 0.03966142]]\n","\n","Average MAE Loss:\n","[0.04049247 0.04190913 0.04111611 0.04059917]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04187587 0.03917987]\n"," [0.04312941 0.04068884]\n"," [0.04224065 0.04004081]\n"," [0.04153616 0.03963514]]\n","\n","Average MAE Loss:\n","[0.04052787 0.04190913 0.04114073 0.04058565]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04223334 0.03968046]\n"," [0.04153616 0.03963514]\n"," [0.04176521 0.03918309]\n"," [0.04156958 0.03942724]]\n","\n","Average MAE Loss:\n","[0.0409569  0.04058565 0.04047415 0.04049841]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04196652 0.03913951]\n"," [0.04153616 0.03963514]\n"," [0.04172907 0.03929617]\n"," [0.04125539 0.03918755]]\n","\n","Average MAE Loss:\n","[0.04055301 0.04058565 0.04051262 0.04022147]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04184104 0.03921087]\n"," [0.04153616 0.03963514]\n"," [0.04173541 0.03934644]\n"," [0.04126382 0.03921731]]\n","\n","Average MAE Loss:\n","[0.04052596 0.04058565 0.04054093 0.04024057]\n","\n","Epoch 00038: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04187937 0.03925229]\n"," [0.04153616 0.03963514]\n"," [0.04175785 0.0393897 ]\n"," [0.04133481 0.03929432]]\n","\n","Average MAE Loss:\n","[0.04056583 0.04058565 0.04057377 0.04031457]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04180519 0.03914499]\n"," [0.04153616 0.03963514]\n"," [0.04178119 0.03942474]\n"," [0.04140919 0.03936472]]\n","\n","Average MAE Loss:\n","[0.04047509 0.04058565 0.04060297 0.04038696]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04179729 0.03918184]\n"," [0.04153616 0.03963514]\n"," [0.04180153 0.0394619 ]\n"," [0.04145351 0.03943951]]\n","\n","Average MAE Loss:\n","[0.04048957 0.04058565 0.04063172 0.04044651]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04184059 0.03918409]\n"," [0.04153616 0.03963514]\n"," [0.04181141 0.03948136]\n"," [0.04146607 0.03946287]]\n","\n","Average MAE Loss:\n","[0.04051234 0.04058565 0.04064638 0.04046447]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04148248 0.03901811]\n"," [0.04129188 0.03905506]\n"," [0.04130137 0.03907176]\n"," [0.04127647 0.03905646]]\n","\n","Average MAE Loss:\n","[0.0402503  0.04017347 0.04018657 0.04016646]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04179042 0.03913756]\n"," [0.04129188 0.03905506]\n"," [0.04132596 0.03910267]\n"," [0.04128405 0.03910935]]\n","\n","Average MAE Loss:\n","[0.04046399 0.04017347 0.04021431 0.0401967 ]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04179519 0.03914779]\n"," [0.04129188 0.03905506]\n"," [0.04135737 0.03914115]\n"," [0.04131128 0.03918045]]\n","\n","Average MAE Loss:\n","[0.04047149 0.04017347 0.04024926 0.04024587]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04178075 0.0391511 ]\n"," [0.04129188 0.03905506]\n"," [0.04139159 0.03918232]\n"," [0.04134637 0.03924419]]\n","\n","Average MAE Loss:\n","[0.04046592 0.04017347 0.04028695 0.04029528]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04177888 0.03915319]\n"," [0.04129188 0.03905506]\n"," [0.04143031 0.03922233]\n"," [0.04138679 0.03930619]]\n","\n","Average MAE Loss:\n","[0.04046604 0.04017347 0.04032632 0.04034649]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04179583 0.03919337]\n"," [0.04129188 0.03905506]\n"," [0.04144936 0.03924083]\n"," [0.04140341 0.03933305]]\n","\n","Average MAE Loss:\n","[0.0404946  0.04017347 0.0403451  0.04036823]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04177905 0.03917634]\n"," [0.04129188 0.03905506]\n"," [0.04146752 0.03925807]\n"," [0.04141311 0.03935496]]\n","\n","Average MAE Loss:\n","[0.0404777  0.04017347 0.0403628  0.04038403]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04137361 0.03900712]\n"," [0.04141311 0.03935496]\n"," [0.04176365 0.03917336]\n"," [0.04139838 0.03921377]]\n","\n","Average MAE Loss:\n","[0.04019037 0.04038403 0.0404685  0.04030607]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04157103 0.0390599 ]\n"," [0.04141311 0.03935496]\n"," [0.04174448 0.03916994]\n"," [0.04133294 0.03916617]]\n","\n","Average MAE Loss:\n","[0.04031547 0.04038403 0.04045721 0.04024955]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00051: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04167601 0.03911136]\n"," [0.04141311 0.03935496]\n"," [0.04173598 0.03916933]\n"," [0.04131496 0.03915612]]\n","\n","Average MAE Loss:\n","[0.04039369 0.04038403 0.04045265 0.04023554]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04171927 0.03912614]\n"," [0.04141311 0.03935496]\n"," [0.04172789 0.03916897]\n"," [0.04130597 0.03915249]]\n","\n","Average MAE Loss:\n","[0.0404227  0.04038403 0.04044843 0.04022923]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04173799 0.03913864]\n"," [0.04141311 0.03935496]\n"," [0.04172037 0.03916909]\n"," [0.04130296 0.03915403]]\n","\n","Average MAE Loss:\n","[0.04043832 0.04038403 0.04044473 0.0402285 ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04175262 0.03915404]\n"," [0.04141311 0.03935496]\n"," [0.04171365 0.03916947]\n"," [0.04130259 0.03915759]]\n","\n","Average MAE Loss:\n","[0.04045333 0.04038403 0.04044156 0.04023009]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00055: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04175707 0.03915652]\n"," [0.04141311 0.03935496]\n"," [0.04171055 0.03916981]\n"," [0.04130282 0.03916021]]\n","\n","Average MAE Loss:\n","[0.04045679 0.04038403 0.04044018 0.04023151]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04141547 0.03901643]\n"," [0.04135706 0.03901347]\n"," [0.04135587 0.0390156 ]\n"," [0.04134853 0.03901043]]\n","\n","Average MAE Loss:\n","[0.04021595 0.04018527 0.04018573 0.04017948]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04151225 0.0390481 ]\n"," [0.04135706 0.03901347]\n"," [0.04135595 0.03901823]\n"," [0.04133828 0.03900749]]\n","\n","Average MAE Loss:\n","[0.04028018 0.04018527 0.04018709 0.04017289]\n","\n","Epoch 00058: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04155843 0.03906878]\n"," [0.04135706 0.03901347]\n"," [0.04135632 0.03902093]\n"," [0.04132975 0.03900579]]\n","\n","Average MAE Loss:\n","[0.0403136  0.04018527 0.04018862 0.04016777]\n","\n","Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00059: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00059: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04159205 0.039087  ]\n"," [0.04135706 0.03901347]\n"," [0.04135652 0.03902228]\n"," [0.04132586 0.0390051 ]]\n","\n","Average MAE Loss:\n","[0.04033953 0.04018527 0.0401894  0.04016548]\n","\n","\n","epochs finished with time:38.93275713920593\n","\n","[[0.04159205 0.039087  ]\n"," [0.04135706 0.03901347]\n"," [0.04135652 0.03902228]\n"," [0.04132586 0.0390051 ]]\n","00:00:59.849600702000316\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10010319 0.09973624]\n"," [0.11635957 0.1145391 ]\n"," [0.08138124 0.08211665]\n"," [0.09005698 0.09005003]]\n","\n","Average MAE Loss:\n","[0.09991971 0.11544933 0.08174894 0.0900535 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.09944655 0.09871746]\n"," [0.11635957 0.1145391 ]\n"," [0.0777366  0.07868388]\n"," [0.08019902 0.07642323]]\n","\n","Average MAE Loss:\n","[0.09908201 0.11544933 0.07821024 0.07831112]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.09901552 0.0983677 ]\n"," [0.11635957 0.1145391 ]\n"," [0.07587604 0.0772184 ]\n"," [0.07802104 0.07444378]]\n","\n","Average MAE Loss:\n","[0.09869161 0.11544933 0.07654722 0.07623241]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.09872436 0.09791365]\n"," [0.11635957 0.1145391 ]\n"," [0.07726764 0.07749708]\n"," [0.07669795 0.07347084]]\n","\n","Average MAE Loss:\n","[0.09831901 0.11544933 0.07738236 0.07508439]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.098619   0.09820344]\n"," [0.11635957 0.1145391 ]\n"," [0.07636625 0.07691087]\n"," [0.07877701 0.07451286]]\n","\n","Average MAE Loss:\n","[0.09841122 0.11544933 0.07663856 0.07664493]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.09865076 0.09834677]\n"," [0.11635957 0.1145391 ]\n"," [0.07711515 0.07741723]\n"," [0.07994574 0.07335907]]\n","\n","Average MAE Loss:\n","[0.09849876 0.11544933 0.07726619 0.0766524 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.09870503 0.09787992]\n"," [0.11635957 0.1145391 ]\n"," [0.07674683 0.07726086]\n"," [0.07520279 0.07475662]]\n","\n","Average MAE Loss:\n","[0.09829247 0.11544933 0.07700385 0.0749797 ]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07500416 0.0767665 ]\n"," [0.07520279 0.07475662]\n"," [0.08292913 0.08441792]\n"," [0.07755936 0.07710342]]\n","\n","Average MAE Loss:\n","[0.07588533 0.0749797  0.08367352 0.07733139]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07382366 0.07495382]\n"," [0.07520279 0.07475662]\n"," [0.07316038 0.072826  ]\n"," [0.07831218 0.07755902]]\n","\n","Average MAE Loss:\n","[0.07438874 0.0749797  0.07299319 0.0779356 ]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07296287 0.07449551]\n"," [0.07520279 0.07475662]\n"," [0.07182466 0.07191118]\n"," [0.08003755 0.07876911]]\n","\n","Average MAE Loss:\n","[0.07372919 0.0749797  0.07186792 0.07940333]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07305148 0.07389723]\n"," [0.07520279 0.07475662]\n"," [0.07128855 0.07139908]\n"," [0.08259013 0.08090728]]\n","\n","Average MAE Loss:\n","[0.07347435 0.0749797  0.07134382 0.08174871]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07298151 0.07390559]\n"," [0.07520279 0.07475662]\n"," [0.07068695 0.07107943]\n"," [0.07499672 0.07551957]]\n","\n","Average MAE Loss:\n","[0.07344355 0.0749797  0.07088319 0.07525814]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07307578 0.07378905]\n"," [0.07520279 0.07475662]\n"," [0.07044786 0.07079825]\n"," [0.07535465 0.07549032]]\n","\n","Average MAE Loss:\n","[0.07343241 0.0749797  0.07062306 0.07542249]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07331209 0.07392489]\n"," [0.07520279 0.07475662]\n"," [0.07061434 0.07089161]\n"," [0.07610386 0.07578305]]\n","\n","Average MAE Loss:\n","[0.07361849 0.0749797  0.07075298 0.07594346]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06958305 0.07067614]\n"," [0.11189005 0.11004281]\n"," [0.06914743 0.07045622]\n"," [0.07187632 0.07148181]]\n","\n","Average MAE Loss:\n","[0.07012959 0.11096643 0.06980183 0.07167906]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06878389 0.06919273]\n"," [0.11189005 0.11004281]\n"," [0.06846789 0.06949045]\n"," [0.06769185 0.06749535]]\n","\n","Average MAE Loss:\n","[0.06898831 0.11096643 0.06897917 0.0675936 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06744821 0.06793498]\n"," [0.11189005 0.11004281]\n"," [0.06643256 0.06763741]\n"," [0.06571151 0.06615514]]\n","\n","Average MAE Loss:\n","[0.06769159 0.11096643 0.06703498 0.06593332]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06713889 0.06769496]\n"," [0.11189005 0.11004281]\n"," [0.06642421 0.06732768]\n"," [0.0664345  0.06592177]]\n","\n","Average MAE Loss:\n","[0.06741692 0.11096643 0.06687595 0.06617814]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06721379 0.06787496]\n"," [0.11189005 0.11004281]\n"," [0.06638627 0.06734339]\n"," [0.06589507 0.06575253]]\n","\n","Average MAE Loss:\n","[0.06754438 0.11096643 0.06686483 0.0658238 ]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06714422 0.06766582]\n"," [0.11189005 0.11004281]\n"," [0.06619799 0.06728895]\n"," [0.06541525 0.06552219]]\n","\n","Average MAE Loss:\n","[0.06740502 0.11096643 0.06674347 0.06546872]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0668222  0.06740208]\n"," [0.11189005 0.11004281]\n"," [0.06629174 0.06723163]\n"," [0.06590937 0.06569253]]\n","\n","Average MAE Loss:\n","[0.06711214 0.11096643 0.06676168 0.06580095]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07066225 0.073316  ]\n"," [0.06590937 0.06569253]\n"," [0.06553842 0.06690651]\n"," [0.06640474 0.06653314]]\n","\n","Average MAE Loss:\n","[0.07198913 0.06580095 0.06622246 0.06646894]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06755292 0.06846793]\n"," [0.06590937 0.06569253]\n"," [0.06590226 0.06735937]\n"," [0.06616072 0.06607553]]\n","\n","Average MAE Loss:\n","[0.06801043 0.06580095 0.06663082 0.06611812]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06742278 0.06778713]\n"," [0.06590937 0.06569253]\n"," [0.06598115 0.06723911]\n"," [0.06598334 0.06588876]]\n","\n","Average MAE Loss:\n","[0.06760496 0.06580095 0.06661013 0.06593605]\n","\n","Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.06794713 0.06835351]\n"," [0.06590937 0.06569253]\n"," [0.06596662 0.06730195]\n"," [0.06611873 0.06590958]]\n","\n","Average MAE Loss:\n","[0.06815032 0.06580095 0.06663429 0.06601415]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06545751 0.06647718]\n"," [0.06590937 0.06569253]\n"," [0.06587098 0.06717921]\n"," [0.06612169 0.06594248]]\n","\n","Average MAE Loss:\n","[0.06596735 0.06580095 0.0665251  0.06603208]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06558101 0.06644753]\n"," [0.06590937 0.06569253]\n"," [0.06592916 0.06716821]\n"," [0.06603295 0.06592115]]\n","\n","Average MAE Loss:\n","[0.06601427 0.06580095 0.06654868 0.06597705]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06585615 0.06657865]\n"," [0.06590937 0.06569253]\n"," [0.06587198 0.0671915 ]\n"," [0.06605879 0.06597415]]\n","\n","Average MAE Loss:\n","[0.0662174  0.06580095 0.06653174 0.06601647]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06379168 0.06498979]\n"," [0.06922631 0.06863954]\n"," [0.06471766 0.06554997]\n"," [0.06588922 0.06600795]]\n","\n","Average MAE Loss:\n","[0.06439074 0.06893292 0.06513381 0.06594858]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06476923 0.06554656]\n"," [0.06922631 0.06863954]\n"," [0.0648721  0.0660525 ]\n"," [0.06474165 0.06524379]]\n","\n","Average MAE Loss:\n","[0.0651579  0.06893292 0.0654623  0.06499272]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06477225 0.06562673]\n"," [0.06922631 0.06863954]\n"," [0.06512239 0.06630515]\n"," [0.06496765 0.06517544]]\n","\n","Average MAE Loss:\n","[0.06519949 0.06893292 0.06571377 0.06507154]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06437297 0.06536118]\n"," [0.06922631 0.06863954]\n"," [0.06503684 0.0662847 ]\n"," [0.06503404 0.06516715]]\n","\n","Average MAE Loss:\n","[0.06486708 0.06893292 0.06566077 0.0651006 ]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06452612 0.06543521]\n"," [0.06922631 0.06863954]\n"," [0.06507413 0.06628967]\n"," [0.06505701 0.06518213]]\n","\n","Average MAE Loss:\n","[0.06498066 0.06893292 0.0656819  0.06511957]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06426641 0.06525182]\n"," [0.06922631 0.06863954]\n"," [0.06504165 0.06627924]\n"," [0.06510977 0.0651923 ]]\n","\n","Average MAE Loss:\n","[0.06475911 0.06893292 0.06566045 0.06515104]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06432717 0.06525593]\n"," [0.06922631 0.06863954]\n"," [0.06506904 0.06629316]\n"," [0.06510634 0.06517991]]\n","\n","Average MAE Loss:\n","[0.06479155 0.06893292 0.0656811  0.06514313]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06423419 0.06512928]\n"," [0.06510634 0.06517991]\n"," [0.06418933 0.06524746]\n"," [0.06498111 0.06608375]]\n","\n","Average MAE Loss:\n","[0.06468174 0.06514313 0.0647184  0.06553243]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06435014 0.06509338]\n"," [0.06510634 0.06517991]\n"," [0.06417333 0.0654182 ]\n"," [0.06491218 0.06581279]]\n","\n","Average MAE Loss:\n","[0.06472176 0.06514313 0.06479577 0.06536248]\n","\n","Epoch 00037: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06436105 0.06516376]\n"," [0.06510634 0.06517991]\n"," [0.06434138 0.06567761]\n"," [0.06485134 0.06561822]]\n","\n","Average MAE Loss:\n","[0.06476241 0.06514313 0.06500949 0.06523478]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06423554 0.0651333 ]\n"," [0.06510634 0.06517991]\n"," [0.06450799 0.06586747]\n"," [0.06481844 0.0655447 ]]\n","\n","Average MAE Loss:\n","[0.06468442 0.06514313 0.06518773 0.06518157]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06428124 0.06517219]\n"," [0.06510634 0.06517991]\n"," [0.06466603 0.06599364]\n"," [0.06478743 0.0654846 ]]\n","\n","Average MAE Loss:\n","[0.06472672 0.06514313 0.06532984 0.06513601]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06425196 0.06517555]\n"," [0.06510634 0.06517991]\n"," [0.06472542 0.06603802]\n"," [0.06476808 0.0654387 ]]\n","\n","Average MAE Loss:\n","[0.06471375 0.06514313 0.06538172 0.06510339]\n","\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06424248 0.06517421]\n"," [0.06510634 0.06517991]\n"," [0.06476213 0.06607443]\n"," [0.06475883 0.06539789]]\n","\n","Average MAE Loss:\n","[0.06470835 0.06514313 0.06541828 0.06507836]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0642906  0.0650757 ]\n"," [0.06434333 0.06513328]\n"," [0.06433391 0.06521059]\n"," [0.06436254 0.06511704]]\n","\n","Average MAE Loss:\n","[0.06468315 0.06473831 0.06477225 0.06473979]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06424574 0.06503513]\n"," [0.06434333 0.06513328]\n"," [0.06433563 0.06534717]\n"," [0.06438957 0.06510584]]\n","\n","Average MAE Loss:\n","[0.06464043 0.06473831 0.0648414  0.06474771]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06423811 0.06502669]\n"," [0.06434333 0.06513328]\n"," [0.06436159 0.06542241]\n"," [0.06441495 0.06509738]]\n","\n","Average MAE Loss:\n","[0.0646324  0.06473831 0.064892   0.06475616]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06423495 0.06502362]\n"," [0.06434333 0.06513328]\n"," [0.06439824 0.06549633]\n"," [0.0644347  0.06509018]]\n","\n","Average MAE Loss:\n","[0.06462929 0.06473831 0.06494729 0.06476244]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06423012 0.06502197]\n"," [0.06434333 0.06513328]\n"," [0.06443897 0.06556156]\n"," [0.0644515  0.06508513]]\n","\n","Average MAE Loss:\n","[0.06462604 0.06473831 0.06500027 0.06476831]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06422449 0.06502185]\n"," [0.06434333 0.06513328]\n"," [0.06448009 0.0656196 ]\n"," [0.06446003 0.06508334]]\n","\n","Average MAE Loss:\n","[0.06462317 0.06473831 0.06504984 0.06477169]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06422365 0.06502278]\n"," [0.06434333 0.06513328]\n"," [0.06449795 0.06564499]\n"," [0.06446871 0.06508136]]\n","\n","Average MAE Loss:\n","[0.06462321 0.06473831 0.06507147 0.06477504]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.5625e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06432577 0.06511433]\n"," [0.06446871 0.06508136]\n"," [0.06420662 0.06503186]\n"," [0.06450483 0.06562635]]\n","\n","Average MAE Loss:\n","[0.06472005 0.06477504 0.06461924 0.06506559]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06430285 0.06508983]\n"," [0.06446871 0.06508136]\n"," [0.06417815 0.06503914]\n"," [0.0645152  0.06560175]]\n","\n","Average MAE Loss:\n","[0.06469634 0.06477504 0.06460865 0.06505848]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06428021 0.06506979]\n"," [0.06446871 0.06508136]\n"," [0.06415489 0.06504875]\n"," [0.06452081 0.06558976]]\n","\n","Average MAE Loss:\n","[0.064675   0.06477504 0.06460182 0.06505529]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06425909 0.06505297]\n"," [0.06446871 0.06508136]\n"," [0.06413915 0.06506595]\n"," [0.064526   0.06557789]]\n","\n","Average MAE Loss:\n","[0.06465603 0.06477504 0.06460255 0.06505195]\n","\n","Epoch 00053: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06424967 0.06504406]\n"," [0.06446871 0.06508136]\n"," [0.06413323 0.06508528]\n"," [0.06453089 0.06556619]]\n","\n","Average MAE Loss:\n","[0.06464686 0.06477504 0.06460926 0.06504854]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0642419  0.06503571]\n"," [0.06446871 0.06508136]\n"," [0.06412992 0.06510574]\n"," [0.06453553 0.06555489]]\n","\n","Average MAE Loss:\n","[0.06463881 0.06477504 0.06461783 0.06504521]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0642363  0.06502827]\n"," [0.06446871 0.06508136]\n"," [0.06413262 0.06513042]\n"," [0.06453783 0.06554939]]\n","\n","Average MAE Loss:\n","[0.06463228 0.06477504 0.06463152 0.06504361]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06427511 0.0651168 ]\n"," [0.06428474 0.06512685]\n"," [0.06428258 0.06514025]\n"," [0.06428916 0.06512426]]\n","\n","Average MAE Loss:\n","[0.06469596 0.0647058  0.06471142 0.06470671]\n","\n","Epoch 00057: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06427054 0.06511195]\n"," [0.06428474 0.06512685]\n"," [0.06427737 0.06515457]\n"," [0.06429374 0.06512215]]\n","\n","Average MAE Loss:\n","[0.06469125 0.0647058  0.06471597 0.06470795]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.064266   0.06510711]\n"," [0.06428474 0.06512685]\n"," [0.0642737  0.06516969]\n"," [0.06429834 0.06512006]]\n","\n","Average MAE Loss:\n","[0.06468655 0.0647058  0.06472169 0.0647092 ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06426111 0.06510242]\n"," [0.06428474 0.06512685]\n"," [0.06427207 0.0651862 ]\n"," [0.06430292 0.06511792]]\n","\n","Average MAE Loss:\n","[0.06468177 0.0647058  0.06472913 0.06471042]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:38.481250047683716\n","\n","[[0.06426111 0.06510242]\n"," [0.06428474 0.06512685]\n"," [0.06427207 0.0651862 ]\n"," [0.06430292 0.06511792]]\n","00:00:57.278133005999734\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.0876302  0.07810691]\n"," [0.09980381 0.08862089]\n"," [0.06664632 0.05805141]\n"," [0.08133205 0.07304595]]\n","\n","Average MAE Loss:\n","[0.08286855 0.09421235 0.06234887 0.077189  ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08704995 0.07755018]\n"," [0.09980381 0.08862089]\n"," [0.06092377 0.05296127]\n"," [0.06256417 0.05421187]]\n","\n","Average MAE Loss:\n","[0.08230007 0.09421235 0.05694252 0.05838802]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08656142 0.07739579]\n"," [0.09980381 0.08862089]\n"," [0.06222128 0.05368961]\n"," [0.0606623  0.05272345]]\n","\n","Average MAE Loss:\n","[0.0819786  0.09421235 0.05795545 0.05669288]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08670259 0.0771831 ]\n"," [0.09980381 0.08862089]\n"," [0.06149464 0.05292086]\n"," [0.05926023 0.05200982]]\n","\n","Average MAE Loss:\n","[0.08194285 0.09421235 0.05720775 0.05563503]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08676803 0.07719994]\n"," [0.09980381 0.08862089]\n"," [0.06227444 0.05325157]\n"," [0.05891856 0.05080059]]\n","\n","Average MAE Loss:\n","[0.08198399 0.09421235 0.05776301 0.05485957]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08675861 0.07694364]\n"," [0.09980381 0.08862089]\n"," [0.06122532 0.05291061]\n"," [0.05900967 0.05051028]]\n","\n","Average MAE Loss:\n","[0.08185112 0.09421235 0.05706796 0.05475997]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08684701 0.07737873]\n"," [0.09980381 0.08862089]\n"," [0.05951662 0.05350574]\n"," [0.0587547  0.05044206]]\n","\n","Average MAE Loss:\n","[0.08211287 0.09421235 0.05651118 0.05459838]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06148761 0.05791071]\n"," [0.0587547  0.05044206]\n"," [0.08706862 0.07712132]\n"," [0.06250993 0.0553286 ]]\n","\n","Average MAE Loss:\n","[0.05969916 0.05459838 0.08209497 0.05891927]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05702335 0.05245878]\n"," [0.0587547  0.05044206]\n"," [0.08713922 0.07673948]\n"," [0.06079008 0.05222441]]\n","\n","Average MAE Loss:\n","[0.05474107 0.05459838 0.08193935 0.05650724]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05689412 0.05188406]\n"," [0.0587547  0.05044206]\n"," [0.08682095 0.07664032]\n"," [0.06089925 0.052023  ]]\n","\n","Average MAE Loss:\n","[0.05438909 0.05459838 0.08173063 0.05646113]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05863632 0.05127861]\n"," [0.0587547  0.05044206]\n"," [0.08623043 0.07655315]\n"," [0.06026005 0.05187715]]\n","\n","Average MAE Loss:\n","[0.05495746 0.05459838 0.08139179 0.0560686 ]\n","\n","Epoch 00011: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05835066 0.05129517]\n"," [0.0587547  0.05044206]\n"," [0.08611608 0.07680325]\n"," [0.06033184 0.05163538]]\n","\n","Average MAE Loss:\n","[0.05482292 0.05459838 0.08145967 0.05598361]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05698576 0.05086029]\n"," [0.0587547  0.05044206]\n"," [0.08604147 0.07639755]\n"," [0.05996122 0.05157332]]\n","\n","Average MAE Loss:\n","[0.05392302 0.05459838 0.08121951 0.05576727]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05783036 0.0509648 ]\n"," [0.0587547  0.05044206]\n"," [0.0860401  0.076441  ]\n"," [0.0595523  0.05163273]]\n","\n","Average MAE Loss:\n","[0.05439758 0.05459838 0.08124055 0.05559252]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05319677 0.04747199]\n"," [0.09550999 0.0843133 ]\n"," [0.06649327 0.0569821 ]\n"," [0.05871315 0.05498219]]\n","\n","Average MAE Loss:\n","[0.05033438 0.08991164 0.06173768 0.05684767]\n","\n","Epoch 00015: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05168691 0.04492842]\n"," [0.09550999 0.0843133 ]\n"," [0.05341683 0.04839644]\n"," [0.05185375 0.04434074]]\n","\n","Average MAE Loss:\n","[0.04830766 0.08991164 0.05090663 0.04809725]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05115961 0.04439794]\n"," [0.09550999 0.0843133 ]\n"," [0.05087835 0.04476354]\n"," [0.05112792 0.04399221]]\n","\n","Average MAE Loss:\n","[0.04777877 0.08991164 0.04782094 0.04756007]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05093048 0.04409028]\n"," [0.09550999 0.0843133 ]\n"," [0.05058198 0.04441112]\n"," [0.05074327 0.04385287]]\n","\n","Average MAE Loss:\n","[0.04751038 0.08991164 0.04749655 0.04729807]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.05061151 0.04404973]\n"," [0.09550999 0.0843133 ]\n"," [0.05049351 0.04439545]\n"," [0.05082716 0.04368159]]\n","\n","Average MAE Loss:\n","[0.04733062 0.08991164 0.04744448 0.04725437]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.05011465 0.04381421]\n"," [0.09550999 0.0843133 ]\n"," [0.05041865 0.04431575]\n"," [0.0507397  0.04371483]]\n","\n","Average MAE Loss:\n","[0.04696443 0.08991164 0.0473672  0.04722726]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0499676  0.043855  ]\n"," [0.09550999 0.0843133 ]\n"," [0.05036989 0.0442796 ]\n"," [0.05064653 0.04373521]]\n","\n","Average MAE Loss:\n","[0.0469113  0.08991164 0.04732474 0.04719087]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05316377 0.05072229]\n"," [0.05064653 0.04373521]\n"," [0.04919821 0.04401948]\n"," [0.05067464 0.04350206]]\n","\n","Average MAE Loss:\n","[0.05194303 0.04719087 0.04660884 0.04708835]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.05221912 0.04591438]\n"," [0.05064653 0.04373521]\n"," [0.04932222 0.04400388]\n"," [0.05059163 0.04377139]]\n","\n","Average MAE Loss:\n","[0.04906675 0.04719087 0.04666305 0.04718151]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05110085 0.04448695]\n"," [0.05064653 0.04373521]\n"," [0.04963876 0.04386105]\n"," [0.05065109 0.04366988]]\n","\n","Average MAE Loss:\n","[0.0477939  0.04719087 0.04674991 0.04716048]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05138148 0.04431845]\n"," [0.05064653 0.04373521]\n"," [0.04984306 0.04390916]\n"," [0.05061512 0.04367021]]\n","\n","Average MAE Loss:\n","[0.04784996 0.04719087 0.04687611 0.04714266]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04967467 0.0440588 ]\n"," [0.05064653 0.04373521]\n"," [0.04995871 0.04397735]\n"," [0.05059662 0.04373452]]\n","\n","Average MAE Loss:\n","[0.04686674 0.04719087 0.04696803 0.04716557]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04975712 0.04384523]\n"," [0.05064653 0.04373521]\n"," [0.05000753 0.04399174]\n"," [0.05057354 0.04374529]]\n","\n","Average MAE Loss:\n","[0.04680117 0.04719087 0.04699964 0.04715942]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04993906 0.04374545]\n"," [0.05064653 0.04373521]\n"," [0.05003866 0.04401299]\n"," [0.05057131 0.04373293]]\n","\n","Average MAE Loss:\n","[0.04684225 0.04719087 0.04702583 0.04715212]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.0493153  0.043268  ]\n"," [0.05422272 0.04547029]\n"," [0.0510284  0.04330517]\n"," [0.05022416 0.04301282]]\n","\n","Average MAE Loss:\n","[0.04629165 0.0498465  0.04716679 0.04661849]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04898373 0.04344384]\n"," [0.05422272 0.04547029]\n"," [0.04942711 0.04343118]\n"," [0.0498051  0.04342688]]\n","\n","Average MAE Loss:\n","[0.04621379 0.0498465  0.04642914 0.04661599]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04913552 0.04323489]\n"," [0.05422272 0.04547029]\n"," [0.04947923 0.0436239 ]\n"," [0.05008288 0.04319161]]\n","\n","Average MAE Loss:\n","[0.0461852  0.0498465  0.04655157 0.04663724]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04894725 0.0432645 ]\n"," [0.05422272 0.04547029]\n"," [0.04957923 0.04342498]\n"," [0.05011848 0.04335501]]\n","\n","Average MAE Loss:\n","[0.04610588 0.0498465  0.0465021  0.04673674]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.0490107  0.04323157]\n"," [0.05422272 0.04547029]\n"," [0.04965909 0.04346036]\n"," [0.05017104 0.04334594]]\n","\n","Average MAE Loss:\n","[0.04612114 0.0498465  0.04655972 0.04675849]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04910748 0.04324601]\n"," [0.05422272 0.04547029]\n"," [0.04969754 0.04350154]\n"," [0.05015291 0.04335984]]\n","\n","Average MAE Loss:\n","[0.04617675 0.0498465  0.04659954 0.04675638]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04900092 0.04324525]\n"," [0.05422272 0.04547029]\n"," [0.04970785 0.04351517]\n"," [0.0501456  0.04335578]]\n","\n","Average MAE Loss:\n","[0.04612308 0.0498465  0.04661151 0.04675069]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04891326 0.04368413]\n"," [0.0501456  0.04335578]\n"," [0.04889483 0.04322299]\n"," [0.04974104 0.04308321]]\n","\n","Average MAE Loss:\n","[0.04629869 0.04675069 0.04605891 0.04641213]\n","\n","Epoch 00036: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04914053 0.04319312]\n"," [0.0501456  0.04335578]\n"," [0.04879548 0.04322524]\n"," [0.04980772 0.04295063]]\n","\n","Average MAE Loss:\n","[0.04616683 0.04675069 0.04601036 0.04637918]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04879472 0.04325232]\n"," [0.0501456  0.04335578]\n"," [0.04877067 0.04325483]\n"," [0.04979624 0.04302923]]\n","\n","Average MAE Loss:\n","[0.04602352 0.04675069 0.04601275 0.04641273]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04889664 0.0431931 ]\n"," [0.0501456  0.04335578]\n"," [0.04879796 0.04328726]\n"," [0.0498311  0.04308038]]\n","\n","Average MAE Loss:\n","[0.04604487 0.04675069 0.04604261 0.04645574]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04886109 0.04322791]\n"," [0.0501456  0.04335578]\n"," [0.04885191 0.04331163]\n"," [0.04985042 0.04313017]]\n","\n","Average MAE Loss:\n","[0.0460445  0.04675069 0.04608177 0.04649029]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04887115 0.04321362]\n"," [0.0501456  0.04335578]\n"," [0.04892092 0.0433283 ]\n"," [0.04988001 0.04317738]]\n","\n","Average MAE Loss:\n","[0.04604239 0.04675069 0.04612461 0.04652869]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04890284 0.04321253]\n"," [0.0501456  0.04335578]\n"," [0.04895493 0.04333546]\n"," [0.04989093 0.04319856]]\n","\n","Average MAE Loss:\n","[0.04605768 0.04675069 0.0461452  0.04654475]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04916279 0.04276859]\n"," [0.04927827 0.0428227 ]\n"," [0.04926108 0.04284881]\n"," [0.04927977 0.04285153]]\n","\n","Average MAE Loss:\n","[0.04596569 0.04605049 0.04605495 0.04606565]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04898409 0.04284511]\n"," [0.04927827 0.0428227 ]\n"," [0.04923508 0.04290474]\n"," [0.04931118 0.04290352]]\n","\n","Average MAE Loss:\n","[0.0459146  0.04605049 0.04606991 0.04610735]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04889541 0.04291729]\n"," [0.04927827 0.0428227 ]\n"," [0.04921989 0.04296279]\n"," [0.04935693 0.04295613]]\n","\n","Average MAE Loss:\n","[0.04590635 0.04605049 0.04609134 0.04615653]\n","\n","Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04885501 0.04295807]\n"," [0.04927827 0.0428227 ]\n"," [0.04921641 0.04298915]\n"," [0.04941153 0.04300616]]\n","\n","Average MAE Loss:\n","[0.04590654 0.04605049 0.04610278 0.04620884]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04882432 0.04300083]\n"," [0.04927827 0.0428227 ]\n"," [0.04921585 0.04301384]\n"," [0.0494705  0.0430631 ]]\n","\n","Average MAE Loss:\n","[0.04591257 0.04605049 0.04611485 0.0462668 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04882429 0.04300984]\n"," [0.04927827 0.0428227 ]\n"," [0.04921749 0.04303927]\n"," [0.04949994 0.04308653]]\n","\n","Average MAE Loss:\n","[0.04591706 0.04605049 0.04612838 0.04629323]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04882891 0.04302573]\n"," [0.04927827 0.0428227 ]\n"," [0.04922035 0.04306458]\n"," [0.04952846 0.04310539]]\n","\n","Average MAE Loss:\n","[0.04592732 0.04605049 0.04614246 0.04631693]\n","\n","Epoch 00049: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00049: reducing learning rate of group 0 to 3.9063e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04920531 0.04278449]\n"," [0.04952846 0.04310539]\n"," [0.0488142  0.0430224 ]\n"," [0.04925631 0.04303491]]\n","\n","Average MAE Loss:\n","[0.0459949  0.04631693 0.0459183  0.04614561]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04909873 0.04278968]\n"," [0.04952846 0.04310539]\n"," [0.04879656 0.04301854]\n"," [0.04930598 0.04300222]]\n","\n","Average MAE Loss:\n","[0.0459442  0.04631693 0.04590755 0.0461541 ]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04901374 0.04282303]\n"," [0.04952846 0.04310539]\n"," [0.04877955 0.04301505]\n"," [0.04932847 0.04299287]]\n","\n","Average MAE Loss:\n","[0.04591838 0.04631693 0.0458973  0.04616067]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04895594 0.04285428]\n"," [0.04952846 0.04310539]\n"," [0.04876342 0.04301321]\n"," [0.04934873 0.04298701]]\n","\n","Average MAE Loss:\n","[0.04590511 0.04631693 0.04588831 0.04616787]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04892408 0.04286287]\n"," [0.04952846 0.04310539]\n"," [0.0487486  0.04301285]\n"," [0.04936724 0.0429833 ]]\n","\n","Average MAE Loss:\n","[0.04589348 0.04631693 0.04588073 0.04617527]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04889849 0.04287325]\n"," [0.04952846 0.04310539]\n"," [0.04873527 0.04301437]\n"," [0.04938415 0.04298107]]\n","\n","Average MAE Loss:\n","[0.04588587 0.04631693 0.04587482 0.04618261]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04888194 0.04288689]\n"," [0.04952846 0.04310539]\n"," [0.04872339 0.04301651]\n"," [0.04939182 0.04298111]]\n","\n","Average MAE Loss:\n","[0.04588442 0.04631693 0.04586995 0.04618646]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04898251 0.04279903]\n"," [0.04899655 0.04281314]\n"," [0.04898974 0.0428232 ]\n"," [0.04900136 0.04281276]]\n","\n","Average MAE Loss:\n","[0.04589077 0.04590485 0.04590647 0.04590706]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04896868 0.04279655]\n"," [0.04899655 0.04281314]\n"," [0.04898296 0.04283481]\n"," [0.04900574 0.04281355]]\n","\n","Average MAE Loss:\n","[0.04588262 0.04590485 0.04590889 0.04590965]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04894757 0.04280236]\n"," [0.04899655 0.04281314]\n"," [0.0489775  0.04284689]\n"," [0.04901038 0.04281485]]\n","\n","Average MAE Loss:\n","[0.04587497 0.04590485 0.04591219 0.04591262]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04891993 0.04281556]\n"," [0.04899655 0.04281314]\n"," [0.04897344 0.04285846]\n"," [0.04901516 0.04281608]]\n","\n","Average MAE Loss:\n","[0.04586775 0.04590485 0.04591595 0.04591562]\n","\n","Epoch 00060: reducing learning rate of group 0 to 1.9531e-06.\n","\n","epochs finished with time:38.04314875602722\n","\n","[[0.04891993 0.04281556]\n"," [0.04899655 0.04281314]\n"," [0.04897344 0.04285846]\n"," [0.04901516 0.04281608]]\n","00:00:53.41001607899989\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05643605 0.07372257]\n"," [0.08241    0.10488542]\n"," [0.09247705 0.10991617]\n"," [0.07581054 0.09699354]]\n","\n","Average MAE Loss:\n","[0.06507931 0.09364771 0.10119661 0.08640204]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04715456 0.0666171 ]\n"," [0.08241    0.10488542]\n"," [0.07076157 0.09252671]\n"," [0.05120014 0.07025217]]\n","\n","Average MAE Loss:\n","[0.05688583 0.09364771 0.08164414 0.06072616]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04687911 0.06622788]\n"," [0.08241    0.10488542]\n"," [0.07116695 0.09257179]\n"," [0.04874507 0.0687757 ]]\n","\n","Average MAE Loss:\n","[0.0565535  0.09364771 0.08186937 0.05876038]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04662097 0.06612793]\n"," [0.08241    0.10488542]\n"," [0.0703409  0.09228829]\n"," [0.04790583 0.0683587 ]]\n","\n","Average MAE Loss:\n","[0.05637445 0.09364771 0.08131459 0.05813226]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04680444 0.06643074]\n"," [0.08241    0.10488542]\n"," [0.07058699 0.09230561]\n"," [0.0492014  0.06857727]]\n","\n","Average MAE Loss:\n","[0.05661759 0.09364771 0.0814463  0.05888934]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04669288 0.06637599]\n"," [0.08241    0.10488542]\n"," [0.07035614 0.09220721]\n"," [0.0479804  0.068286  ]]\n","\n","Average MAE Loss:\n","[0.05653443 0.09364771 0.08128167 0.0581332 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04697342 0.06662017]\n"," [0.08241    0.10488542]\n"," [0.07031135 0.0919871 ]\n"," [0.04806891 0.06855149]]\n","\n","Average MAE Loss:\n","[0.05679679 0.09364771 0.08114922 0.0583102 ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04871762 0.06792695]\n"," [0.04806891 0.06855149]\n"," [0.04863857 0.06854562]\n"," [0.07068335 0.09217272]]\n","\n","Average MAE Loss:\n","[0.05832229 0.0583102  0.0585921  0.08142804]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04822526 0.0668154 ]\n"," [0.04806891 0.06855149]\n"," [0.04709369 0.06731196]\n"," [0.06945198 0.09108394]]\n","\n","Average MAE Loss:\n","[0.05752033 0.0583102  0.05720282 0.08026796]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04566019 0.06540865]\n"," [0.04806891 0.06855149]\n"," [0.04661117 0.06714908]\n"," [0.06931873 0.09089576]]\n","\n","Average MAE Loss:\n","[0.05553442 0.0583102  0.05688012 0.08010725]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04582468 0.06543755]\n"," [0.04806891 0.06855149]\n"," [0.04656038 0.0672674 ]\n"," [0.06922039 0.0909344 ]]\n","\n","Average MAE Loss:\n","[0.05563111 0.0583102  0.05691389 0.0800774 ]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04568836 0.06554049]\n"," [0.04806891 0.06855149]\n"," [0.04661248 0.06746343]\n"," [0.06916721 0.09080242]]\n","\n","Average MAE Loss:\n","[0.05561442 0.0583102  0.05703796 0.07998481]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04559202 0.06553947]\n"," [0.04806891 0.06855149]\n"," [0.04691381 0.06757537]\n"," [0.06956372 0.09090562]]\n","\n","Average MAE Loss:\n","[0.05556575 0.0583102  0.05724459 0.08023467]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04552282 0.06522749]\n"," [0.04806891 0.06855149]\n"," [0.04650478 0.06718177]\n"," [0.0693169  0.09078466]]\n","\n","Average MAE Loss:\n","[0.05537516 0.0583102  0.05684327 0.08005078]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04879305 0.06542082]\n"," [0.07773025 0.1002041 ]\n"," [0.044943   0.06569824]\n"," [0.05122653 0.07223503]]\n","\n","Average MAE Loss:\n","[0.05710693 0.08896717 0.05532062 0.06173078]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04210233 0.06125556]\n"," [0.07773025 0.1002041 ]\n"," [0.04138952 0.06193154]\n"," [0.0432888  0.06226066]]\n","\n","Average MAE Loss:\n","[0.05167894 0.08896717 0.05166053 0.05277473]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04123702 0.06023924]\n"," [0.07773025 0.1002041 ]\n"," [0.04120733 0.06200082]\n"," [0.04225977 0.06139559]]\n","\n","Average MAE Loss:\n","[0.05073813 0.08896717 0.05160407 0.05182768]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04144087 0.06008811]\n"," [0.07773025 0.1002041 ]\n"," [0.04134512 0.06170946]\n"," [0.04201108 0.06122107]]\n","\n","Average MAE Loss:\n","[0.05076449 0.08896717 0.05152729 0.05161608]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0414204  0.06000926]\n"," [0.07773025 0.1002041 ]\n"," [0.04128133 0.06168391]\n"," [0.04213576 0.06132007]]\n","\n","Average MAE Loss:\n","[0.05071483 0.08896717 0.05148262 0.05172791]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04142807 0.06010393]\n"," [0.07773025 0.1002041 ]\n"," [0.04122032 0.06153449]\n"," [0.04207808 0.06134481]]\n","\n","Average MAE Loss:\n","[0.050766   0.08896717 0.0513774  0.05171145]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04135057 0.06000841]\n"," [0.07773025 0.1002041 ]\n"," [0.04123208 0.06160764]\n"," [0.04212239 0.06131107]]\n","\n","Average MAE Loss:\n","[0.05067949 0.08896717 0.05141986 0.05171673]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05642982 0.07821909]\n"," [0.04212239 0.06131107]\n"," [0.04144048 0.06184651]\n"," [0.04102538 0.06103383]]\n","\n","Average MAE Loss:\n","[0.06732446 0.05171673 0.05164349 0.05102961]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04368912 0.06162569]\n"," [0.04212239 0.06131107]\n"," [0.04102444 0.06136774]\n"," [0.0412095  0.06092799]]\n","\n","Average MAE Loss:\n","[0.0526574  0.05171673 0.05119609 0.05106875]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04185307 0.06041763]\n"," [0.04212239 0.06131107]\n"," [0.0409708  0.06135668]\n"," [0.04124886 0.0609364 ]]\n","\n","Average MAE Loss:\n","[0.05113535 0.05171673 0.05116374 0.05109263]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04162524 0.0604908 ]\n"," [0.04212239 0.06131107]\n"," [0.04119173 0.06131962]\n"," [0.04136267 0.06098818]]\n","\n","Average MAE Loss:\n","[0.05105802 0.05171673 0.05125567 0.05117542]\n","\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04191036 0.06039008]\n"," [0.04212239 0.06131107]\n"," [0.04152467 0.06161715]\n"," [0.04143076 0.06098167]]\n","\n","Average MAE Loss:\n","[0.05115022 0.05171673 0.05157091 0.05120622]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04180841 0.06045143]\n"," [0.04212239 0.06131107]\n"," [0.04161195 0.06150023]\n"," [0.04143353 0.06099342]]\n","\n","Average MAE Loss:\n","[0.05112992 0.05171673 0.05155609 0.05121347]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04180753 0.06034239]\n"," [0.04212239 0.06131107]\n"," [0.04142302 0.06146991]\n"," [0.04145316 0.06100711]]\n","\n","Average MAE Loss:\n","[0.05107496 0.05171673 0.05144646 0.05123013]\n","\n","Epoch 00028: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04096194 0.05965795]\n"," [0.04181172 0.06221826]\n"," [0.04144107 0.06097613]\n"," [0.04071648 0.06035209]]\n","\n","Average MAE Loss:\n","[0.05030995 0.05201499 0.0512086  0.05053429]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04066589 0.05945071]\n"," [0.04181172 0.06221826]\n"," [0.04053543 0.06076109]\n"," [0.04114831 0.0603474 ]]\n","\n","Average MAE Loss:\n","[0.0500583  0.05201499 0.05064826 0.05074785]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04081018 0.05936139]\n"," [0.04181172 0.06221826]\n"," [0.04056371 0.06063586]\n"," [0.0408741  0.06022942]]\n","\n","Average MAE Loss:\n","[0.05008578 0.05201499 0.05059979 0.05055176]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04068678 0.05930897]\n"," [0.04181172 0.06221826]\n"," [0.04101804 0.06082216]\n"," [0.04098329 0.06024614]]\n","\n","Average MAE Loss:\n","[0.04999788 0.05201499 0.0509201  0.05061471]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04068784 0.0593008 ]\n"," [0.04181172 0.06221826]\n"," [0.04056374 0.06066128]\n"," [0.04100804 0.06026945]]\n","\n","Average MAE Loss:\n","[0.04999432 0.05201499 0.05061251 0.05063874]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04072127 0.05930828]\n"," [0.04181172 0.06221826]\n"," [0.0407581  0.06073966]\n"," [0.04101093 0.06026669]]\n","\n","Average MAE Loss:\n","[0.05001478 0.05201499 0.05074888 0.05063881]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04068745 0.05928251]\n"," [0.04181172 0.06221826]\n"," [0.04092932 0.06078182]\n"," [0.04101859 0.06026804]]\n","\n","Average MAE Loss:\n","[0.04998498 0.05201499 0.05085557 0.05064331]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04093451 0.05969465]\n"," [0.04101859 0.06026804]\n"," [0.04037346 0.05968796]\n"," [0.04058874 0.06045161]]\n","\n","Average MAE Loss:\n","[0.05031458 0.05064331 0.05003071 0.05052018]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04066073 0.05940432]\n"," [0.04101859 0.06026804]\n"," [0.04039709 0.06032272]\n"," [0.04040309 0.06018037]]\n","\n","Average MAE Loss:\n","[0.05003252 0.05064331 0.0503599  0.05029173]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04080219 0.05935443]\n"," [0.04101859 0.06026804]\n"," [0.04051864 0.06056757]\n"," [0.04042936 0.06006103]]\n","\n","Average MAE Loss:\n","[0.05007831 0.05064331 0.05054311 0.05024519]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04070137 0.05933261]\n"," [0.04101859 0.06026804]\n"," [0.04067612 0.06070416]\n"," [0.04048503 0.06000615]]\n","\n","Average MAE Loss:\n","[0.05001699 0.05064331 0.05069014 0.05024559]\n","\n","Epoch 00039: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.0407819  0.05935652]\n"," [0.04101859 0.06026804]\n"," [0.04066883 0.06073464]\n"," [0.04054608 0.0599894 ]]\n","\n","Average MAE Loss:\n","[0.05006921 0.05064331 0.05070174 0.05026774]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04076945 0.05935421]\n"," [0.04101859 0.06026804]\n"," [0.04076967 0.06077407]\n"," [0.04061513 0.06000059]]\n","\n","Average MAE Loss:\n","[0.05006183 0.05064331 0.05077187 0.05030786]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04077432 0.05935512]\n"," [0.04101859 0.06026804]\n"," [0.04076724 0.06080642]\n"," [0.04067063 0.06002278]]\n","\n","Average MAE Loss:\n","[0.05006472 0.05064331 0.05078683 0.05034671]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04038687 0.0594768 ]\n"," [0.04037463 0.05974215]\n"," [0.04036401 0.05988272]\n"," [0.04039127 0.05973352]]\n","\n","Average MAE Loss:\n","[0.04993184 0.05005839 0.05012336 0.0500624 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04054411 0.05931029]\n"," [0.04037463 0.05974215]\n"," [0.04045895 0.06013576]\n"," [0.0404217  0.05974075]]\n","\n","Average MAE Loss:\n","[0.0499272  0.05005839 0.05029736 0.05008123]\n","\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04066607 0.05928114]\n"," [0.04037463 0.05974215]\n"," [0.04051171 0.06023262]\n"," [0.04046709 0.05976316]]\n","\n","Average MAE Loss:\n","[0.04997361 0.05005839 0.05037216 0.05011512]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04070028 0.05928677]\n"," [0.04037463 0.05974215]\n"," [0.04055156 0.06031648]\n"," [0.04051364 0.05978715]]\n","\n","Average MAE Loss:\n","[0.04999353 0.05005839 0.05043402 0.0501504 ]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04071827 0.05929081]\n"," [0.04037463 0.05974215]\n"," [0.04059077 0.06039636]\n"," [0.04055831 0.05981178]]\n","\n","Average MAE Loss:\n","[0.05000454 0.05005839 0.05049356 0.05018504]\n","\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04073275 0.05929416]\n"," [0.04037463 0.05974215]\n"," [0.04061885 0.06044582]\n"," [0.04057835 0.05982459]]\n","\n","Average MAE Loss:\n","[0.05001345 0.05005839 0.05053233 0.05020147]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04074553 0.05929947]\n"," [0.04037463 0.05974215]\n"," [0.04064071 0.06047639]\n"," [0.04059761 0.05983689]]\n","\n","Average MAE Loss:\n","[0.0500225  0.05005839 0.05055855 0.05021725]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04035338 0.0595945 ]\n"," [0.04059761 0.05983689]\n"," [0.04069766 0.05932991]\n"," [0.04058679 0.06041969]]\n","\n","Average MAE Loss:\n","[0.04997394 0.05021725 0.05001379 0.05050324]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04041461 0.05944411]\n"," [0.04059761 0.05983689]\n"," [0.04062077 0.05937892]\n"," [0.04051402 0.06033572]]\n","\n","Average MAE Loss:\n","[0.04992936 0.05021725 0.04999984 0.05042487]\n","\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04045044 0.05939953]\n"," [0.04059761 0.05983689]\n"," [0.04055412 0.05943838]\n"," [0.04048482 0.06029779]]\n","\n","Average MAE Loss:\n","[0.04992499 0.05021725 0.04999625 0.05039131]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04048072 0.05936715]\n"," [0.04059761 0.05983689]\n"," [0.04050534 0.05949729]\n"," [0.04046149 0.06026474]]\n","\n","Average MAE Loss:\n","[0.04992393 0.05021725 0.05000132 0.05036311]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04051082 0.05934167]\n"," [0.04059761 0.05983689]\n"," [0.04047178 0.0595702 ]\n"," [0.04044238 0.06023586]]\n","\n","Average MAE Loss:\n","[0.04992624 0.05021725 0.05002099 0.05033912]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04054039 0.05932829]\n"," [0.04059761 0.05983689]\n"," [0.04045602 0.05964663]\n"," [0.04042745 0.06020983]]\n","\n","Average MAE Loss:\n","[0.04993434 0.05021725 0.05005132 0.05031864]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04056704 0.05932287]\n"," [0.04059761 0.05983689]\n"," [0.04045049 0.05968041]\n"," [0.04042159 0.0601982 ]]\n","\n","Average MAE Loss:\n","[0.04994496 0.05021725 0.05006545 0.05030989]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04033133 0.05956478]\n"," [0.0403305  0.05960184]\n"," [0.04032994 0.05964031]\n"," [0.04032903 0.05960026]]\n","\n","Average MAE Loss:\n","[0.04994806 0.04996617 0.04998513 0.04996464]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04034013 0.05952197]\n"," [0.0403305  0.05960184]\n"," [0.04033357 0.05968022]\n"," [0.04032778 0.05960104]]\n","\n","Average MAE Loss:\n","[0.04993105 0.04996617 0.0500069  0.04996441]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04035741 0.05948531]\n"," [0.0403305  0.05960184]\n"," [0.04033669 0.05972046]\n"," [0.04032707 0.05960222]]\n","\n","Average MAE Loss:\n","[0.04992136 0.04996617 0.05002858 0.04996465]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04037819 0.05945528]\n"," [0.0403305  0.05960184]\n"," [0.04034338 0.05975846]\n"," [0.04032673 0.05960367]]\n","\n","Average MAE Loss:\n","[0.04991673 0.04996617 0.05005092 0.0499652 ]\n","\n","\n","epochs finished with time:39.29844641685486\n","\n","[[0.04037819 0.05945528]\n"," [0.0403305  0.05960184]\n"," [0.04034338 0.05975846]\n"," [0.04032673 0.05960367]]\n","00:00:54.78896729799999\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08620408 0.08987418]\n"," [0.10227572 0.10665325]\n"," [0.06739103 0.06996016]\n"," [0.07494998 0.07988729]]\n","\n","Average MAE Loss:\n","[0.08803913 0.10446449 0.0686756  0.07741863]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08500375 0.08935361]\n"," [0.10227572 0.10665325]\n"," [0.06181394 0.06412692]\n"," [0.06211015 0.06308409]]\n","\n","Average MAE Loss:\n","[0.08717868 0.10446449 0.06297043 0.06259712]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08536768 0.08941396]\n"," [0.10227572 0.10665325]\n"," [0.06213722 0.06455851]\n"," [0.06171874 0.06324714]]\n","\n","Average MAE Loss:\n","[0.08739082 0.10446449 0.06334786 0.06248294]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08542887 0.08958162]\n"," [0.10227572 0.10665325]\n"," [0.061882   0.0644711 ]\n"," [0.06096504 0.06251045]]\n","\n","Average MAE Loss:\n","[0.08750524 0.10446449 0.06317655 0.06173774]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08546048 0.08948743]\n"," [0.10227572 0.10665325]\n"," [0.06197553 0.06461914]\n"," [0.06195066 0.06378173]]\n","\n","Average MAE Loss:\n","[0.08747396 0.10446449 0.06329733 0.0628662 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08550751 0.08954473]\n"," [0.10227572 0.10665325]\n"," [0.06141108 0.06419605]\n"," [0.06188003 0.06376306]]\n","\n","Average MAE Loss:\n","[0.08752612 0.10446449 0.06280356 0.06282154]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08642748 0.09046051]\n"," [0.10227572 0.10665325]\n"," [0.06188722 0.06448822]\n"," [0.06146022 0.06388205]]\n","\n","Average MAE Loss:\n","[0.08844399 0.10446449 0.06318772 0.06267113]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06148196 0.06532691]\n"," [0.06146022 0.06388205]\n"," [0.08650795 0.09054519]\n"," [0.0630181  0.06466792]]\n","\n","Average MAE Loss:\n","[0.06340444 0.06267113 0.08852657 0.06384301]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05885879 0.06239721]\n"," [0.06146022 0.06388205]\n"," [0.0856511  0.08959537]\n"," [0.06304406 0.06573356]]\n","\n","Average MAE Loss:\n","[0.060628   0.06267113 0.08762323 0.06438881]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05801856 0.06142887]\n"," [0.06146022 0.06388205]\n"," [0.08544335 0.08914606]\n"," [0.06308243 0.06590956]]\n","\n","Average MAE Loss:\n","[0.05972371 0.06267113 0.08729471 0.064496  ]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05885407 0.0622624 ]\n"," [0.06146022 0.06388205]\n"," [0.08525909 0.08888908]\n"," [0.06302629 0.06582736]]\n","\n","Average MAE Loss:\n","[0.06055823 0.06267113 0.08707409 0.06442682]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05851428 0.06179411]\n"," [0.06146022 0.06388205]\n"," [0.08518156 0.08895428]\n"," [0.06226084 0.06495479]]\n","\n","Average MAE Loss:\n","[0.06015419 0.06267113 0.08706792 0.06360782]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05890269 0.0618735 ]\n"," [0.06146022 0.06388205]\n"," [0.0850544  0.08875464]\n"," [0.0602268  0.06154001]]\n","\n","Average MAE Loss:\n","[0.06038809 0.06267113 0.08690452 0.06088341]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05930151 0.06270021]\n"," [0.06146022 0.06388205]\n"," [0.08487624 0.08849725]\n"," [0.06066267 0.06241872]]\n","\n","Average MAE Loss:\n","[0.06100086 0.06267113 0.08668675 0.06154069]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06602943 0.07016944]\n"," [0.09969132 0.10407981]\n"," [0.06622178 0.07023258]\n"," [0.0754855  0.07960098]]\n","\n","Average MAE Loss:\n","[0.06809944 0.10188556 0.06822718 0.07754324]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05101812 0.05301441]\n"," [0.09969132 0.10407981]\n"," [0.04953283 0.05079426]\n"," [0.05137177 0.05147129]]\n","\n","Average MAE Loss:\n","[0.05201626 0.10188556 0.05016355 0.05142153]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04809308 0.0493727 ]\n"," [0.09969132 0.10407981]\n"," [0.04891692 0.05001476]\n"," [0.05167684 0.05309479]]\n","\n","Average MAE Loss:\n","[0.04873289 0.10188556 0.04946584 0.05238582]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04894982 0.05043685]\n"," [0.09969132 0.10407981]\n"," [0.04897152 0.05005348]\n"," [0.0506344  0.05147884]]\n","\n","Average MAE Loss:\n","[0.04969334 0.10188556 0.0495125  0.05105662]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04847989 0.0497172 ]\n"," [0.09969132 0.10407981]\n"," [0.0485492  0.04955739]\n"," [0.05075765 0.05168653]]\n","\n","Average MAE Loss:\n","[0.04909855 0.10188556 0.04905329 0.05122209]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04850926 0.04988885]\n"," [0.09969132 0.10407981]\n"," [0.04866814 0.0499332 ]\n"," [0.05080416 0.05179974]]\n","\n","Average MAE Loss:\n","[0.04919905 0.10188556 0.04930067 0.05130195]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04859539 0.04992443]\n"," [0.09969132 0.10407981]\n"," [0.04868408 0.04976974]\n"," [0.05067659 0.05160281]]\n","\n","Average MAE Loss:\n","[0.04925991 0.10188556 0.04922691 0.0511397 ]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0950992  0.09948886]\n"," [0.05067659 0.05160281]\n"," [0.0485635  0.04971545]\n"," [0.04923515 0.05023012]]\n","\n","Average MAE Loss:\n","[0.09729403 0.0511397  0.04913948 0.04973263]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0856518  0.09011689]\n"," [0.05067659 0.05160281]\n"," [0.0487863  0.05007716]\n"," [0.04964709 0.05015371]]\n","\n","Average MAE Loss:\n","[0.08788434 0.0511397  0.04943173 0.0499004 ]\n","\n","Epoch 00023: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06593806 0.07014555]\n"," [0.05067659 0.05160281]\n"," [0.04860686 0.04992133]\n"," [0.04991003 0.05055401]]\n","\n","Average MAE Loss:\n","[0.0680418  0.0511397  0.04926409 0.05023202]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04913115 0.05074129]\n"," [0.05067659 0.05160281]\n"," [0.04856526 0.04987727]\n"," [0.04988592 0.05057205]]\n","\n","Average MAE Loss:\n","[0.04993622 0.0511397  0.04922127 0.05022899]\n","\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04943129 0.05123179]\n"," [0.05067659 0.05160281]\n"," [0.04851762 0.04985684]\n"," [0.04991161 0.05059727]]\n","\n","Average MAE Loss:\n","[0.05033154 0.0511397  0.04918723 0.05025444]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0487697  0.05012172]\n"," [0.05067659 0.05160281]\n"," [0.04858789 0.04988658]\n"," [0.04988393 0.05058574]]\n","\n","Average MAE Loss:\n","[0.04944571 0.0511397  0.04923723 0.05023483]\n","\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04874676 0.05004592]\n"," [0.05067659 0.05160281]\n"," [0.04859563 0.04987767]\n"," [0.04987559 0.05060191]]\n","\n","Average MAE Loss:\n","[0.04939634 0.0511397  0.04923665 0.05023875]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05055601 0.05235394]\n"," [0.05389015 0.05645216]\n"," [0.04966398 0.05104032]\n"," [0.04847679 0.04876697]]\n","\n","Average MAE Loss:\n","[0.05145498 0.05517116 0.05035215 0.04862188]\n","\n","Epoch 00029: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04915129 0.05050632]\n"," [0.05389015 0.05645216]\n"," [0.04798397 0.04851786]\n"," [0.04944728 0.05030294]]\n","\n","Average MAE Loss:\n","[0.04982881 0.05517116 0.04825092 0.04987511]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04852292 0.04966801]\n"," [0.05389015 0.05645216]\n"," [0.04822902 0.04915291]\n"," [0.04938394 0.05003188]]\n","\n","Average MAE Loss:\n","[0.04909546 0.05517116 0.04869096 0.04970791]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04827055 0.04934007]\n"," [0.05389015 0.05645216]\n"," [0.0482081  0.04917458]\n"," [0.04950906 0.05025456]]\n","\n","Average MAE Loss:\n","[0.04880531 0.05517116 0.04869134 0.04988181]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.0481543  0.04919642]\n"," [0.05389015 0.05645216]\n"," [0.04816792 0.04917396]\n"," [0.04947282 0.05024605]]\n","\n","Average MAE Loss:\n","[0.04867536 0.05517116 0.04867094 0.04985944]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04805289 0.04906994]\n"," [0.05389015 0.05645216]\n"," [0.0481935  0.04926936]\n"," [0.04947776 0.05026343]]\n","\n","Average MAE Loss:\n","[0.04856142 0.05517116 0.04873143 0.0498706 ]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04798404 0.04897558]\n"," [0.05389015 0.05645216]\n"," [0.04818662 0.04927216]\n"," [0.04947925 0.05025556]]\n","\n","Average MAE Loss:\n","[0.04847981 0.05517116 0.04872939 0.0498674 ]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05213253 0.05435331]\n"," [0.04947925 0.05025556]\n"," [0.04786385 0.0487666 ]\n"," [0.04840891 0.04954338]]\n","\n","Average MAE Loss:\n","[0.05324292 0.0498674  0.04831523 0.04897614]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05015908 0.05184416]\n"," [0.04947925 0.05025556]\n"," [0.04774433 0.04855869]\n"," [0.04856088 0.04958635]]\n","\n","Average MAE Loss:\n","[0.05100162 0.0498674  0.04815151 0.04907362]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04902173 0.05035103]\n"," [0.04947925 0.05025556]\n"," [0.04769415 0.04849667]\n"," [0.04860646 0.0495687 ]]\n","\n","Average MAE Loss:\n","[0.04968638 0.0498674  0.04809541 0.04908758]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04849862 0.0496455 ]\n"," [0.04947925 0.05025556]\n"," [0.04769321 0.04852523]\n"," [0.04867132 0.04959181]]\n","\n","Average MAE Loss:\n","[0.04907206 0.0498674  0.04810922 0.04913157]\n","\n","Epoch 00039: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04837485 0.0494879 ]\n"," [0.04947925 0.05025556]\n"," [0.04772017 0.04858965]\n"," [0.04874811 0.04963651]]\n","\n","Average MAE Loss:\n","[0.04893137 0.0498674  0.04815491 0.04919231]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04829494 0.04939295]\n"," [0.04947925 0.05025556]\n"," [0.04776559 0.04866705]\n"," [0.04883643 0.04969531]]\n","\n","Average MAE Loss:\n","[0.04884395 0.0498674  0.04821632 0.04926587]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04823143 0.04930903]\n"," [0.04947925 0.05025556]\n"," [0.04780976 0.04872857]\n"," [0.04888137 0.04972675]]\n","\n","Average MAE Loss:\n","[0.04877023 0.0498674  0.04826917 0.04930406]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04834976 0.04925003]\n"," [0.04843474 0.04934053]\n"," [0.04834755 0.04923268]\n"," [0.04847503 0.04936064]]\n","\n","Average MAE Loss:\n","[0.04879989 0.04888764 0.04879012 0.04891784]\n","\n","Epoch 00043: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04831019 0.04921183]\n"," [0.04843474 0.04934053]\n"," [0.04822608 0.04908605]\n"," [0.04852023 0.04938442]]\n","\n","Average MAE Loss:\n","[0.04876101 0.04888764 0.04865607 0.04895232]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04827278 0.04917859]\n"," [0.04843474 0.04934053]\n"," [0.04813639 0.04898244]\n"," [0.04856509 0.04941346]]\n","\n","Average MAE Loss:\n","[0.04872568 0.04888764 0.04855942 0.04898928]\n","\n","Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04823771 0.04914426]\n"," [0.04843474 0.04934053]\n"," [0.0480761  0.04892449]\n"," [0.04858981 0.04943069]]\n","\n","Average MAE Loss:\n","[0.04869099 0.04888764 0.0485003  0.04901025]\n","\n","Epoch 00046: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04820812 0.04911253]\n"," [0.04843474 0.04934053]\n"," [0.04805791 0.04890997]\n"," [0.04861125 0.0494432 ]]\n","\n","Average MAE Loss:\n","[0.04866032 0.04888764 0.04848394 0.04902722]\n","\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04819523 0.04909933]\n"," [0.04843474 0.04934053]\n"," [0.0480443  0.04890097]\n"," [0.0486336  0.04945749]]\n","\n","Average MAE Loss:\n","[0.04864728 0.04888764 0.04847263 0.04904554]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04818342 0.04908695]\n"," [0.04843474 0.04934053]\n"," [0.04803442 0.04889707]\n"," [0.04865573 0.04947283]]\n","\n","Average MAE Loss:\n","[0.04863518 0.04888764 0.04846575 0.04906428]\n","\n","Epoch 00049: reducing learning rate of group 0 to 3.9063e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04841397 0.04931911]\n"," [0.04865573 0.04947283]\n"," [0.04814112 0.04902895]\n"," [0.04805546 0.04892262]]\n","\n","Average MAE Loss:\n","[0.04886654 0.04906428 0.04858503 0.04848904]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04839216 0.0492983 ]\n"," [0.04865573 0.04947283]\n"," [0.04811171 0.04898649]\n"," [0.04808816 0.04896457]]\n","\n","Average MAE Loss:\n","[0.04884523 0.04906428 0.0485491  0.04852637]\n","\n","Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04838137 0.04928788]\n"," [0.04865573 0.04947283]\n"," [0.04808431 0.04894586]\n"," [0.04811854 0.04900368]]\n","\n","Average MAE Loss:\n","[0.04883462 0.04906428 0.04851509 0.04856111]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04837067 0.04927746]\n"," [0.04865573 0.04947283]\n"," [0.04806051 0.04891004]\n"," [0.04814705 0.04903912]]\n","\n","Average MAE Loss:\n","[0.04882406 0.04906428 0.04848528 0.04859309]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04836021 0.0492674 ]\n"," [0.04865573 0.04947283]\n"," [0.04803872 0.04887736]\n"," [0.04817457 0.04907162]]\n","\n","Average MAE Loss:\n","[0.0488138  0.04906428 0.04845804 0.0486231 ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04834991 0.04925767]\n"," [0.04865573 0.04947283]\n"," [0.04802837 0.04886204]\n"," [0.04818773 0.04908659]]\n","\n","Average MAE Loss:\n","[0.04880379 0.04906428 0.0484452  0.04863716]\n","\n","Epoch 00055: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04834484 0.04925289]\n"," [0.04865573 0.04947283]\n"," [0.04801802 0.04884701]\n"," [0.04820026 0.04910048]]\n","\n","Average MAE Loss:\n","[0.04879886 0.04906428 0.04843251 0.04865037]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04827316 0.04912576]\n"," [0.04827764 0.04912997]\n"," [0.04826464 0.04911391]\n"," [0.0482896  0.04914201]]\n","\n","Average MAE Loss:\n","[0.04869946 0.04870381 0.04868927 0.0487158 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04826929 0.0491227 ]\n"," [0.04827764 0.04912997]\n"," [0.04824916 0.04909511]\n"," [0.04830097 0.04915263]]\n","\n","Average MAE Loss:\n","[0.048696   0.04870381 0.04867214 0.0487268 ]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.0482655  0.04911979]\n"," [0.04827764 0.04912997]\n"," [0.04824153 0.04908576]\n"," [0.04830662 0.04915787]]\n","\n","Average MAE Loss:\n","[0.04869265 0.04870381 0.04866365 0.04873225]\n","\n","Epoch 00059: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04826358 0.04911831]\n"," [0.04827764 0.04912997]\n"," [0.04823403 0.04907664]\n"," [0.04831224 0.04916307]]\n","\n","Average MAE Loss:\n","[0.04869094 0.04870381 0.04865534 0.04873765]\n","\n","\n","epochs finished with time:39.2915940284729\n","\n","[[0.04826358 0.04911831]\n"," [0.04827764 0.04912997]\n"," [0.04823403 0.04907664]\n"," [0.04831224 0.04916307]]\n","00:00:54.84948389600004\n"]}],"source":["# seed = 10 table = 4/8 fixed folds CNN_2 dropout\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='CNN2_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218565,"status":"ok","timestamp":1690368527411,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"FQ3rRlf3rmMB","outputId":"15e21a94-9c8c-41ca-d62a-72efa41485b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.25\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05980194 0.05644431]\n"," [0.05575079 0.0533058 ]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.05812313 0.0545283 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.051752   0.05004802]\n"," [0.05443482 0.05243137]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.05090001 0.0534331 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05155187 0.04938125]\n"," [0.05168367 0.04962032]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.05046656 0.050652  ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05093988 0.04871966]\n"," [0.05181687 0.05005123]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.04982977 0.05093405]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05140737 0.04912954]\n"," [0.05031849 0.04852774]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.05026846 0.04942311]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05081621 0.04879475]\n"," [0.04984302 0.04795679]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.04980548 0.04889991]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05126519 0.04883164]\n"," [0.05033181 0.04833591]]\n","\n","Average MAE Loss:\n","[0.08429033 0.08433132 0.05004841 0.04933386]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05033181 0.04833591]\n"," [0.05705867 0.05363393]\n"," [0.05195426 0.04996295]]\n","\n","Average MAE Loss:\n","[0.08433132 0.04933386 0.0553463  0.0509586 ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05033181 0.04833591]\n"," [0.05148584 0.04936919]\n"," [0.05096031 0.04906904]]\n","\n","Average MAE Loss:\n","[0.08433132 0.04933386 0.05042751 0.05001467]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05033181 0.04833591]\n"," [0.05015614 0.04791628]\n"," [0.05098611 0.04896863]]\n","\n","Average MAE Loss:\n","[0.08433132 0.04933386 0.04903621 0.04997737]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05033181 0.04833591]\n"," [0.04984977 0.04778761]\n"," [0.04967861 0.04756207]]\n","\n","Average MAE Loss:\n","[0.08433132 0.04933386 0.04881869 0.04862034]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05033181 0.04833591]\n"," [0.05014843 0.04780228]\n"," [0.05008778 0.0478222 ]]\n","\n","Average MAE Loss:\n","[0.08433132 0.04933386 0.04897535 0.04895499]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05033181 0.04833591]\n"," [0.05039114 0.04797048]\n"," [0.04965491 0.0475872 ]]\n","\n","Average MAE Loss:\n","[0.08433132 0.04933386 0.04918081 0.04862105]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.05033181 0.04833591]\n"," [0.04999064 0.04774226]\n"," [0.04936051 0.04750475]]\n","\n","Average MAE Loss:\n","[0.08433132 0.04933386 0.04886645 0.04843263]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.0832451  0.08022739]\n"," [0.04433177 0.04252901]\n"," [0.04698467 0.04409077]]\n","\n","Average MAE Loss:\n","[0.08173625 0.08173625 0.04343039 0.04553772]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.0832451  0.08022739]\n"," [0.04415251 0.04314432]\n"," [0.04329285 0.04171789]]\n","\n","Average MAE Loss:\n","[0.08173625 0.08173625 0.04364842 0.04250537]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.0832451  0.08022739]\n"," [0.04281189 0.04074617]\n"," [0.04318438 0.04105636]]\n","\n","Average MAE Loss:\n","[0.08173625 0.08173625 0.04177903 0.04212037]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.0832451  0.08022739]\n"," [0.04276613 0.04142531]\n"," [0.04265601 0.04066918]]\n","\n","Average MAE Loss:\n","[0.08173625 0.08173625 0.04209572 0.0416626 ]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.0832451  0.08022739]\n"," [0.04290707 0.04070766]\n"," [0.04257442 0.04073515]]\n","\n","Average MAE Loss:\n","[0.08173625 0.08173625 0.04180736 0.04165479]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.0832451  0.08022739]\n"," [0.0430548  0.04090862]\n"," [0.04252159 0.04066373]]\n","\n","Average MAE Loss:\n","[0.08173625 0.08173625 0.04198171 0.04159266]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.0832451  0.08022739]\n"," [0.04255724 0.04072626]\n"," [0.04230759 0.04055754]]\n","\n","Average MAE Loss:\n","[0.08173625 0.08173625 0.04164175 0.04143257]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.04230759 0.04055754]\n"," [0.04882545 0.04734751]\n"," [0.04149671 0.03980616]]\n","\n","Average MAE Loss:\n","[0.08173625 0.04143257 0.04808648 0.04065143]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.04230759 0.04055754]\n"," [0.04733693 0.04608195]\n"," [0.04158097 0.04000827]]\n","\n","Average MAE Loss:\n","[0.08173625 0.04143257 0.04670944 0.04079462]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.04230759 0.04055754]\n"," [0.04529586 0.04357302]\n"," [0.04159386 0.04013077]]\n","\n","Average MAE Loss:\n","[0.08173625 0.04143257 0.04443444 0.04086231]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.04230759 0.04055754]\n"," [0.04517217 0.0433859 ]\n"," [0.0414813  0.04001765]]\n","\n","Average MAE Loss:\n","[0.08173625 0.04143257 0.04427903 0.04074947]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.04230759 0.04055754]\n"," [0.04501384 0.04290616]\n"," [0.04145983 0.03987853]]\n","\n","Average MAE Loss:\n","[0.08173625 0.04143257 0.04396    0.04066918]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.04230759 0.04055754]\n"," [0.04486967 0.04289666]\n"," [0.04153311 0.04010909]]\n","\n","Average MAE Loss:\n","[0.08173625 0.04143257 0.04388316 0.0408211 ]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0832451  0.08022739]\n"," [0.04230759 0.04055754]\n"," [0.04484755 0.04297095]\n"," [0.04143223 0.03994892]]\n","\n","Average MAE Loss:\n","[0.08173625 0.04143257 0.04390925 0.04069058]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.05553314 0.05309823]\n"," [0.04258584 0.04075932]\n"," [0.04521274 0.04399427]]\n","\n","Average MAE Loss:\n","[0.05431569 0.05431569 0.04167258 0.04460351]\n","\n","Epoch 00029: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.05553314 0.05309823]\n"," [0.0423042  0.04035684]\n"," [0.04136401 0.03986627]]\n","\n","Average MAE Loss:\n","[0.05431569 0.05431569 0.04133052 0.04061514]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.05553314 0.05309823]\n"," [0.04247825 0.04067969]\n"," [0.04137775 0.0399491 ]]\n","\n","Average MAE Loss:\n","[0.05431569 0.05431569 0.04157897 0.04066342]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.05553314 0.05309823]\n"," [0.04214322 0.04025461]\n"," [0.04140091 0.0399684 ]]\n","\n","Average MAE Loss:\n","[0.05431569 0.05431569 0.04119892 0.04068466]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.05553314 0.05309823]\n"," [0.04220689 0.04032528]\n"," [0.04135659 0.03988752]]\n","\n","Average MAE Loss:\n","[0.05431569 0.05431569 0.04126608 0.04062205]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.05553314 0.05309823]\n"," [0.04220898 0.04029849]\n"," [0.0414463  0.04001709]]\n","\n","Average MAE Loss:\n","[0.05431569 0.05431569 0.04125374 0.0407317 ]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.05553314 0.05309823]\n"," [0.04216201 0.04024438]\n"," [0.0414548  0.04008275]]\n","\n","Average MAE Loss:\n","[0.05431569 0.05431569 0.0412032  0.04076877]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.0414548  0.04008275]\n"," [0.04205022 0.04042903]\n"," [0.0413758  0.03964487]]\n","\n","Average MAE Loss:\n","[0.05431569 0.04076877 0.04123962 0.04051033]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.0414548  0.04008275]\n"," [0.04260991 0.04109724]\n"," [0.04113906 0.03954369]]\n","\n","Average MAE Loss:\n","[0.05431569 0.04076877 0.04185357 0.04034138]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.0414548  0.04008275]\n"," [0.04185316 0.04013199]\n"," [0.04125097 0.03966556]]\n","\n","Average MAE Loss:\n","[0.05431569 0.04076877 0.04099257 0.04045826]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.0414548  0.04008275]\n"," [0.0420729  0.04035212]\n"," [0.04136355 0.03980136]]\n","\n","Average MAE Loss:\n","[0.05431569 0.04076877 0.04121251 0.04058245]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.0414548  0.04008275]\n"," [0.0421084  0.04033999]\n"," [0.04138654 0.03984812]]\n","\n","Average MAE Loss:\n","[0.05431569 0.04076877 0.04122419 0.04061733]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.0414548  0.04008275]\n"," [0.04214555 0.04037524]\n"," [0.04139578 0.03988886]]\n","\n","Average MAE Loss:\n","[0.05431569 0.04076877 0.0412604  0.04064232]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05553314 0.05309823]\n"," [0.0414548  0.04008275]\n"," [0.04212493 0.04036446]\n"," [0.04142734 0.03995287]]\n","\n","Average MAE Loss:\n","[0.05431569 0.04076877 0.0412447  0.04069011]\n","\n","Epoch 00042: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04228467 0.04031625]\n"," [0.04132514 0.03954862]\n"," [0.04125758 0.0395817 ]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04130046 0.04043688 0.04041964]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04228467 0.04031625]\n"," [0.04159527 0.04000004]\n"," [0.04126399 0.03986417]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04130046 0.04079766 0.04056408]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04228467 0.04031625]\n"," [0.0417659  0.04011135]\n"," [0.04121889 0.03977824]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04130046 0.04093862 0.04049857]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04228467 0.04031625]\n"," [0.04178109 0.04006356]\n"," [0.04124385 0.03979314]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04130046 0.04092232 0.04051849]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04228467 0.04031625]\n"," [0.04187105 0.04013447]\n"," [0.04128484 0.03983437]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04130046 0.04100276 0.0405596 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04228467 0.04031625]\n"," [0.04190419 0.04016686]\n"," [0.04131521 0.03985938]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04130046 0.04103553 0.04058729]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04228467 0.04031625]\n"," [0.04193066 0.0401958 ]\n"," [0.04133794 0.03987797]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04130046 0.04106323 0.04060795]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.5625e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04133794 0.03987797]\n"," [0.04169118 0.03979519]\n"," [0.04180003 0.04007731]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04060795 0.04074319 0.04093867]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04133794 0.03987797]\n"," [0.04126124 0.03954113]\n"," [0.04162737 0.03991145]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04060795 0.04040119 0.04076941]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04133794 0.03987797]\n"," [0.04131733 0.03967414]\n"," [0.04148484 0.03979107]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04060795 0.04049573 0.04063795]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04133794 0.03987797]\n"," [0.04149239 0.0398501 ]\n"," [0.04138211 0.03971478]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04060795 0.04067125 0.04054844]\n","\n","Epoch 00053: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04133794 0.03987797]\n"," [0.04160061 0.03993689]\n"," [0.04134147 0.03968671]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04060795 0.04076875 0.04051409]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04133794 0.03987797]\n"," [0.04167376 0.03997899]\n"," [0.04130685 0.03966297]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04060795 0.04082638 0.04048491]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04228467 0.04031625]\n"," [0.04133794 0.03987797]\n"," [0.04171006 0.04000411]\n"," [0.04127811 0.03964154]]\n","\n","Average MAE Loss:\n","[0.04130046 0.04060795 0.04085708 0.04045983]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.041142   0.03946356]\n"," [0.041142   0.03946356]\n"," [0.04115388 0.0395011 ]\n"," [0.04112282 0.03946805]]\n","\n","Average MAE Loss:\n","[0.04030278 0.04030278 0.04032749 0.04029543]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.041142   0.03946356]\n"," [0.041142   0.03946356]\n"," [0.04121568 0.03958204]\n"," [0.04110974 0.03948417]]\n","\n","Average MAE Loss:\n","[0.04030278 0.04030278 0.04039886 0.04029696]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.041142   0.03946356]\n"," [0.041142   0.03946356]\n"," [0.04130205 0.03966998]\n"," [0.04110788 0.03950647]]\n","\n","Average MAE Loss:\n","[0.04030278 0.04030278 0.04048602 0.04030718]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.041142   0.03946356]\n"," [0.041142   0.03946356]\n"," [0.04138483 0.0397419 ]\n"," [0.04110885 0.03952736]]\n","\n","Average MAE Loss:\n","[0.04030278 0.04030278 0.04056337 0.0403181 ]\n","\n","\n","epochs finished with time:26.264599561691284\n","\n","[[0.041142   0.03946356]\n"," [0.041142   0.03946356]\n"," [0.04138483 0.0397419 ]\n"," [0.04110885 0.03952736]]\n","00:00:46.986143143999925\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.08138124 0.08211665]\n"," [0.09005698 0.09005003]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.08174894 0.0900535 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.0777366  0.07868388]\n"," [0.08019902 0.07642323]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.07821024 0.07831112]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.07587604 0.0772184 ]\n"," [0.07802104 0.07444378]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.07654722 0.07623241]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.07726764 0.07749708]\n"," [0.07669795 0.07347084]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.07738236 0.07508439]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.07636625 0.07691087]\n"," [0.07877701 0.07451286]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.07663856 0.07664493]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.07711515 0.07741723]\n"," [0.07994574 0.07335907]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.07726619 0.0766524 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.11631048 0.11447674]\n"," [0.11635957 0.1145391 ]\n"," [0.07674683 0.07726086]\n"," [0.07520279 0.07475662]]\n","\n","Average MAE Loss:\n","[0.11539361 0.11544933 0.07700385 0.0749797 ]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07520279 0.07475662]\n"," [0.07806234 0.07852676]\n"," [0.07755936 0.07710342]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0749797  0.07829455 0.07733139]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07520279 0.07475662]\n"," [0.07624278 0.07685564]\n"," [0.07831218 0.07755902]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0749797  0.07654921 0.0779356 ]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07520279 0.07475662]\n"," [0.07596861 0.07623645]\n"," [0.08003755 0.07876911]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0749797  0.07610253 0.07940333]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07520279 0.07475662]\n"," [0.07563215 0.07593544]\n"," [0.08259013 0.08090728]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0749797  0.0757838  0.08174871]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07520279 0.07475662]\n"," [0.07600381 0.07594023]\n"," [0.07499672 0.07551957]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0749797  0.07597202 0.07525814]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07520279 0.07475662]\n"," [0.0754725  0.07581514]\n"," [0.07535465 0.07549032]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0749797  0.07564382 0.07542249]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.11635957 0.1145391 ]\n"," [0.07520279 0.07475662]\n"," [0.07554927 0.07584803]\n"," [0.07610386 0.07578305]]\n","\n","Average MAE Loss:\n","[0.11544933 0.0749797  0.07569865 0.07594346]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.11338376 0.11176826]\n"," [0.07042981 0.07200294]\n"," [0.07357691 0.07254441]]\n","\n","Average MAE Loss:\n","[0.11257601 0.11257601 0.07121637 0.07306066]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.11338376 0.11176826]\n"," [0.06855952 0.06955377]\n"," [0.07035418 0.06944231]]\n","\n","Average MAE Loss:\n","[0.11257601 0.11257601 0.06905665 0.06989824]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.11338376 0.11176826]\n"," [0.06789439 0.06906114]\n"," [0.0666626  0.06738423]]\n","\n","Average MAE Loss:\n","[0.11257601 0.11257601 0.06847776 0.06702342]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.11338376 0.11176826]\n"," [0.06744317 0.06851428]\n"," [0.06666038 0.0668696 ]]\n","\n","Average MAE Loss:\n","[0.11257601 0.11257601 0.06797873 0.06676499]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.11338376 0.11176826]\n"," [0.06742905 0.06856991]\n"," [0.06716015 0.06712769]]\n","\n","Average MAE Loss:\n","[0.11257601 0.11257601 0.06799948 0.06714392]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.11338376 0.11176826]\n"," [0.06738143 0.06853035]\n"," [0.06679909 0.06694179]]\n","\n","Average MAE Loss:\n","[0.11257601 0.11257601 0.06795589 0.06687044]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.11338376 0.11176826]\n"," [0.06729469 0.06831841]\n"," [0.06651835 0.06683267]]\n","\n","Average MAE Loss:\n","[0.11257601 0.11257601 0.06780655 0.06667551]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.06651835 0.06683267]\n"," [0.06997491 0.07069512]\n"," [0.06754703 0.06755588]]\n","\n","Average MAE Loss:\n","[0.11257601 0.06667551 0.07033502 0.06755145]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.06651835 0.06683267]\n"," [0.06907815 0.07104635]\n"," [0.06779626 0.06728101]]\n","\n","Average MAE Loss:\n","[0.11257601 0.06667551 0.07006225 0.06753863]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.06651835 0.06683267]\n"," [0.06898294 0.06997498]\n"," [0.06685779 0.06691874]]\n","\n","Average MAE Loss:\n","[0.11257601 0.06667551 0.06947896 0.06688826]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.06651835 0.06683267]\n"," [0.06823866 0.06963035]\n"," [0.06683605 0.06698815]]\n","\n","Average MAE Loss:\n","[0.11257601 0.06667551 0.06893451 0.0669121 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.06651835 0.06683267]\n"," [0.06782774 0.06906068]\n"," [0.06689728 0.06693594]]\n","\n","Average MAE Loss:\n","[0.11257601 0.06667551 0.06844421 0.06691661]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.06651835 0.06683267]\n"," [0.06784034 0.06909503]\n"," [0.06688521 0.06689352]]\n","\n","Average MAE Loss:\n","[0.11257601 0.06667551 0.06846769 0.06688937]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.11338376 0.11176826]\n"," [0.06651835 0.06683267]\n"," [0.06786466 0.06907657]\n"," [0.06672145 0.06691391]]\n","\n","Average MAE Loss:\n","[0.11257601 0.06667551 0.06847062 0.06681768]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.07885537 0.07723771]\n"," [0.065673   0.06736186]\n"," [0.06551208 0.06641592]]\n","\n","Average MAE Loss:\n","[0.07804654 0.07804654 0.06651743 0.065964  ]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.07885537 0.07723771]\n"," [0.06642852 0.0678995 ]\n"," [0.06633579 0.06625087]]\n","\n","Average MAE Loss:\n","[0.07804654 0.07804654 0.06716401 0.06629333]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.07885537 0.07723771]\n"," [0.066006   0.06748796]\n"," [0.06601163 0.06621816]]\n","\n","Average MAE Loss:\n","[0.07804654 0.07804654 0.06674698 0.0661149 ]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.07885537 0.07723771]\n"," [0.06611685 0.06745548]\n"," [0.06576158 0.06614789]]\n","\n","Average MAE Loss:\n","[0.07804654 0.07804654 0.06678616 0.06595474]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.07885537 0.07723771]\n"," [0.0660675  0.06741387]\n"," [0.06606128 0.06631356]]\n","\n","Average MAE Loss:\n","[0.07804654 0.07804654 0.06674069 0.06618742]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.07885537 0.07723771]\n"," [0.06603784 0.06736441]\n"," [0.06580549 0.06612069]]\n","\n","Average MAE Loss:\n","[0.07804654 0.07804654 0.06670112 0.06596309]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.07885537 0.07723771]\n"," [0.06601481 0.06737095]\n"," [0.06586754 0.06619338]]\n","\n","Average MAE Loss:\n","[0.07804654 0.07804654 0.06669288 0.06603046]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.06586754 0.06619338]\n"," [0.06761462 0.0676141 ]\n"," [0.0657653  0.06645368]]\n","\n","Average MAE Loss:\n","[0.07804654 0.06603046 0.06761436 0.06610949]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.06586754 0.06619338]\n"," [0.06536112 0.06696564]\n"," [0.06574453 0.06619996]]\n","\n","Average MAE Loss:\n","[0.07804654 0.06603046 0.06616338 0.06597224]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.06586754 0.06619338]\n"," [0.06580954 0.06720511]\n"," [0.06581175 0.0661311 ]]\n","\n","Average MAE Loss:\n","[0.07804654 0.06603046 0.06650732 0.06597143]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.06586754 0.06619338]\n"," [0.06583976 0.06723867]\n"," [0.06577867 0.06609756]]\n","\n","Average MAE Loss:\n","[0.07804654 0.06603046 0.06653921 0.06593812]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.06586754 0.06619338]\n"," [0.06587481 0.0672903 ]\n"," [0.06581911 0.06609005]]\n","\n","Average MAE Loss:\n","[0.07804654 0.06603046 0.06658256 0.06595458]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.06586754 0.06619338]\n"," [0.06592388 0.067341  ]\n"," [0.06580657 0.06609033]]\n","\n","Average MAE Loss:\n","[0.07804654 0.06603046 0.06663244 0.06594845]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.07885537 0.07723771]\n"," [0.06586754 0.06619338]\n"," [0.06591354 0.06731511]\n"," [0.06585932 0.06608435]]\n","\n","Average MAE Loss:\n","[0.07804654 0.06603046 0.06661433 0.06597183]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06762369 0.06743362]\n"," [0.06623664 0.06655668]\n"," [0.06571703 0.06621205]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06752865 0.06639666 0.06596454]\n","\n","Epoch 00043: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06762369 0.06743362]\n"," [0.06549625 0.06646211]\n"," [0.06555495 0.06607363]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06752865 0.06597918 0.06581429]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06762369 0.06743362]\n"," [0.06552689 0.06668797]\n"," [0.06569822 0.06606044]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06752865 0.06610743 0.06587933]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06762369 0.06743362]\n"," [0.0656301  0.06683829]\n"," [0.06572186 0.06606765]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06752865 0.06623419 0.06589476]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06762369 0.06743362]\n"," [0.06569196 0.06693975]\n"," [0.06572935 0.06608097]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06752865 0.06631585 0.06590516]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06762369 0.06743362]\n"," [0.06574867 0.06701151]\n"," [0.06576231 0.06609558]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06752865 0.06638009 0.06592895]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06762369 0.06743362]\n"," [0.06575727 0.06702428]\n"," [0.06576245 0.06609984]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06752865 0.06639078 0.06593114]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06576245 0.06609984]\n"," [0.06681129 0.06691051]\n"," [0.06566172 0.06682266]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06593114 0.0668609  0.06624219]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06576245 0.06609984]\n"," [0.06600136 0.06647632]\n"," [0.06558962 0.06656772]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06593114 0.06623884 0.06607867]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06576245 0.06609984]\n"," [0.06564059 0.06643797]\n"," [0.06554486 0.06638709]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06593114 0.06603928 0.06596597]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06576245 0.06609984]\n"," [0.06557818 0.06647582]\n"," [0.06552723 0.06632702]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06593114 0.066027   0.06592713]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06576245 0.06609984]\n"," [0.06555201 0.06651711]\n"," [0.06551285 0.06628519]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06593114 0.06603456 0.06589902]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06576245 0.06609984]\n"," [0.06553906 0.06655448]\n"," [0.06550425 0.06624708]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06593114 0.06604677 0.06587567]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06762369 0.06743362]\n"," [0.06576245 0.06609984]\n"," [0.06554182 0.06659262]\n"," [0.06550005 0.06621898]]\n","\n","Average MAE Loss:\n","[0.06752865 0.06593114 0.06606722 0.06585951]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06586912 0.06631827]\n"," [0.06586912 0.06631827]\n"," [0.06581019 0.06630769]\n"," [0.06583116 0.06628531]]\n","\n","Average MAE Loss:\n","[0.0660937  0.0660937  0.06605894 0.06605823]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06586912 0.06631827]\n"," [0.06586912 0.06631827]\n"," [0.06572387 0.06628647]\n"," [0.06577475 0.06624489]]\n","\n","Average MAE Loss:\n","[0.0660937  0.0660937  0.06600517 0.06600982]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06586912 0.06631827]\n"," [0.06586912 0.06631827]\n"," [0.06564863 0.06627591]\n"," [0.06572604 0.06620986]]\n","\n","Average MAE Loss:\n","[0.0660937  0.0660937  0.06596227 0.06596795]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06586912 0.06631827]\n"," [0.06586912 0.06631827]\n"," [0.0655876  0.06627723]\n"," [0.06569186 0.06618536]]\n","\n","Average MAE Loss:\n","[0.0660937  0.0660937  0.06593242 0.06593861]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:26.317161083221436\n","\n","[[0.06586912 0.06631827]\n"," [0.06586912 0.06631827]\n"," [0.0655876  0.06627723]\n"," [0.06569186 0.06618536]]\n","00:00:45.24672517499994\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06664632 0.05805141]\n"," [0.08133205 0.07304595]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.06234887 0.077189  ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06092377 0.05296127]\n"," [0.06256417 0.05421187]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05694252 0.05838802]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06222128 0.05368961]\n"," [0.0606623  0.05272345]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05795545 0.05669288]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06149464 0.05292086]\n"," [0.05926023 0.05200982]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05720775 0.05563503]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06227444 0.05325157]\n"," [0.05891856 0.05080059]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05776301 0.05485957]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.06122532 0.05291061]\n"," [0.05900967 0.05051028]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05706796 0.05475997]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.09974948 0.08858001]\n"," [0.09980381 0.08862089]\n"," [0.05951662 0.05350574]\n"," [0.0587547  0.05044206]]\n","\n","Average MAE Loss:\n","[0.09416474 0.09421235 0.05651118 0.05459838]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.0587547  0.05044206]\n"," [0.06435572 0.05628843]\n"," [0.06250993 0.0553286 ]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05459838 0.06032207 0.05891927]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.0587547  0.05044206]\n"," [0.06372146 0.05554489]\n"," [0.06079008 0.05222441]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05459838 0.05963317 0.05650724]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.0587547  0.05044206]\n"," [0.06327801 0.05475701]\n"," [0.06089925 0.052023  ]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05459838 0.05901751 0.05646113]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.0587547  0.05044206]\n"," [0.06397049 0.05487154]\n"," [0.06026005 0.05187715]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05459838 0.05942102 0.0560686 ]\n","\n","Epoch 00011: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.0587547  0.05044206]\n"," [0.06191831 0.05460216]\n"," [0.06033184 0.05163538]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05459838 0.05826024 0.05598361]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.0587547  0.05044206]\n"," [0.06130072 0.05463935]\n"," [0.05996122 0.05157332]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05459838 0.05797004 0.05576727]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.09980381 0.08862089]\n"," [0.0587547  0.05044206]\n"," [0.06132381 0.05347248]\n"," [0.0595523  0.05163273]]\n","\n","Average MAE Loss:\n","[0.09421235 0.05459838 0.05739814 0.05559252]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.09626901 0.08518109]\n"," [0.05371414 0.04940078]\n"," [0.05685432 0.04763196]]\n","\n","Average MAE Loss:\n","[0.09072505 0.09072505 0.05155746 0.05224314]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.09626901 0.08518109]\n"," [0.05136223 0.04446292]\n"," [0.05312733 0.04487294]]\n","\n","Average MAE Loss:\n","[0.09072505 0.09072505 0.04791257 0.04900014]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.09626901 0.08518109]\n"," [0.05063858 0.04442538]\n"," [0.05242673 0.04411998]]\n","\n","Average MAE Loss:\n","[0.09072505 0.09072505 0.04753198 0.04827335]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.09626901 0.08518109]\n"," [0.05031198 0.0442359 ]\n"," [0.05172142 0.04380054]]\n","\n","Average MAE Loss:\n","[0.09072505 0.09072505 0.04727394 0.04776098]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.09626901 0.08518109]\n"," [0.05033039 0.04370597]\n"," [0.05127186 0.04371757]]\n","\n","Average MAE Loss:\n","[0.09072505 0.09072505 0.04701818 0.04749471]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.09626901 0.08518109]\n"," [0.05016791 0.04383768]\n"," [0.0513389  0.04368803]]\n","\n","Average MAE Loss:\n","[0.09072505 0.09072505 0.04700279 0.04751346]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.09626901 0.08518109]\n"," [0.05006724 0.04390115]\n"," [0.05148807 0.04365532]]\n","\n","Average MAE Loss:\n","[0.09072505 0.09072505 0.04698419 0.0475717 ]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.05148807 0.04365532]\n"," [0.08033117 0.06919272]\n"," [0.05185492 0.04381662]]\n","\n","Average MAE Loss:\n","[0.09072505 0.0475717  0.07476195 0.04783577]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.05148807 0.04365532]\n"," [0.05539443 0.04681694]\n"," [0.05160546 0.04344397]]\n","\n","Average MAE Loss:\n","[0.09072505 0.0475717  0.05110568 0.04752471]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.05148807 0.04365532]\n"," [0.05152372 0.04559329]\n"," [0.05047717 0.04336726]]\n","\n","Average MAE Loss:\n","[0.09072505 0.0475717  0.0485585  0.04692222]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.05148807 0.04365532]\n"," [0.05114853 0.04497557]\n"," [0.05038542 0.04315411]]\n","\n","Average MAE Loss:\n","[0.09072505 0.0475717  0.04806205 0.04676976]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.05148807 0.04365532]\n"," [0.05101965 0.0444067 ]\n"," [0.05046798 0.04300593]]\n","\n","Average MAE Loss:\n","[0.09072505 0.0475717  0.04771318 0.04673695]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.05148807 0.04365532]\n"," [0.05096162 0.04451427]\n"," [0.05035834 0.04313403]]\n","\n","Average MAE Loss:\n","[0.09072505 0.0475717  0.04773794 0.04674619]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.09626901 0.08518109]\n"," [0.05148807 0.04365532]\n"," [0.05100676 0.04446667]\n"," [0.05035523 0.04310063]]\n","\n","Average MAE Loss:\n","[0.09072505 0.0475717  0.04773672 0.04672793]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.06313989 0.05248652]\n"," [0.05040611 0.04319867]\n"," [0.05130216 0.04532196]]\n","\n","Average MAE Loss:\n","[0.05781321 0.05781321 0.04680239 0.04831206]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.06313989 0.05248652]\n"," [0.05017492 0.04322887]\n"," [0.05074447 0.04312127]]\n","\n","Average MAE Loss:\n","[0.05781321 0.05781321 0.0467019  0.04693287]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.06313989 0.05248652]\n"," [0.05001003 0.04326278]\n"," [0.05054878 0.04336336]]\n","\n","Average MAE Loss:\n","[0.05781321 0.05781321 0.0466364  0.04695607]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.06313989 0.05248652]\n"," [0.04987421 0.04338922]\n"," [0.05035791 0.04322229]]\n","\n","Average MAE Loss:\n","[0.05781321 0.05781321 0.04663171 0.0467901 ]\n","\n","Epoch 00032: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.06313989 0.05248652]\n"," [0.04982567 0.0433592 ]\n"," [0.05032673 0.0431271 ]]\n","\n","Average MAE Loss:\n","[0.05781321 0.05781321 0.04659243 0.04672692]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.06313989 0.05248652]\n"," [0.04985131 0.04331513]\n"," [0.05028143 0.04317144]]\n","\n","Average MAE Loss:\n","[0.05781321 0.05781321 0.04658322 0.04672644]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.06313989 0.05248652]\n"," [0.04985003 0.04332944]\n"," [0.05029687 0.04312846]]\n","\n","Average MAE Loss:\n","[0.05781321 0.05781321 0.04658974 0.04671266]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.05029687 0.04312846]\n"," [0.05215865 0.0436521 ]\n"," [0.04999179 0.04274599]]\n","\n","Average MAE Loss:\n","[0.05781321 0.04671266 0.04790537 0.04636889]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.05029687 0.04312846]\n"," [0.05050719 0.04468913]\n"," [0.05009306 0.0431002 ]]\n","\n","Average MAE Loss:\n","[0.05781321 0.04671266 0.04759816 0.04659663]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.05029687 0.04312846]\n"," [0.05020125 0.04332503]\n"," [0.05023342 0.04303113]]\n","\n","Average MAE Loss:\n","[0.05781321 0.04671266 0.04676314 0.04663227]\n","\n","Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.05029687 0.04312846]\n"," [0.05011034 0.04335439]\n"," [0.05019552 0.04306595]]\n","\n","Average MAE Loss:\n","[0.05781321 0.04671266 0.04673237 0.04663073]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.05029687 0.04312846]\n"," [0.0500683  0.0434542 ]\n"," [0.05025944 0.0431003 ]]\n","\n","Average MAE Loss:\n","[0.05781321 0.04671266 0.04676125 0.04667987]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.05029687 0.04312846]\n"," [0.05002943 0.04344465]\n"," [0.05020156 0.04311422]]\n","\n","Average MAE Loss:\n","[0.05781321 0.04671266 0.04673704 0.04665789]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06313989 0.05248652]\n"," [0.05029687 0.04312846]\n"," [0.05001534 0.04344274]\n"," [0.05022044 0.04311017]]\n","\n","Average MAE Loss:\n","[0.05781321 0.04671266 0.04672904 0.04666531]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05157823 0.04305788]\n"," [0.05079583 0.04273498]\n"," [0.05046905 0.04274381]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04731805 0.04676541 0.04660643]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05157823 0.04305788]\n"," [0.05013668 0.04275998]\n"," [0.05014303 0.0431245 ]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04731805 0.04644833 0.04663377]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05157823 0.04305788]\n"," [0.04994219 0.04301694]\n"," [0.05016142 0.04307019]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04731805 0.04647956 0.0466158 ]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05157823 0.04305788]\n"," [0.04992033 0.04316831]\n"," [0.0501757  0.04307682]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04731805 0.04654432 0.04662626]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05157823 0.04305788]\n"," [0.04991747 0.04321089]\n"," [0.05016564 0.04309329]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04731805 0.04656418 0.04662947]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05157823 0.04305788]\n"," [0.04991105 0.04325467]\n"," [0.05016211 0.0430917 ]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04731805 0.04658286 0.04662691]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05157823 0.04305788]\n"," [0.04990544 0.04327275]\n"," [0.05015802 0.04309162]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04731805 0.0465891  0.04662482]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05015802 0.04309162]\n"," [0.05113015 0.04283625]\n"," [0.04986905 0.0431103 ]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04662482 0.0469832  0.04648967]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05015802 0.04309162]\n"," [0.05061388 0.04266943]\n"," [0.04984079 0.04295096]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04662482 0.04664166 0.04639587]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05015802 0.04309162]\n"," [0.05027385 0.04267311]\n"," [0.0498143  0.04288269]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04662482 0.04647348 0.04634849]\n","\n","Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05015802 0.04309162]\n"," [0.05016937 0.0427067 ]\n"," [0.04979661 0.04286572]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04662482 0.04643804 0.04633117]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05015802 0.04309162]\n"," [0.05009152 0.04274255]\n"," [0.0497933  0.0428668 ]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04662482 0.04641703 0.04633005]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05015802 0.04309162]\n"," [0.05003456 0.0427817 ]\n"," [0.04980388 0.04287724]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04662482 0.04640813 0.04634056]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05157823 0.04305788]\n"," [0.05015802 0.04309162]\n"," [0.04999496 0.04282596]\n"," [0.0498136  0.04289367]]\n","\n","Average MAE Loss:\n","[0.04731805 0.04662482 0.04641046 0.04635363]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05019015 0.04269288]\n"," [0.05019015 0.04269288]\n"," [0.05010968 0.04271584]\n"," [0.05012182 0.04275443]]\n","\n","Average MAE Loss:\n","[0.04644152 0.04644152 0.04641276 0.04643812]\n","\n","Epoch 00057: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05019015 0.04269288]\n"," [0.05019015 0.04269288]\n"," [0.05002992 0.04275101]\n"," [0.0500755  0.04279397]]\n","\n","Average MAE Loss:\n","[0.04644152 0.04644152 0.04639046 0.04643474]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05019015 0.04269288]\n"," [0.05019015 0.04269288]\n"," [0.04997007 0.04278723]\n"," [0.05003969 0.04282707]]\n","\n","Average MAE Loss:\n","[0.04644152 0.04644152 0.04637865 0.04643338]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05019015 0.04269288]\n"," [0.05019015 0.04269288]\n"," [0.04992691 0.04282411]\n"," [0.05001707 0.0428545 ]]\n","\n","Average MAE Loss:\n","[0.04644152 0.04644152 0.04637551 0.04643579]\n","\n","\n","epochs finished with time:26.080867767333984\n","\n","[[0.05019015 0.04269288]\n"," [0.05019015 0.04269288]\n"," [0.04992691 0.04282411]\n"," [0.05001707 0.0428545 ]]\n","00:00:41.600429544000235\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.09247705 0.10991617]\n"," [0.07581054 0.09699354]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.10119661 0.08640204]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.07076157 0.09252671]\n"," [0.05120014 0.07025217]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.08164414 0.06072616]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.07116695 0.09257179]\n"," [0.04874507 0.0687757 ]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.08186937 0.05876038]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.0703409  0.09228829]\n"," [0.04790583 0.0683587 ]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.08131459 0.05813226]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.07058699 0.09230561]\n"," [0.0492014  0.06857727]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.0814463  0.05888934]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.07035614 0.09220721]\n"," [0.0479804  0.068286  ]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.08128167 0.0581332 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08237317 0.10481952]\n"," [0.08241    0.10488542]\n"," [0.07031135 0.0919871 ]\n"," [0.04806891 0.06855149]]\n","\n","Average MAE Loss:\n","[0.09359635 0.09364771 0.08114922 0.0583102 ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.04806891 0.06855149]\n"," [0.04976278 0.07126863]\n"," [0.07068335 0.09217272]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0583102  0.06051571 0.08142804]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.04806891 0.06855149]\n"," [0.04752346 0.06785179]\n"," [0.06945198 0.09108394]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0583102  0.05768763 0.08026796]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.04806891 0.06855149]\n"," [0.04624866 0.06695596]\n"," [0.06931873 0.09089576]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0583102  0.05660231 0.08010725]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.04806891 0.06855149]\n"," [0.04607747 0.06712106]\n"," [0.06922039 0.0909344 ]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0583102  0.05659927 0.0800774 ]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.04806891 0.06855149]\n"," [0.04592235 0.06702696]\n"," [0.06916721 0.09080242]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0583102  0.05647466 0.07998481]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.04806891 0.06855149]\n"," [0.0460017  0.06716324]\n"," [0.06956372 0.09090562]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0583102  0.05658247 0.08023467]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.08241    0.10488542]\n"," [0.04806891 0.06855149]\n"," [0.04636696 0.06750744]\n"," [0.0693169  0.09078466]]\n","\n","Average MAE Loss:\n","[0.09364771 0.0583102  0.0569372  0.08005078]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.07942612 0.10195038]\n"," [0.04326878 0.06378902]\n"," [0.0506062  0.06899397]]\n","\n","Average MAE Loss:\n","[0.09068825 0.09068825 0.0535289  0.05980008]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.07942612 0.10195038]\n"," [0.04121488 0.06119259]\n"," [0.04267479 0.06290936]]\n","\n","Average MAE Loss:\n","[0.09068825 0.09068825 0.05120374 0.05279208]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.07942612 0.10195038]\n"," [0.04061463 0.06129308]\n"," [0.04184258 0.0620771 ]]\n","\n","Average MAE Loss:\n","[0.09068825 0.09068825 0.05095386 0.05195984]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.07942612 0.10195038]\n"," [0.04062539 0.06122934]\n"," [0.04174106 0.06178829]]\n","\n","Average MAE Loss:\n","[0.09068825 0.09068825 0.05092737 0.05176467]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.07942612 0.10195038]\n"," [0.0407718  0.0615751 ]\n"," [0.04226839 0.06191052]]\n","\n","Average MAE Loss:\n","[0.09068825 0.09068825 0.05117345 0.05208946]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.07942612 0.10195038]\n"," [0.04059286 0.0610655 ]\n"," [0.0420172  0.06183765]]\n","\n","Average MAE Loss:\n","[0.09068825 0.09068825 0.05082918 0.05192743]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.07942612 0.10195038]\n"," [0.04037724 0.06094251]\n"," [0.04176923 0.06171156]]\n","\n","Average MAE Loss:\n","[0.09068825 0.09068825 0.05065988 0.0517404 ]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.04176923 0.06171156]\n"," [0.05222296 0.07038827]\n"," [0.04001685 0.05996176]]\n","\n","Average MAE Loss:\n","[0.09068825 0.0517404  0.06130562 0.04998931]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.04176923 0.06171156]\n"," [0.04335317 0.06369276]\n"," [0.04066588 0.06011489]]\n","\n","Average MAE Loss:\n","[0.09068825 0.0517404  0.05352296 0.05039039]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.04176923 0.06171156]\n"," [0.04247655 0.06313003]\n"," [0.04048105 0.0600815 ]]\n","\n","Average MAE Loss:\n","[0.09068825 0.0517404  0.05280329 0.05028128]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.04176923 0.06171156]\n"," [0.04206039 0.06295118]\n"," [0.04069494 0.06015129]]\n","\n","Average MAE Loss:\n","[0.09068825 0.0517404  0.05250579 0.05042312]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.04176923 0.06171156]\n"," [0.04200108 0.06247266]\n"," [0.04069999 0.06013225]]\n","\n","Average MAE Loss:\n","[0.09068825 0.0517404  0.05223687 0.05041612]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.04176923 0.06171156]\n"," [0.04278833 0.06304134]\n"," [0.04068959 0.06013107]]\n","\n","Average MAE Loss:\n","[0.09068825 0.0517404  0.05291484 0.05041033]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07942612 0.10195038]\n"," [0.04176923 0.06171156]\n"," [0.04214511 0.06268773]\n"," [0.0406892  0.06012775]]\n","\n","Average MAE Loss:\n","[0.09068825 0.0517404  0.05241642 0.05040848]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.05045817 0.07229612]\n"," [0.04083122 0.06094822]\n"," [0.04174687 0.06132139]]\n","\n","Average MAE Loss:\n","[0.06137714 0.06137714 0.05088972 0.05153413]\n","\n","Epoch 00029: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.05045817 0.07229612]\n"," [0.04004239 0.06010013]\n"," [0.04037765 0.06029159]]\n","\n","Average MAE Loss:\n","[0.06137714 0.06137714 0.05007126 0.05033462]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00030: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.05045817 0.07229612]\n"," [0.04042951 0.06070314]\n"," [0.04063878 0.06018139]]\n","\n","Average MAE Loss:\n","[0.06137714 0.06137714 0.05056633 0.05041008]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.05045817 0.07229612]\n"," [0.0403985  0.06062151]\n"," [0.04081956 0.06018126]]\n","\n","Average MAE Loss:\n","[0.06137714 0.06137714 0.05051001 0.05050041]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.05045817 0.07229612]\n"," [0.04066896 0.06078976]\n"," [0.04071932 0.06015038]]\n","\n","Average MAE Loss:\n","[0.06137714 0.06137714 0.05072936 0.05043485]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.05045817 0.07229612]\n"," [0.04046724 0.06060544]\n"," [0.04084587 0.06018236]]\n","\n","Average MAE Loss:\n","[0.06137714 0.06137714 0.05053634 0.05051411]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.05045817 0.07229612]\n"," [0.04062296 0.06073329]\n"," [0.04081418 0.06017123]]\n","\n","Average MAE Loss:\n","[0.06137714 0.06137714 0.05067812 0.05049271]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.04081418 0.06017123]\n"," [0.04115362 0.06179013]\n"," [0.0403938  0.0605261 ]]\n","\n","Average MAE Loss:\n","[0.06137714 0.05049271 0.05147188 0.05045995]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.04081418 0.06017123]\n"," [0.04072423 0.06051841]\n"," [0.04020446 0.06031105]]\n","\n","Average MAE Loss:\n","[0.06137714 0.05049271 0.05062132 0.05025775]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.04081418 0.06017123]\n"," [0.04018887 0.06032698]\n"," [0.04015733 0.0601727 ]]\n","\n","Average MAE Loss:\n","[0.06137714 0.05049271 0.05025793 0.05016501]\n","\n","Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00038: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.04081418 0.06017123]\n"," [0.0403908  0.06049408]\n"," [0.04015619 0.06011984]]\n","\n","Average MAE Loss:\n","[0.06137714 0.05049271 0.05044244 0.05013802]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.04081418 0.06017123]\n"," [0.04038522 0.06055892]\n"," [0.04016639 0.0600791 ]]\n","\n","Average MAE Loss:\n","[0.06137714 0.05049271 0.05047207 0.05012275]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.04081418 0.06017123]\n"," [0.04042732 0.06060527]\n"," [0.04017978 0.06004745]]\n","\n","Average MAE Loss:\n","[0.06137714 0.05049271 0.0505163  0.05011361]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05045817 0.07229612]\n"," [0.04081418 0.06017123]\n"," [0.04046319 0.06062876]\n"," [0.04019278 0.06001886]]\n","\n","Average MAE Loss:\n","[0.06137714 0.05049271 0.05054598 0.05010582]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00042: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.04051986 0.06113476]\n"," [0.04005976 0.06051616]\n"," [0.0403791  0.06094786]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05082731 0.05028796 0.05066348]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.04051986 0.06113476]\n"," [0.03983019 0.06008627]\n"," [0.04020288 0.06069818]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05082731 0.04995823 0.05045053]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.04051986 0.06113476]\n"," [0.03996244 0.06009299]\n"," [0.04007971 0.06049028]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05082731 0.05002771 0.05028499]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.04051986 0.06113476]\n"," [0.0400992  0.0601472 ]\n"," [0.03999553 0.06032461]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05082731 0.0501232  0.05016007]\n","\n","Epoch 00046: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.04051986 0.06113476]\n"," [0.040149   0.06018862]\n"," [0.03996696 0.06026123]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05082731 0.05016881 0.0501141 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.04051986 0.06113476]\n"," [0.04016045 0.06022441]\n"," [0.03994676 0.06020968]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05082731 0.05019243 0.05007822]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.04051986 0.06113476]\n"," [0.04016745 0.0602445 ]\n"," [0.03993083 0.06016684]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05082731 0.05020597 0.05004883]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.03993083 0.06016684]\n"," [0.04025658 0.06081176]\n"," [0.04017727 0.06023314]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05004883 0.05053417 0.0502052 ]\n","\n","Epoch 00050: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.03993083 0.06016684]\n"," [0.03998626 0.06041818]\n"," [0.04016436 0.06021897]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05004883 0.05020222 0.05019167]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.03993083 0.06016684]\n"," [0.03984996 0.0601711 ]\n"," [0.04014945 0.06020427]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05004883 0.05001053 0.05017686]\n","\n","Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.03993083 0.06016684]\n"," [0.0398223  0.06010951]\n"," [0.04013536 0.06019029]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05004883 0.0499659  0.05016282]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.03993083 0.06016684]\n"," [0.03981432 0.06007934]\n"," [0.04012245 0.06017692]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05004883 0.04994683 0.05014968]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.03993083 0.06016684]\n"," [0.03982499 0.06006839]\n"," [0.04011676 0.06017049]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05004883 0.04994669 0.05014363]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04051986 0.06113476]\n"," [0.03993083 0.06016684]\n"," [0.03984872 0.06006886]\n"," [0.04011142 0.06016423]]\n","\n","Average MAE Loss:\n","[0.05082731 0.05004883 0.04995879 0.05013782]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03982516 0.06012399]\n"," [0.03982516 0.06012399]\n"," [0.03980844 0.06008377]\n"," [0.03982468 0.06011871]]\n","\n","Average MAE Loss:\n","[0.04997457 0.04997457 0.04994611 0.04997169]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03982516 0.06012399]\n"," [0.03982516 0.06012399]\n"," [0.03981003 0.06006022]\n"," [0.03982331 0.06011082]]\n","\n","Average MAE Loss:\n","[0.04997457 0.04997457 0.04993512 0.04996707]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03982516 0.06012399]\n"," [0.03982516 0.06012399]\n"," [0.03983261 0.06005451]\n"," [0.03982203 0.06010291]]\n","\n","Average MAE Loss:\n","[0.04997457 0.04997457 0.04994356 0.04996247]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03982516 0.06012399]\n"," [0.03982516 0.06012399]\n"," [0.03986764 0.06006034]\n"," [0.03982095 0.0600951 ]]\n","\n","Average MAE Loss:\n","[0.04997457 0.04997457 0.04996399 0.04995803]\n","\n","\n","epochs finished with time:26.575764656066895\n","\n","[[0.03982516 0.06012399]\n"," [0.03982516 0.06012399]\n"," [0.03986764 0.06006034]\n"," [0.03982095 0.0600951 ]]\n","00:00:42.08842331999995\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06739103 0.06996016]\n"," [0.07494998 0.07988729]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.0686756  0.07741863]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06181394 0.06412692]\n"," [0.06211015 0.06308409]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06297043 0.06259712]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06213722 0.06455851]\n"," [0.06171874 0.06324714]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06334786 0.06248294]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.061882   0.0644711 ]\n"," [0.06096504 0.06251045]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06317655 0.06173774]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06197553 0.06461914]\n"," [0.06195066 0.06378173]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06329733 0.0628662 ]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06141108 0.06419605]\n"," [0.06188003 0.06376306]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06280356 0.06282154]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10222475 0.10660766]\n"," [0.10227572 0.10665325]\n"," [0.06188722 0.06448822]\n"," [0.06146022 0.06388205]]\n","\n","Average MAE Loss:\n","[0.10441621 0.10446449 0.06318772 0.06267113]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06146022 0.06388205]\n"," [0.06387113 0.06445302]\n"," [0.0630181  0.06466792]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06267113 0.06416208 0.06384301]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06146022 0.06388205]\n"," [0.06210331 0.06441877]\n"," [0.06304406 0.06573356]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06267113 0.06326104 0.06438881]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06146022 0.06388205]\n"," [0.06202774 0.06422524]\n"," [0.06308243 0.06590956]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06267113 0.06312649 0.064496  ]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06146022 0.06388205]\n"," [0.06126142 0.06376767]\n"," [0.06302629 0.06582736]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06267113 0.06251455 0.06442682]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06146022 0.06388205]\n"," [0.06125339 0.06359376]\n"," [0.06226084 0.06495479]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06267113 0.06242357 0.06360782]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06146022 0.06388205]\n"," [0.06020383 0.06231792]\n"," [0.0602268  0.06154001]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06267113 0.06126088 0.06088341]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.10227572 0.10665325]\n"," [0.06146022 0.06388205]\n"," [0.06004649 0.06226414]\n"," [0.06066267 0.06241872]]\n","\n","Average MAE Loss:\n","[0.10446449 0.06267113 0.06115532 0.06154069]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.09962279 0.10408529]\n"," [0.05541075 0.05731386]\n"," [0.05483109 0.05334591]]\n","\n","Average MAE Loss:\n","[0.10185404 0.10185404 0.0563623  0.0540885 ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.09962279 0.10408529]\n"," [0.05213589 0.05351967]\n"," [0.05267223 0.0539881 ]]\n","\n","Average MAE Loss:\n","[0.10185404 0.10185404 0.05282778 0.05333016]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.09962279 0.10408529]\n"," [0.05112164 0.05235565]\n"," [0.05230807 0.05401585]]\n","\n","Average MAE Loss:\n","[0.10185404 0.10185404 0.05173865 0.05316196]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.09962279 0.10408529]\n"," [0.05071319 0.05163734]\n"," [0.05208134 0.05352398]]\n","\n","Average MAE Loss:\n","[0.10185404 0.10185404 0.05117527 0.05280266]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.09962279 0.10408529]\n"," [0.05059627 0.05167246]\n"," [0.05174069 0.05296345]]\n","\n","Average MAE Loss:\n","[0.10185404 0.10185404 0.05113437 0.05235207]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.09962279 0.10408529]\n"," [0.05051925 0.05164059]\n"," [0.05158729 0.05255642]]\n","\n","Average MAE Loss:\n","[0.10185404 0.10185404 0.05107992 0.05207185]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.09962279 0.10408529]\n"," [0.05037273 0.05179754]\n"," [0.05179224 0.05296874]]\n","\n","Average MAE Loss:\n","[0.10185404 0.10185404 0.05108514 0.05238049]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.05179224 0.05296874]\n"," [0.05770104 0.05911591]\n"," [0.05054548 0.05196896]]\n","\n","Average MAE Loss:\n","[0.10185404 0.05238049 0.05840847 0.05125722]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.05179224 0.05296874]\n"," [0.05494221 0.0566135 ]\n"," [0.05101228 0.0518402 ]]\n","\n","Average MAE Loss:\n","[0.10185404 0.05238049 0.05577786 0.05142624]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.05179224 0.05296874]\n"," [0.05252741 0.0538501 ]\n"," [0.05128219 0.05205074]]\n","\n","Average MAE Loss:\n","[0.10185404 0.05238049 0.05318875 0.05166646]\n","\n","Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.05179224 0.05296874]\n"," [0.05136437 0.05248315]\n"," [0.05132446 0.05227051]]\n","\n","Average MAE Loss:\n","[0.10185404 0.05238049 0.05192376 0.05179749]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.05179224 0.05296874]\n"," [0.05110471 0.052173  ]\n"," [0.05136638 0.05226631]]\n","\n","Average MAE Loss:\n","[0.10185404 0.05238049 0.05163885 0.05181634]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.05179224 0.05296874]\n"," [0.05115899 0.05228509]\n"," [0.0513028  0.0521853 ]]\n","\n","Average MAE Loss:\n","[0.10185404 0.05238049 0.05172204 0.05174405]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.09962279 0.10408529]\n"," [0.05179224 0.05296874]\n"," [0.05131312 0.05245281]\n"," [0.05129915 0.05219831]]\n","\n","Average MAE Loss:\n","[0.10185404 0.05238049 0.05188296 0.05174873]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.06787481 0.07180044]\n"," [0.04942284 0.04890653]\n"," [0.04988442 0.04956929]]\n","\n","Average MAE Loss:\n","[0.06983763 0.06983763 0.04916469 0.04972686]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.06787481 0.07180044]\n"," [0.04994502 0.05109524]\n"," [0.05046365 0.0510764 ]]\n","\n","Average MAE Loss:\n","[0.06983763 0.06983763 0.05052013 0.05077003]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.06787481 0.07180044]\n"," [0.04987439 0.05097637]\n"," [0.05106032 0.05216753]]\n","\n","Average MAE Loss:\n","[0.06983763 0.06983763 0.05042538 0.05161393]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.06787481 0.07180044]\n"," [0.04945042 0.05046863]\n"," [0.0505863  0.05153368]]\n","\n","Average MAE Loss:\n","[0.06983763 0.06983763 0.04995953 0.05105999]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.06787481 0.07180044]\n"," [0.0495184  0.05067873]\n"," [0.05053187 0.05119031]]\n","\n","Average MAE Loss:\n","[0.06983763 0.06983763 0.05009857 0.05086109]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.06787481 0.07180044]\n"," [0.04953098 0.05074844]\n"," [0.05078841 0.05174358]]\n","\n","Average MAE Loss:\n","[0.06983763 0.06983763 0.05013971 0.05126599]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.06787481 0.07180044]\n"," [0.04949728 0.05068043]\n"," [0.05062126 0.05152602]]\n","\n","Average MAE Loss:\n","[0.06983763 0.06983763 0.05008886 0.05107364]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.05062126 0.05152602]\n"," [0.05604935 0.05857362]\n"," [0.04982745 0.0510527 ]]\n","\n","Average MAE Loss:\n","[0.06983763 0.05107364 0.05731148 0.05044007]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.05062126 0.05152602]\n"," [0.04939635 0.04942302]\n"," [0.04973545 0.05074215]]\n","\n","Average MAE Loss:\n","[0.06983763 0.05107364 0.04940968 0.0502388 ]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.05062126 0.05152602]\n"," [0.04970907 0.05054583]\n"," [0.04985364 0.05086269]]\n","\n","Average MAE Loss:\n","[0.06983763 0.05107364 0.05012745 0.05035817]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.05062126 0.05152602]\n"," [0.04977301 0.05078752]\n"," [0.04996643 0.05095914]]\n","\n","Average MAE Loss:\n","[0.06983763 0.05107364 0.05028027 0.05046279]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.05062126 0.05152602]\n"," [0.04963129 0.05060502]\n"," [0.05008434 0.0510547 ]]\n","\n","Average MAE Loss:\n","[0.06983763 0.05107364 0.05011815 0.05056952]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.05062126 0.05152602]\n"," [0.04958371 0.05058545]\n"," [0.05017661 0.0511192 ]]\n","\n","Average MAE Loss:\n","[0.06983763 0.05107364 0.05008458 0.05064791]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06787481 0.07180044]\n"," [0.05062126 0.05152602]\n"," [0.04957376 0.05059133]\n"," [0.05021705 0.05115912]]\n","\n","Average MAE Loss:\n","[0.06983763 0.05107364 0.05008255 0.05068809]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05329786 0.05541762]\n"," [0.05202921 0.05377153]\n"," [0.05206325 0.05378852]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05435774 0.05290037 0.05292588]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05329786 0.05541762]\n"," [0.05060754 0.05189395]\n"," [0.05077153 0.0520013 ]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05435774 0.05125074 0.05138642]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05329786 0.05541762]\n"," [0.04992686 0.05094346]\n"," [0.05034021 0.05130284]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05435774 0.05043516 0.05082153]\n","\n","Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05329786 0.05541762]\n"," [0.0498024  0.05077643]\n"," [0.05032648 0.05127361]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05435774 0.05028941 0.05080005]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05329786 0.05541762]\n"," [0.04975203 0.05071885]\n"," [0.0503524  0.05131545]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05435774 0.05023544 0.05083392]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05329786 0.05541762]\n"," [0.04971853 0.05068689]\n"," [0.05036952 0.05133904]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05435774 0.05020271 0.05085428]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05329786 0.05541762]\n"," [0.04968759 0.0506583 ]\n"," [0.05038163 0.05135331]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05435774 0.05017295 0.05086747]\n","\n","Epoch 00049: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00049: reducing learning rate of group 0 to 3.9063e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05038163 0.05135331]\n"," [0.05295803 0.05498295]\n"," [0.0497182  0.05070671]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05086747 0.05397049 0.05021246]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05038163 0.05135331]\n"," [0.05245953 0.05433572]\n"," [0.04976521 0.05077876]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05086747 0.05339763 0.05027198]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05038163 0.05135331]\n"," [0.05200062 0.05374768]\n"," [0.04980537 0.05083787]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05086747 0.05287415 0.05032162]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05038163 0.05135331]\n"," [0.05160039 0.05323497]\n"," [0.04983849 0.05088153]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05086747 0.05241768 0.05036001]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00053: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05038163 0.05135331]\n"," [0.05142204 0.0530063 ]\n"," [0.0498531  0.05089917]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05086747 0.05221417 0.05037613]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05038163 0.05135331]\n"," [0.05126205 0.05279725]\n"," [0.04986703 0.05091502]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05086747 0.05202965 0.05039102]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05329786 0.05541762]\n"," [0.05038163 0.05135331]\n"," [0.05111416 0.05260386]\n"," [0.04988111 0.0509305 ]]\n","\n","Average MAE Loss:\n","[0.05435774 0.05086747 0.05185901 0.05040581]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.05101413 0.0524489 ]\n"," [0.05101413 0.0524489 ]\n"," [0.05088538 0.05227539]\n"," [0.05096599 0.05237655]]\n","\n","Average MAE Loss:\n","[0.05173151 0.05173151 0.05158038 0.05167127]\n","\n","Epoch 00057: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00057: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.05101413 0.0524489 ]\n"," [0.05101413 0.0524489 ]\n"," [0.05082692 0.05219583]\n"," [0.05092266 0.05231315]]\n","\n","Average MAE Loss:\n","[0.05173151 0.05173151 0.05151137 0.05161791]\n","\n","Epoch 00058: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.05101413 0.0524489 ]\n"," [0.05101413 0.0524489 ]\n"," [0.05077207 0.05212048]\n"," [0.0508798  0.0522494 ]]\n","\n","Average MAE Loss:\n","[0.05173151 0.05173151 0.05144627 0.0515646 ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.05101413 0.0524489 ]\n"," [0.05101413 0.0524489 ]\n"," [0.05072083 0.0520493 ]\n"," [0.05083927 0.05218792]]\n","\n","Average MAE Loss:\n","[0.05173151 0.05173151 0.05138507 0.05151359]\n","\n","\n","epochs finished with time:26.63526225090027\n","\n","[[0.05101413 0.0524489 ]\n"," [0.05101413 0.0524489 ]\n"," [0.05072083 0.0520493 ]\n"," [0.05083927 0.05218792]]\n","00:00:42.25719574999994\n"]}],"source":["# seed = 10 table = 2/8 fixed folds CNN2\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='CNN2_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sl0GgAoLrmOz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-46xld4rmRb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343327,"status":"ok","timestamp":1690030575338,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"UW_QuLHxgLST","outputId":"c8f4467a-015a-42fb-bc36-9af3e5d996ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07632864 0.07342375]\n"," [0.0858584  0.08280423]\n"," [0.06958028 0.06679701]\n"," [0.0743964  0.07121451]]\n","\n","Average MAE Loss:\n","[0.0748762  0.08433132 0.06818865 0.07280545]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07513348 0.07181109]\n"," [0.0858584  0.08280423]\n"," [0.07235302 0.06956607]\n"," [0.06359088 0.06163194]]\n","\n","Average MAE Loss:\n","[0.07347229 0.08433132 0.07095955 0.06261141]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07448399 0.07126369]\n"," [0.0858584  0.08280423]\n"," [0.06782266 0.06516868]\n"," [0.0609222  0.05872008]]\n","\n","Average MAE Loss:\n","[0.07287384 0.08433132 0.06649567 0.05982114]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07388061 0.07068176]\n"," [0.0858584  0.08280423]\n"," [0.07369673 0.07058889]\n"," [0.06265584 0.06064655]]\n","\n","Average MAE Loss:\n","[0.07228118 0.08433132 0.07214281 0.0616512 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07453573 0.07119271]\n"," [0.0858584  0.08280423]\n"," [0.05734262 0.0552469 ]\n"," [0.06024759 0.05800711]]\n","\n","Average MAE Loss:\n","[0.07286422 0.08433132 0.05629476 0.05912735]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.073799   0.0706793 ]\n"," [0.0858584  0.08280423]\n"," [0.04867573 0.04644208]\n"," [0.05935991 0.05783711]]\n","\n","Average MAE Loss:\n","[0.07223915 0.08433132 0.0475589  0.05859851]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07428281 0.07088701]\n"," [0.0858584  0.08280423]\n"," [0.04724127 0.04498371]\n"," [0.0604039  0.05862359]]\n","\n","Average MAE Loss:\n","[0.07258491 0.08433132 0.04611249 0.05951374]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04972744 0.04714534]\n"," [0.0604039  0.05862359]\n"," [0.07491075 0.07177119]\n"," [0.05180121 0.0501091 ]]\n","\n","Average MAE Loss:\n","[0.04843639 0.05951374 0.07334097 0.05095516]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04718961 0.04459137]\n"," [0.0604039  0.05862359]\n"," [0.07431881 0.07129671]\n"," [0.04496939 0.04275458]]\n","\n","Average MAE Loss:\n","[0.04589049 0.05951374 0.07280776 0.04386199]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0434461  0.04076683]\n"," [0.0604039  0.05862359]\n"," [0.07374063 0.07059958]\n"," [0.04392302 0.04210071]]\n","\n","Average MAE Loss:\n","[0.04210646 0.05951374 0.0721701  0.04301187]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04269106 0.03962925]\n"," [0.0604039  0.05862359]\n"," [0.07351367 0.07055718]\n"," [0.0433307  0.0418694 ]]\n","\n","Average MAE Loss:\n","[0.04116016 0.05951374 0.07203543 0.04260005]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04207568 0.03922179]\n"," [0.0604039  0.05862359]\n"," [0.07135236 0.06828899]\n"," [0.04405708 0.04185355]]\n","\n","Average MAE Loss:\n","[0.04064873 0.05951374 0.06982068 0.04295532]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04119296 0.03836432]\n"," [0.0604039  0.05862359]\n"," [0.06883727 0.06587378]\n"," [0.04561517 0.04306157]]\n","\n","Average MAE Loss:\n","[0.03977864 0.05951374 0.06735552 0.04433837]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04093579 0.03822326]\n"," [0.0604039  0.05862359]\n"," [0.06835367 0.0654985 ]\n"," [0.04322739 0.04120218]]\n","\n","Average MAE Loss:\n","[0.03957953 0.05951374 0.06692608 0.04221479]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0427876  0.04057249]\n"," [0.07787746 0.07467472]\n"," [0.04424101 0.04216821]\n"," [0.04226351 0.04006189]]\n","\n","Average MAE Loss:\n","[0.04168005 0.07627609 0.04320461 0.0411627 ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04192806 0.03899966]\n"," [0.07787746 0.07467472]\n"," [0.04298512 0.04106842]\n"," [0.04176103 0.04006177]]\n","\n","Average MAE Loss:\n","[0.04046386 0.07627609 0.04202677 0.0409114 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04238208 0.03969533]\n"," [0.07787746 0.07467472]\n"," [0.04200916 0.04044659]\n"," [0.04367896 0.04152496]]\n","\n","Average MAE Loss:\n","[0.04103871 0.07627609 0.04122788 0.04260196]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04060331 0.03793208]\n"," [0.07787746 0.07467472]\n"," [0.04173271 0.04008572]\n"," [0.04100735 0.03914315]]\n","\n","Average MAE Loss:\n","[0.0392677  0.07627609 0.04090922 0.04007525]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04089226 0.0382009 ]\n"," [0.07787746 0.07467472]\n"," [0.04163848 0.03954263]\n"," [0.04126511 0.03932206]]\n","\n","Average MAE Loss:\n","[0.03954658 0.07627609 0.04059055 0.04029358]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04106433 0.03825183]\n"," [0.07787746 0.07467472]\n"," [0.04149432 0.03974574]\n"," [0.04079547 0.03920872]]\n","\n","Average MAE Loss:\n","[0.03965808 0.07627609 0.04062003 0.04000209]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04222476 0.04002103]\n"," [0.07787746 0.07467472]\n"," [0.04166644 0.04002907]\n"," [0.04014626 0.03882202]]\n","\n","Average MAE Loss:\n","[0.0411229  0.07627609 0.04084776 0.03948414]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04382438 0.04121201]\n"," [0.04014626 0.03882202]\n"," [0.04102246 0.03877644]\n"," [0.04114891 0.03939917]]\n","\n","Average MAE Loss:\n","[0.0425182  0.03948414 0.03989945 0.04027404]\n","\n","Epoch 00022: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04142551 0.03879889]\n"," [0.04014626 0.03882202]\n"," [0.04083307 0.0389414 ]\n"," [0.04147345 0.0394767 ]]\n","\n","Average MAE Loss:\n","[0.0401122  0.03948414 0.03988723 0.04047508]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04095717 0.03862025]\n"," [0.04014626 0.03882202]\n"," [0.04075671 0.03899031]\n"," [0.04087992 0.0392065 ]]\n","\n","Average MAE Loss:\n","[0.03978871 0.03948414 0.03987351 0.04004321]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04077431 0.03819759]\n"," [0.04014626 0.03882202]\n"," [0.04037092 0.03837866]\n"," [0.04057937 0.03883366]]\n","\n","Average MAE Loss:\n","[0.03948595 0.03948414 0.03937479 0.03970651]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04033605 0.03775675]\n"," [0.04014626 0.03882202]\n"," [0.04044596 0.03839512]\n"," [0.04051984 0.03858378]]\n","\n","Average MAE Loss:\n","[0.0390464  0.03948414 0.03942054 0.03955181]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04045742 0.03785232]\n"," [0.04014626 0.03882202]\n"," [0.04070227 0.03859346]\n"," [0.04050544 0.03847279]]\n","\n","Average MAE Loss:\n","[0.03915487 0.03948414 0.03964786 0.03948912]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04039753 0.03794866]\n"," [0.04014626 0.03882202]\n"," [0.0407003  0.0387472 ]\n"," [0.04093963 0.03930181]]\n","\n","Average MAE Loss:\n","[0.03917309 0.03948414 0.03972375 0.04012072]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03928712 0.03688098]\n"," [0.04023694 0.0380279 ]\n"," [0.03954396 0.03754335]\n"," [0.03974776 0.03808849]]\n","\n","Average MAE Loss:\n","[0.03808405 0.03913242 0.03854365 0.03891813]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0395375  0.03691568]\n"," [0.04023694 0.0380279 ]\n"," [0.03996662 0.03797845]\n"," [0.03925868 0.03770062]]\n","\n","Average MAE Loss:\n","[0.03822659 0.03913242 0.03897254 0.03847965]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03951409 0.0368937 ]\n"," [0.04023694 0.0380279 ]\n"," [0.04030801 0.03832337]\n"," [0.03927381 0.03776396]]\n","\n","Average MAE Loss:\n","[0.0382039  0.03913242 0.03931569 0.03851888]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03935034 0.03696306]\n"," [0.04023694 0.0380279 ]\n"," [0.04034771 0.03854704]\n"," [0.03908356 0.03749309]]\n","\n","Average MAE Loss:\n","[0.0381567  0.03913242 0.03944737 0.03828833]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.0393705  0.03670639]\n"," [0.04023694 0.0380279 ]\n"," [0.03995715 0.03804994]\n"," [0.03933262 0.03740577]]\n","\n","Average MAE Loss:\n","[0.03803844 0.03913242 0.03900355 0.03836919]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03941562 0.03690437]\n"," [0.04023694 0.0380279 ]\n"," [0.04046717 0.03860354]\n"," [0.03928438 0.03746575]]\n","\n","Average MAE Loss:\n","[0.03816    0.03913242 0.03953535 0.03837506]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03934457 0.03662065]\n"," [0.04023694 0.0380279 ]\n"," [0.03969164 0.03782267]\n"," [0.03905607 0.03736461]]\n","\n","Average MAE Loss:\n","[0.03798261 0.03913242 0.03875715 0.03821034]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03927144 0.03683098]\n"," [0.03905607 0.03736461]\n"," [0.03914112 0.03674767]\n"," [0.03888593 0.03710813]]\n","\n","Average MAE Loss:\n","[0.03805121 0.03821034 0.0379444  0.03799703]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03965374 0.03699084]\n"," [0.03905607 0.03736461]\n"," [0.03943725 0.03724206]\n"," [0.03882133 0.03714734]]\n","\n","Average MAE Loss:\n","[0.03832229 0.03821034 0.03833965 0.03798433]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03938785 0.03688748]\n"," [0.03905607 0.03736461]\n"," [0.03979768 0.03776921]\n"," [0.03884616 0.0373175 ]]\n","\n","Average MAE Loss:\n","[0.03813767 0.03821034 0.03878344 0.03808183]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03956833 0.03713331]\n"," [0.03905607 0.03736461]\n"," [0.0397557  0.03779125]\n"," [0.03881326 0.03726074]]\n","\n","Average MAE Loss:\n","[0.03835082 0.03821034 0.03877347 0.038037  ]\n","\n","Epoch 00039: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.0392446  0.03669344]\n"," [0.03905607 0.03736461]\n"," [0.03967571 0.03774038]\n"," [0.03893473 0.03745279]]\n","\n","Average MAE Loss:\n","[0.03796902 0.03821034 0.03870804 0.03819376]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03921295 0.03671373]\n"," [0.03905607 0.03736461]\n"," [0.0395956  0.03758356]\n"," [0.0391075  0.03765687]]\n","\n","Average MAE Loss:\n","[0.03796334 0.03821034 0.03858958 0.03838218]\n","\n","Epoch 00041: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03926891 0.03672105]\n"," [0.03905607 0.03736461]\n"," [0.03961162 0.03758714]\n"," [0.03907224 0.03754607]]\n","\n","Average MAE Loss:\n","[0.03799498 0.03821034 0.03859938 0.03830915]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03877776 0.03668398]\n"," [0.03873683 0.03672434]\n"," [0.03875405 0.036767  ]\n"," [0.03868264 0.03689312]]\n","\n","Average MAE Loss:\n","[0.03773087 0.03773059 0.03776052 0.03778788]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03910237 0.03673605]\n"," [0.03873683 0.03672434]\n"," [0.03914091 0.03716605]\n"," [0.03881349 0.03710747]]\n","\n","Average MAE Loss:\n","[0.03791921 0.03773059 0.03815348 0.03796048]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03907588 0.03645093]\n"," [0.03873683 0.03672434]\n"," [0.03927063 0.03729744]\n"," [0.03877254 0.0370877 ]]\n","\n","Average MAE Loss:\n","[0.03776341 0.03773059 0.03828404 0.03793012]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03911728 0.03653046]\n"," [0.03873683 0.03672434]\n"," [0.03942881 0.03746427]\n"," [0.03878781 0.03711248]]\n","\n","Average MAE Loss:\n","[0.03782387 0.03773059 0.03844654 0.03795015]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03918217 0.03677486]\n"," [0.03873683 0.03672434]\n"," [0.03960169 0.03769359]\n"," [0.03915733 0.03750824]]\n","\n","Average MAE Loss:\n","[0.03797852 0.03773059 0.03864764 0.03833279]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0391226  0.03668302]\n"," [0.03873683 0.03672434]\n"," [0.03960068 0.03770958]\n"," [0.03909637 0.03745231]]\n","\n","Average MAE Loss:\n","[0.03790281 0.03773059 0.03865513 0.03827434]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03911611 0.03662444]\n"," [0.03873683 0.03672434]\n"," [0.03953175 0.03762419]\n"," [0.03894288 0.03728432]]\n","\n","Average MAE Loss:\n","[0.03787028 0.03773059 0.03857797 0.0381136 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03870706 0.03664349]\n"," [0.03894288 0.03728432]\n"," [0.03907634 0.0366006 ]\n"," [0.03912793 0.03734332]]\n","\n","Average MAE Loss:\n","[0.03767527 0.0381136  0.03783847 0.03823562]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03879667 0.03660709]\n"," [0.03894288 0.03728432]\n"," [0.03902065 0.03664799]\n"," [0.03879411 0.03710366]]\n","\n","Average MAE Loss:\n","[0.03770188 0.0381136  0.03783432 0.03794889]\n","\n","Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00051: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03892487 0.03660155]\n"," [0.03894288 0.03728432]\n"," [0.03900237 0.03669558]\n"," [0.03871568 0.03704843]]\n","\n","Average MAE Loss:\n","[0.03776321 0.0381136  0.03784897 0.03788205]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03906784 0.03670793]\n"," [0.03894288 0.03728432]\n"," [0.03901098 0.03673536]\n"," [0.03868105 0.03705231]]\n","\n","Average MAE Loss:\n","[0.03788789 0.0381136  0.03787317 0.03786668]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03920389 0.03687689]\n"," [0.03894288 0.03728432]\n"," [0.0390572  0.03680601]\n"," [0.03875678 0.03713757]]\n","\n","Average MAE Loss:\n","[0.03804039 0.0381136  0.0379316  0.03794717]\n","\n","Epoch 00054: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03913431 0.03680302]\n"," [0.03894288 0.03728432]\n"," [0.03907202 0.03684245]\n"," [0.03863827 0.03703178]]\n","\n","Average MAE Loss:\n","[0.03796866 0.0381136  0.03795724 0.03783502]\n","\n","Epoch 00055: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03904483 0.03665415]\n"," [0.03894288 0.03728432]\n"," [0.03907523 0.03685545]\n"," [0.03862128 0.03700303]]\n","\n","Average MAE Loss:\n","[0.03784949 0.0381136  0.03796534 0.03781215]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03863803 0.03657355]\n"," [0.03863196 0.03666389]\n"," [0.03864395 0.03668451]\n"," [0.03863175 0.03668306]]\n","\n","Average MAE Loss:\n","[0.03760579 0.03764793 0.03766423 0.03765741]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03870724 0.03656907]\n"," [0.03863196 0.03666389]\n"," [0.03866244 0.03670622]\n"," [0.03862135 0.03670062]]\n","\n","Average MAE Loss:\n","[0.03763816 0.03764793 0.03768433 0.03766099]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03879955 0.03659991]\n"," [0.03863196 0.03666389]\n"," [0.03869108 0.03673755]\n"," [0.03863402 0.03672796]]\n","\n","Average MAE Loss:\n","[0.03769973 0.03764793 0.03771431 0.03768099]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03886033 0.03661897]\n"," [0.03863196 0.03666389]\n"," [0.03872477 0.0367752 ]\n"," [0.03863776 0.03674501]]\n","\n","Average MAE Loss:\n","[0.03773965 0.03764793 0.03774998 0.03769138]\n","\n","\n","epochs finished with time:48.50591015815735\n","\n","[[0.03886033 0.03661897]\n"," [0.03863196 0.03666389]\n"," [0.03872477 0.0367752 ]\n"," [0.03863776 0.03674501]]\n","00:01:9.567797229000007\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.1051002  0.10465337]\n"," [0.11635957 0.1145391 ]\n"," [0.09952453 0.09869113]\n"," [0.10200732 0.10030704]]\n","\n","Average MAE Loss:\n","[0.10487679 0.11544933 0.09910783 0.10115718]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10280758 0.10167047]\n"," [0.11635957 0.1145391 ]\n"," [0.09746099 0.09753405]\n"," [0.09320068 0.09067798]]\n","\n","Average MAE Loss:\n","[0.10223902 0.11544933 0.09749752 0.09193933]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10173333 0.10133227]\n"," [0.11635957 0.1145391 ]\n"," [0.0962926  0.09537244]\n"," [0.09015805 0.08800419]]\n","\n","Average MAE Loss:\n","[0.1015328  0.11544933 0.09583252 0.08908112]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10150443 0.10091184]\n"," [0.11635957 0.1145391 ]\n"," [0.09681808 0.09657897]\n"," [0.09273496 0.08997425]]\n","\n","Average MAE Loss:\n","[0.10120814 0.11544933 0.09669853 0.09135461]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10133255 0.10041306]\n"," [0.11635957 0.1145391 ]\n"," [0.09626582 0.09572197]\n"," [0.08984276 0.08801428]]\n","\n","Average MAE Loss:\n","[0.1008728  0.11544933 0.09599389 0.08892852]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10197219 0.1009302 ]\n"," [0.11635957 0.1145391 ]\n"," [0.09569094 0.09538373]\n"," [0.08885735 0.08720815]]\n","\n","Average MAE Loss:\n","[0.10145119 0.11544933 0.09553733 0.08803275]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10224161 0.10091203]\n"," [0.11635957 0.1145391 ]\n"," [0.09509907 0.09551613]\n"," [0.08898268 0.08678844]]\n","\n","Average MAE Loss:\n","[0.10157682 0.11544933 0.0953076  0.08788556]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08302654 0.08144822]\n"," [0.08898268 0.08678844]\n"," [0.10292342 0.10168363]\n"," [0.09596735 0.09470826]]\n","\n","Average MAE Loss:\n","[0.08223738 0.08788556 0.10230352 0.09533781]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07058014 0.07169561]\n"," [0.08898268 0.08678844]\n"," [0.1064072  0.10489904]\n"," [0.09514089 0.09428928]]\n","\n","Average MAE Loss:\n","[0.07113788 0.08788556 0.10565312 0.09471509]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06862756 0.07059004]\n"," [0.08898268 0.08678844]\n"," [0.10185401 0.10074377]\n"," [0.09489465 0.09370788]]\n","\n","Average MAE Loss:\n","[0.0696088  0.08788556 0.10129889 0.09430127]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07006348 0.07004431]\n"," [0.08898268 0.08678844]\n"," [0.10151976 0.10069772]\n"," [0.0990618  0.09755247]]\n","\n","Average MAE Loss:\n","[0.0700539  0.08788556 0.10110874 0.09830714]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0678542  0.06907737]\n"," [0.08898268 0.08678844]\n"," [0.09852099 0.09813064]\n"," [0.09400227 0.09374199]]\n","\n","Average MAE Loss:\n","[0.06846579 0.08788556 0.09832582 0.09387213]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06679989 0.0677434 ]\n"," [0.08898268 0.08678844]\n"," [0.09283487 0.09225558]\n"," [0.09421204 0.09288996]]\n","\n","Average MAE Loss:\n","[0.06727164 0.08788556 0.09254522 0.093551  ]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06668224 0.0683146 ]\n"," [0.08898268 0.08678844]\n"," [0.06961539 0.06921225]\n"," [0.09334145 0.09258167]]\n","\n","Average MAE Loss:\n","[0.06749842 0.08788556 0.06941382 0.09296156]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06648118 0.06807259]\n"," [0.11038756 0.10853616]\n"," [0.06961273 0.07022437]\n"," [0.07204684 0.07167533]]\n","\n","Average MAE Loss:\n","[0.06727689 0.10946186 0.06991855 0.07186109]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06638132 0.06778978]\n"," [0.11038756 0.10853616]\n"," [0.06896459 0.06981794]\n"," [0.07005122 0.06982237]]\n","\n","Average MAE Loss:\n","[0.06708555 0.10946186 0.06939126 0.06993679]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07116482 0.07051104]\n"," [0.11038756 0.10853616]\n"," [0.06680928 0.06792831]\n"," [0.06780677 0.06802938]]\n","\n","Average MAE Loss:\n","[0.07083793 0.10946186 0.06736879 0.06791808]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06482664 0.06547775]\n"," [0.11038756 0.10853616]\n"," [0.06613665 0.06739374]\n"," [0.06707342 0.06761596]]\n","\n","Average MAE Loss:\n","[0.0651522  0.10946186 0.0667652  0.06734469]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06448671 0.06516855]\n"," [0.11038756 0.10853616]\n"," [0.0664894  0.06774714]\n"," [0.06791177 0.06792923]]\n","\n","Average MAE Loss:\n","[0.06482763 0.10946186 0.06711827 0.0679205 ]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06596247 0.06629925]\n"," [0.11038756 0.10853616]\n"," [0.06586819 0.06709505]\n"," [0.06691418 0.06733605]]\n","\n","Average MAE Loss:\n","[0.06613086 0.10946186 0.06648162 0.06712511]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06475637 0.06585162]\n"," [0.11038756 0.10853616]\n"," [0.06647056 0.06741195]\n"," [0.07031226 0.06943233]]\n","\n","Average MAE Loss:\n","[0.065304   0.10946186 0.06694126 0.06987229]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07332363 0.07274355]\n"," [0.07031226 0.06943233]\n"," [0.06620392 0.06737072]\n"," [0.06674268 0.06682134]]\n","\n","Average MAE Loss:\n","[0.07303359 0.06987229 0.06678732 0.06678201]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07539912 0.07481716]\n"," [0.07031226 0.06943233]\n"," [0.06478941 0.06614413]\n"," [0.06668331 0.06638286]]\n","\n","Average MAE Loss:\n","[0.07510814 0.06987229 0.06546677 0.06653308]\n","\n","Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06551321 0.06686366]\n"," [0.07031226 0.06943233]\n"," [0.06513723 0.06614599]\n"," [0.06640046 0.06608711]]\n","\n","Average MAE Loss:\n","[0.06618843 0.06987229 0.06564161 0.06624379]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.06628003 0.06662494]\n"," [0.07031226 0.06943233]\n"," [0.06461021 0.06597278]\n"," [0.06702189 0.06673521]]\n","\n","Average MAE Loss:\n","[0.06645249 0.06987229 0.0652915  0.06687855]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06534342 0.06623726]\n"," [0.07031226 0.06943233]\n"," [0.06527188 0.06640717]\n"," [0.06640662 0.06597114]]\n","\n","Average MAE Loss:\n","[0.06579034 0.06987229 0.06583953 0.06618888]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06607953 0.066601  ]\n"," [0.07031226 0.06943233]\n"," [0.0653904  0.06629643]\n"," [0.06542959 0.06566934]]\n","\n","Average MAE Loss:\n","[0.06634026 0.06987229 0.06584342 0.06554947]\n","\n","Epoch 00027: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06544026 0.06628555]\n"," [0.07031226 0.06943233]\n"," [0.06496924 0.06616569]\n"," [0.06696557 0.06639391]]\n","\n","Average MAE Loss:\n","[0.0658629  0.06987229 0.06556747 0.06667974]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06374884 0.06436169]\n"," [0.0703606  0.06956213]\n"," [0.06381671 0.06507378]\n"," [0.06390562 0.06452599]]\n","\n","Average MAE Loss:\n","[0.06405526 0.06996136 0.06444525 0.0642158 ]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06274819 0.06394581]\n"," [0.0703606  0.06956213]\n"," [0.06564859 0.06622114]\n"," [0.06461486 0.06465453]]\n","\n","Average MAE Loss:\n","[0.063347   0.06996136 0.06593486 0.06463469]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06318887 0.06406007]\n"," [0.0703606  0.06956213]\n"," [0.06472656 0.06585376]\n"," [0.0650754  0.06493047]]\n","\n","Average MAE Loss:\n","[0.06362447 0.06996136 0.06529016 0.06500293]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06313832 0.06408799]\n"," [0.0703606  0.06956213]\n"," [0.06419904 0.06560642]\n"," [0.06443017 0.06434754]]\n","\n","Average MAE Loss:\n","[0.06361316 0.06996136 0.06490273 0.06438885]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06292648 0.06402715]\n"," [0.0703606  0.06956213]\n"," [0.06453328 0.06565351]\n"," [0.06455036 0.06450858]]\n","\n","Average MAE Loss:\n","[0.06347682 0.06996136 0.06509339 0.06452947]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06311292 0.06399664]\n"," [0.0703606  0.06956213]\n"," [0.0642267  0.06535357]\n"," [0.06406869 0.06431996]]\n","\n","Average MAE Loss:\n","[0.06355478 0.06996136 0.06479014 0.06419432]\n","\n","Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06297845 0.06389264]\n"," [0.0703606  0.06956213]\n"," [0.06400696 0.06523066]\n"," [0.06433784 0.06443099]]\n","\n","Average MAE Loss:\n","[0.06343555 0.06996136 0.06461881 0.06438442]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06652089 0.06641303]\n"," [0.06433784 0.06443099]\n"," [0.06290591 0.06409391]\n"," [0.06436685 0.0650864 ]]\n","\n","Average MAE Loss:\n","[0.06646696 0.06438442 0.06349991 0.06472662]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06371479 0.06438796]\n"," [0.06433784 0.06443099]\n"," [0.06346623 0.06464674]\n"," [0.0646376  0.06508884]]\n","\n","Average MAE Loss:\n","[0.06405137 0.06438442 0.06405648 0.06486322]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06344321 0.06410328]\n"," [0.06433784 0.06443099]\n"," [0.06380456 0.06509337]\n"," [0.06371174 0.06442878]]\n","\n","Average MAE Loss:\n","[0.06377325 0.06438442 0.06444896 0.06407026]\n","\n","Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06329492 0.06404064]\n"," [0.06433784 0.06443099]\n"," [0.06390772 0.06525239]\n"," [0.06333047 0.06423011]]\n","\n","Average MAE Loss:\n","[0.06366778 0.06438442 0.06458006 0.06378029]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06321081 0.06403803]\n"," [0.06433784 0.06443099]\n"," [0.06444125 0.0656355 ]\n"," [0.06381657 0.06429162]]\n","\n","Average MAE Loss:\n","[0.06362442 0.06438442 0.06503837 0.06405409]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06329653 0.06407495]\n"," [0.06433784 0.06443099]\n"," [0.06404552 0.06536822]\n"," [0.06400692 0.06433021]]\n","\n","Average MAE Loss:\n","[0.06368574 0.06438442 0.06470687 0.06416857]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06330115 0.06409832]\n"," [0.06433784 0.06443099]\n"," [0.06398948 0.06531319]\n"," [0.06398268 0.0643149 ]]\n","\n","Average MAE Loss:\n","[0.06369973 0.06438442 0.06465134 0.06414879]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06355286 0.06414342]\n"," [0.06365043 0.06420543]\n"," [0.06373112 0.06432303]\n"," [0.06363887 0.0641513 ]]\n","\n","Average MAE Loss:\n","[0.06384814 0.06392793 0.06402707 0.06389509]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06344625 0.06405701]\n"," [0.06365043 0.06420543]\n"," [0.06360694 0.06445794]\n"," [0.06396029 0.0643379 ]]\n","\n","Average MAE Loss:\n","[0.06375163 0.06392793 0.06403244 0.0641491 ]\n","\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06341566 0.06402394]\n"," [0.06365043 0.06420543]\n"," [0.06366819 0.06454176]\n"," [0.06415036 0.06446033]]\n","\n","Average MAE Loss:\n","[0.0637198  0.06392793 0.06410497 0.06430534]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06334709 0.06397774]\n"," [0.06365043 0.06420543]\n"," [0.06365188 0.06460474]\n"," [0.06382729 0.06423233]]\n","\n","Average MAE Loss:\n","[0.06366241 0.06392793 0.06412831 0.06402981]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06330796 0.06395563]\n"," [0.06365043 0.06420543]\n"," [0.06350741 0.06457878]\n"," [0.06395591 0.06425034]]\n","\n","Average MAE Loss:\n","[0.06363179 0.06392793 0.0640431  0.06410312]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06332868 0.06396428]\n"," [0.06365043 0.06420543]\n"," [0.06352159 0.06464714]\n"," [0.06406187 0.06427999]]\n","\n","Average MAE Loss:\n","[0.06364648 0.06392793 0.06408436 0.06417093]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06330379 0.06395016]\n"," [0.06365043 0.06420543]\n"," [0.06356387 0.06468769]\n"," [0.06431063 0.06439136]]\n","\n","Average MAE Loss:\n","[0.06362698 0.06392793 0.06412578 0.064351  ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06360871 0.06416648]\n"," [0.06431063 0.06439136]\n"," [0.06323119 0.06393393]\n"," [0.06372666 0.06466147]]\n","\n","Average MAE Loss:\n","[0.06388759 0.064351   0.06358256 0.06419407]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06358092 0.06414413]\n"," [0.06431063 0.06439136]\n"," [0.06317603 0.06394824]\n"," [0.06379557 0.06459194]]\n","\n","Average MAE Loss:\n","[0.06386252 0.064351   0.06356214 0.06419375]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06354361 0.06411742]\n"," [0.06431063 0.06439136]\n"," [0.06316328 0.06397573]\n"," [0.06379149 0.06454109]]\n","\n","Average MAE Loss:\n","[0.06383052 0.064351   0.0635695  0.06416629]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06349624 0.06408623]\n"," [0.06431063 0.06439136]\n"," [0.06316566 0.0640045 ]\n"," [0.06364473 0.06441374]]\n","\n","Average MAE Loss:\n","[0.06379124 0.064351   0.06358508 0.06402923]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06344188 0.06405379]\n"," [0.06431063 0.06439136]\n"," [0.06316026 0.06403236]\n"," [0.06358745 0.06434594]]\n","\n","Average MAE Loss:\n","[0.06374783 0.064351   0.06359631 0.06396669]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06342071 0.06404085]\n"," [0.06431063 0.06439136]\n"," [0.06319827 0.06408184]\n"," [0.06362814 0.06432649]]\n","\n","Average MAE Loss:\n","[0.06373078 0.064351   0.06364006 0.06397732]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06340718 0.06403247]\n"," [0.06431063 0.06439136]\n"," [0.06316913 0.0641004 ]\n"," [0.06365483 0.06432781]]\n","\n","Average MAE Loss:\n","[0.06371982 0.064351   0.06363477 0.06399132]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06354277 0.06410882]\n"," [0.06354866 0.06411484]\n"," [0.06351067 0.06411293]\n"," [0.06357733 0.06411836]]\n","\n","Average MAE Loss:\n","[0.0638258  0.06383175 0.0638118  0.06384785]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06352849 0.0640981 ]\n"," [0.06354866 0.06411484]\n"," [0.06351002 0.06412201]\n"," [0.06360877 0.06412213]]\n","\n","Average MAE Loss:\n","[0.06381329 0.06383175 0.06381602 0.06386545]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.0635211  0.06409304]\n"," [0.06354866 0.06411484]\n"," [0.06349301 0.06412355]\n"," [0.06361913 0.06411507]]\n","\n","Average MAE Loss:\n","[0.06380707 0.06383175 0.06380828 0.0638671 ]\n","\n","Epoch 00059: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06351251 0.06408712]\n"," [0.06354866 0.06411484]\n"," [0.06348159 0.06412784]\n"," [0.06362426 0.0641134 ]]\n","\n","Average MAE Loss:\n","[0.06379982 0.06383175 0.06380472 0.06386883]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:51.335312604904175\n","\n","[[0.06351251 0.06408712]\n"," [0.06354866 0.06411484]\n"," [0.06348159 0.06412784]\n"," [0.06362426 0.0641134 ]]\n","00:01:12.95701455499966\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08830144 0.07970948]\n"," [0.09980381 0.08862089]\n"," [0.06927281 0.06189309]\n"," [0.07071539 0.06305885]]\n","\n","Average MAE Loss:\n","[0.08400546 0.09421235 0.06558295 0.06688712]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08642344 0.07695837]\n"," [0.09980381 0.08862089]\n"," [0.07824612 0.06779962]\n"," [0.08297358 0.07966137]]\n","\n","Average MAE Loss:\n","[0.08169091 0.09421235 0.07302287 0.08131748]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08635314 0.07696089]\n"," [0.09980381 0.08862089]\n"," [0.06074338 0.05259609]\n"," [0.07209318 0.06207909]]\n","\n","Average MAE Loss:\n","[0.08165701 0.09421235 0.05666973 0.06708613]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08687573 0.07678039]\n"," [0.09980381 0.08862089]\n"," [0.06432989 0.05516734]\n"," [0.06804418 0.06030658]]\n","\n","Average MAE Loss:\n","[0.08182806 0.09421235 0.05974862 0.06417538]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08684813 0.07672396]\n"," [0.09980381 0.08862089]\n"," [0.0601736  0.05073009]\n"," [0.065246   0.05691763]]\n","\n","Average MAE Loss:\n","[0.08178605 0.09421235 0.05545185 0.06108182]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08538164 0.07623932]\n"," [0.09980381 0.08862089]\n"," [0.05851556 0.05008878]\n"," [0.06427832 0.05682096]]\n","\n","Average MAE Loss:\n","[0.08081048 0.09421235 0.05430217 0.06054964]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08541274 0.0761136 ]\n"," [0.09980381 0.08862089]\n"," [0.05672393 0.04962793]\n"," [0.06489596 0.05725427]]\n","\n","Average MAE Loss:\n","[0.08076317 0.09421235 0.05317593 0.06107511]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06253901 0.05486087]\n"," [0.06489596 0.05725427]\n"," [0.08974637 0.07898139]\n"," [0.06115323 0.05182053]]\n","\n","Average MAE Loss:\n","[0.05869994 0.06107511 0.08436388 0.05648688]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05077501 0.04670916]\n"," [0.06489596 0.05725427]\n"," [0.08835939 0.07781141]\n"," [0.05905121 0.05172189]]\n","\n","Average MAE Loss:\n","[0.04874208 0.06107511 0.0830854  0.05538655]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05653154 0.0483816 ]\n"," [0.06489596 0.05725427]\n"," [0.08553418 0.07520616]\n"," [0.05718609 0.04863071]]\n","\n","Average MAE Loss:\n","[0.05245657 0.06107511 0.08037017 0.0529084 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04950114 0.04399405]\n"," [0.06489596 0.05725427]\n"," [0.08752036 0.07698424]\n"," [0.05873146 0.05356842]]\n","\n","Average MAE Loss:\n","[0.0467476  0.06107511 0.0822523  0.05614994]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05313637 0.04563626]\n"," [0.06489596 0.05725427]\n"," [0.08494155 0.07517193]\n"," [0.06071661 0.05014013]]\n","\n","Average MAE Loss:\n","[0.04938632 0.06107511 0.08005674 0.05542837]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04869107 0.04329805]\n"," [0.06489596 0.05725427]\n"," [0.08438087 0.07527923]\n"," [0.05632466 0.04751454]]\n","\n","Average MAE Loss:\n","[0.04599456 0.06107511 0.07983005 0.0519196 ]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04854925 0.04320127]\n"," [0.06489596 0.05725427]\n"," [0.08461858 0.07457646]\n"," [0.05876787 0.04925379]]\n","\n","Average MAE Loss:\n","[0.04587526 0.06107511 0.07959752 0.05401083]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0509274  0.04656326]\n"," [0.09230314 0.08110308]\n"," [0.05321887 0.04896763]\n"," [0.05379963 0.04486658]]\n","\n","Average MAE Loss:\n","[0.04874533 0.08670311 0.05109325 0.04933311]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04898667 0.04420761]\n"," [0.09230314 0.08110308]\n"," [0.05148132 0.04432148]\n"," [0.0510485  0.04336619]]\n","\n","Average MAE Loss:\n","[0.04659714 0.08670311 0.0479014  0.04720735]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05078918 0.04397505]\n"," [0.09230314 0.08110308]\n"," [0.05042783 0.04384052]\n"," [0.05053057 0.04237865]]\n","\n","Average MAE Loss:\n","[0.04738212 0.08670311 0.04713417 0.04645461]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04873824 0.04325587]\n"," [0.09230314 0.08110308]\n"," [0.05107279 0.04335602]\n"," [0.04993574 0.04411637]]\n","\n","Average MAE Loss:\n","[0.04599706 0.08670311 0.0472144  0.04702605]\n","\n","Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04884602 0.0428074 ]\n"," [0.09230314 0.08110308]\n"," [0.05069879 0.04361061]\n"," [0.04965272 0.0423381 ]]\n","\n","Average MAE Loss:\n","[0.04582671 0.08670311 0.0471547  0.04599541]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04884744 0.04282714]\n"," [0.09230314 0.08110308]\n"," [0.04987314 0.0434989 ]\n"," [0.04901573 0.04219723]]\n","\n","Average MAE Loss:\n","[0.04583729 0.08670311 0.04668602 0.04560648]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04824534 0.04292385]\n"," [0.09230314 0.08110308]\n"," [0.0497256  0.04328829]\n"," [0.04895353 0.0418833 ]]\n","\n","Average MAE Loss:\n","[0.0455846  0.08670311 0.04650695 0.04541842]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.08287462 0.07166512]\n"," [0.04895353 0.0418833 ]\n"," [0.04900455 0.04341166]\n"," [0.05119125 0.04359972]]\n","\n","Average MAE Loss:\n","[0.07726987 0.04541842 0.04620811 0.04739548]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0531506  0.04657633]\n"," [0.04895353 0.0418833 ]\n"," [0.04981017 0.04314766]\n"," [0.05295425 0.04425023]]\n","\n","Average MAE Loss:\n","[0.04986346 0.04541842 0.04647892 0.04860224]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04966787 0.04300148]\n"," [0.04895353 0.0418833 ]\n"," [0.04943562 0.04293113]\n"," [0.04965334 0.04352098]]\n","\n","Average MAE Loss:\n","[0.04633468 0.04541842 0.04618337 0.04658716]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0475735  0.04268221]\n"," [0.04895353 0.0418833 ]\n"," [0.04938748 0.04299432]\n"," [0.05060442 0.04262377]]\n","\n","Average MAE Loss:\n","[0.04512785 0.04541842 0.0461909  0.0466141 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0476652  0.04225956]\n"," [0.04895353 0.0418833 ]\n"," [0.04911199 0.04293427]\n"," [0.04926277 0.04317789]]\n","\n","Average MAE Loss:\n","[0.04496238 0.04541842 0.04602313 0.04622033]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04775371 0.04222813]\n"," [0.04895353 0.0418833 ]\n"," [0.04927828 0.04418801]\n"," [0.04973387 0.04221134]]\n","\n","Average MAE Loss:\n","[0.04499092 0.04541842 0.04673314 0.0459726 ]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04732367 0.04185357]\n"," [0.04895353 0.0418833 ]\n"," [0.04917357 0.04293839]\n"," [0.05095265 0.04287047]]\n","\n","Average MAE Loss:\n","[0.04458862 0.04541842 0.04605598 0.04691156]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04665654 0.04117715]\n"," [0.05103844 0.04293534]\n"," [0.04789872 0.04127318]\n"," [0.04732366 0.04129356]]\n","\n","Average MAE Loss:\n","[0.04391684 0.04698689 0.04458595 0.04430861]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04681987 0.0411763 ]\n"," [0.05103844 0.04293534]\n"," [0.04749374 0.04246228]\n"," [0.04826389 0.04115397]]\n","\n","Average MAE Loss:\n","[0.04399808 0.04698689 0.04497801 0.04470893]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04609942 0.041557  ]\n"," [0.05103844 0.04293534]\n"," [0.0479459  0.04100312]\n"," [0.04842707 0.04129182]]\n","\n","Average MAE Loss:\n","[0.04382821 0.04698689 0.04447451 0.04485944]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04752259 0.04150632]\n"," [0.05103844 0.04293534]\n"," [0.0474367  0.0423734 ]\n"," [0.04794763 0.04164762]]\n","\n","Average MAE Loss:\n","[0.04451445 0.04698689 0.04490505 0.04479763]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04630876 0.04133774]\n"," [0.05103844 0.04293534]\n"," [0.04750384 0.04206179]\n"," [0.04795235 0.04117757]]\n","\n","Average MAE Loss:\n","[0.04382325 0.04698689 0.04478281 0.04456496]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04687786 0.04107863]\n"," [0.05103844 0.04293534]\n"," [0.0475199  0.04106928]\n"," [0.04816597 0.04097034]]\n","\n","Average MAE Loss:\n","[0.04397824 0.04698689 0.04429459 0.04456815]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04669723 0.04112821]\n"," [0.05103844 0.04293534]\n"," [0.04739581 0.04199442]\n"," [0.04790073 0.04111172]]\n","\n","Average MAE Loss:\n","[0.04391272 0.04698689 0.04469511 0.04450623]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04640093 0.04142523]\n"," [0.04790073 0.04111172]\n"," [0.04650378 0.04120651]\n"," [0.04756463 0.04080364]]\n","\n","Average MAE Loss:\n","[0.04391308 0.04450623 0.04385515 0.04418414]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04669648 0.04115878]\n"," [0.04790073 0.04111172]\n"," [0.04735808 0.04207861]\n"," [0.04758707 0.04077418]]\n","\n","Average MAE Loss:\n","[0.04392763 0.04450623 0.04471834 0.04418063]\n","\n","Epoch 00037: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04598429 0.04130867]\n"," [0.04790073 0.04111172]\n"," [0.04792572 0.04173055]\n"," [0.04760363 0.04103232]]\n","\n","Average MAE Loss:\n","[0.04364648 0.04450623 0.04482813 0.04431798]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04609093 0.04120611]\n"," [0.04790073 0.04111172]\n"," [0.04804656 0.04181615]\n"," [0.04776476 0.04131275]]\n","\n","Average MAE Loss:\n","[0.04364852 0.04450623 0.04493135 0.04453875]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04687078 0.0410471 ]\n"," [0.04790073 0.04111172]\n"," [0.04788749 0.0422349 ]\n"," [0.04779111 0.04143206]]\n","\n","Average MAE Loss:\n","[0.04395894 0.04450623 0.04506119 0.04461158]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04658313 0.04105584]\n"," [0.04790073 0.04111172]\n"," [0.04786169 0.04157212]\n"," [0.04771175 0.04122321]]\n","\n","Average MAE Loss:\n","[0.04381949 0.04450623 0.0447169  0.04446748]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04629101 0.04114744]\n"," [0.04790073 0.04111172]\n"," [0.04768726 0.04167084]\n"," [0.0476692  0.04112006]]\n","\n","Average MAE Loss:\n","[0.04371922 0.04450623 0.04467905 0.04439463]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04696207 0.04060563]\n"," [0.04708074 0.04068023]\n"," [0.04697444 0.04118987]\n"," [0.04713136 0.04071102]]\n","\n","Average MAE Loss:\n","[0.04378385 0.04388049 0.04408216 0.04392119]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04655574 0.04069587]\n"," [0.04708074 0.04068023]\n"," [0.04730648 0.04166824]\n"," [0.04714474 0.04079176]]\n","\n","Average MAE Loss:\n","[0.04362581 0.04388049 0.04448736 0.04396825]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04649537 0.04068719]\n"," [0.04708074 0.04068023]\n"," [0.0474062  0.04145849]\n"," [0.0473446  0.04072883]]\n","\n","Average MAE Loss:\n","[0.04359128 0.04388049 0.04443235 0.04403671]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04645993 0.04073977]\n"," [0.04708074 0.04068023]\n"," [0.04747494 0.04180136]\n"," [0.04747052 0.04073732]]\n","\n","Average MAE Loss:\n","[0.04359985 0.04388049 0.04463815 0.04410392]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04638345 0.04081966]\n"," [0.04708074 0.04068023]\n"," [0.04747593 0.04190739]\n"," [0.04751819 0.04078968]]\n","\n","Average MAE Loss:\n","[0.04360156 0.04388049 0.04469166 0.04415393]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04626435 0.04088546]\n"," [0.04708074 0.04068023]\n"," [0.04744799 0.04174232]\n"," [0.0475171  0.04083287]]\n","\n","Average MAE Loss:\n","[0.04357491 0.04388049 0.04459516 0.04417498]\n","\n","Epoch 00048: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04611915 0.04089973]\n"," [0.04708074 0.04068023]\n"," [0.04740681 0.04175134]\n"," [0.04757607 0.04086262]]\n","\n","Average MAE Loss:\n","[0.04350944 0.04388049 0.04457908 0.04421935]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04686604 0.04060464]\n"," [0.04757607 0.04086262]\n"," [0.0461375  0.04084665]\n"," [0.04735468 0.04145551]]\n","\n","Average MAE Loss:\n","[0.04373534 0.04421935 0.04349208 0.0444051 ]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04695562 0.04065339]\n"," [0.04757607 0.04086262]\n"," [0.04618727 0.04075757]\n"," [0.04729665 0.04124507]]\n","\n","Average MAE Loss:\n","[0.0438045  0.04421935 0.04347242 0.04427086]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04656765 0.04075259]\n"," [0.04757607 0.04086262]\n"," [0.04618647 0.04081258]\n"," [0.04729941 0.04113561]]\n","\n","Average MAE Loss:\n","[0.04366012 0.04421935 0.04349952 0.04421751]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04631936 0.04080688]\n"," [0.04757607 0.04086262]\n"," [0.04622326 0.04087214]\n"," [0.04733066 0.04102268]]\n","\n","Average MAE Loss:\n","[0.04356312 0.04421935 0.0435477  0.04417667]\n","\n","Epoch 00053: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04639453 0.04078683]\n"," [0.04757607 0.04086262]\n"," [0.04638352 0.04087341]\n"," [0.04740791 0.04089777]]\n","\n","Average MAE Loss:\n","[0.04359068 0.04421935 0.04362846 0.04415284]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04617249 0.04086909]\n"," [0.04757607 0.04086262]\n"," [0.04651754 0.04089043]\n"," [0.04747918 0.04084016]]\n","\n","Average MAE Loss:\n","[0.04352079 0.04421935 0.04370398 0.04415967]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04615337 0.04089354]\n"," [0.04757607 0.04086262]\n"," [0.0465081  0.04099146]\n"," [0.04746496 0.04082736]]\n","\n","Average MAE Loss:\n","[0.04352345 0.04421935 0.04374978 0.04414616]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04660866 0.04057424]\n"," [0.04673865 0.04062206]\n"," [0.04671837 0.04071122]\n"," [0.04673417 0.0406213 ]]\n","\n","Average MAE Loss:\n","[0.04359145 0.04368036 0.04371479 0.04367774]\n","\n","Epoch 00057: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04657165 0.04055769]\n"," [0.04673865 0.04062206]\n"," [0.04667444 0.04083082]\n"," [0.04676142 0.04061615]]\n","\n","Average MAE Loss:\n","[0.04356467 0.04368036 0.04375263 0.04368879]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04648291 0.04057883]\n"," [0.04673865 0.04062206]\n"," [0.04669809 0.04084442]\n"," [0.04680558 0.04060876]]\n","\n","Average MAE Loss:\n","[0.04353087 0.04368036 0.04377125 0.04370717]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04648075 0.04057467]\n"," [0.04673865 0.04062206]\n"," [0.04672554 0.04084794]\n"," [0.04681332 0.04062116]]\n","\n","Average MAE Loss:\n","[0.04352771 0.04368036 0.04378674 0.04371724]\n","\n","\n","epochs finished with time:48.41046762466431\n","\n","[[0.04648075 0.04057467]\n"," [0.04673865 0.04062206]\n"," [0.04672554 0.04084794]\n"," [0.04681332 0.04062116]]\n","00:01:4.895034828000007\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07328996 0.09464665]\n"," [0.08241    0.10488542]\n"," [0.06067696 0.08308118]\n"," [0.07661575 0.0947879 ]]\n","\n","Average MAE Loss:\n","[0.0839683  0.09364771 0.07187907 0.08570182]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.0720094  0.09389698]\n"," [0.08241    0.10488542]\n"," [0.0515374  0.07224213]\n"," [0.06961866 0.09078448]]\n","\n","Average MAE Loss:\n","[0.08295319 0.09364771 0.06188977 0.08020157]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0714259  0.09259639]\n"," [0.08241    0.10488542]\n"," [0.06316677 0.08504313]\n"," [0.05019438 0.06954204]]\n","\n","Average MAE Loss:\n","[0.08201114 0.09364771 0.07410495 0.05986821]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07118794 0.09242541]\n"," [0.08241    0.10488542]\n"," [0.04803627 0.06945026]\n"," [0.05370009 0.07202943]]\n","\n","Average MAE Loss:\n","[0.08180667 0.09364771 0.05874326 0.06286476]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07073766 0.09203434]\n"," [0.08241    0.10488542]\n"," [0.04924886 0.06834155]\n"," [0.04793086 0.06769159]]\n","\n","Average MAE Loss:\n","[0.081386   0.09364771 0.0587952  0.05781123]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07123731 0.09301711]\n"," [0.08241    0.10488542]\n"," [0.04587758 0.06700508]\n"," [0.04864307 0.06894819]]\n","\n","Average MAE Loss:\n","[0.08212721 0.09364771 0.05644133 0.05879563]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07091521 0.0922157 ]\n"," [0.08241    0.10488542]\n"," [0.05340003 0.07126264]\n"," [0.04910738 0.06986059]]\n","\n","Average MAE Loss:\n","[0.08156546 0.09364771 0.06233134 0.05948398]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04499379 0.06326002]\n"," [0.04910738 0.06986059]\n"," [0.07177034 0.0930725 ]\n"," [0.04914594 0.06892666]]\n","\n","Average MAE Loss:\n","[0.0541269  0.05948398 0.08242142 0.0590363 ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04692651 0.06779861]\n"," [0.04910738 0.06986059]\n"," [0.07384745 0.09405684]\n"," [0.05286074 0.07122852]]\n","\n","Average MAE Loss:\n","[0.05736256 0.05948398 0.08395215 0.06204463]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04611055 0.06387431]\n"," [0.04910738 0.06986059]\n"," [0.06994291 0.09127753]\n"," [0.04680377 0.06640603]]\n","\n","Average MAE Loss:\n","[0.05499243 0.05948398 0.08061022 0.0566049 ]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04202376 0.06100739]\n"," [0.04910738 0.06986059]\n"," [0.06912436 0.09043156]\n"," [0.04523326 0.06524433]]\n","\n","Average MAE Loss:\n","[0.05151558 0.05948398 0.07977796 0.05523879]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04330179 0.06128253]\n"," [0.04910738 0.06986059]\n"," [0.06895077 0.0905666 ]\n"," [0.04487885 0.06458876]]\n","\n","Average MAE Loss:\n","[0.05229216 0.05948398 0.07975869 0.0547338 ]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04249659 0.06263047]\n"," [0.04910738 0.06986059]\n"," [0.0689014  0.09003927]\n"," [0.04419073 0.06448945]]\n","\n","Average MAE Loss:\n","[0.05256353 0.05948398 0.07947034 0.05434009]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04184218 0.06112289]\n"," [0.04910738 0.06986059]\n"," [0.06875537 0.08988644]\n"," [0.04435392 0.06500368]]\n","\n","Average MAE Loss:\n","[0.05148253 0.05948398 0.07932091 0.0546788 ]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04261507 0.06303001]\n"," [0.07520018 0.09762832]\n"," [0.06021908 0.08216074]\n"," [0.04774286 0.06831927]]\n","\n","Average MAE Loss:\n","[0.05282254 0.08641425 0.07118991 0.05803106]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04069584 0.06030494]\n"," [0.07520018 0.09762832]\n"," [0.04313644 0.06194118]\n"," [0.04179432 0.06182466]]\n","\n","Average MAE Loss:\n","[0.05050039 0.08641425 0.05253881 0.05180949]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04108137 0.05933603]\n"," [0.07520018 0.09762832]\n"," [0.04038695 0.06095664]\n"," [0.04371372 0.06178708]]\n","\n","Average MAE Loss:\n","[0.0502087  0.08641425 0.05067179 0.0527504 ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0402525  0.05947022]\n"," [0.07520018 0.09762832]\n"," [0.04055094 0.06032101]\n"," [0.04064358 0.06068444]]\n","\n","Average MAE Loss:\n","[0.04986136 0.08641425 0.05043598 0.05066401]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04072862 0.0589745 ]\n"," [0.07520018 0.09762832]\n"," [0.04012416 0.06045061]\n"," [0.04071965 0.06024432]]\n","\n","Average MAE Loss:\n","[0.04985156 0.08641425 0.05028739 0.05048198]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03909869 0.05814897]\n"," [0.07520018 0.09762832]\n"," [0.03989753 0.06026214]\n"," [0.04041163 0.06028018]]\n","\n","Average MAE Loss:\n","[0.04862383 0.08641425 0.05007984 0.05034591]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03937739 0.05820157]\n"," [0.07520018 0.09762832]\n"," [0.03996497 0.06004111]\n"," [0.04073024 0.06018285]]\n","\n","Average MAE Loss:\n","[0.04878948 0.08641425 0.05000304 0.05045654]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04534164 0.06342237]\n"," [0.04073024 0.06018285]\n"," [0.03901917 0.0583419 ]\n"," [0.04036307 0.05983517]]\n","\n","Average MAE Loss:\n","[0.05438201 0.05045654 0.04868054 0.05009912]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04118853 0.05973144]\n"," [0.04073024 0.06018285]\n"," [0.03935943 0.05887914]\n"," [0.04060371 0.06004009]]\n","\n","Average MAE Loss:\n","[0.05045998 0.05045654 0.04911929 0.0503219 ]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04049426 0.05910522]\n"," [0.04073024 0.06018285]\n"," [0.03928577 0.05904992]\n"," [0.04047005 0.05987186]]\n","\n","Average MAE Loss:\n","[0.04979974 0.05045654 0.04916784 0.05017095]\n","\n","Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04020906 0.05859777]\n"," [0.04073024 0.06018285]\n"," [0.03930004 0.0592181 ]\n"," [0.04002468 0.05980937]]\n","\n","Average MAE Loss:\n","[0.04940341 0.05045654 0.04925907 0.04991703]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0396532  0.05851654]\n"," [0.04073024 0.06018285]\n"," [0.03978277 0.05945409]\n"," [0.04035742 0.05971297]]\n","\n","Average MAE Loss:\n","[0.04908487 0.05045654 0.04961843 0.0500352 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03996643 0.05835025]\n"," [0.04073024 0.06018285]\n"," [0.03946453 0.05921169]\n"," [0.04035576 0.05964171]]\n","\n","Average MAE Loss:\n","[0.04915834 0.05045654 0.04933811 0.04999874]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03964961 0.05818756]\n"," [0.04073024 0.06018285]\n"," [0.03944452 0.05914834]\n"," [0.04005789 0.05966359]]\n","\n","Average MAE Loss:\n","[0.04891858 0.05045654 0.04929643 0.04986074]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03842527 0.05747446]\n"," [0.04146985 0.06197885]\n"," [0.03904906 0.05914473]\n"," [0.03885992 0.05826472]]\n","\n","Average MAE Loss:\n","[0.04794987 0.05172435 0.0490969  0.04856232]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03859794 0.05702025]\n"," [0.04146985 0.06197885]\n"," [0.03809284 0.05795498]\n"," [0.03928705 0.05838484]]\n","\n","Average MAE Loss:\n","[0.0478091  0.05172435 0.04802391 0.04883594]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03875802 0.05696328]\n"," [0.04146985 0.06197885]\n"," [0.03818327 0.05792994]\n"," [0.03914046 0.05838467]]\n","\n","Average MAE Loss:\n","[0.04786065 0.05172435 0.0480566  0.04876256]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03886997 0.05691271]\n"," [0.04146985 0.06197885]\n"," [0.03832522 0.05808067]\n"," [0.03978032 0.05858739]]\n","\n","Average MAE Loss:\n","[0.04789134 0.05172435 0.04820294 0.04918385]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03855203 0.05686894]\n"," [0.04146985 0.06197885]\n"," [0.03840376 0.05821508]\n"," [0.03911796 0.05837999]]\n","\n","Average MAE Loss:\n","[0.04771049 0.05172435 0.04830942 0.04874898]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03862177 0.0568848 ]\n"," [0.04146985 0.06197885]\n"," [0.03832821 0.05822702]\n"," [0.03921472 0.05828278]]\n","\n","Average MAE Loss:\n","[0.04775328 0.05172435 0.04827761 0.04874875]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03879547 0.05698302]\n"," [0.04146985 0.06197885]\n"," [0.03837612 0.05819929]\n"," [0.03916385 0.05822581]]\n","\n","Average MAE Loss:\n","[0.04788924 0.05172435 0.04828771 0.04869483]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03856872 0.05767756]\n"," [0.03916385 0.05822581]\n"," [0.03870505 0.05698048]\n"," [0.03835255 0.05790935]]\n","\n","Average MAE Loss:\n","[0.04812314 0.04869483 0.04784276 0.04813095]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03938628 0.05743007]\n"," [0.03916385 0.05822581]\n"," [0.0384748  0.0570396 ]\n"," [0.03874193 0.05797128]]\n","\n","Average MAE Loss:\n","[0.04840817 0.04869483 0.0477572  0.04835661]\n","\n","Epoch 00037: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03864758 0.05717006]\n"," [0.03916385 0.05822581]\n"," [0.03828191 0.05714978]\n"," [0.03856027 0.05804771]]\n","\n","Average MAE Loss:\n","[0.04790882 0.04869483 0.04771584 0.04830399]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03840611 0.05707468]\n"," [0.03916385 0.05822581]\n"," [0.03826667 0.05731796]\n"," [0.0387758  0.05808623]]\n","\n","Average MAE Loss:\n","[0.0477404  0.04869483 0.04779231 0.04843101]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03839354 0.05699412]\n"," [0.03916385 0.05822581]\n"," [0.0382521  0.05749653]\n"," [0.03888964 0.0579769 ]]\n","\n","Average MAE Loss:\n","[0.04769383 0.04869483 0.04787431 0.04843327]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03846327 0.05698431]\n"," [0.03916385 0.05822581]\n"," [0.03826472 0.05762099]\n"," [0.03899384 0.05802571]]\n","\n","Average MAE Loss:\n","[0.04772379 0.04869483 0.04794285 0.04850977]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03863429 0.05692053]\n"," [0.03916385 0.05822581]\n"," [0.0383626  0.05773605]\n"," [0.03894972 0.05808473]]\n","\n","Average MAE Loss:\n","[0.04777741 0.04869483 0.04804932 0.04851723]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03830531 0.05713281]\n"," [0.03830303 0.05739874]\n"," [0.03825989 0.05740337]\n"," [0.03841209 0.05743064]]\n","\n","Average MAE Loss:\n","[0.04771906 0.04785089 0.04783163 0.04792137]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03837635 0.05698577]\n"," [0.03830303 0.05739874]\n"," [0.0381895  0.05741987]\n"," [0.03867374 0.05762346]]\n","\n","Average MAE Loss:\n","[0.04768106 0.04785089 0.04780468 0.0481486 ]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03834687 0.05694661]\n"," [0.03830303 0.05739874]\n"," [0.03812567 0.05745104]\n"," [0.03865546 0.0577128 ]]\n","\n","Average MAE Loss:\n","[0.04764674 0.04785089 0.04778836 0.04818413]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03835678 0.05693335]\n"," [0.03830303 0.05739874]\n"," [0.03806254 0.0574885 ]\n"," [0.0385811  0.05779514]]\n","\n","Average MAE Loss:\n","[0.04764506 0.04785089 0.04777552 0.04818812]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03850557 0.05684871]\n"," [0.03830303 0.05739874]\n"," [0.0380328  0.05750507]\n"," [0.03860089 0.05786144]]\n","\n","Average MAE Loss:\n","[0.04767714 0.04785089 0.04776894 0.04823116]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03864587 0.0568545 ]\n"," [0.03830303 0.05739874]\n"," [0.03803026 0.05753143]\n"," [0.03864543 0.05786894]]\n","\n","Average MAE Loss:\n","[0.04775019 0.04785089 0.04778085 0.04825718]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03869119 0.05688821]\n"," [0.03830303 0.05739874]\n"," [0.03803218 0.0575469 ]\n"," [0.03872929 0.05787874]]\n","\n","Average MAE Loss:\n","[0.0477897  0.04785089 0.04778954 0.04830402]\n","\n","Epoch 00049: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03829934 0.05725913]\n"," [0.03872929 0.05787874]\n"," [0.03861335 0.05687396]\n"," [0.03807692 0.05752632]]\n","\n","Average MAE Loss:\n","[0.04777923 0.04830402 0.04774365 0.04780162]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03834177 0.05717513]\n"," [0.03872929 0.05787874]\n"," [0.03856492 0.05687151]\n"," [0.03816094 0.05753262]]\n","\n","Average MAE Loss:\n","[0.04775845 0.04830402 0.04771821 0.04784678]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03845526 0.05714241]\n"," [0.03872929 0.05787874]\n"," [0.03852952 0.05687319]\n"," [0.03824819 0.05754754]]\n","\n","Average MAE Loss:\n","[0.04779883 0.04830402 0.04770136 0.04789786]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03854147 0.05709367]\n"," [0.03872929 0.05787874]\n"," [0.0384974  0.05687413]\n"," [0.0383242  0.05757388]]\n","\n","Average MAE Loss:\n","[0.04781757 0.04830402 0.04768576 0.04794904]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03852183 0.05705907]\n"," [0.03872929 0.05787874]\n"," [0.03846135 0.056878  ]\n"," [0.03836021 0.05760361]]\n","\n","Average MAE Loss:\n","[0.04779045 0.04830402 0.04766968 0.04798191]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0384803  0.05703468]\n"," [0.03872929 0.05787874]\n"," [0.03843938 0.05688613]\n"," [0.03837758 0.05762269]]\n","\n","Average MAE Loss:\n","[0.04775749 0.04830402 0.04766276 0.04800014]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03848467 0.05702169]\n"," [0.03872929 0.05787874]\n"," [0.03842055 0.05689519]\n"," [0.03837615 0.05763996]]\n","\n","Average MAE Loss:\n","[0.04775318 0.04830402 0.04765787 0.04800805]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03829244 0.05712327]\n"," [0.03828363 0.0571769 ]\n"," [0.03825284 0.05718312]\n"," [0.03828765 0.05719336]]\n","\n","Average MAE Loss:\n","[0.04770785 0.04773027 0.04771798 0.04774051]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03827956 0.05708772]\n"," [0.03828363 0.0571769 ]\n"," [0.03823457 0.05718968]\n"," [0.03829377 0.05722061]]\n","\n","Average MAE Loss:\n","[0.04768364 0.04773027 0.04771212 0.04775719]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03826922 0.05705445]\n"," [0.03828363 0.0571769 ]\n"," [0.03821825 0.05719637]\n"," [0.03833973 0.05725474]]\n","\n","Average MAE Loss:\n","[0.04766184 0.04773027 0.04770731 0.04779723]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03827422 0.05702572]\n"," [0.03828363 0.0571769 ]\n"," [0.03819948 0.05720406]\n"," [0.03839714 0.05730206]]\n","\n","Average MAE Loss:\n","[0.04764997 0.04773027 0.04770177 0.0478496 ]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:50.448440074920654\n","\n","[[0.03827422 0.05702572]\n"," [0.03828363 0.0571769 ]\n"," [0.03819948 0.05720406]\n"," [0.03839714 0.05730206]]\n","00:01:7.391430575999948\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08888319 0.09293   ]\n"," [0.10227573 0.10665325]\n"," [0.08501425 0.08736908]\n"," [0.09345094 0.09651124]]\n","\n","Average MAE Loss:\n","[0.09090659 0.10446449 0.08619167 0.09498109]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08699241 0.09100685]\n"," [0.10227573 0.10665325]\n"," [0.08957329 0.09133884]\n"," [0.08904549 0.09200577]]\n","\n","Average MAE Loss:\n","[0.08899963 0.10446449 0.09045606 0.09052563]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0877803  0.09184983]\n"," [0.10227573 0.10665325]\n"," [0.08956696 0.09332559]\n"," [0.06509369 0.06767572]]\n","\n","Average MAE Loss:\n","[0.08981507 0.10446449 0.09144627 0.06638471]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0865787  0.0900552 ]\n"," [0.10227573 0.10665325]\n"," [0.07542891 0.07947798]\n"," [0.06609046 0.06804416]]\n","\n","Average MAE Loss:\n","[0.08831695 0.10446449 0.07745345 0.06706731]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08653335 0.08967769]\n"," [0.10227573 0.10665325]\n"," [0.06296941 0.06653083]\n"," [0.05952063 0.05896779]]\n","\n","Average MAE Loss:\n","[0.08810552 0.10446449 0.06475012 0.05924421]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08542877 0.08872359]\n"," [0.10227573 0.10665325]\n"," [0.06134892 0.06491733]\n"," [0.07504208 0.08021937]]\n","\n","Average MAE Loss:\n","[0.08707618 0.10446449 0.06313312 0.07763073]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08612695 0.0899872 ]\n"," [0.10227573 0.10665325]\n"," [0.05996934 0.06238931]\n"," [0.05862805 0.05935526]]\n","\n","Average MAE Loss:\n","[0.08805708 0.10446449 0.06117933 0.05899166]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05741184 0.06145129]\n"," [0.05862805 0.05935526]\n"," [0.08684732 0.09017387]\n"," [0.05886124 0.05924587]]\n","\n","Average MAE Loss:\n","[0.05943156 0.05899166 0.0885106  0.05905355]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05163787 0.05475722]\n"," [0.05862805 0.05935526]\n"," [0.09164692 0.09567702]\n"," [0.0577818  0.05715827]]\n","\n","Average MAE Loss:\n","[0.05319754 0.05899166 0.09366197 0.05747003]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0522167  0.05670734]\n"," [0.05862805 0.05935526]\n"," [0.0845352  0.0873228 ]\n"," [0.07202324 0.07385682]]\n","\n","Average MAE Loss:\n","[0.05446202 0.05899166 0.085929   0.07294003]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04769569 0.04920317]\n"," [0.05862805 0.05935526]\n"," [0.08482019 0.08839101]\n"," [0.05914311 0.06102017]]\n","\n","Average MAE Loss:\n","[0.04844943 0.05899166 0.0866056  0.06008164]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04780515 0.05089468]\n"," [0.05862805 0.05935526]\n"," [0.08463465 0.08836387]\n"," [0.06966811 0.07414109]]\n","\n","Average MAE Loss:\n","[0.04934991 0.05899166 0.08649926 0.0719046 ]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0494834  0.05206425]\n"," [0.05862805 0.05935526]\n"," [0.08483338 0.08901699]\n"," [0.0637477  0.06692044]]\n","\n","Average MAE Loss:\n","[0.05077383 0.05899166 0.08692518 0.06533407]\n","\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04900797 0.05121386]\n"," [0.05862805 0.05935526]\n"," [0.08282682 0.08607589]\n"," [0.05874147 0.06076559]]\n","\n","Average MAE Loss:\n","[0.05011092 0.05899166 0.08445136 0.05975353]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04990343 0.05343521]\n"," [0.09508442 0.09945839]\n"," [0.05324511 0.05454786]\n"," [0.06173017 0.06506536]]\n","\n","Average MAE Loss:\n","[0.05166932 0.09727141 0.05389649 0.06339777]\n","\n","Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04966668 0.05136417]\n"," [0.09508442 0.09945839]\n"," [0.05341966 0.05537259]\n"," [0.05176068 0.05352322]]\n","\n","Average MAE Loss:\n","[0.05051542 0.09727141 0.05439613 0.05264195]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05085132 0.05317966]\n"," [0.09508442 0.09945839]\n"," [0.04901437 0.05075438]\n"," [0.05096468 0.05223473]]\n","\n","Average MAE Loss:\n","[0.05201549 0.09727141 0.04988437 0.0515997 ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04837245 0.04980937]\n"," [0.09508442 0.09945839]\n"," [0.04938448 0.05094202]\n"," [0.05135207 0.05269502]]\n","\n","Average MAE Loss:\n","[0.04909091 0.09727141 0.05016325 0.05202355]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04882492 0.05073656]\n"," [0.09508442 0.09945839]\n"," [0.04948535 0.0503566 ]\n"," [0.04972489 0.05060817]]\n","\n","Average MAE Loss:\n","[0.04978074 0.09727141 0.04992098 0.05016653]\n","\n","Epoch 00019: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04751234 0.04896985]\n"," [0.09508442 0.09945839]\n"," [0.0488165  0.04955419]\n"," [0.05110932 0.05290816]]\n","\n","Average MAE Loss:\n","[0.04824109 0.09727141 0.04918534 0.05200874]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04733745 0.04855586]\n"," [0.09508442 0.09945839]\n"," [0.04822961 0.04901982]\n"," [0.04880169 0.04973138]]\n","\n","Average MAE Loss:\n","[0.04794666 0.09727141 0.04862472 0.04926653]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09033878 0.09473218]\n"," [0.04880169 0.04973138]\n"," [0.04942372 0.05120693]\n"," [0.04975154 0.05121489]]\n","\n","Average MAE Loss:\n","[0.09253548 0.04926653 0.05031532 0.05048322]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06940025 0.07356236]\n"," [0.04880169 0.04973138]\n"," [0.04915734 0.05109581]\n"," [0.05028433 0.0514291 ]]\n","\n","Average MAE Loss:\n","[0.07148131 0.04926653 0.05012657 0.05085671]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04796903 0.04924208]\n"," [0.04880169 0.04973138]\n"," [0.04858521 0.05000008]\n"," [0.04998191 0.05097665]]\n","\n","Average MAE Loss:\n","[0.04860555 0.04926653 0.04929265 0.05047928]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05113591 0.05380936]\n"," [0.04880169 0.04973138]\n"," [0.04800093 0.04971065]\n"," [0.04889181 0.049353  ]]\n","\n","Average MAE Loss:\n","[0.05247264 0.04926653 0.04885579 0.0491224 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04927084 0.05120333]\n"," [0.04880169 0.04973138]\n"," [0.04756229 0.04905804]\n"," [0.04869314 0.04925046]]\n","\n","Average MAE Loss:\n","[0.05023708 0.04926653 0.04831016 0.0489718 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0486355  0.05011633]\n"," [0.04880169 0.04973138]\n"," [0.04841948 0.05040255]\n"," [0.05111004 0.0528479 ]]\n","\n","Average MAE Loss:\n","[0.04937591 0.04926653 0.04941101 0.05197897]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04867668 0.05006669]\n"," [0.04880169 0.04973138]\n"," [0.04700518 0.04818293]\n"," [0.04860009 0.0489732 ]]\n","\n","Average MAE Loss:\n","[0.04937169 0.04926653 0.04759405 0.04878665]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04955667 0.05142495]\n"," [0.05253371 0.05507156]\n"," [0.04758959 0.0489135 ]\n"," [0.04743553 0.04749924]]\n","\n","Average MAE Loss:\n","[0.05049081 0.05380264 0.04825155 0.04746739]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04830724 0.04985403]\n"," [0.05253371 0.05507156]\n"," [0.04732363 0.04889116]\n"," [0.05061251 0.05255139]]\n","\n","Average MAE Loss:\n","[0.04908064 0.05380264 0.04810739 0.05158195]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04775602 0.049141  ]\n"," [0.05253371 0.05507156]\n"," [0.04662907 0.04778163]\n"," [0.04869752 0.04986651]]\n","\n","Average MAE Loss:\n","[0.04844851 0.05380264 0.04720535 0.04928202]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04718327 0.04836308]\n"," [0.05253371 0.05507156]\n"," [0.04713873 0.0485076 ]\n"," [0.04998837 0.05187448]]\n","\n","Average MAE Loss:\n","[0.04777318 0.05380264 0.04782317 0.05093143]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04677912 0.04779019]\n"," [0.05253371 0.05507156]\n"," [0.04634045 0.0473694 ]\n"," [0.04832117 0.04911804]]\n","\n","Average MAE Loss:\n","[0.04728466 0.05380264 0.04685492 0.0487196 ]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04660633 0.04755573]\n"," [0.05253371 0.05507156]\n"," [0.04683347 0.0482548 ]\n"," [0.04842635 0.04917298]]\n","\n","Average MAE Loss:\n","[0.04708103 0.05380264 0.04754413 0.04879966]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04672541 0.04774429]\n"," [0.05253371 0.05507156]\n"," [0.04647721 0.04740103]\n"," [0.04861005 0.0495302 ]]\n","\n","Average MAE Loss:\n","[0.04723485 0.05380264 0.04693912 0.04907013]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05109344 0.053386  ]\n"," [0.04861005 0.0495302 ]\n"," [0.04585552 0.04650041]\n"," [0.04679719 0.04772402]]\n","\n","Average MAE Loss:\n","[0.05223972 0.04907013 0.04617797 0.04726061]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.0488195  0.05054396]\n"," [0.04861005 0.0495302 ]\n"," [0.04633692 0.04671269]\n"," [0.04794668 0.04905064]]\n","\n","Average MAE Loss:\n","[0.04968173 0.04907013 0.0465248  0.04849866]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04772211 0.04912501]\n"," [0.04861005 0.0495302 ]\n"," [0.04685228 0.04771855]\n"," [0.04818103 0.04910048]]\n","\n","Average MAE Loss:\n","[0.04842356 0.04907013 0.04728542 0.04864075]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.0474053  0.04872092]\n"," [0.04861005 0.0495302 ]\n"," [0.04803271 0.0494818 ]\n"," [0.04857199 0.04978549]]\n","\n","Average MAE Loss:\n","[0.04806311 0.04907013 0.04875726 0.04917874]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04717654 0.04844262]\n"," [0.04861005 0.0495302 ]\n"," [0.04776779 0.04933432]\n"," [0.04791171 0.04871441]]\n","\n","Average MAE Loss:\n","[0.04780958 0.04907013 0.04855105 0.04831306]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04712582 0.04838761]\n"," [0.04861005 0.0495302 ]\n"," [0.04742728 0.0486349 ]\n"," [0.04811902 0.04895647]]\n","\n","Average MAE Loss:\n","[0.04775672 0.04907013 0.04803109 0.04853774]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04709866 0.04838193]\n"," [0.04861005 0.0495302 ]\n"," [0.04699253 0.04804886]\n"," [0.04869484 0.04984302]]\n","\n","Average MAE Loss:\n","[0.0477403  0.04907013 0.04752069 0.04926893]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04763649 0.04886314]\n"," [0.04764564 0.0488377 ]\n"," [0.0469992  0.04804909]\n"," [0.04770172 0.0488623 ]]\n","\n","Average MAE Loss:\n","[0.04824981 0.04824167 0.04752415 0.04828201]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04762411 0.04888683]\n"," [0.04764564 0.0488377 ]\n"," [0.04653981 0.04750588]\n"," [0.04789429 0.04906325]]\n","\n","Average MAE Loss:\n","[0.04825547 0.04824167 0.04702284 0.04847877]\n","\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04756886 0.04883769]\n"," [0.04764564 0.0488377 ]\n"," [0.04664026 0.04769705]\n"," [0.04823188 0.04953003]]\n","\n","Average MAE Loss:\n","[0.04820327 0.04824167 0.04716866 0.04888095]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04752517 0.04878376]\n"," [0.04764564 0.0488377 ]\n"," [0.04682222 0.04802992]\n"," [0.04809323 0.04933245]]\n","\n","Average MAE Loss:\n","[0.04815446 0.04824167 0.04742607 0.04871284]\n","\n","Epoch 00046: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04747539 0.04871767]\n"," [0.04764564 0.0488377 ]\n"," [0.04700229 0.04832467]\n"," [0.04775001 0.04877258]]\n","\n","Average MAE Loss:\n","[0.04809653 0.04824167 0.04766348 0.0482613 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04746521 0.04870744]\n"," [0.04764564 0.0488377 ]\n"," [0.04708625 0.04844569]\n"," [0.04770437 0.04866299]]\n","\n","Average MAE Loss:\n","[0.04808632 0.04824167 0.04776597 0.04818368]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04742778 0.04865877]\n"," [0.04764564 0.0488377 ]\n"," [0.04712471 0.0485148 ]\n"," [0.04772027 0.04865512]]\n","\n","Average MAE Loss:\n","[0.04804328 0.04824167 0.04781976 0.0481877 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04762333 0.04882143]\n"," [0.04772027 0.04865512]\n"," [0.04732442 0.04853177]\n"," [0.04704263 0.04840622]]\n","\n","Average MAE Loss:\n","[0.04822238 0.0481877  0.04792809 0.04772443]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04760906 0.04880831]\n"," [0.04772027 0.04865512]\n"," [0.04711998 0.04826592]\n"," [0.04703564 0.04838515]]\n","\n","Average MAE Loss:\n","[0.04820869 0.0481877  0.04769295 0.04771039]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04758377 0.04877802]\n"," [0.04772027 0.04865512]\n"," [0.04691575 0.0480223 ]\n"," [0.04698404 0.04826996]]\n","\n","Average MAE Loss:\n","[0.04818089 0.0481877  0.04746903 0.047627  ]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04755648 0.04874384]\n"," [0.04772027 0.04865512]\n"," [0.04687126 0.04795778]\n"," [0.04703419 0.04833258]]\n","\n","Average MAE Loss:\n","[0.04815016 0.0481877  0.04741452 0.04768338]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04753411 0.04871777]\n"," [0.04772027 0.04865512]\n"," [0.04689931 0.04801843]\n"," [0.04713877 0.04848322]]\n","\n","Average MAE Loss:\n","[0.04812594 0.0481877  0.04745887 0.047811  ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04752399 0.04870597]\n"," [0.04772027 0.04865512]\n"," [0.04688262 0.04802241]\n"," [0.04721747 0.0485916 ]]\n","\n","Average MAE Loss:\n","[0.04811498 0.0481877  0.04745251 0.04790453]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04751632 0.04869885]\n"," [0.04772027 0.04865512]\n"," [0.04683174 0.04797876]\n"," [0.0472047  0.04855102]]\n","\n","Average MAE Loss:\n","[0.04810758 0.0481877  0.04740525 0.04787786]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04723233 0.04837128]\n"," [0.04724234 0.04838331]\n"," [0.04725298 0.04841614]\n"," [0.04721835 0.04832898]]\n","\n","Average MAE Loss:\n","[0.04780181 0.04781283 0.04783456 0.04777367]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04722814 0.04836784]\n"," [0.04724234 0.04838331]\n"," [0.04721979 0.04837603]\n"," [0.04725105 0.04836472]]\n","\n","Average MAE Loss:\n","[0.04779799 0.04781283 0.04779791 0.04780789]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04722852 0.04837153]\n"," [0.04724234 0.04838331]\n"," [0.04719007 0.04834147]\n"," [0.04726733 0.04836989]]\n","\n","Average MAE Loss:\n","[0.04780002 0.04781283 0.04776577 0.04781861]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04722804 0.04837262]\n"," [0.04724234 0.04838331]\n"," [0.04713559 0.04827474]\n"," [0.04731172 0.04843235]]\n","\n","Average MAE Loss:\n","[0.04780033 0.04781283 0.04770517 0.04787204]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:49.93716859817505\n","\n","[[0.04722804 0.04837262]\n"," [0.04724234 0.04838331]\n"," [0.04713559 0.04827474]\n"," [0.04731172 0.04843235]]\n","00:01:8.28104083900007\n"]}],"source":["# seed = 10 table = 4/8 fixed folds CNN_2 dropout\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Try_CNN_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351903,"status":"ok","timestamp":1690027602993,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"PgxCXS_teT76","outputId":"c25bf01d-a4e3-4c08-bc40-cd1fe803fe25"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07632864 0.07342375]\n"," [0.0858584  0.08280423]\n"," [0.06958028 0.06679701]\n"," [0.0743964  0.07121451]]\n","\n","Average MAE Loss:\n","[0.0748762  0.08433132 0.06818865 0.07280545]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07513348 0.07181109]\n"," [0.0858584  0.08280423]\n"," [0.07235302 0.06956607]\n"," [0.06359088 0.06163194]]\n","\n","Average MAE Loss:\n","[0.07347229 0.08433132 0.07095955 0.06261141]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07448399 0.07126369]\n"," [0.0858584  0.08280423]\n"," [0.06782266 0.06516868]\n"," [0.0609222  0.05872008]]\n","\n","Average MAE Loss:\n","[0.07287384 0.08433132 0.06649567 0.05982114]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07388061 0.07068176]\n"," [0.0858584  0.08280423]\n"," [0.07369673 0.07058889]\n"," [0.06265584 0.06064655]]\n","\n","Average MAE Loss:\n","[0.07228118 0.08433132 0.07214281 0.0616512 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07453573 0.07119271]\n"," [0.0858584  0.08280423]\n"," [0.05734262 0.0552469 ]\n"," [0.06024759 0.05800711]]\n","\n","Average MAE Loss:\n","[0.07286422 0.08433132 0.05629476 0.05912735]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.073799   0.0706793 ]\n"," [0.0858584  0.08280423]\n"," [0.04867573 0.04644208]\n"," [0.05935991 0.05783711]]\n","\n","Average MAE Loss:\n","[0.07223915 0.08433132 0.0475589  0.05859851]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07428281 0.07088701]\n"," [0.0858584  0.08280423]\n"," [0.04724127 0.04498371]\n"," [0.0604039  0.05862359]]\n","\n","Average MAE Loss:\n","[0.07258491 0.08433132 0.04611249 0.05951374]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04972744 0.04714534]\n"," [0.0604039  0.05862359]\n"," [0.07491075 0.07177119]\n"," [0.05180121 0.0501091 ]]\n","\n","Average MAE Loss:\n","[0.04843639 0.05951374 0.07334097 0.05095516]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04718961 0.04459137]\n"," [0.0604039  0.05862359]\n"," [0.07431881 0.07129671]\n"," [0.04496939 0.04275458]]\n","\n","Average MAE Loss:\n","[0.04589049 0.05951374 0.07280776 0.04386199]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0434461  0.04076683]\n"," [0.0604039  0.05862359]\n"," [0.07374063 0.07059958]\n"," [0.04392302 0.04210071]]\n","\n","Average MAE Loss:\n","[0.04210646 0.05951374 0.0721701  0.04301187]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04269106 0.03962925]\n"," [0.0604039  0.05862359]\n"," [0.07351367 0.07055718]\n"," [0.0433307  0.0418694 ]]\n","\n","Average MAE Loss:\n","[0.04116016 0.05951374 0.07203543 0.04260005]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04207568 0.03922179]\n"," [0.0604039  0.05862359]\n"," [0.07135236 0.06828899]\n"," [0.04405708 0.04185355]]\n","\n","Average MAE Loss:\n","[0.04064873 0.05951374 0.06982068 0.04295532]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04119296 0.03836432]\n"," [0.0604039  0.05862359]\n"," [0.06883727 0.06587378]\n"," [0.04561517 0.04306157]]\n","\n","Average MAE Loss:\n","[0.03977864 0.05951374 0.06735552 0.04433837]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04093579 0.03822326]\n"," [0.0604039  0.05862359]\n"," [0.06835367 0.0654985 ]\n"," [0.04322739 0.04120218]]\n","\n","Average MAE Loss:\n","[0.03957953 0.05951374 0.06692608 0.04221479]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0427876  0.04057249]\n"," [0.07787746 0.07467472]\n"," [0.04424101 0.04216821]\n"," [0.04226351 0.04006189]]\n","\n","Average MAE Loss:\n","[0.04168005 0.07627609 0.04320461 0.0411627 ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04192806 0.03899966]\n"," [0.07787746 0.07467472]\n"," [0.04298512 0.04106842]\n"," [0.04176103 0.04006177]]\n","\n","Average MAE Loss:\n","[0.04046386 0.07627609 0.04202677 0.0409114 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04238208 0.03969533]\n"," [0.07787746 0.07467472]\n"," [0.04200916 0.04044659]\n"," [0.04367896 0.04152496]]\n","\n","Average MAE Loss:\n","[0.04103871 0.07627609 0.04122788 0.04260196]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04060331 0.03793208]\n"," [0.07787746 0.07467472]\n"," [0.04173271 0.04008572]\n"," [0.04100735 0.03914315]]\n","\n","Average MAE Loss:\n","[0.0392677  0.07627609 0.04090922 0.04007525]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04089226 0.0382009 ]\n"," [0.07787746 0.07467472]\n"," [0.04163848 0.03954263]\n"," [0.04126511 0.03932206]]\n","\n","Average MAE Loss:\n","[0.03954658 0.07627609 0.04059055 0.04029358]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04106433 0.03825183]\n"," [0.07787746 0.07467472]\n"," [0.04149432 0.03974574]\n"," [0.04079547 0.03920872]]\n","\n","Average MAE Loss:\n","[0.03965808 0.07627609 0.04062003 0.04000209]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04222476 0.04002103]\n"," [0.07787746 0.07467472]\n"," [0.04166644 0.04002907]\n"," [0.04014626 0.03882202]]\n","\n","Average MAE Loss:\n","[0.0411229  0.07627609 0.04084776 0.03948414]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04382438 0.04121201]\n"," [0.04014626 0.03882202]\n"," [0.04102246 0.03877644]\n"," [0.04114891 0.03939917]]\n","\n","Average MAE Loss:\n","[0.0425182  0.03948414 0.03989945 0.04027404]\n","\n","Epoch 00022: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04142551 0.03879889]\n"," [0.04014626 0.03882202]\n"," [0.04083307 0.0389414 ]\n"," [0.04147345 0.0394767 ]]\n","\n","Average MAE Loss:\n","[0.0401122  0.03948414 0.03988723 0.04047508]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04095717 0.03862025]\n"," [0.04014626 0.03882202]\n"," [0.04075671 0.03899031]\n"," [0.04087992 0.0392065 ]]\n","\n","Average MAE Loss:\n","[0.03978871 0.03948414 0.03987351 0.04004321]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04077431 0.03819759]\n"," [0.04014626 0.03882202]\n"," [0.04037092 0.03837866]\n"," [0.04057937 0.03883366]]\n","\n","Average MAE Loss:\n","[0.03948595 0.03948414 0.03937479 0.03970651]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04033605 0.03775675]\n"," [0.04014626 0.03882202]\n"," [0.04044596 0.03839512]\n"," [0.04051984 0.03858378]]\n","\n","Average MAE Loss:\n","[0.0390464  0.03948414 0.03942054 0.03955181]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04045742 0.03785232]\n"," [0.04014626 0.03882202]\n"," [0.04070227 0.03859346]\n"," [0.04050544 0.03847279]]\n","\n","Average MAE Loss:\n","[0.03915487 0.03948414 0.03964786 0.03948912]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04039753 0.03794866]\n"," [0.04014626 0.03882202]\n"," [0.0407003  0.0387472 ]\n"," [0.04093963 0.03930181]]\n","\n","Average MAE Loss:\n","[0.03917309 0.03948414 0.03972375 0.04012072]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03928712 0.03688098]\n"," [0.04023694 0.0380279 ]\n"," [0.03954396 0.03754335]\n"," [0.03974776 0.03808849]]\n","\n","Average MAE Loss:\n","[0.03808405 0.03913242 0.03854365 0.03891813]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0395375  0.03691568]\n"," [0.04023694 0.0380279 ]\n"," [0.03996662 0.03797845]\n"," [0.03925868 0.03770062]]\n","\n","Average MAE Loss:\n","[0.03822659 0.03913242 0.03897254 0.03847965]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03951409 0.0368937 ]\n"," [0.04023694 0.0380279 ]\n"," [0.04030801 0.03832337]\n"," [0.03927381 0.03776396]]\n","\n","Average MAE Loss:\n","[0.0382039  0.03913242 0.03931569 0.03851888]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03935034 0.03696306]\n"," [0.04023694 0.0380279 ]\n"," [0.04034771 0.03854704]\n"," [0.03908356 0.03749309]]\n","\n","Average MAE Loss:\n","[0.0381567  0.03913242 0.03944737 0.03828833]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.0393705  0.03670639]\n"," [0.04023694 0.0380279 ]\n"," [0.03995715 0.03804994]\n"," [0.03933262 0.03740577]]\n","\n","Average MAE Loss:\n","[0.03803844 0.03913242 0.03900355 0.03836919]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03941562 0.03690437]\n"," [0.04023694 0.0380279 ]\n"," [0.04046717 0.03860354]\n"," [0.03928438 0.03746575]]\n","\n","Average MAE Loss:\n","[0.03816    0.03913242 0.03953535 0.03837506]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03934457 0.03662065]\n"," [0.04023694 0.0380279 ]\n"," [0.03969164 0.03782267]\n"," [0.03905607 0.03736461]]\n","\n","Average MAE Loss:\n","[0.03798261 0.03913242 0.03875715 0.03821034]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03927144 0.03683098]\n"," [0.03905607 0.03736461]\n"," [0.03914112 0.03674767]\n"," [0.03888593 0.03710813]]\n","\n","Average MAE Loss:\n","[0.03805121 0.03821034 0.0379444  0.03799703]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03965374 0.03699084]\n"," [0.03905607 0.03736461]\n"," [0.03943725 0.03724206]\n"," [0.03882133 0.03714734]]\n","\n","Average MAE Loss:\n","[0.03832229 0.03821034 0.03833965 0.03798433]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03938785 0.03688748]\n"," [0.03905607 0.03736461]\n"," [0.03979768 0.03776921]\n"," [0.03884616 0.0373175 ]]\n","\n","Average MAE Loss:\n","[0.03813767 0.03821034 0.03878344 0.03808183]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03956833 0.03713331]\n"," [0.03905607 0.03736461]\n"," [0.0397557  0.03779125]\n"," [0.03881326 0.03726074]]\n","\n","Average MAE Loss:\n","[0.03835082 0.03821034 0.03877347 0.038037  ]\n","\n","Epoch 00039: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.0392446  0.03669344]\n"," [0.03905607 0.03736461]\n"," [0.03967571 0.03774038]\n"," [0.03893473 0.03745279]]\n","\n","Average MAE Loss:\n","[0.03796902 0.03821034 0.03870804 0.03819376]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03921295 0.03671373]\n"," [0.03905607 0.03736461]\n"," [0.0395956  0.03758356]\n"," [0.0391075  0.03765687]]\n","\n","Average MAE Loss:\n","[0.03796334 0.03821034 0.03858958 0.03838218]\n","\n","Epoch 00041: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03926891 0.03672105]\n"," [0.03905607 0.03736461]\n"," [0.03961162 0.03758714]\n"," [0.03907224 0.03754607]]\n","\n","Average MAE Loss:\n","[0.03799498 0.03821034 0.03859938 0.03830915]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03877776 0.03668398]\n"," [0.03873683 0.03672434]\n"," [0.03875405 0.036767  ]\n"," [0.03868264 0.03689312]]\n","\n","Average MAE Loss:\n","[0.03773087 0.03773059 0.03776052 0.03778788]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03910237 0.03673605]\n"," [0.03873683 0.03672434]\n"," [0.03914091 0.03716605]\n"," [0.03881349 0.03710747]]\n","\n","Average MAE Loss:\n","[0.03791921 0.03773059 0.03815348 0.03796048]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03907588 0.03645093]\n"," [0.03873683 0.03672434]\n"," [0.03927063 0.03729744]\n"," [0.03877254 0.0370877 ]]\n","\n","Average MAE Loss:\n","[0.03776341 0.03773059 0.03828404 0.03793012]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03911728 0.03653046]\n"," [0.03873683 0.03672434]\n"," [0.03942881 0.03746427]\n"," [0.03878781 0.03711248]]\n","\n","Average MAE Loss:\n","[0.03782387 0.03773059 0.03844654 0.03795015]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03918217 0.03677486]\n"," [0.03873683 0.03672434]\n"," [0.03960169 0.03769359]\n"," [0.03915733 0.03750824]]\n","\n","Average MAE Loss:\n","[0.03797852 0.03773059 0.03864764 0.03833279]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0391226  0.03668302]\n"," [0.03873683 0.03672434]\n"," [0.03960068 0.03770958]\n"," [0.03909637 0.03745231]]\n","\n","Average MAE Loss:\n","[0.03790281 0.03773059 0.03865513 0.03827434]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03911611 0.03662444]\n"," [0.03873683 0.03672434]\n"," [0.03953175 0.03762419]\n"," [0.03894288 0.03728432]]\n","\n","Average MAE Loss:\n","[0.03787028 0.03773059 0.03857797 0.0381136 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03870706 0.03664349]\n"," [0.03894288 0.03728432]\n"," [0.03907634 0.0366006 ]\n"," [0.03912793 0.03734332]]\n","\n","Average MAE Loss:\n","[0.03767527 0.0381136  0.03783847 0.03823562]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03879667 0.03660709]\n"," [0.03894288 0.03728432]\n"," [0.03902065 0.03664799]\n"," [0.03879411 0.03710366]]\n","\n","Average MAE Loss:\n","[0.03770188 0.0381136  0.03783432 0.03794889]\n","\n","Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00051: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03892487 0.03660155]\n"," [0.03894288 0.03728432]\n"," [0.03900237 0.03669558]\n"," [0.03871568 0.03704843]]\n","\n","Average MAE Loss:\n","[0.03776321 0.0381136  0.03784897 0.03788205]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03906784 0.03670793]\n"," [0.03894288 0.03728432]\n"," [0.03901098 0.03673536]\n"," [0.03868105 0.03705231]]\n","\n","Average MAE Loss:\n","[0.03788789 0.0381136  0.03787317 0.03786668]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03920389 0.03687689]\n"," [0.03894288 0.03728432]\n"," [0.0390572  0.03680601]\n"," [0.03875678 0.03713757]]\n","\n","Average MAE Loss:\n","[0.03804039 0.0381136  0.0379316  0.03794717]\n","\n","Epoch 00054: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03913431 0.03680302]\n"," [0.03894288 0.03728432]\n"," [0.03907202 0.03684245]\n"," [0.03863827 0.03703178]]\n","\n","Average MAE Loss:\n","[0.03796866 0.0381136  0.03795724 0.03783502]\n","\n","Epoch 00055: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03904483 0.03665415]\n"," [0.03894288 0.03728432]\n"," [0.03907523 0.03685545]\n"," [0.03862128 0.03700303]]\n","\n","Average MAE Loss:\n","[0.03784949 0.0381136  0.03796534 0.03781215]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03863803 0.03657355]\n"," [0.03863196 0.03666389]\n"," [0.03864395 0.03668451]\n"," [0.03863175 0.03668306]]\n","\n","Average MAE Loss:\n","[0.03760579 0.03764793 0.03766423 0.03765741]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03870724 0.03656907]\n"," [0.03863196 0.03666389]\n"," [0.03866244 0.03670622]\n"," [0.03862135 0.03670062]]\n","\n","Average MAE Loss:\n","[0.03763816 0.03764793 0.03768433 0.03766099]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03879955 0.03659991]\n"," [0.03863196 0.03666389]\n"," [0.03869108 0.03673755]\n"," [0.03863402 0.03672796]]\n","\n","Average MAE Loss:\n","[0.03769973 0.03764793 0.03771431 0.03768099]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03886033 0.03661897]\n"," [0.03863196 0.03666389]\n"," [0.03872477 0.0367752 ]\n"," [0.03863776 0.03674501]]\n","\n","Average MAE Loss:\n","[0.03773965 0.03764793 0.03774998 0.03769138]\n","\n","\n","epochs finished with time:48.842079401016235\n","\n","[[0.03886033 0.03661897]\n"," [0.03863196 0.03666389]\n"," [0.03872477 0.0367752 ]\n"," [0.03863776 0.03674501]]\n","00:01:12.882696865999606\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.1051002  0.10465337]\n"," [0.11635957 0.1145391 ]\n"," [0.09952453 0.09869113]\n"," [0.10200732 0.10030704]]\n","\n","Average MAE Loss:\n","[0.10487679 0.11544933 0.09910783 0.10115718]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10280758 0.10167047]\n"," [0.11635957 0.1145391 ]\n"," [0.09746099 0.09753405]\n"," [0.09320068 0.09067798]]\n","\n","Average MAE Loss:\n","[0.10223902 0.11544933 0.09749752 0.09193933]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10173333 0.10133227]\n"," [0.11635957 0.1145391 ]\n"," [0.0962926  0.09537244]\n"," [0.09015805 0.08800419]]\n","\n","Average MAE Loss:\n","[0.1015328  0.11544933 0.09583252 0.08908112]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10150443 0.10091184]\n"," [0.11635957 0.1145391 ]\n"," [0.09681808 0.09657897]\n"," [0.09273496 0.08997425]]\n","\n","Average MAE Loss:\n","[0.10120814 0.11544933 0.09669853 0.09135461]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10133255 0.10041306]\n"," [0.11635957 0.1145391 ]\n"," [0.09626582 0.09572197]\n"," [0.08984276 0.08801428]]\n","\n","Average MAE Loss:\n","[0.1008728  0.11544933 0.09599389 0.08892852]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10197219 0.1009302 ]\n"," [0.11635957 0.1145391 ]\n"," [0.09569094 0.09538373]\n"," [0.08885735 0.08720815]]\n","\n","Average MAE Loss:\n","[0.10145119 0.11544933 0.09553733 0.08803275]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10224161 0.10091203]\n"," [0.11635957 0.1145391 ]\n"," [0.09509907 0.09551613]\n"," [0.08898268 0.08678844]]\n","\n","Average MAE Loss:\n","[0.10157682 0.11544933 0.0953076  0.08788556]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08302654 0.08144822]\n"," [0.08898268 0.08678844]\n"," [0.10292342 0.10168363]\n"," [0.09596735 0.09470826]]\n","\n","Average MAE Loss:\n","[0.08223738 0.08788556 0.10230352 0.09533781]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07058014 0.07169561]\n"," [0.08898268 0.08678844]\n"," [0.1064072  0.10489904]\n"," [0.09514089 0.09428928]]\n","\n","Average MAE Loss:\n","[0.07113788 0.08788556 0.10565312 0.09471509]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06862756 0.07059004]\n"," [0.08898268 0.08678844]\n"," [0.10185401 0.10074377]\n"," [0.09489465 0.09370788]]\n","\n","Average MAE Loss:\n","[0.0696088  0.08788556 0.10129889 0.09430127]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07006348 0.07004431]\n"," [0.08898268 0.08678844]\n"," [0.10151976 0.10069772]\n"," [0.0990618  0.09755247]]\n","\n","Average MAE Loss:\n","[0.0700539  0.08788556 0.10110874 0.09830714]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0678542  0.06907737]\n"," [0.08898268 0.08678844]\n"," [0.09852099 0.09813064]\n"," [0.09400227 0.09374199]]\n","\n","Average MAE Loss:\n","[0.06846579 0.08788556 0.09832582 0.09387213]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06679989 0.0677434 ]\n"," [0.08898268 0.08678844]\n"," [0.09283487 0.09225558]\n"," [0.09421204 0.09288996]]\n","\n","Average MAE Loss:\n","[0.06727164 0.08788556 0.09254522 0.093551  ]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06668224 0.0683146 ]\n"," [0.08898268 0.08678844]\n"," [0.06961539 0.06921225]\n"," [0.09334145 0.09258167]]\n","\n","Average MAE Loss:\n","[0.06749842 0.08788556 0.06941382 0.09296156]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06648118 0.06807259]\n"," [0.11038756 0.10853616]\n"," [0.06961273 0.07022437]\n"," [0.07204684 0.07167533]]\n","\n","Average MAE Loss:\n","[0.06727689 0.10946186 0.06991855 0.07186109]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06638132 0.06778978]\n"," [0.11038756 0.10853616]\n"," [0.06896459 0.06981794]\n"," [0.07005122 0.06982237]]\n","\n","Average MAE Loss:\n","[0.06708555 0.10946186 0.06939126 0.06993679]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07116482 0.07051104]\n"," [0.11038756 0.10853616]\n"," [0.06680928 0.06792831]\n"," [0.06780677 0.06802938]]\n","\n","Average MAE Loss:\n","[0.07083793 0.10946186 0.06736879 0.06791808]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06482664 0.06547775]\n"," [0.11038756 0.10853616]\n"," [0.06613665 0.06739374]\n"," [0.06707342 0.06761596]]\n","\n","Average MAE Loss:\n","[0.0651522  0.10946186 0.0667652  0.06734469]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06448671 0.06516855]\n"," [0.11038756 0.10853616]\n"," [0.0664894  0.06774714]\n"," [0.06791177 0.06792923]]\n","\n","Average MAE Loss:\n","[0.06482763 0.10946186 0.06711827 0.0679205 ]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06596247 0.06629925]\n"," [0.11038756 0.10853616]\n"," [0.06586819 0.06709505]\n"," [0.06691418 0.06733605]]\n","\n","Average MAE Loss:\n","[0.06613086 0.10946186 0.06648162 0.06712511]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06475637 0.06585162]\n"," [0.11038756 0.10853616]\n"," [0.06647056 0.06741195]\n"," [0.07031226 0.06943233]]\n","\n","Average MAE Loss:\n","[0.065304   0.10946186 0.06694126 0.06987229]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07332363 0.07274355]\n"," [0.07031226 0.06943233]\n"," [0.06620392 0.06737072]\n"," [0.06674268 0.06682134]]\n","\n","Average MAE Loss:\n","[0.07303359 0.06987229 0.06678732 0.06678201]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07539912 0.07481716]\n"," [0.07031226 0.06943233]\n"," [0.06478941 0.06614413]\n"," [0.06668331 0.06638286]]\n","\n","Average MAE Loss:\n","[0.07510814 0.06987229 0.06546677 0.06653308]\n","\n","Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06551321 0.06686366]\n"," [0.07031226 0.06943233]\n"," [0.06513723 0.06614599]\n"," [0.06640046 0.06608711]]\n","\n","Average MAE Loss:\n","[0.06618843 0.06987229 0.06564161 0.06624379]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.06628003 0.06662494]\n"," [0.07031226 0.06943233]\n"," [0.06461021 0.06597278]\n"," [0.06702189 0.06673521]]\n","\n","Average MAE Loss:\n","[0.06645249 0.06987229 0.0652915  0.06687855]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06534342 0.06623726]\n"," [0.07031226 0.06943233]\n"," [0.06527188 0.06640717]\n"," [0.06640662 0.06597114]]\n","\n","Average MAE Loss:\n","[0.06579034 0.06987229 0.06583953 0.06618888]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06607953 0.066601  ]\n"," [0.07031226 0.06943233]\n"," [0.0653904  0.06629643]\n"," [0.06542959 0.06566934]]\n","\n","Average MAE Loss:\n","[0.06634026 0.06987229 0.06584342 0.06554947]\n","\n","Epoch 00027: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06544026 0.06628555]\n"," [0.07031226 0.06943233]\n"," [0.06496924 0.06616569]\n"," [0.06696557 0.06639391]]\n","\n","Average MAE Loss:\n","[0.0658629  0.06987229 0.06556747 0.06667974]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06374884 0.06436169]\n"," [0.0703606  0.06956213]\n"," [0.06381671 0.06507378]\n"," [0.06390562 0.06452599]]\n","\n","Average MAE Loss:\n","[0.06405526 0.06996136 0.06444525 0.0642158 ]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06274819 0.06394581]\n"," [0.0703606  0.06956213]\n"," [0.06564859 0.06622114]\n"," [0.06461486 0.06465453]]\n","\n","Average MAE Loss:\n","[0.063347   0.06996136 0.06593486 0.06463469]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06318887 0.06406007]\n"," [0.0703606  0.06956213]\n"," [0.06472656 0.06585376]\n"," [0.0650754  0.06493047]]\n","\n","Average MAE Loss:\n","[0.06362447 0.06996136 0.06529016 0.06500293]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06313832 0.06408799]\n"," [0.0703606  0.06956213]\n"," [0.06419904 0.06560642]\n"," [0.06443017 0.06434754]]\n","\n","Average MAE Loss:\n","[0.06361316 0.06996136 0.06490273 0.06438885]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06292648 0.06402715]\n"," [0.0703606  0.06956213]\n"," [0.06453328 0.06565351]\n"," [0.06455036 0.06450858]]\n","\n","Average MAE Loss:\n","[0.06347682 0.06996136 0.06509339 0.06452947]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06311292 0.06399664]\n"," [0.0703606  0.06956213]\n"," [0.0642267  0.06535357]\n"," [0.06406869 0.06431996]]\n","\n","Average MAE Loss:\n","[0.06355478 0.06996136 0.06479014 0.06419432]\n","\n","Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06297845 0.06389264]\n"," [0.0703606  0.06956213]\n"," [0.06400696 0.06523066]\n"," [0.06433784 0.06443099]]\n","\n","Average MAE Loss:\n","[0.06343555 0.06996136 0.06461881 0.06438442]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06652089 0.06641303]\n"," [0.06433784 0.06443099]\n"," [0.06290591 0.06409391]\n"," [0.06436685 0.0650864 ]]\n","\n","Average MAE Loss:\n","[0.06646696 0.06438442 0.06349991 0.06472662]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06371479 0.06438796]\n"," [0.06433784 0.06443099]\n"," [0.06346623 0.06464674]\n"," [0.0646376  0.06508884]]\n","\n","Average MAE Loss:\n","[0.06405137 0.06438442 0.06405648 0.06486322]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06344321 0.06410328]\n"," [0.06433784 0.06443099]\n"," [0.06380456 0.06509337]\n"," [0.06371174 0.06442878]]\n","\n","Average MAE Loss:\n","[0.06377325 0.06438442 0.06444896 0.06407026]\n","\n","Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06329492 0.06404064]\n"," [0.06433784 0.06443099]\n"," [0.06390772 0.06525239]\n"," [0.06333047 0.06423011]]\n","\n","Average MAE Loss:\n","[0.06366778 0.06438442 0.06458006 0.06378029]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06321081 0.06403803]\n"," [0.06433784 0.06443099]\n"," [0.06444125 0.0656355 ]\n"," [0.06381657 0.06429162]]\n","\n","Average MAE Loss:\n","[0.06362442 0.06438442 0.06503837 0.06405409]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06329653 0.06407495]\n"," [0.06433784 0.06443099]\n"," [0.06404552 0.06536822]\n"," [0.06400692 0.06433021]]\n","\n","Average MAE Loss:\n","[0.06368574 0.06438442 0.06470687 0.06416857]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06330115 0.06409832]\n"," [0.06433784 0.06443099]\n"," [0.06398948 0.06531319]\n"," [0.06398268 0.0643149 ]]\n","\n","Average MAE Loss:\n","[0.06369973 0.06438442 0.06465134 0.06414879]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06355286 0.06414342]\n"," [0.06365043 0.06420543]\n"," [0.06373112 0.06432303]\n"," [0.06363887 0.0641513 ]]\n","\n","Average MAE Loss:\n","[0.06384814 0.06392793 0.06402707 0.06389509]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06344625 0.06405701]\n"," [0.06365043 0.06420543]\n"," [0.06360694 0.06445794]\n"," [0.06396029 0.0643379 ]]\n","\n","Average MAE Loss:\n","[0.06375163 0.06392793 0.06403244 0.0641491 ]\n","\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06341566 0.06402394]\n"," [0.06365043 0.06420543]\n"," [0.06366819 0.06454176]\n"," [0.06415036 0.06446033]]\n","\n","Average MAE Loss:\n","[0.0637198  0.06392793 0.06410497 0.06430534]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06334709 0.06397774]\n"," [0.06365043 0.06420543]\n"," [0.06365188 0.06460474]\n"," [0.06382729 0.06423233]]\n","\n","Average MAE Loss:\n","[0.06366241 0.06392793 0.06412831 0.06402981]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06330796 0.06395563]\n"," [0.06365043 0.06420543]\n"," [0.06350741 0.06457878]\n"," [0.06395591 0.06425034]]\n","\n","Average MAE Loss:\n","[0.06363179 0.06392793 0.0640431  0.06410312]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06332868 0.06396428]\n"," [0.06365043 0.06420543]\n"," [0.06352159 0.06464714]\n"," [0.06406187 0.06427999]]\n","\n","Average MAE Loss:\n","[0.06364648 0.06392793 0.06408436 0.06417093]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06330379 0.06395016]\n"," [0.06365043 0.06420543]\n"," [0.06356387 0.06468769]\n"," [0.06431063 0.06439136]]\n","\n","Average MAE Loss:\n","[0.06362698 0.06392793 0.06412578 0.064351  ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06360871 0.06416648]\n"," [0.06431063 0.06439136]\n"," [0.06323119 0.06393393]\n"," [0.06372666 0.06466147]]\n","\n","Average MAE Loss:\n","[0.06388759 0.064351   0.06358256 0.06419407]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06358092 0.06414413]\n"," [0.06431063 0.06439136]\n"," [0.06317603 0.06394824]\n"," [0.06379557 0.06459194]]\n","\n","Average MAE Loss:\n","[0.06386252 0.064351   0.06356214 0.06419375]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06354361 0.06411742]\n"," [0.06431063 0.06439136]\n"," [0.06316328 0.06397573]\n"," [0.06379149 0.06454109]]\n","\n","Average MAE Loss:\n","[0.06383052 0.064351   0.0635695  0.06416629]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06349624 0.06408623]\n"," [0.06431063 0.06439136]\n"," [0.06316566 0.0640045 ]\n"," [0.06364473 0.06441374]]\n","\n","Average MAE Loss:\n","[0.06379124 0.064351   0.06358508 0.06402923]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06344188 0.06405379]\n"," [0.06431063 0.06439136]\n"," [0.06316026 0.06403236]\n"," [0.06358745 0.06434594]]\n","\n","Average MAE Loss:\n","[0.06374783 0.064351   0.06359631 0.06396669]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06342071 0.06404085]\n"," [0.06431063 0.06439136]\n"," [0.06319827 0.06408184]\n"," [0.06362814 0.06432649]]\n","\n","Average MAE Loss:\n","[0.06373078 0.064351   0.06364006 0.06397732]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06340718 0.06403247]\n"," [0.06431063 0.06439136]\n"," [0.06316913 0.0641004 ]\n"," [0.06365483 0.06432781]]\n","\n","Average MAE Loss:\n","[0.06371982 0.064351   0.06363477 0.06399132]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06354277 0.06410882]\n"," [0.06354866 0.06411484]\n"," [0.06351067 0.06411293]\n"," [0.06357733 0.06411836]]\n","\n","Average MAE Loss:\n","[0.0638258  0.06383175 0.0638118  0.06384785]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06352849 0.0640981 ]\n"," [0.06354866 0.06411484]\n"," [0.06351002 0.06412201]\n"," [0.06360877 0.06412213]]\n","\n","Average MAE Loss:\n","[0.06381329 0.06383175 0.06381602 0.06386545]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.0635211  0.06409304]\n"," [0.06354866 0.06411484]\n"," [0.06349301 0.06412355]\n"," [0.06361913 0.06411507]]\n","\n","Average MAE Loss:\n","[0.06380707 0.06383175 0.06380828 0.0638671 ]\n","\n","Epoch 00059: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06351251 0.06408712]\n"," [0.06354866 0.06411484]\n"," [0.06348159 0.06412784]\n"," [0.06362426 0.0641134 ]]\n","\n","Average MAE Loss:\n","[0.06379982 0.06383175 0.06380472 0.06386883]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:49.39903712272644\n","\n","[[0.06351251 0.06408712]\n"," [0.06354866 0.06411484]\n"," [0.06348159 0.06412784]\n"," [0.06362426 0.0641134 ]]\n","00:01:11.811679715001446\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08830144 0.07970948]\n"," [0.09980381 0.08862089]\n"," [0.06927281 0.06189309]\n"," [0.07071539 0.06305885]]\n","\n","Average MAE Loss:\n","[0.08400546 0.09421235 0.06558295 0.06688712]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08642344 0.07695837]\n"," [0.09980381 0.08862089]\n"," [0.07824612 0.06779962]\n"," [0.08297358 0.07966137]]\n","\n","Average MAE Loss:\n","[0.08169091 0.09421235 0.07302287 0.08131748]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08635314 0.07696089]\n"," [0.09980381 0.08862089]\n"," [0.06074338 0.05259609]\n"," [0.07209318 0.06207909]]\n","\n","Average MAE Loss:\n","[0.08165701 0.09421235 0.05666973 0.06708613]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08687573 0.07678039]\n"," [0.09980381 0.08862089]\n"," [0.06432989 0.05516734]\n"," [0.06804418 0.06030658]]\n","\n","Average MAE Loss:\n","[0.08182806 0.09421235 0.05974862 0.06417538]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08684813 0.07672396]\n"," [0.09980381 0.08862089]\n"," [0.0601736  0.05073009]\n"," [0.065246   0.05691763]]\n","\n","Average MAE Loss:\n","[0.08178605 0.09421235 0.05545185 0.06108182]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08538164 0.07623932]\n"," [0.09980381 0.08862089]\n"," [0.05851556 0.05008878]\n"," [0.06427832 0.05682096]]\n","\n","Average MAE Loss:\n","[0.08081048 0.09421235 0.05430217 0.06054964]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08541274 0.0761136 ]\n"," [0.09980381 0.08862089]\n"," [0.05672393 0.04962793]\n"," [0.06489596 0.05725427]]\n","\n","Average MAE Loss:\n","[0.08076317 0.09421235 0.05317593 0.06107511]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06253901 0.05486087]\n"," [0.06489596 0.05725427]\n"," [0.08974637 0.07898139]\n"," [0.06115323 0.05182053]]\n","\n","Average MAE Loss:\n","[0.05869994 0.06107511 0.08436388 0.05648688]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05077501 0.04670916]\n"," [0.06489596 0.05725427]\n"," [0.08835939 0.07781141]\n"," [0.05905121 0.05172189]]\n","\n","Average MAE Loss:\n","[0.04874208 0.06107511 0.0830854  0.05538655]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05653154 0.0483816 ]\n"," [0.06489596 0.05725427]\n"," [0.08553418 0.07520616]\n"," [0.05718609 0.04863071]]\n","\n","Average MAE Loss:\n","[0.05245657 0.06107511 0.08037017 0.0529084 ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04950114 0.04399405]\n"," [0.06489596 0.05725427]\n"," [0.08752036 0.07698424]\n"," [0.05873146 0.05356842]]\n","\n","Average MAE Loss:\n","[0.0467476  0.06107511 0.0822523  0.05614994]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05313637 0.04563626]\n"," [0.06489596 0.05725427]\n"," [0.08494155 0.07517193]\n"," [0.06071661 0.05014013]]\n","\n","Average MAE Loss:\n","[0.04938632 0.06107511 0.08005674 0.05542837]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04869107 0.04329805]\n"," [0.06489596 0.05725427]\n"," [0.08438087 0.07527923]\n"," [0.05632466 0.04751454]]\n","\n","Average MAE Loss:\n","[0.04599456 0.06107511 0.07983005 0.0519196 ]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04854925 0.04320127]\n"," [0.06489596 0.05725427]\n"," [0.08461858 0.07457646]\n"," [0.05876787 0.04925379]]\n","\n","Average MAE Loss:\n","[0.04587526 0.06107511 0.07959752 0.05401083]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.0509274  0.04656326]\n"," [0.09230314 0.08110308]\n"," [0.05321887 0.04896763]\n"," [0.05379963 0.04486658]]\n","\n","Average MAE Loss:\n","[0.04874533 0.08670311 0.05109325 0.04933311]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04898667 0.04420761]\n"," [0.09230314 0.08110308]\n"," [0.05148132 0.04432148]\n"," [0.0510485  0.04336619]]\n","\n","Average MAE Loss:\n","[0.04659714 0.08670311 0.0479014  0.04720735]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05078918 0.04397505]\n"," [0.09230314 0.08110308]\n"," [0.05042783 0.04384052]\n"," [0.05053057 0.04237865]]\n","\n","Average MAE Loss:\n","[0.04738212 0.08670311 0.04713417 0.04645461]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04873824 0.04325587]\n"," [0.09230314 0.08110308]\n"," [0.05107279 0.04335602]\n"," [0.04993574 0.04411637]]\n","\n","Average MAE Loss:\n","[0.04599706 0.08670311 0.0472144  0.04702605]\n","\n","Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04884602 0.0428074 ]\n"," [0.09230314 0.08110308]\n"," [0.05069879 0.04361061]\n"," [0.04965272 0.0423381 ]]\n","\n","Average MAE Loss:\n","[0.04582671 0.08670311 0.0471547  0.04599541]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04884744 0.04282714]\n"," [0.09230314 0.08110308]\n"," [0.04987314 0.0434989 ]\n"," [0.04901573 0.04219723]]\n","\n","Average MAE Loss:\n","[0.04583729 0.08670311 0.04668602 0.04560648]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04824534 0.04292385]\n"," [0.09230314 0.08110308]\n"," [0.0497256  0.04328829]\n"," [0.04895353 0.0418833 ]]\n","\n","Average MAE Loss:\n","[0.0455846  0.08670311 0.04650695 0.04541842]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.08287462 0.07166512]\n"," [0.04895353 0.0418833 ]\n"," [0.04900455 0.04341166]\n"," [0.05119125 0.04359972]]\n","\n","Average MAE Loss:\n","[0.07726987 0.04541842 0.04620811 0.04739548]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0531506  0.04657633]\n"," [0.04895353 0.0418833 ]\n"," [0.04981017 0.04314766]\n"," [0.05295425 0.04425023]]\n","\n","Average MAE Loss:\n","[0.04986346 0.04541842 0.04647892 0.04860224]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04966787 0.04300148]\n"," [0.04895353 0.0418833 ]\n"," [0.04943562 0.04293113]\n"," [0.04965334 0.04352098]]\n","\n","Average MAE Loss:\n","[0.04633468 0.04541842 0.04618337 0.04658716]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0475735  0.04268221]\n"," [0.04895353 0.0418833 ]\n"," [0.04938748 0.04299432]\n"," [0.05060442 0.04262377]]\n","\n","Average MAE Loss:\n","[0.04512785 0.04541842 0.0461909  0.0466141 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0476652  0.04225956]\n"," [0.04895353 0.0418833 ]\n"," [0.04911199 0.04293427]\n"," [0.04926277 0.04317789]]\n","\n","Average MAE Loss:\n","[0.04496238 0.04541842 0.04602313 0.04622033]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04775371 0.04222813]\n"," [0.04895353 0.0418833 ]\n"," [0.04927828 0.04418801]\n"," [0.04973387 0.04221134]]\n","\n","Average MAE Loss:\n","[0.04499092 0.04541842 0.04673314 0.0459726 ]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04732367 0.04185357]\n"," [0.04895353 0.0418833 ]\n"," [0.04917357 0.04293839]\n"," [0.05095265 0.04287047]]\n","\n","Average MAE Loss:\n","[0.04458862 0.04541842 0.04605598 0.04691156]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04665654 0.04117715]\n"," [0.05103844 0.04293534]\n"," [0.04789872 0.04127318]\n"," [0.04732366 0.04129356]]\n","\n","Average MAE Loss:\n","[0.04391684 0.04698689 0.04458595 0.04430861]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04681987 0.0411763 ]\n"," [0.05103844 0.04293534]\n"," [0.04749374 0.04246228]\n"," [0.04826389 0.04115397]]\n","\n","Average MAE Loss:\n","[0.04399808 0.04698689 0.04497801 0.04470893]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04609942 0.041557  ]\n"," [0.05103844 0.04293534]\n"," [0.0479459  0.04100312]\n"," [0.04842707 0.04129182]]\n","\n","Average MAE Loss:\n","[0.04382821 0.04698689 0.04447451 0.04485944]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04752259 0.04150632]\n"," [0.05103844 0.04293534]\n"," [0.0474367  0.0423734 ]\n"," [0.04794763 0.04164762]]\n","\n","Average MAE Loss:\n","[0.04451445 0.04698689 0.04490505 0.04479763]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04630876 0.04133774]\n"," [0.05103844 0.04293534]\n"," [0.04750384 0.04206179]\n"," [0.04795235 0.04117757]]\n","\n","Average MAE Loss:\n","[0.04382325 0.04698689 0.04478281 0.04456496]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04687786 0.04107863]\n"," [0.05103844 0.04293534]\n"," [0.0475199  0.04106928]\n"," [0.04816597 0.04097034]]\n","\n","Average MAE Loss:\n","[0.04397824 0.04698689 0.04429459 0.04456815]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04669723 0.04112821]\n"," [0.05103844 0.04293534]\n"," [0.04739581 0.04199442]\n"," [0.04790073 0.04111172]]\n","\n","Average MAE Loss:\n","[0.04391272 0.04698689 0.04469511 0.04450623]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04640093 0.04142523]\n"," [0.04790073 0.04111172]\n"," [0.04650378 0.04120651]\n"," [0.04756463 0.04080364]]\n","\n","Average MAE Loss:\n","[0.04391308 0.04450623 0.04385515 0.04418414]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04669648 0.04115878]\n"," [0.04790073 0.04111172]\n"," [0.04735808 0.04207861]\n"," [0.04758707 0.04077418]]\n","\n","Average MAE Loss:\n","[0.04392763 0.04450623 0.04471834 0.04418063]\n","\n","Epoch 00037: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04598429 0.04130867]\n"," [0.04790073 0.04111172]\n"," [0.04792572 0.04173055]\n"," [0.04760363 0.04103232]]\n","\n","Average MAE Loss:\n","[0.04364648 0.04450623 0.04482813 0.04431798]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04609093 0.04120611]\n"," [0.04790073 0.04111172]\n"," [0.04804656 0.04181615]\n"," [0.04776476 0.04131275]]\n","\n","Average MAE Loss:\n","[0.04364852 0.04450623 0.04493135 0.04453875]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04687078 0.0410471 ]\n"," [0.04790073 0.04111172]\n"," [0.04788749 0.0422349 ]\n"," [0.04779111 0.04143206]]\n","\n","Average MAE Loss:\n","[0.04395894 0.04450623 0.04506119 0.04461158]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04658313 0.04105584]\n"," [0.04790073 0.04111172]\n"," [0.04786169 0.04157212]\n"," [0.04771175 0.04122321]]\n","\n","Average MAE Loss:\n","[0.04381949 0.04450623 0.0447169  0.04446748]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04629101 0.04114744]\n"," [0.04790073 0.04111172]\n"," [0.04768726 0.04167084]\n"," [0.0476692  0.04112006]]\n","\n","Average MAE Loss:\n","[0.04371922 0.04450623 0.04467905 0.04439463]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04696207 0.04060563]\n"," [0.04708074 0.04068023]\n"," [0.04697444 0.04118987]\n"," [0.04713136 0.04071102]]\n","\n","Average MAE Loss:\n","[0.04378385 0.04388049 0.04408216 0.04392119]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04655574 0.04069587]\n"," [0.04708074 0.04068023]\n"," [0.04730648 0.04166824]\n"," [0.04714474 0.04079176]]\n","\n","Average MAE Loss:\n","[0.04362581 0.04388049 0.04448736 0.04396825]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04649537 0.04068719]\n"," [0.04708074 0.04068023]\n"," [0.0474062  0.04145849]\n"," [0.0473446  0.04072883]]\n","\n","Average MAE Loss:\n","[0.04359128 0.04388049 0.04443235 0.04403671]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04645993 0.04073977]\n"," [0.04708074 0.04068023]\n"," [0.04747494 0.04180136]\n"," [0.04747052 0.04073732]]\n","\n","Average MAE Loss:\n","[0.04359985 0.04388049 0.04463815 0.04410392]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04638345 0.04081966]\n"," [0.04708074 0.04068023]\n"," [0.04747593 0.04190739]\n"," [0.04751819 0.04078968]]\n","\n","Average MAE Loss:\n","[0.04360156 0.04388049 0.04469166 0.04415393]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04626435 0.04088546]\n"," [0.04708074 0.04068023]\n"," [0.04744799 0.04174232]\n"," [0.0475171  0.04083287]]\n","\n","Average MAE Loss:\n","[0.04357491 0.04388049 0.04459516 0.04417498]\n","\n","Epoch 00048: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04611915 0.04089973]\n"," [0.04708074 0.04068023]\n"," [0.04740681 0.04175134]\n"," [0.04757607 0.04086262]]\n","\n","Average MAE Loss:\n","[0.04350944 0.04388049 0.04457908 0.04421935]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04686604 0.04060464]\n"," [0.04757607 0.04086262]\n"," [0.0461375  0.04084665]\n"," [0.04735468 0.04145551]]\n","\n","Average MAE Loss:\n","[0.04373534 0.04421935 0.04349208 0.0444051 ]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04695562 0.04065339]\n"," [0.04757607 0.04086262]\n"," [0.04618727 0.04075757]\n"," [0.04729665 0.04124507]]\n","\n","Average MAE Loss:\n","[0.0438045  0.04421935 0.04347242 0.04427086]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04656765 0.04075259]\n"," [0.04757607 0.04086262]\n"," [0.04618647 0.04081258]\n"," [0.04729941 0.04113561]]\n","\n","Average MAE Loss:\n","[0.04366012 0.04421935 0.04349952 0.04421751]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04631936 0.04080688]\n"," [0.04757607 0.04086262]\n"," [0.04622326 0.04087214]\n"," [0.04733066 0.04102268]]\n","\n","Average MAE Loss:\n","[0.04356312 0.04421935 0.0435477  0.04417667]\n","\n","Epoch 00053: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04639453 0.04078683]\n"," [0.04757607 0.04086262]\n"," [0.04638352 0.04087341]\n"," [0.04740791 0.04089777]]\n","\n","Average MAE Loss:\n","[0.04359068 0.04421935 0.04362846 0.04415284]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04617249 0.04086909]\n"," [0.04757607 0.04086262]\n"," [0.04651754 0.04089043]\n"," [0.04747918 0.04084016]]\n","\n","Average MAE Loss:\n","[0.04352079 0.04421935 0.04370398 0.04415967]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04615337 0.04089354]\n"," [0.04757607 0.04086262]\n"," [0.0465081  0.04099146]\n"," [0.04746496 0.04082736]]\n","\n","Average MAE Loss:\n","[0.04352345 0.04421935 0.04374978 0.04414616]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04660866 0.04057424]\n"," [0.04673865 0.04062206]\n"," [0.04671837 0.04071122]\n"," [0.04673417 0.0406213 ]]\n","\n","Average MAE Loss:\n","[0.04359145 0.04368036 0.04371479 0.04367774]\n","\n","Epoch 00057: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04657165 0.04055769]\n"," [0.04673865 0.04062206]\n"," [0.04667444 0.04083082]\n"," [0.04676142 0.04061615]]\n","\n","Average MAE Loss:\n","[0.04356467 0.04368036 0.04375263 0.04368879]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04648291 0.04057883]\n"," [0.04673865 0.04062206]\n"," [0.04669809 0.04084442]\n"," [0.04680558 0.04060876]]\n","\n","Average MAE Loss:\n","[0.04353087 0.04368036 0.04377125 0.04370717]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04648075 0.04057467]\n"," [0.04673865 0.04062206]\n"," [0.04672554 0.04084794]\n"," [0.04681332 0.04062116]]\n","\n","Average MAE Loss:\n","[0.04352771 0.04368036 0.04378674 0.04371724]\n","\n","\n","epochs finished with time:48.52736973762512\n","\n","[[0.04648075 0.04057467]\n"," [0.04673865 0.04062206]\n"," [0.04672554 0.04084794]\n"," [0.04681332 0.04062116]]\n","00:01:7.195332847000827\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07328996 0.09464665]\n"," [0.08241    0.10488542]\n"," [0.06067696 0.08308118]\n"," [0.07661575 0.0947879 ]]\n","\n","Average MAE Loss:\n","[0.0839683  0.09364771 0.07187907 0.08570182]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.0720094  0.09389698]\n"," [0.08241    0.10488542]\n"," [0.0515374  0.07224213]\n"," [0.06961866 0.09078448]]\n","\n","Average MAE Loss:\n","[0.08295319 0.09364771 0.06188977 0.08020157]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0714259  0.09259639]\n"," [0.08241    0.10488542]\n"," [0.06316677 0.08504313]\n"," [0.05019438 0.06954204]]\n","\n","Average MAE Loss:\n","[0.08201114 0.09364771 0.07410495 0.05986821]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07118794 0.09242541]\n"," [0.08241    0.10488542]\n"," [0.04803627 0.06945026]\n"," [0.05370009 0.07202943]]\n","\n","Average MAE Loss:\n","[0.08180667 0.09364771 0.05874326 0.06286476]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07073766 0.09203434]\n"," [0.08241    0.10488542]\n"," [0.04924886 0.06834155]\n"," [0.04793086 0.06769159]]\n","\n","Average MAE Loss:\n","[0.081386   0.09364771 0.0587952  0.05781123]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07123731 0.09301711]\n"," [0.08241    0.10488542]\n"," [0.04587758 0.06700508]\n"," [0.04864307 0.06894819]]\n","\n","Average MAE Loss:\n","[0.08212721 0.09364771 0.05644133 0.05879563]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07091521 0.0922157 ]\n"," [0.08241    0.10488542]\n"," [0.05340003 0.07126264]\n"," [0.04910738 0.06986059]]\n","\n","Average MAE Loss:\n","[0.08156546 0.09364771 0.06233134 0.05948398]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04499379 0.06326002]\n"," [0.04910738 0.06986059]\n"," [0.07177034 0.0930725 ]\n"," [0.04914594 0.06892666]]\n","\n","Average MAE Loss:\n","[0.0541269  0.05948398 0.08242142 0.0590363 ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04692651 0.06779861]\n"," [0.04910738 0.06986059]\n"," [0.07384745 0.09405684]\n"," [0.05286074 0.07122852]]\n","\n","Average MAE Loss:\n","[0.05736256 0.05948398 0.08395215 0.06204463]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04611055 0.06387431]\n"," [0.04910738 0.06986059]\n"," [0.06994291 0.09127753]\n"," [0.04680377 0.06640603]]\n","\n","Average MAE Loss:\n","[0.05499243 0.05948398 0.08061022 0.0566049 ]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04202376 0.06100739]\n"," [0.04910738 0.06986059]\n"," [0.06912436 0.09043156]\n"," [0.04523326 0.06524433]]\n","\n","Average MAE Loss:\n","[0.05151558 0.05948398 0.07977796 0.05523879]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04330179 0.06128253]\n"," [0.04910738 0.06986059]\n"," [0.06895077 0.0905666 ]\n"," [0.04487885 0.06458876]]\n","\n","Average MAE Loss:\n","[0.05229216 0.05948398 0.07975869 0.0547338 ]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04249659 0.06263047]\n"," [0.04910738 0.06986059]\n"," [0.0689014  0.09003927]\n"," [0.04419073 0.06448945]]\n","\n","Average MAE Loss:\n","[0.05256353 0.05948398 0.07947034 0.05434009]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04184218 0.06112289]\n"," [0.04910738 0.06986059]\n"," [0.06875537 0.08988644]\n"," [0.04435392 0.06500368]]\n","\n","Average MAE Loss:\n","[0.05148253 0.05948398 0.07932091 0.0546788 ]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04261507 0.06303001]\n"," [0.07520018 0.09762832]\n"," [0.06021908 0.08216074]\n"," [0.04774286 0.06831927]]\n","\n","Average MAE Loss:\n","[0.05282254 0.08641425 0.07118991 0.05803106]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04069584 0.06030494]\n"," [0.07520018 0.09762832]\n"," [0.04313644 0.06194118]\n"," [0.04179432 0.06182466]]\n","\n","Average MAE Loss:\n","[0.05050039 0.08641425 0.05253881 0.05180949]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04108137 0.05933603]\n"," [0.07520018 0.09762832]\n"," [0.04038695 0.06095664]\n"," [0.04371372 0.06178708]]\n","\n","Average MAE Loss:\n","[0.0502087  0.08641425 0.05067179 0.0527504 ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0402525  0.05947022]\n"," [0.07520018 0.09762832]\n"," [0.04055094 0.06032101]\n"," [0.04064358 0.06068444]]\n","\n","Average MAE Loss:\n","[0.04986136 0.08641425 0.05043598 0.05066401]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04072862 0.0589745 ]\n"," [0.07520018 0.09762832]\n"," [0.04012416 0.06045061]\n"," [0.04071965 0.06024432]]\n","\n","Average MAE Loss:\n","[0.04985156 0.08641425 0.05028739 0.05048198]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03909869 0.05814897]\n"," [0.07520018 0.09762832]\n"," [0.03989753 0.06026214]\n"," [0.04041163 0.06028018]]\n","\n","Average MAE Loss:\n","[0.04862383 0.08641425 0.05007984 0.05034591]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03937739 0.05820157]\n"," [0.07520018 0.09762832]\n"," [0.03996497 0.06004111]\n"," [0.04073024 0.06018285]]\n","\n","Average MAE Loss:\n","[0.04878948 0.08641425 0.05000304 0.05045654]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04534164 0.06342237]\n"," [0.04073024 0.06018285]\n"," [0.03901917 0.0583419 ]\n"," [0.04036307 0.05983517]]\n","\n","Average MAE Loss:\n","[0.05438201 0.05045654 0.04868054 0.05009912]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04118853 0.05973144]\n"," [0.04073024 0.06018285]\n"," [0.03935943 0.05887914]\n"," [0.04060371 0.06004009]]\n","\n","Average MAE Loss:\n","[0.05045998 0.05045654 0.04911929 0.0503219 ]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04049426 0.05910522]\n"," [0.04073024 0.06018285]\n"," [0.03928577 0.05904992]\n"," [0.04047005 0.05987186]]\n","\n","Average MAE Loss:\n","[0.04979974 0.05045654 0.04916784 0.05017095]\n","\n","Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04020906 0.05859777]\n"," [0.04073024 0.06018285]\n"," [0.03930004 0.0592181 ]\n"," [0.04002468 0.05980937]]\n","\n","Average MAE Loss:\n","[0.04940341 0.05045654 0.04925907 0.04991703]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0396532  0.05851654]\n"," [0.04073024 0.06018285]\n"," [0.03978277 0.05945409]\n"," [0.04035742 0.05971297]]\n","\n","Average MAE Loss:\n","[0.04908487 0.05045654 0.04961843 0.0500352 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03996643 0.05835025]\n"," [0.04073024 0.06018285]\n"," [0.03946453 0.05921169]\n"," [0.04035576 0.05964171]]\n","\n","Average MAE Loss:\n","[0.04915834 0.05045654 0.04933811 0.04999874]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03964961 0.05818756]\n"," [0.04073024 0.06018285]\n"," [0.03944452 0.05914834]\n"," [0.04005789 0.05966359]]\n","\n","Average MAE Loss:\n","[0.04891858 0.05045654 0.04929643 0.04986074]\n","\n","Epoch 00028: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03842527 0.05747446]\n"," [0.04146985 0.06197885]\n"," [0.03904906 0.05914473]\n"," [0.03885992 0.05826472]]\n","\n","Average MAE Loss:\n","[0.04794987 0.05172435 0.0490969  0.04856232]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03859794 0.05702025]\n"," [0.04146985 0.06197885]\n"," [0.03809284 0.05795498]\n"," [0.03928705 0.05838484]]\n","\n","Average MAE Loss:\n","[0.0478091  0.05172435 0.04802391 0.04883594]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03875802 0.05696328]\n"," [0.04146985 0.06197885]\n"," [0.03818327 0.05792994]\n"," [0.03914046 0.05838467]]\n","\n","Average MAE Loss:\n","[0.04786065 0.05172435 0.0480566  0.04876256]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03886997 0.05691271]\n"," [0.04146985 0.06197885]\n"," [0.03832522 0.05808067]\n"," [0.03978032 0.05858739]]\n","\n","Average MAE Loss:\n","[0.04789134 0.05172435 0.04820294 0.04918385]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03855203 0.05686894]\n"," [0.04146985 0.06197885]\n"," [0.03840376 0.05821508]\n"," [0.03911796 0.05837999]]\n","\n","Average MAE Loss:\n","[0.04771049 0.05172435 0.04830942 0.04874898]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03862177 0.0568848 ]\n"," [0.04146985 0.06197885]\n"," [0.03832821 0.05822702]\n"," [0.03921472 0.05828278]]\n","\n","Average MAE Loss:\n","[0.04775328 0.05172435 0.04827761 0.04874875]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03879547 0.05698302]\n"," [0.04146985 0.06197885]\n"," [0.03837612 0.05819929]\n"," [0.03916385 0.05822581]]\n","\n","Average MAE Loss:\n","[0.04788924 0.05172435 0.04828771 0.04869483]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03856872 0.05767756]\n"," [0.03916385 0.05822581]\n"," [0.03870505 0.05698048]\n"," [0.03835255 0.05790935]]\n","\n","Average MAE Loss:\n","[0.04812314 0.04869483 0.04784276 0.04813095]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03938628 0.05743007]\n"," [0.03916385 0.05822581]\n"," [0.0384748  0.0570396 ]\n"," [0.03874193 0.05797128]]\n","\n","Average MAE Loss:\n","[0.04840817 0.04869483 0.0477572  0.04835661]\n","\n","Epoch 00037: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03864758 0.05717006]\n"," [0.03916385 0.05822581]\n"," [0.03828191 0.05714978]\n"," [0.03856027 0.05804771]]\n","\n","Average MAE Loss:\n","[0.04790882 0.04869483 0.04771584 0.04830399]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03840611 0.05707468]\n"," [0.03916385 0.05822581]\n"," [0.03826667 0.05731796]\n"," [0.0387758  0.05808623]]\n","\n","Average MAE Loss:\n","[0.0477404  0.04869483 0.04779231 0.04843101]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03839354 0.05699412]\n"," [0.03916385 0.05822581]\n"," [0.0382521  0.05749653]\n"," [0.03888964 0.0579769 ]]\n","\n","Average MAE Loss:\n","[0.04769383 0.04869483 0.04787431 0.04843327]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03846327 0.05698431]\n"," [0.03916385 0.05822581]\n"," [0.03826472 0.05762099]\n"," [0.03899384 0.05802571]]\n","\n","Average MAE Loss:\n","[0.04772379 0.04869483 0.04794285 0.04850977]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03863429 0.05692053]\n"," [0.03916385 0.05822581]\n"," [0.0383626  0.05773605]\n"," [0.03894972 0.05808473]]\n","\n","Average MAE Loss:\n","[0.04777741 0.04869483 0.04804932 0.04851723]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03830531 0.05713281]\n"," [0.03830303 0.05739874]\n"," [0.03825989 0.05740337]\n"," [0.03841209 0.05743064]]\n","\n","Average MAE Loss:\n","[0.04771906 0.04785089 0.04783163 0.04792137]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03837635 0.05698577]\n"," [0.03830303 0.05739874]\n"," [0.0381895  0.05741987]\n"," [0.03867374 0.05762346]]\n","\n","Average MAE Loss:\n","[0.04768106 0.04785089 0.04780468 0.0481486 ]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03834687 0.05694661]\n"," [0.03830303 0.05739874]\n"," [0.03812567 0.05745104]\n"," [0.03865546 0.0577128 ]]\n","\n","Average MAE Loss:\n","[0.04764674 0.04785089 0.04778836 0.04818413]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03835678 0.05693335]\n"," [0.03830303 0.05739874]\n"," [0.03806254 0.0574885 ]\n"," [0.0385811  0.05779514]]\n","\n","Average MAE Loss:\n","[0.04764506 0.04785089 0.04777552 0.04818812]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03850557 0.05684871]\n"," [0.03830303 0.05739874]\n"," [0.0380328  0.05750507]\n"," [0.03860089 0.05786144]]\n","\n","Average MAE Loss:\n","[0.04767714 0.04785089 0.04776894 0.04823116]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03864587 0.0568545 ]\n"," [0.03830303 0.05739874]\n"," [0.03803026 0.05753143]\n"," [0.03864543 0.05786894]]\n","\n","Average MAE Loss:\n","[0.04775019 0.04785089 0.04778085 0.04825718]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03869119 0.05688821]\n"," [0.03830303 0.05739874]\n"," [0.03803218 0.0575469 ]\n"," [0.03872929 0.05787874]]\n","\n","Average MAE Loss:\n","[0.0477897  0.04785089 0.04778954 0.04830402]\n","\n","Epoch 00049: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03829934 0.05725913]\n"," [0.03872929 0.05787874]\n"," [0.03861335 0.05687396]\n"," [0.03807692 0.05752632]]\n","\n","Average MAE Loss:\n","[0.04777923 0.04830402 0.04774365 0.04780162]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03834177 0.05717513]\n"," [0.03872929 0.05787874]\n"," [0.03856492 0.05687151]\n"," [0.03816094 0.05753262]]\n","\n","Average MAE Loss:\n","[0.04775845 0.04830402 0.04771821 0.04784678]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03845526 0.05714241]\n"," [0.03872929 0.05787874]\n"," [0.03852952 0.05687319]\n"," [0.03824819 0.05754754]]\n","\n","Average MAE Loss:\n","[0.04779883 0.04830402 0.04770136 0.04789786]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03854147 0.05709367]\n"," [0.03872929 0.05787874]\n"," [0.0384974  0.05687413]\n"," [0.0383242  0.05757388]]\n","\n","Average MAE Loss:\n","[0.04781757 0.04830402 0.04768576 0.04794904]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03852183 0.05705907]\n"," [0.03872929 0.05787874]\n"," [0.03846135 0.056878  ]\n"," [0.03836021 0.05760361]]\n","\n","Average MAE Loss:\n","[0.04779045 0.04830402 0.04766968 0.04798191]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0384803  0.05703468]\n"," [0.03872929 0.05787874]\n"," [0.03843938 0.05688613]\n"," [0.03837758 0.05762269]]\n","\n","Average MAE Loss:\n","[0.04775749 0.04830402 0.04766276 0.04800014]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03848467 0.05702169]\n"," [0.03872929 0.05787874]\n"," [0.03842055 0.05689519]\n"," [0.03837615 0.05763996]]\n","\n","Average MAE Loss:\n","[0.04775318 0.04830402 0.04765787 0.04800805]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03829244 0.05712327]\n"," [0.03828363 0.0571769 ]\n"," [0.03825284 0.05718312]\n"," [0.03828765 0.05719336]]\n","\n","Average MAE Loss:\n","[0.04770785 0.04773027 0.04771798 0.04774051]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03827956 0.05708772]\n"," [0.03828363 0.0571769 ]\n"," [0.03823457 0.05718968]\n"," [0.03829377 0.05722061]]\n","\n","Average MAE Loss:\n","[0.04768364 0.04773027 0.04771212 0.04775719]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03826922 0.05705445]\n"," [0.03828363 0.0571769 ]\n"," [0.03821825 0.05719637]\n"," [0.03833973 0.05725474]]\n","\n","Average MAE Loss:\n","[0.04766184 0.04773027 0.04770731 0.04779723]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03827422 0.05702572]\n"," [0.03828363 0.0571769 ]\n"," [0.03819948 0.05720406]\n"," [0.03839714 0.05730206]]\n","\n","Average MAE Loss:\n","[0.04764997 0.04773027 0.04770177 0.0478496 ]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:51.687361001968384\n","\n","[[0.03827422 0.05702572]\n"," [0.03828363 0.0571769 ]\n"," [0.03819948 0.05720406]\n"," [0.03839714 0.05730206]]\n","00:01:10.443388976000278\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08888319 0.09293   ]\n"," [0.10227573 0.10665325]\n"," [0.08501425 0.08736908]\n"," [0.09345094 0.09651124]]\n","\n","Average MAE Loss:\n","[0.09090659 0.10446449 0.08619167 0.09498109]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08699241 0.09100685]\n"," [0.10227573 0.10665325]\n"," [0.08957329 0.09133884]\n"," [0.08904549 0.09200577]]\n","\n","Average MAE Loss:\n","[0.08899963 0.10446449 0.09045606 0.09052563]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0877803  0.09184983]\n"," [0.10227573 0.10665325]\n"," [0.08956696 0.09332559]\n"," [0.06509369 0.06767572]]\n","\n","Average MAE Loss:\n","[0.08981507 0.10446449 0.09144627 0.06638471]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0865787  0.0900552 ]\n"," [0.10227573 0.10665325]\n"," [0.07542891 0.07947798]\n"," [0.06609046 0.06804416]]\n","\n","Average MAE Loss:\n","[0.08831695 0.10446449 0.07745345 0.06706731]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08653335 0.08967769]\n"," [0.10227573 0.10665325]\n"," [0.06296941 0.06653083]\n"," [0.05952063 0.05896779]]\n","\n","Average MAE Loss:\n","[0.08810552 0.10446449 0.06475012 0.05924421]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08542877 0.08872359]\n"," [0.10227573 0.10665325]\n"," [0.06134892 0.06491733]\n"," [0.07504208 0.08021937]]\n","\n","Average MAE Loss:\n","[0.08707618 0.10446449 0.06313312 0.07763073]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08612695 0.0899872 ]\n"," [0.10227573 0.10665325]\n"," [0.05996934 0.06238931]\n"," [0.05862805 0.05935526]]\n","\n","Average MAE Loss:\n","[0.08805708 0.10446449 0.06117933 0.05899166]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05741184 0.06145129]\n"," [0.05862805 0.05935526]\n"," [0.08684732 0.09017387]\n"," [0.05886124 0.05924587]]\n","\n","Average MAE Loss:\n","[0.05943156 0.05899166 0.0885106  0.05905355]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05163787 0.05475722]\n"," [0.05862805 0.05935526]\n"," [0.09164692 0.09567702]\n"," [0.0577818  0.05715827]]\n","\n","Average MAE Loss:\n","[0.05319754 0.05899166 0.09366197 0.05747003]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0522167  0.05670734]\n"," [0.05862805 0.05935526]\n"," [0.0845352  0.0873228 ]\n"," [0.07202324 0.07385682]]\n","\n","Average MAE Loss:\n","[0.05446202 0.05899166 0.085929   0.07294003]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04769569 0.04920317]\n"," [0.05862805 0.05935526]\n"," [0.08482019 0.08839101]\n"," [0.05914311 0.06102017]]\n","\n","Average MAE Loss:\n","[0.04844943 0.05899166 0.0866056  0.06008164]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04780515 0.05089468]\n"," [0.05862805 0.05935526]\n"," [0.08463465 0.08836387]\n"," [0.06966811 0.07414109]]\n","\n","Average MAE Loss:\n","[0.04934991 0.05899166 0.08649926 0.0719046 ]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0494834  0.05206425]\n"," [0.05862805 0.05935526]\n"," [0.08483338 0.08901699]\n"," [0.0637477  0.06692044]]\n","\n","Average MAE Loss:\n","[0.05077383 0.05899166 0.08692518 0.06533407]\n","\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04900797 0.05121386]\n"," [0.05862805 0.05935526]\n"," [0.08282682 0.08607589]\n"," [0.05874147 0.06076559]]\n","\n","Average MAE Loss:\n","[0.05011092 0.05899166 0.08445136 0.05975353]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04990343 0.05343521]\n"," [0.09508442 0.09945839]\n"," [0.05324511 0.05454786]\n"," [0.06173017 0.06506536]]\n","\n","Average MAE Loss:\n","[0.05166932 0.09727141 0.05389649 0.06339777]\n","\n","Epoch 00015: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04966668 0.05136417]\n"," [0.09508442 0.09945839]\n"," [0.05341966 0.05537259]\n"," [0.05176068 0.05352322]]\n","\n","Average MAE Loss:\n","[0.05051542 0.09727141 0.05439613 0.05264195]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05085132 0.05317966]\n"," [0.09508442 0.09945839]\n"," [0.04901437 0.05075438]\n"," [0.05096468 0.05223473]]\n","\n","Average MAE Loss:\n","[0.05201549 0.09727141 0.04988437 0.0515997 ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04837245 0.04980937]\n"," [0.09508442 0.09945839]\n"," [0.04938448 0.05094202]\n"," [0.05135207 0.05269502]]\n","\n","Average MAE Loss:\n","[0.04909091 0.09727141 0.05016325 0.05202355]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04882492 0.05073656]\n"," [0.09508442 0.09945839]\n"," [0.04948535 0.0503566 ]\n"," [0.04972489 0.05060817]]\n","\n","Average MAE Loss:\n","[0.04978074 0.09727141 0.04992098 0.05016653]\n","\n","Epoch 00019: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04751234 0.04896985]\n"," [0.09508442 0.09945839]\n"," [0.0488165  0.04955419]\n"," [0.05110932 0.05290816]]\n","\n","Average MAE Loss:\n","[0.04824109 0.09727141 0.04918534 0.05200874]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04733745 0.04855586]\n"," [0.09508442 0.09945839]\n"," [0.04822961 0.04901982]\n"," [0.04880169 0.04973138]]\n","\n","Average MAE Loss:\n","[0.04794666 0.09727141 0.04862472 0.04926653]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09033878 0.09473218]\n"," [0.04880169 0.04973138]\n"," [0.04942372 0.05120693]\n"," [0.04975154 0.05121489]]\n","\n","Average MAE Loss:\n","[0.09253548 0.04926653 0.05031532 0.05048322]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06940025 0.07356236]\n"," [0.04880169 0.04973138]\n"," [0.04915734 0.05109581]\n"," [0.05028433 0.0514291 ]]\n","\n","Average MAE Loss:\n","[0.07148131 0.04926653 0.05012657 0.05085671]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04796903 0.04924208]\n"," [0.04880169 0.04973138]\n"," [0.04858521 0.05000008]\n"," [0.04998191 0.05097665]]\n","\n","Average MAE Loss:\n","[0.04860555 0.04926653 0.04929265 0.05047928]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05113591 0.05380936]\n"," [0.04880169 0.04973138]\n"," [0.04800093 0.04971065]\n"," [0.04889181 0.049353  ]]\n","\n","Average MAE Loss:\n","[0.05247264 0.04926653 0.04885579 0.0491224 ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04927084 0.05120333]\n"," [0.04880169 0.04973138]\n"," [0.04756229 0.04905804]\n"," [0.04869314 0.04925046]]\n","\n","Average MAE Loss:\n","[0.05023708 0.04926653 0.04831016 0.0489718 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0486355  0.05011633]\n"," [0.04880169 0.04973138]\n"," [0.04841948 0.05040255]\n"," [0.05111004 0.0528479 ]]\n","\n","Average MAE Loss:\n","[0.04937591 0.04926653 0.04941101 0.05197897]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04867668 0.05006669]\n"," [0.04880169 0.04973138]\n"," [0.04700518 0.04818293]\n"," [0.04860009 0.0489732 ]]\n","\n","Average MAE Loss:\n","[0.04937169 0.04926653 0.04759405 0.04878665]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04955667 0.05142495]\n"," [0.05253371 0.05507156]\n"," [0.04758959 0.0489135 ]\n"," [0.04743553 0.04749924]]\n","\n","Average MAE Loss:\n","[0.05049081 0.05380264 0.04825155 0.04746739]\n","\n","Epoch 00029: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04830724 0.04985403]\n"," [0.05253371 0.05507156]\n"," [0.04732363 0.04889116]\n"," [0.05061251 0.05255139]]\n","\n","Average MAE Loss:\n","[0.04908064 0.05380264 0.04810739 0.05158195]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04775602 0.049141  ]\n"," [0.05253371 0.05507156]\n"," [0.04662907 0.04778163]\n"," [0.04869752 0.04986651]]\n","\n","Average MAE Loss:\n","[0.04844851 0.05380264 0.04720535 0.04928202]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04718327 0.04836308]\n"," [0.05253371 0.05507156]\n"," [0.04713873 0.0485076 ]\n"," [0.04998837 0.05187448]]\n","\n","Average MAE Loss:\n","[0.04777318 0.05380264 0.04782317 0.05093143]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04677912 0.04779019]\n"," [0.05253371 0.05507156]\n"," [0.04634045 0.0473694 ]\n"," [0.04832117 0.04911804]]\n","\n","Average MAE Loss:\n","[0.04728466 0.05380264 0.04685492 0.0487196 ]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04660633 0.04755573]\n"," [0.05253371 0.05507156]\n"," [0.04683347 0.0482548 ]\n"," [0.04842635 0.04917298]]\n","\n","Average MAE Loss:\n","[0.04708103 0.05380264 0.04754413 0.04879966]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04672541 0.04774429]\n"," [0.05253371 0.05507156]\n"," [0.04647721 0.04740103]\n"," [0.04861005 0.0495302 ]]\n","\n","Average MAE Loss:\n","[0.04723485 0.05380264 0.04693912 0.04907013]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05109344 0.053386  ]\n"," [0.04861005 0.0495302 ]\n"," [0.04585552 0.04650041]\n"," [0.04679719 0.04772402]]\n","\n","Average MAE Loss:\n","[0.05223972 0.04907013 0.04617797 0.04726061]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.0488195  0.05054396]\n"," [0.04861005 0.0495302 ]\n"," [0.04633692 0.04671269]\n"," [0.04794668 0.04905064]]\n","\n","Average MAE Loss:\n","[0.04968173 0.04907013 0.0465248  0.04849866]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04772211 0.04912501]\n"," [0.04861005 0.0495302 ]\n"," [0.04685228 0.04771855]\n"," [0.04818103 0.04910048]]\n","\n","Average MAE Loss:\n","[0.04842356 0.04907013 0.04728542 0.04864075]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.0474053  0.04872092]\n"," [0.04861005 0.0495302 ]\n"," [0.04803271 0.0494818 ]\n"," [0.04857199 0.04978549]]\n","\n","Average MAE Loss:\n","[0.04806311 0.04907013 0.04875726 0.04917874]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04717654 0.04844262]\n"," [0.04861005 0.0495302 ]\n"," [0.04776779 0.04933432]\n"," [0.04791171 0.04871441]]\n","\n","Average MAE Loss:\n","[0.04780958 0.04907013 0.04855105 0.04831306]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04712582 0.04838761]\n"," [0.04861005 0.0495302 ]\n"," [0.04742728 0.0486349 ]\n"," [0.04811902 0.04895647]]\n","\n","Average MAE Loss:\n","[0.04775672 0.04907013 0.04803109 0.04853774]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04709866 0.04838193]\n"," [0.04861005 0.0495302 ]\n"," [0.04699253 0.04804886]\n"," [0.04869484 0.04984302]]\n","\n","Average MAE Loss:\n","[0.0477403  0.04907013 0.04752069 0.04926893]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04763649 0.04886314]\n"," [0.04764564 0.0488377 ]\n"," [0.0469992  0.04804909]\n"," [0.04770172 0.0488623 ]]\n","\n","Average MAE Loss:\n","[0.04824981 0.04824167 0.04752415 0.04828201]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04762411 0.04888683]\n"," [0.04764564 0.0488377 ]\n"," [0.04653981 0.04750588]\n"," [0.04789429 0.04906325]]\n","\n","Average MAE Loss:\n","[0.04825547 0.04824167 0.04702284 0.04847877]\n","\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04756886 0.04883769]\n"," [0.04764564 0.0488377 ]\n"," [0.04664026 0.04769705]\n"," [0.04823188 0.04953003]]\n","\n","Average MAE Loss:\n","[0.04820327 0.04824167 0.04716866 0.04888095]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04752517 0.04878376]\n"," [0.04764564 0.0488377 ]\n"," [0.04682222 0.04802992]\n"," [0.04809323 0.04933245]]\n","\n","Average MAE Loss:\n","[0.04815446 0.04824167 0.04742607 0.04871284]\n","\n","Epoch 00046: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04747539 0.04871767]\n"," [0.04764564 0.0488377 ]\n"," [0.04700229 0.04832467]\n"," [0.04775001 0.04877258]]\n","\n","Average MAE Loss:\n","[0.04809653 0.04824167 0.04766348 0.0482613 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04746521 0.04870744]\n"," [0.04764564 0.0488377 ]\n"," [0.04708625 0.04844569]\n"," [0.04770437 0.04866299]]\n","\n","Average MAE Loss:\n","[0.04808632 0.04824167 0.04776597 0.04818368]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04742778 0.04865877]\n"," [0.04764564 0.0488377 ]\n"," [0.04712471 0.0485148 ]\n"," [0.04772027 0.04865512]]\n","\n","Average MAE Loss:\n","[0.04804328 0.04824167 0.04781976 0.0481877 ]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04762333 0.04882143]\n"," [0.04772027 0.04865512]\n"," [0.04732442 0.04853177]\n"," [0.04704263 0.04840622]]\n","\n","Average MAE Loss:\n","[0.04822238 0.0481877  0.04792809 0.04772443]\n","\n","Epoch 00050: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04760906 0.04880831]\n"," [0.04772027 0.04865512]\n"," [0.04711998 0.04826592]\n"," [0.04703564 0.04838515]]\n","\n","Average MAE Loss:\n","[0.04820869 0.0481877  0.04769295 0.04771039]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04758377 0.04877802]\n"," [0.04772027 0.04865512]\n"," [0.04691575 0.0480223 ]\n"," [0.04698404 0.04826996]]\n","\n","Average MAE Loss:\n","[0.04818089 0.0481877  0.04746903 0.047627  ]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00052: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04755648 0.04874384]\n"," [0.04772027 0.04865512]\n"," [0.04687126 0.04795778]\n"," [0.04703419 0.04833258]]\n","\n","Average MAE Loss:\n","[0.04815016 0.0481877  0.04741452 0.04768338]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04753411 0.04871777]\n"," [0.04772027 0.04865512]\n"," [0.04689931 0.04801843]\n"," [0.04713877 0.04848322]]\n","\n","Average MAE Loss:\n","[0.04812594 0.0481877  0.04745887 0.047811  ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04752399 0.04870597]\n"," [0.04772027 0.04865512]\n"," [0.04688262 0.04802241]\n"," [0.04721747 0.0485916 ]]\n","\n","Average MAE Loss:\n","[0.04811498 0.0481877  0.04745251 0.04790453]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04751632 0.04869885]\n"," [0.04772027 0.04865512]\n"," [0.04683174 0.04797876]\n"," [0.0472047  0.04855102]]\n","\n","Average MAE Loss:\n","[0.04810758 0.0481877  0.04740525 0.04787786]\n","\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00056: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04723233 0.04837128]\n"," [0.04724234 0.04838331]\n"," [0.04725298 0.04841614]\n"," [0.04721835 0.04832898]]\n","\n","Average MAE Loss:\n","[0.04780181 0.04781283 0.04783456 0.04777367]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04722814 0.04836784]\n"," [0.04724234 0.04838331]\n"," [0.04721979 0.04837603]\n"," [0.04725105 0.04836472]]\n","\n","Average MAE Loss:\n","[0.04779799 0.04781283 0.04779791 0.04780789]\n","\n","Epoch 00058: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04722852 0.04837153]\n"," [0.04724234 0.04838331]\n"," [0.04719007 0.04834147]\n"," [0.04726733 0.04836989]]\n","\n","Average MAE Loss:\n","[0.04780002 0.04781283 0.04776577 0.04781861]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04722804 0.04837262]\n"," [0.04724234 0.04838331]\n"," [0.04713559 0.04827474]\n"," [0.04731172 0.04843235]]\n","\n","Average MAE Loss:\n","[0.04780033 0.04781283 0.04770517 0.04787204]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00060: reducing learning rate of group 0 to 3.9063e-06.\n","\n","epochs finished with time:50.63228440284729\n","\n","[[0.04722804 0.04837262]\n"," [0.04724234 0.04838331]\n"," [0.04713559 0.04827474]\n"," [0.04731172 0.04843235]]\n","00:01:9.357153582999672\n"]}],"source":["# seed = 10 table = 4/8 fixed folds CNN_2 dropout\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Try_Simple2_CNN_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":40810,"status":"error","timestamp":1690054182997,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"5nrg9ycdEd1k","outputId":"87afe309-03cd-43c1-de3b-c677c17a09bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.25\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.06059251 0.05770964]\n"," [0.05660847 0.05246856]]\n","\n","Average MAE Loss:\n","[0.08429034 0.08433132 0.05915108 0.05453852]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.06812717 0.06469029]\n"," [0.06685961 0.06678156]]\n","\n","Average MAE Loss:\n","[0.08429034 0.08433132 0.06640873 0.06682059]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.06874192 0.06777871]\n"," [0.06031567 0.05880165]]\n","\n","Average MAE Loss:\n","[0.08429034 0.08433132 0.06826032 0.05955866]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05386474 0.05147865]\n"," [0.05813973 0.05580227]]\n","\n","Average MAE Loss:\n","[0.08429034 0.08433132 0.05267169 0.056971  ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05339262 0.05086935]\n"," [0.05068143 0.04814859]]\n","\n","Average MAE Loss:\n","[0.08429034 0.08433132 0.05213098 0.04941501]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05196031 0.0498676 ]\n"," [0.05725744 0.05458816]]\n","\n","Average MAE Loss:\n","[0.08429034 0.08433132 0.05091396 0.0559228 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08581297 0.0827677 ]\n"," [0.0858584  0.08280423]\n"," [0.05126821 0.04901228]\n"," [0.0554676  0.05344784]]\n","\n","Average MAE Loss:\n","[0.08429034 0.08433132 0.05014024 0.05445772]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.0554676  0.05344784]\n"," [0.0647701  0.06162503]\n"," [0.05231979 0.05039274]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05445772 0.06319757 0.05135626]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.0554676  0.05344784]\n"," [0.06504308 0.06218221]\n"," [0.05031599 0.04845012]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05445772 0.06361264 0.04938306]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.0554676  0.05344784]\n"," [0.06608958 0.0641251 ]\n"," [0.04990632 0.04813762]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05445772 0.06510734 0.04902197]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.0554676  0.05344784]\n"," [0.06463163 0.06208923]\n"," [0.05251589 0.05011873]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05445772 0.06336043 0.05131731]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.0554676  0.05344784]\n"," [0.06275543 0.0602154 ]\n"," [0.05349337 0.05133369]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05445772 0.06148542 0.05241353]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.0554676  0.05344784]\n"," [0.06504086 0.0622841 ]\n"," [0.0470114  0.04486229]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05445772 0.06366248 0.04593685]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0858584  0.08280423]\n"," [0.0554676  0.05344784]\n"," [0.06360861 0.06087573]\n"," [0.04714897 0.04545235]]\n","\n","Average MAE Loss:\n","[0.08433132 0.05445772 0.06224217 0.04630066]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.07941195 0.07623343]\n"," [0.05197476 0.05000376]\n"," [0.05155264 0.04924202]]\n","\n","Average MAE Loss:\n","[0.07782269 0.07782269 0.05098926 0.05039733]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.07941195 0.07623343]\n"," [0.04493816 0.04292813]\n"," [0.04853723 0.04646038]]\n","\n","Average MAE Loss:\n","[0.07782269 0.07782269 0.04393315 0.0474988 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.07941195 0.07623343]\n"," [0.04459563 0.0424949 ]\n"," [0.04557802 0.04386473]]\n","\n","Average MAE Loss:\n","[0.07782269 0.07782269 0.04354527 0.04472138]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.07941195 0.07623343]\n"," [0.04465002 0.04239986]\n"," [0.04430881 0.0423075 ]]\n","\n","Average MAE Loss:\n","[0.07782269 0.07782269 0.04352494 0.04330815]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.07941195 0.07623343]\n"," [0.0442013  0.0423887 ]\n"," [0.04510819 0.04372849]]\n","\n","Average MAE Loss:\n","[0.07782269 0.07782269 0.043295   0.04441834]\n","\n","Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.07941195 0.07623343]\n"," [0.04411072 0.04225993]\n"," [0.04362119 0.04170687]]\n","\n","Average MAE Loss:\n","[0.07782269 0.07782269 0.04318532 0.04266403]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.07941195 0.07623343]\n"," [0.04446834 0.04234119]\n"," [0.04491804 0.04292374]]\n","\n","Average MAE Loss:\n","[0.07782269 0.07782269 0.04340477 0.04392089]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.04491804 0.04292374]\n"," [0.07252396 0.06936954]\n"," [0.04467083 0.04307933]]\n","\n","Average MAE Loss:\n","[0.07782269 0.04392089 0.07094675 0.04387508]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.04491804 0.04292374]\n"," [0.04591022 0.04362035]\n"," [0.04496005 0.04283042]]\n","\n","Average MAE Loss:\n","[0.07782269 0.04392089 0.04476529 0.04389524]\n","\n","Epoch 00023: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.04491804 0.04292374]\n"," [0.04457434 0.0426277 ]\n"," [0.04400608 0.0425094 ]]\n","\n","Average MAE Loss:\n","[0.07782269 0.04392089 0.04360102 0.04325774]\n","\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.04491804 0.04292374]\n"," [0.04423594 0.04234194]\n"," [0.04379935 0.04199045]]\n","\n","Average MAE Loss:\n","[0.07782269 0.04392089 0.04328894 0.0428949 ]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.04491804 0.04292374]\n"," [0.04427179 0.04214135]\n"," [0.04320943 0.04132239]]\n","\n","Average MAE Loss:\n","[0.07782269 0.04392089 0.04320657 0.04226591]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.04491804 0.04292374]\n"," [0.04403512 0.04189974]\n"," [0.04297444 0.04121937]]\n","\n","Average MAE Loss:\n","[0.07782269 0.04392089 0.04296743 0.04209691]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07941195 0.07623343]\n"," [0.04491804 0.04292374]\n"," [0.04395007 0.0420014 ]\n"," [0.04284706 0.0409976 ]]\n","\n","Average MAE Loss:\n","[0.07782269 0.04392089 0.04297573 0.04192233]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.05694516 0.05436633]\n"," [0.04966737 0.04734567]\n"," [0.04440911 0.04255731]]\n","\n","Average MAE Loss:\n","[0.05565574 0.05565574 0.04850652 0.04348321]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.05694516 0.05436633]\n"," [0.04242049 0.0405008 ]\n"," [0.04283524 0.04094113]]\n","\n","Average MAE Loss:\n","[0.05565574 0.05565574 0.04146065 0.04188818]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.05694516 0.05436633]\n"," [0.04403965 0.04218074]\n"," [0.04271678 0.04104785]]\n","\n","Average MAE Loss:\n","[0.05565574 0.05565574 0.04311019 0.04188232]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.05694516 0.05436633]\n"," [0.04280943 0.04091081]\n"," [0.04240942 0.04073858]]\n","\n","Average MAE Loss:\n","[0.05565574 0.05565574 0.04186012 0.041574  ]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.05694516 0.05436633]\n"," [0.04315081 0.04139567]\n"," [0.04270753 0.04078747]]\n","\n","Average MAE Loss:\n","[0.05565574 0.05565574 0.04227324 0.0417475 ]\n","\n","Epoch 00033: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.05694516 0.05436633]\n"," [0.04284484 0.04112212]\n"," [0.042619   0.04086946]]\n","\n","Average MAE Loss:\n","[0.05565574 0.05565574 0.04198348 0.04174423]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.05694516 0.05436633]\n"," [0.04288478 0.04111297]\n"," [0.04233631 0.04084503]]\n","\n","Average MAE Loss:\n","[0.05565574 0.05565574 0.04199887 0.04159067]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.04233631 0.04084503]\n"," [0.05376505 0.0512609 ]\n"," [0.04264122 0.04097537]]\n","\n","Average MAE Loss:\n","[0.05565574 0.04159067 0.05251297 0.04180829]\n","\n","Epoch 00036: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.04233631 0.04084503]\n"," [0.048885   0.04651151]\n"," [0.04250126 0.0410135 ]]\n","\n","Average MAE Loss:\n","[0.05565574 0.04159067 0.04769825 0.04175738]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.04233631 0.04084503]\n"," [0.04476236 0.04245219]\n"," [0.04222707 0.04062778]]\n","\n","Average MAE Loss:\n","[0.05565574 0.04159067 0.04360727 0.04142742]\n","\n","Epoch 00038: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.04233631 0.04084503]\n"," [0.04341878 0.04121197]\n"," [0.0423519  0.04057203]]\n","\n","Average MAE Loss:\n","[0.05565574 0.04159067 0.04231538 0.04146197]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.04233631 0.04084503]\n"," [0.04261172 0.04059624]\n"," [0.04244291 0.04062313]]\n","\n","Average MAE Loss:\n","[0.05565574 0.04159067 0.04160398 0.04153302]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.04233631 0.04084503]\n"," [0.0423363  0.04039809]\n"," [0.04231508 0.04060351]]\n","\n","Average MAE Loss:\n","[0.05565574 0.04159067 0.0413672  0.0414593 ]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05694516 0.05436633]\n"," [0.04233631 0.04084503]\n"," [0.04226228 0.04038438]\n"," [0.04231883 0.04065235]]\n","\n","Average MAE Loss:\n","[0.05565574 0.04159067 0.04132333 0.04148559]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04386948 0.04169216]\n"," [0.04314023 0.04097157]\n"," [0.04244047 0.04042024]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04278082 0.0420559  0.04143036]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04386948 0.04169216]\n"," [0.04246247 0.04041719]\n"," [0.04207683 0.04034489]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04278082 0.04143983 0.04121086]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04386948 0.04169216]\n"," [0.04213227 0.04019559]\n"," [0.04216386 0.04056246]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04278082 0.04116393 0.04136316]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04386948 0.04169216]\n"," [0.0420173  0.04016944]\n"," [0.04212225 0.04056944]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04278082 0.04109337 0.04134585]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04386948 0.04169216]\n"," [0.04200826 0.04020819]\n"," [0.04221106 0.04064751]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04278082 0.04110823 0.04142928]\n","\n","Epoch 00047: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04386948 0.04169216]\n"," [0.04208342 0.04030132]\n"," [0.04208705 0.04050433]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04278082 0.04119237 0.04129569]\n","\n","Epoch 00048: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00048: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04386948 0.04169216]\n"," [0.04216095 0.04037732]\n"," [0.04208221 0.04046083]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04278082 0.04126914 0.04127152]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04208221 0.04046083]\n"," [0.04324733 0.0410893 ]\n"," [0.04207725 0.04035329]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04127152 0.04216831 0.04121527]\n","\n","Epoch 00050: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04208221 0.04046083]\n"," [0.0428617  0.04073028]\n"," [0.04200012 0.04027937]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04127152 0.04179599 0.04113975]\n","\n","Epoch 00051: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04208221 0.04046083]\n"," [0.04257083 0.04049045]\n"," [0.04198729 0.04023643]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04127152 0.04153064 0.04111186]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04208221 0.04046083]\n"," [0.0423748  0.04033503]\n"," [0.04196718 0.0402292 ]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04127152 0.04135491 0.04109819]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04208221 0.04046083]\n"," [0.04224904 0.04023299]\n"," [0.04204686 0.04030184]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04127152 0.04124101 0.04117435]\n","\n","Epoch 00054: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00054: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04208221 0.04046083]\n"," [0.04218825 0.04019106]\n"," [0.04217564 0.04047845]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04127152 0.04118966 0.04132704]\n","\n","Epoch 00055: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04386948 0.04169216]\n"," [0.04208221 0.04046083]\n"," [0.04214751 0.0401711 ]\n"," [0.0422846  0.04059715]]\n","\n","Average MAE Loss:\n","[0.04278082 0.04127152 0.0411593  0.04144087]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04208983 0.04023897]\n"," [0.04208983 0.04023897]\n"," [0.04205187 0.04021193]\n"," [0.04199567 0.0402357 ]]\n","\n","Average MAE Loss:\n","[0.0411644  0.0411644  0.0411319  0.04111568]\n","\n","Epoch 00057: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04208983 0.04023897]\n"," [0.04208983 0.04023897]\n"," [0.0420176  0.0401895 ]\n"," [0.04197484 0.04024031]]\n","\n","Average MAE Loss:\n","[0.0411644  0.0411644  0.04110355 0.04110757]\n","\n","Epoch 00058: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04208983 0.04023897]\n"," [0.04208983 0.04023897]\n"," [0.04199872 0.04017953]\n"," [0.04197525 0.04024996]]\n","\n","Average MAE Loss:\n","[0.0411644  0.0411644  0.04108912 0.0411126 ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04208983 0.04023897]\n"," [0.04208983 0.04023897]\n"," [0.04198069 0.04016985]\n"," [0.04198442 0.04028156]]\n","\n","Average MAE Loss:\n","[0.0411644  0.0411644  0.04107527 0.04113299]\n","\n","\n","epochs finished with time:36.388179063797\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mreset_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: majorTicks","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-f7e404460604>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Simple2_CNN_Dropout_4D-FED-GNN++'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_cnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbosity_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-e40c19b352cf>\u001b[0m in \u001b[0;36mtrain_cnns\u001b[0;34m(args, dataset, seed, ratio, verbose, train_validate_verbose, train_validate_verbosity_epochs)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epochs finished with time:{epochs_end}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mvalidate_gnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhospitals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mtic1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtic0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtic1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-3c7c584f0ee8>\u001b[0m in \u001b[0;36mvalidate_gnns\u001b[0;34m(args, hospitals, test_data, f, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m                         plot_matrix(data[t+1].cpu().detach().numpy(),f'Real Graph, Hospital:{h_i}, Subject:{subject_index}, Timepoint:{t+1}',\n\u001b[1;32m     27\u001b[0m                                   args.save_path+'real_and_predicted_graphs/'+f'hospital_{h_i}_subject_{subject_index}_timepoint_{t+1}_fold_{f}_real_graph',verbose)\n\u001b[0;32m---> 28\u001b[0;31m                         plot_matrix(pred.cpu().detach().numpy(),f'Predicted Graph, Hospital:{h_i}, Subject:{subject_index}, Timepoint:{t+1}',\n\u001b[0m\u001b[1;32m     29\u001b[0m                                   args.save_path+'real_and_predicted_graphs/'+f'hospital_{h_i}_subject_{subject_index}_timepoint_{t+1}_fold_{f}_predicted_graph',verbose)\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/plot.py\u001b[0m in \u001b[0;36mplot_matrix\u001b[0;34m(out, strategy, save_path, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpcolor\u001b[0;34m(shading, alpha, norm, cmap, vmin, vmax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2757\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m         vmin=None, vmax=None, data=None, **kwargs):\n\u001b[0;32m-> 2759\u001b[0;31m     __ret = gca().pcolor(\n\u001b[0m\u001b[1;32m   2760\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2308\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \"\"\"\n\u001b[1;32m   1627\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m             projection_class, pkw = self._process_projection_requirements(\n\u001b[1;32m    756\u001b[0m                 *args, **kwargs)\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterization_zorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;31m# funcs used to format x and y - fall back on major formatters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axis_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Also resets the scale to linear.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mspine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mspine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.grid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 mpl.rcParams['axes.grid.which'] in ('both', 'minor'))\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mreset_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# Restore the lazy tick lists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# seed = 10 table = 2/8 fixed folds CNN_2\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='Simple2_CNN_Dropout_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476461,"status":"ok","timestamp":1690024825704,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"WUDUNge4TyWX","outputId":"ea1161ab-a117-45d1-8469-05ba9ca2dd29"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07423845 0.07099184]\n"," [0.0858584  0.08280423]\n"," [0.05980123 0.05644355]\n"," [0.05581843 0.05272923]]\n","\n","Average MAE Loss:\n","[0.07261514 0.08433132 0.05812239 0.05427383]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.05363391 0.05099773]\n"," [0.0858584  0.08280423]\n"," [0.05174453 0.05002043]\n"," [0.05459361 0.0530942 ]]\n","\n","Average MAE Loss:\n","[0.05231582 0.08433132 0.05088248 0.05384391]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.05071445 0.04856771]\n"," [0.0858584  0.08280423]\n"," [0.05152505 0.04936152]\n"," [0.05067437 0.04823685]]\n","\n","Average MAE Loss:\n","[0.04964108 0.08433132 0.05044329 0.04945561]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04997948 0.04759381]\n"," [0.0858584  0.08280423]\n"," [0.05093668 0.0487306 ]\n"," [0.05141417 0.04806925]]\n","\n","Average MAE Loss:\n","[0.04878664 0.08433132 0.04983364 0.04974171]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0500147  0.04775137]\n"," [0.0858584  0.08280423]\n"," [0.05142777 0.04908976]\n"," [0.05089709 0.04926352]]\n","\n","Average MAE Loss:\n","[0.04888303 0.08433132 0.05025876 0.05008031]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04984426 0.04756029]\n"," [0.0858584  0.08280423]\n"," [0.05195149 0.04954054]\n"," [0.05021278 0.04813805]]\n","\n","Average MAE Loss:\n","[0.04870227 0.08433132 0.05074602 0.04917542]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04978387 0.0475375 ]\n"," [0.0858584  0.08280423]\n"," [0.05213331 0.04966946]\n"," [0.04994978 0.04761404]]\n","\n","Average MAE Loss:\n","[0.04866068 0.08433132 0.05090138 0.04878191]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05132031 0.04862428]\n"," [0.04994978 0.04761404]\n"," [0.05368216 0.05190161]\n"," [0.0532057  0.05083628]]\n","\n","Average MAE Loss:\n","[0.0499723  0.04878191 0.05279189 0.05202099]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04965237 0.04658288]\n"," [0.04994978 0.04761404]\n"," [0.04902516 0.04679853]\n"," [0.05176257 0.05016699]]\n","\n","Average MAE Loss:\n","[0.04811763 0.04878191 0.04791185 0.05096478]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04930237 0.04651421]\n"," [0.04994978 0.04761404]\n"," [0.0492055  0.04692865]\n"," [0.05031494 0.04823303]]\n","\n","Average MAE Loss:\n","[0.04790829 0.04878191 0.04806708 0.04927399]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04815833 0.04520739]\n"," [0.04994978 0.04761404]\n"," [0.04836914 0.04632328]\n"," [0.05030263 0.0483486 ]]\n","\n","Average MAE Loss:\n","[0.04668286 0.04878191 0.04734621 0.04932561]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04831246 0.04579744]\n"," [0.04994978 0.04761404]\n"," [0.04826978 0.04640598]\n"," [0.04987131 0.04770904]]\n","\n","Average MAE Loss:\n","[0.04705495 0.04878191 0.04733788 0.04879018]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04747864 0.04447937]\n"," [0.04994978 0.04761404]\n"," [0.04812617 0.04606829]\n"," [0.04991871 0.04777769]]\n","\n","Average MAE Loss:\n","[0.04597901 0.04878191 0.04709723 0.0488482 ]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04685891 0.04410433]\n"," [0.04994978 0.04761404]\n"," [0.04810708 0.04584313]\n"," [0.0495932  0.04762554]]\n","\n","Average MAE Loss:\n","[0.04548162 0.04878191 0.04697511 0.04860937]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04524255 0.04283716]\n"," [0.08067861 0.07761942]\n"," [0.04750715 0.04525454]\n"," [0.04790218 0.04574448]]\n","\n","Average MAE Loss:\n","[0.04403985 0.07914901 0.04638085 0.04682333]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04288803 0.04021031]\n"," [0.08067861 0.07761942]\n"," [0.04322493 0.04103319]\n"," [0.04295015 0.04120953]]\n","\n","Average MAE Loss:\n","[0.04154917 0.07914901 0.04212906 0.04207984]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0421276  0.03932959]\n"," [0.08067861 0.07761942]\n"," [0.04265082 0.04034508]\n"," [0.04148562 0.03962263]]\n","\n","Average MAE Loss:\n","[0.0407286  0.07914901 0.04149795 0.04055412]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04191348 0.03912925]\n"," [0.08067861 0.07761942]\n"," [0.04251345 0.04033809]\n"," [0.04124433 0.03952575]]\n","\n","Average MAE Loss:\n","[0.04052137 0.07914901 0.04142577 0.04038504]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04208272 0.03917422]\n"," [0.08067861 0.07761942]\n"," [0.04222594 0.03988415]\n"," [0.04116006 0.03948767]]\n","\n","Average MAE Loss:\n","[0.04062847 0.07914901 0.04105505 0.04032387]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04223823 0.03926163]\n"," [0.08067861 0.07761942]\n"," [0.04224848 0.03996144]\n"," [0.04114883 0.03950925]]\n","\n","Average MAE Loss:\n","[0.04074993 0.07914901 0.04110496 0.04032904]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04221784 0.03926952]\n"," [0.08067861 0.07761942]\n"," [0.04229404 0.04002659]\n"," [0.04111777 0.03943783]]\n","\n","Average MAE Loss:\n","[0.04074368 0.07914901 0.04116031 0.0402778 ]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05269892 0.0502803 ]\n"," [0.04111777 0.03943783]\n"," [0.04194393 0.03956791]\n"," [0.04111244 0.03919655]]\n","\n","Average MAE Loss:\n","[0.05148961 0.0402778  0.04075592 0.0401545 ]\n","\n","Epoch 00022: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04350215 0.0407428 ]\n"," [0.04111777 0.03943783]\n"," [0.04228659 0.03996357]\n"," [0.04115298 0.03935858]]\n","\n","Average MAE Loss:\n","[0.04212247 0.0402778  0.04112508 0.04025578]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04223789 0.03945739]\n"," [0.04111777 0.03943783]\n"," [0.04185323 0.03954335]\n"," [0.04105309 0.03933092]]\n","\n","Average MAE Loss:\n","[0.04084764 0.0402778  0.04069829 0.04019201]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04188648 0.03921099]\n"," [0.04111777 0.03943783]\n"," [0.04191006 0.0394926 ]\n"," [0.04102615 0.03928277]]\n","\n","Average MAE Loss:\n","[0.04054873 0.0402778  0.04070133 0.04015446]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04190423 0.03917226]\n"," [0.04111777 0.03943783]\n"," [0.0420356  0.03963426]\n"," [0.0410724  0.03927807]]\n","\n","Average MAE Loss:\n","[0.04053825 0.0402778  0.04083493 0.04017524]\n","\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04181876 0.03921174]\n"," [0.04111777 0.03943783]\n"," [0.04193359 0.03953875]\n"," [0.04106188 0.03932137]]\n","\n","Average MAE Loss:\n","[0.04051525 0.0402778  0.04073617 0.04019163]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04172277 0.03910022]\n"," [0.04111777 0.03943783]\n"," [0.04189196 0.03947502]\n"," [0.04103373 0.03927132]]\n","\n","Average MAE Loss:\n","[0.04041149 0.0402778  0.04068349 0.04015252]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04105171 0.03852148]\n"," [0.04183582 0.03946308]\n"," [0.04177547 0.0396939 ]\n"," [0.04092999 0.03887704]]\n","\n","Average MAE Loss:\n","[0.0397866  0.04064945 0.04073468 0.03990352]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04109998 0.03832801]\n"," [0.04183582 0.03946308]\n"," [0.04164818 0.0392924 ]\n"," [0.04065436 0.03875646]]\n","\n","Average MAE Loss:\n","[0.039714   0.04064945 0.04047029 0.03970541]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04105897 0.03833935]\n"," [0.04183582 0.03946308]\n"," [0.0414429  0.03900641]\n"," [0.04074641 0.03891955]]\n","\n","Average MAE Loss:\n","[0.03969916 0.04064945 0.04022465 0.03983298]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04102527 0.03832265]\n"," [0.04183582 0.03946308]\n"," [0.04162405 0.03932731]\n"," [0.04071314 0.03881701]]\n","\n","Average MAE Loss:\n","[0.03967396 0.04064945 0.04047568 0.03976507]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04103646 0.03834635]\n"," [0.04183582 0.03946308]\n"," [0.04144309 0.03910298]\n"," [0.04073399 0.03885723]]\n","\n","Average MAE Loss:\n","[0.0396914  0.04064945 0.04027304 0.03979561]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04104093 0.03833499]\n"," [0.04183582 0.03946308]\n"," [0.04151393 0.03917572]\n"," [0.04073178 0.03886666]]\n","\n","Average MAE Loss:\n","[0.03968796 0.04064945 0.04034482 0.03979922]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.0410267  0.03832729]\n"," [0.04183582 0.03946308]\n"," [0.04147542 0.0391422 ]\n"," [0.04075965 0.03889465]]\n","\n","Average MAE Loss:\n","[0.039677   0.04064945 0.04030881 0.03982715]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04098367 0.03848216]\n"," [0.04075965 0.03889465]\n"," [0.04088264 0.03846009]\n"," [0.0409259  0.03867142]]\n","\n","Average MAE Loss:\n","[0.03973292 0.03982715 0.03967136 0.03979866]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04102035 0.03837663]\n"," [0.04075965 0.03889465]\n"," [0.04129196 0.03897731]\n"," [0.04051675 0.0383783 ]]\n","\n","Average MAE Loss:\n","[0.03969849 0.03982715 0.04013463 0.03944752]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04098712 0.03827965]\n"," [0.04075965 0.03889465]\n"," [0.04139131 0.03908877]\n"," [0.04040535 0.03837519]]\n","\n","Average MAE Loss:\n","[0.03963338 0.03982715 0.04024004 0.03939027]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04102622 0.03832778]\n"," [0.04075965 0.03889465]\n"," [0.04140303 0.03907206]\n"," [0.04045855 0.03845197]]\n","\n","Average MAE Loss:\n","[0.039677   0.03982715 0.04023755 0.03945526]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04100977 0.0383021 ]\n"," [0.04075965 0.03889465]\n"," [0.04138461 0.03907687]\n"," [0.04053581 0.03853827]]\n","\n","Average MAE Loss:\n","[0.03965593 0.03982715 0.04023074 0.03953704]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04101207 0.03830834]\n"," [0.04075965 0.03889465]\n"," [0.04140729 0.03911766]\n"," [0.04058099 0.03859586]]\n","\n","Average MAE Loss:\n","[0.0396602  0.03982715 0.04026247 0.03958843]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04101017 0.03830918]\n"," [0.04075965 0.03889465]\n"," [0.04138104 0.03907582]\n"," [0.04061595 0.03864067]]\n","\n","Average MAE Loss:\n","[0.03965967 0.03982715 0.04022843 0.03962831]\n","\n","Epoch 00042: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00042: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04043812 0.03818988]\n"," [0.04040463 0.0382469 ]\n"," [0.04060115 0.03838548]\n"," [0.04037673 0.038254  ]]\n","\n","Average MAE Loss:\n","[0.039314   0.03932576 0.03949332 0.03931537]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04053895 0.03814838]\n"," [0.04040463 0.0382469 ]\n"," [0.04095004 0.038703  ]\n"," [0.04039509 0.03830985]]\n","\n","Average MAE Loss:\n","[0.03934367 0.03932576 0.03982652 0.03935247]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04067229 0.03815909]\n"," [0.04040463 0.0382469 ]\n"," [0.04112462 0.03888919]\n"," [0.04043432 0.0383786 ]]\n","\n","Average MAE Loss:\n","[0.03941569 0.03932576 0.0400069  0.03940646]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04076929 0.0382059 ]\n"," [0.04040463 0.0382469 ]\n"," [0.04123609 0.03899676]\n"," [0.04048553 0.03844587]]\n","\n","Average MAE Loss:\n","[0.0394876  0.03932576 0.04011642 0.0394657 ]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04082689 0.03824583]\n"," [0.04040463 0.0382469 ]\n"," [0.04127444 0.03905424]\n"," [0.04053644 0.03851635]]\n","\n","Average MAE Loss:\n","[0.03953636 0.03932576 0.04016434 0.03952639]\n","\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04084792 0.03826048]\n"," [0.04040463 0.0382469 ]\n"," [0.04129383 0.03907953]\n"," [0.04055946 0.03854756]]\n","\n","Average MAE Loss:\n","[0.0395542  0.03932576 0.04018668 0.03955351]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04086268 0.03826674]\n"," [0.04040463 0.0382469 ]\n"," [0.04131151 0.03910589]\n"," [0.04057126 0.03856847]]\n","\n","Average MAE Loss:\n","[0.03956471 0.03932576 0.0402087  0.03956986]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04041609 0.03820821]\n"," [0.04057126 0.03856847]\n"," [0.04080898 0.03826855]\n"," [0.04111852 0.03893208]]\n","\n","Average MAE Loss:\n","[0.03931215 0.03956986 0.03953876 0.0400253 ]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.040454   0.03817229]\n"," [0.04057126 0.03856847]\n"," [0.04081792 0.03837515]\n"," [0.04089928 0.03874513]]\n","\n","Average MAE Loss:\n","[0.03931314 0.03956986 0.03959654 0.0398222 ]\n","\n","Epoch 00051: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00051: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04048118 0.03816266]\n"," [0.04057126 0.03856847]\n"," [0.04084229 0.03843018]\n"," [0.04081546 0.03867947]]\n","\n","Average MAE Loss:\n","[0.03932192 0.03956986 0.03963624 0.03974747]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04050884 0.03815654]\n"," [0.04057126 0.03856847]\n"," [0.04087363 0.03847568]\n"," [0.04074496 0.03862395]]\n","\n","Average MAE Loss:\n","[0.03933269 0.03956986 0.03967465 0.03968445]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04053879 0.03815287]\n"," [0.04057126 0.03856847]\n"," [0.04089813 0.03851888]\n"," [0.04068656 0.03857862]]\n","\n","Average MAE Loss:\n","[0.03934583 0.03956986 0.0397085  0.03963259]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04057131 0.03815338]\n"," [0.04057126 0.03856847]\n"," [0.04092651 0.03856485]\n"," [0.04063559 0.03854037]]\n","\n","Average MAE Loss:\n","[0.03936235 0.03956986 0.03974568 0.03958798]\n","\n","Epoch 00055: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04058742 0.03815546]\n"," [0.04057126 0.03856847]\n"," [0.04094116 0.03858567]\n"," [0.04061389 0.03852379]]\n","\n","Average MAE Loss:\n","[0.03937144 0.03956986 0.03976342 0.03956884]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04047787 0.03826387]\n"," [0.04047309 0.03827485]\n"," [0.04050561 0.03830914]\n"," [0.04045829 0.0382639 ]]\n","\n","Average MAE Loss:\n","[0.03937087 0.03937397 0.03940738 0.0393611 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04048131 0.03824933]\n"," [0.04047309 0.03827485]\n"," [0.04055584 0.03835926]\n"," [0.04044496 0.03825533]]\n","\n","Average MAE Loss:\n","[0.03936532 0.03937397 0.03945755 0.03935015]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04048523 0.03823667]\n"," [0.04047309 0.03827485]\n"," [0.0406041  0.03840461]\n"," [0.04043462 0.03824917]]\n","\n","Average MAE Loss:\n","[0.03936095 0.03937397 0.03950435 0.03934189]\n","\n","Epoch 00059: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00059: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04048766 0.03823112]\n"," [0.04047309 0.03827485]\n"," [0.04064455 0.0384392 ]\n"," [0.04043025 0.03824706]]\n","\n","Average MAE Loss:\n","[0.03935939 0.03937397 0.03954187 0.03933866]\n","\n","\n","epochs finished with time:47.240620136260986\n","\n","[[0.04048766 0.03823112]\n"," [0.04047309 0.03827485]\n"," [0.04064455 0.0384392 ]\n"," [0.04043025 0.03824706]]\n","00:01:42.32971677000023\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10023183 0.09995283]\n"," [0.11635957 0.1145391 ]\n"," [0.08138219 0.08211793]\n"," [0.0858035  0.08518215]]\n","\n","Average MAE Loss:\n","[0.10009233 0.11544933 0.08175006 0.08549283]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.0997584  0.09890331]\n"," [0.11635957 0.1145391 ]\n"," [0.07772769 0.07869295]\n"," [0.07812119 0.07848051]]\n","\n","Average MAE Loss:\n","[0.09933086 0.11544933 0.07821032 0.07830085]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0991338  0.09865162]\n"," [0.11635957 0.1145391 ]\n"," [0.0758495  0.07716186]\n"," [0.0783863  0.07675232]]\n","\n","Average MAE Loss:\n","[0.09889271 0.11544933 0.07650568 0.07756931]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.09895364 0.0981838 ]\n"," [0.11635957 0.1145391 ]\n"," [0.07747886 0.07761401]\n"," [0.07671383 0.07571688]]\n","\n","Average MAE Loss:\n","[0.09856872 0.11544933 0.07754643 0.07621536]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.09901913 0.09832123]\n"," [0.11635957 0.1145391 ]\n"," [0.07637434 0.07693516]\n"," [0.07849365 0.07718843]]\n","\n","Average MAE Loss:\n","[0.09867018 0.11544933 0.07665475 0.07784104]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.0990749  0.0983408 ]\n"," [0.11635957 0.1145391 ]\n"," [0.07707755 0.07743562]\n"," [0.0796064  0.0781455 ]]\n","\n","Average MAE Loss:\n","[0.09870785 0.11544933 0.07725658 0.07887595]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.09907899 0.09853556]\n"," [0.11635957 0.1145391 ]\n"," [0.07668121 0.0772646 ]\n"," [0.08013484 0.07825427]]\n","\n","Average MAE Loss:\n","[0.09880727 0.11544933 0.0769729  0.07919455]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0747786  0.07720158]\n"," [0.08013484 0.07825427]\n"," [0.07301816 0.07309736]\n"," [0.07807933 0.07709641]]\n","\n","Average MAE Loss:\n","[0.07599009 0.07919455 0.07305776 0.07758787]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07445565 0.07638949]\n"," [0.08013484 0.07825427]\n"," [0.07171631 0.07201657]\n"," [0.07790083 0.07706134]]\n","\n","Average MAE Loss:\n","[0.07542257 0.07919455 0.07186644 0.07748109]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07443188 0.07609127]\n"," [0.08013484 0.07825427]\n"," [0.07192079 0.07197561]\n"," [0.07903813 0.07775239]]\n","\n","Average MAE Loss:\n","[0.07526158 0.07919455 0.0719482  0.07839526]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07453935 0.07505209]\n"," [0.08013484 0.07825427]\n"," [0.07195166 0.07205089]\n"," [0.07643597 0.07597305]]\n","\n","Average MAE Loss:\n","[0.07479572 0.07919455 0.07200128 0.07620451]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07463257 0.07483842]\n"," [0.08013484 0.07825427]\n"," [0.07231587 0.07226097]\n"," [0.07483572 0.07530836]]\n","\n","Average MAE Loss:\n","[0.0747355  0.07919455 0.07228842 0.07507204]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07404174 0.07442644]\n"," [0.08013484 0.07825427]\n"," [0.07183141 0.07189356]\n"," [0.07592713 0.07551408]]\n","\n","Average MAE Loss:\n","[0.07423409 0.07919455 0.07186248 0.0757206 ]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07442048 0.07474849]\n"," [0.08013484 0.07825427]\n"," [0.07077105 0.07125386]\n"," [0.07548044 0.07535588]]\n","\n","Average MAE Loss:\n","[0.07458448 0.07919455 0.07101246 0.07541816]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06834719 0.06969384]\n"," [0.11275191 0.11092151]\n"," [0.09515195 0.09370844]\n"," [0.07098286 0.07168069]]\n","\n","Average MAE Loss:\n","[0.06902052 0.11183671 0.0944302  0.07133178]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.06722767 0.06750311]\n"," [0.11275191 0.11092151]\n"," [0.06683065 0.06726859]\n"," [0.06781509 0.06716551]]\n","\n","Average MAE Loss:\n","[0.06736539 0.11183671 0.06704962 0.0674903 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06661167 0.06750172]\n"," [0.11275191 0.11092151]\n"," [0.06625426 0.06732194]\n"," [0.06675643 0.06626935]]\n","\n","Average MAE Loss:\n","[0.0670567  0.11183671 0.0667881  0.06651289]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0667549  0.06709265]\n"," [0.11275191 0.11092151]\n"," [0.06627982 0.06735408]\n"," [0.06575727 0.06569157]]\n","\n","Average MAE Loss:\n","[0.06692378 0.11183671 0.06681695 0.06572442]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06659811 0.0671686 ]\n"," [0.11275191 0.11092151]\n"," [0.06598932 0.06719868]\n"," [0.06602827 0.06573837]]\n","\n","Average MAE Loss:\n","[0.06688336 0.11183671 0.066594   0.06588332]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06584421 0.06653606]\n"," [0.11275191 0.11092151]\n"," [0.06615209 0.06729075]\n"," [0.0661445  0.06587188]]\n","\n","Average MAE Loss:\n","[0.06619014 0.11183671 0.06672142 0.06600819]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06565813 0.066497  ]\n"," [0.11275191 0.11092151]\n"," [0.06613391 0.06722968]\n"," [0.06566349 0.06561344]]\n","\n","Average MAE Loss:\n","[0.06607757 0.11183671 0.06668179 0.06563846]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.06849977 0.0694521 ]\n"," [0.06566349 0.06561344]\n"," [0.06451775 0.06598991]\n"," [0.06651575 0.066566  ]]\n","\n","Average MAE Loss:\n","[0.06897594 0.06563846 0.06525383 0.06654088]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06798633 0.0690948 ]\n"," [0.06566349 0.06561344]\n"," [0.06512613 0.06653026]\n"," [0.06601982 0.0659009 ]]\n","\n","Average MAE Loss:\n","[0.06854057 0.06563846 0.06582819 0.06596036]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0670342  0.06753651]\n"," [0.06566349 0.06561344]\n"," [0.06539555 0.06672781]\n"," [0.06592332 0.06584432]]\n","\n","Average MAE Loss:\n","[0.06728536 0.06563846 0.06606168 0.06588382]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0668938  0.06743339]\n"," [0.06566349 0.06561344]\n"," [0.06544229 0.06674557]\n"," [0.06619226 0.06588551]]\n","\n","Average MAE Loss:\n","[0.06716359 0.06563846 0.06609393 0.06603889]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06538098 0.06646207]\n"," [0.06566349 0.06561344]\n"," [0.06546472 0.06678056]\n"," [0.06590236 0.06582227]]\n","\n","Average MAE Loss:\n","[0.06592152 0.06563846 0.06612264 0.06586232]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06541913 0.06638057]\n"," [0.06566349 0.06561344]\n"," [0.06546184 0.06676659]\n"," [0.06596963 0.06588464]]\n","\n","Average MAE Loss:\n","[0.06589985 0.06563846 0.06611422 0.06592714]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06569393 0.06650736]\n"," [0.06566349 0.06561344]\n"," [0.06546685 0.06675977]\n"," [0.06595257 0.06589575]]\n","\n","Average MAE Loss:\n","[0.06610065 0.06563846 0.06611331 0.06592416]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06398425 0.06490504]\n"," [0.07140676 0.07042677]\n"," [0.06494067 0.0656428 ]\n"," [0.06453488 0.06522743]]\n","\n","Average MAE Loss:\n","[0.06444464 0.07091676 0.06529174 0.06488115]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.0646394  0.06527633]\n"," [0.07140676 0.07042677]\n"," [0.06447935 0.06557983]\n"," [0.06519536 0.06512235]]\n","\n","Average MAE Loss:\n","[0.06495786 0.07091676 0.06502959 0.06515886]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06457932 0.0653012 ]\n"," [0.07140676 0.07042677]\n"," [0.06487607 0.06589587]\n"," [0.06512296 0.06505225]]\n","\n","Average MAE Loss:\n","[0.06494026 0.07091676 0.06538597 0.06508761]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06427132 0.06515739]\n"," [0.07140676 0.07042677]\n"," [0.06480559 0.06594622]\n"," [0.0652261  0.06508759]]\n","\n","Average MAE Loss:\n","[0.06471435 0.07091676 0.0653759  0.06515685]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06451423 0.06525997]\n"," [0.07140676 0.07042677]\n"," [0.06490126 0.06602287]\n"," [0.06507832 0.0650096 ]]\n","\n","Average MAE Loss:\n","[0.0648871  0.07091676 0.06546207 0.06504396]\n","\n","Epoch 00033: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06421369 0.06509476]\n"," [0.07140676 0.07042677]\n"," [0.06491678 0.06604293]\n"," [0.06513367 0.06501071]]\n","\n","Average MAE Loss:\n","[0.06465423 0.07091676 0.06547986 0.06507219]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06428838 0.06508949]\n"," [0.07140676 0.07042677]\n"," [0.06492426 0.06604542]\n"," [0.06514234 0.06501962]]\n","\n","Average MAE Loss:\n","[0.06468894 0.07091676 0.06548484 0.06508098]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06408412 0.06504626]\n"," [0.06514234 0.06501962]\n"," [0.06420172 0.06506211]\n"," [0.06491036 0.06571796]]\n","\n","Average MAE Loss:\n","[0.06456519 0.06508098 0.06463192 0.06531416]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06438708 0.06494521]\n"," [0.06514234 0.06501962]\n"," [0.0641023  0.06507447]\n"," [0.06481364 0.06537398]]\n","\n","Average MAE Loss:\n","[0.06466614 0.06508098 0.06458839 0.06509381]\n","\n","Epoch 00037: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00037: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06441272 0.06501882]\n"," [0.06514234 0.06501962]\n"," [0.06409309 0.06516687]\n"," [0.06473786 0.06525027]]\n","\n","Average MAE Loss:\n","[0.06471577 0.06508098 0.06462998 0.06499406]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06426042 0.06499202]\n"," [0.06514234 0.06501962]\n"," [0.06414043 0.06529096]\n"," [0.06471396 0.06517807]]\n","\n","Average MAE Loss:\n","[0.06462622 0.06508098 0.06471569 0.06494601]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.064298   0.06502637]\n"," [0.06514234 0.06501962]\n"," [0.06419984 0.0654116 ]\n"," [0.06472551 0.06513637]]\n","\n","Average MAE Loss:\n","[0.06466219 0.06508098 0.06480572 0.06493094]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.0642516  0.06502184]\n"," [0.06514234 0.06501962]\n"," [0.06427318 0.06553154]\n"," [0.06474185 0.0651101 ]]\n","\n","Average MAE Loss:\n","[0.06463672 0.06508098 0.06490236 0.06492597]\n","\n","Epoch 00041: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06424426 0.06501829]\n"," [0.06514234 0.06501962]\n"," [0.06431885 0.06558157]\n"," [0.06475378 0.0650981 ]]\n","\n","Average MAE Loss:\n","[0.06463128 0.06508098 0.06495021 0.06492594]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06429335 0.06487456]\n"," [0.06440504 0.06493109]\n"," [0.06436095 0.06495727]\n"," [0.06440675 0.06491024]]\n","\n","Average MAE Loss:\n","[0.06458396 0.06466807 0.06465911 0.0646585 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06421561 0.0648424 ]\n"," [0.06440504 0.06493109]\n"," [0.06429645 0.06500023]\n"," [0.06442418 0.06489648]]\n","\n","Average MAE Loss:\n","[0.06452901 0.06466807 0.06464834 0.06466033]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0641965  0.06482852]\n"," [0.06440504 0.06493109]\n"," [0.06427361 0.06506711]\n"," [0.06444793 0.0648891 ]]\n","\n","Average MAE Loss:\n","[0.06451251 0.06466807 0.06467036 0.06466852]\n","\n","Epoch 00045: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00045: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06419353 0.06482892]\n"," [0.06440504 0.06493109]\n"," [0.06427603 0.06510392]\n"," [0.06447585 0.06488711]]\n","\n","Average MAE Loss:\n","[0.06451123 0.06466807 0.06468997 0.06468148]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06418921 0.06483092]\n"," [0.06440504 0.06493109]\n"," [0.06428093 0.06514115]\n"," [0.06450585 0.06488739]]\n","\n","Average MAE Loss:\n","[0.06451007 0.06466807 0.06471104 0.06469662]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06418656 0.06483854]\n"," [0.06440504 0.06493109]\n"," [0.06428922 0.06517782]\n"," [0.06452111 0.06488952]]\n","\n","Average MAE Loss:\n","[0.06451255 0.06466807 0.06473352 0.06470532]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06418905 0.06484779]\n"," [0.06440504 0.06493109]\n"," [0.06429964 0.06521486]\n"," [0.06453618 0.0648914 ]]\n","\n","Average MAE Loss:\n","[0.06451842 0.06466807 0.06475725 0.06471379]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06437226 0.06491459]\n"," [0.06453618 0.0648914 ]\n"," [0.0641729  0.0648505 ]\n"," [0.06432354 0.06519473]]\n","\n","Average MAE Loss:\n","[0.06464342 0.06471379 0.0645117  0.06475914]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06432474 0.06488999]\n"," [0.06453618 0.0648914 ]\n"," [0.06415545 0.06485297]\n"," [0.06435518 0.06517074]]\n","\n","Average MAE Loss:\n","[0.06460737 0.06471379 0.06450421 0.06476296]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06428562 0.06486796]\n"," [0.06453618 0.0648914 ]\n"," [0.06414016 0.06485662]\n"," [0.06436995 0.06515852]]\n","\n","Average MAE Loss:\n","[0.06457679 0.06471379 0.06449839 0.06476423]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06425645 0.06485217]\n"," [0.06453618 0.0648914 ]\n"," [0.06412793 0.06486103]\n"," [0.06438355 0.06514688]]\n","\n","Average MAE Loss:\n","[0.06455431 0.06471379 0.06449448 0.06476521]\n","\n","Epoch 00053: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06424561 0.06484641]\n"," [0.06453618 0.0648914 ]\n"," [0.06411679 0.06486707]\n"," [0.06439617 0.06513638]]\n","\n","Average MAE Loss:\n","[0.06454601 0.06471379 0.06449193 0.06476628]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06423572 0.06484109]\n"," [0.06453618 0.0648914 ]\n"," [0.06410737 0.06487609]\n"," [0.06440778 0.06512584]]\n","\n","Average MAE Loss:\n","[0.0645384  0.06471379 0.06449173 0.06476681]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06422594 0.06483631]\n"," [0.06453618 0.0648914 ]\n"," [0.06409947 0.0648862 ]\n"," [0.06441335 0.06512064]]\n","\n","Average MAE Loss:\n","[0.06453112 0.06471379 0.06449284 0.06476699]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06423357 0.06485025]\n"," [0.06424595 0.06485763]\n"," [0.06424007 0.06487269]\n"," [0.06425418 0.06485482]]\n","\n","Average MAE Loss:\n","[0.06454191 0.06455179 0.06455638 0.0645545 ]\n","\n","Epoch 00057: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00057: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06422839 0.0648478 ]\n"," [0.06424595 0.06485763]\n"," [0.0642351  0.06487971]\n"," [0.06426209 0.06485302]]\n","\n","Average MAE Loss:\n","[0.06453809 0.06455179 0.0645574  0.06455756]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06422355 0.06484585]\n"," [0.06424595 0.06485763]\n"," [0.06423021 0.06488658]\n"," [0.06426935 0.0648515 ]]\n","\n","Average MAE Loss:\n","[0.0645347  0.06455179 0.0645584  0.06456043]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06421912 0.06484403]\n"," [0.06424595 0.06485763]\n"," [0.06422625 0.06489371]\n"," [0.06427661 0.06485025]]\n","\n","Average MAE Loss:\n","[0.06453157 0.06455179 0.06455998 0.06456343]\n","\n","\n","epochs finished with time:47.471158504486084\n","\n","[[0.06421912 0.06484403]\n"," [0.06424595 0.06485763]\n"," [0.06422625 0.06489371]\n"," [0.06427661 0.06485025]]\n","00:01:40.03824703400005\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.0869564  0.07862292]\n"," [0.09980381 0.08862089]\n"," [0.06664631 0.0580514 ]\n"," [0.07952718 0.07225591]]\n","\n","Average MAE Loss:\n","[0.08278966 0.09421235 0.06234885 0.07589154]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.06126168 0.0537638 ]\n"," [0.09980381 0.08862089]\n"," [0.06092376 0.05296128]\n"," [0.06255808 0.05508271]]\n","\n","Average MAE Loss:\n","[0.05751274 0.09421235 0.05694252 0.0588204 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.05947608 0.05247716]\n"," [0.09980381 0.08862089]\n"," [0.06222124 0.05368958]\n"," [0.05923454 0.05229075]]\n","\n","Average MAE Loss:\n","[0.05597662 0.09421235 0.05795541 0.05576265]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.05942487 0.0522146 ]\n"," [0.09980381 0.08862089]\n"," [0.06149461 0.05292083]\n"," [0.05823749 0.05139388]]\n","\n","Average MAE Loss:\n","[0.05581973 0.09421235 0.05720772 0.05481568]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.05971862 0.05213394]\n"," [0.09980381 0.08862089]\n"," [0.0622744  0.05325154]\n"," [0.05833009 0.05111021]]\n","\n","Average MAE Loss:\n","[0.05592628 0.09421235 0.05776297 0.05472015]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.05971298 0.05216065]\n"," [0.09980381 0.08862089]\n"," [0.06122537 0.05291065]\n"," [0.05812965 0.05078277]]\n","\n","Average MAE Loss:\n","[0.05593682 0.09421235 0.05706801 0.05445621]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.06003623 0.05233081]\n"," [0.09980381 0.08862089]\n"," [0.0595166  0.05350571]\n"," [0.05819828 0.05161207]]\n","\n","Average MAE Loss:\n","[0.05618352 0.09421235 0.05651116 0.05490518]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0604394  0.05498553]\n"," [0.05819828 0.05161207]\n"," [0.06043445 0.05240344]\n"," [0.06161832 0.05293575]]\n","\n","Average MAE Loss:\n","[0.05771247 0.05490518 0.05641895 0.05727704]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06092967 0.05274831]\n"," [0.05819828 0.05161207]\n"," [0.06033052 0.0518846 ]\n"," [0.06024812 0.05320754]]\n","\n","Average MAE Loss:\n","[0.05683899 0.05490518 0.05610756 0.05672783]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05874615 0.05152598]\n"," [0.05819828 0.05161207]\n"," [0.05943664 0.05193304]\n"," [0.05984071 0.05192256]]\n","\n","Average MAE Loss:\n","[0.05513606 0.05490518 0.05568484 0.05588163]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05841485 0.05138346]\n"," [0.05819828 0.05161207]\n"," [0.0589934  0.05306903]\n"," [0.0599228  0.05146138]]\n","\n","Average MAE Loss:\n","[0.05489916 0.05490518 0.05603121 0.05569209]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05857814 0.05136687]\n"," [0.05819828 0.05161207]\n"," [0.05867196 0.05202086]\n"," [0.06019251 0.05156089]]\n","\n","Average MAE Loss:\n","[0.05497251 0.05490518 0.05534641 0.0558767 ]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05859998 0.05126562]\n"," [0.05819828 0.05161207]\n"," [0.05882555 0.05165834]\n"," [0.05984114 0.0514877 ]]\n","\n","Average MAE Loss:\n","[0.0549328  0.05490518 0.05524194 0.05566442]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05810219 0.05114568]\n"," [0.05819828 0.05161207]\n"," [0.05886287 0.05231099]\n"," [0.05942862 0.05147893]]\n","\n","Average MAE Loss:\n","[0.05462394 0.05490518 0.05558693 0.05545378]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06073939 0.05317859]\n"," [0.09626727 0.08505377]\n"," [0.05473221 0.05102079]\n"," [0.06894557 0.05838396]]\n","\n","Average MAE Loss:\n","[0.05695899 0.09066052 0.0528765  0.06366476]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05242003 0.04537125]\n"," [0.09626727 0.08505377]\n"," [0.05215059 0.04432673]\n"," [0.05230965 0.04573777]]\n","\n","Average MAE Loss:\n","[0.04889564 0.09066052 0.04823866 0.04902371]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.05012415 0.04460404]\n"," [0.09626727 0.08505377]\n"," [0.05116322 0.04508843]\n"," [0.0517603  0.04405179]]\n","\n","Average MAE Loss:\n","[0.0473641  0.09066052 0.04812582 0.04790604]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.05026377 0.04384309]\n"," [0.09626727 0.08505377]\n"," [0.05041962 0.04436808]\n"," [0.05130108 0.04389279]]\n","\n","Average MAE Loss:\n","[0.04705343 0.09066052 0.04739385 0.04759693]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.05010913 0.04389391]\n"," [0.09626727 0.08505377]\n"," [0.0505822  0.04389755]\n"," [0.05118009 0.04382185]]\n","\n","Average MAE Loss:\n","[0.04700152 0.09066052 0.04723988 0.04750097]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04983273 0.04391148]\n"," [0.09626727 0.08505377]\n"," [0.05056905 0.04429429]\n"," [0.05116596 0.04372693]]\n","\n","Average MAE Loss:\n","[0.04687211 0.09066052 0.04743167 0.04744644]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04995739 0.04377304]\n"," [0.09626727 0.08505377]\n"," [0.05041973 0.04419012]\n"," [0.05116684 0.04375874]]\n","\n","Average MAE Loss:\n","[0.04686522 0.09066052 0.04730492 0.04746279]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07582716 0.0646335 ]\n"," [0.05116684 0.04375874]\n"," [0.05108882 0.04479852]\n"," [0.05058666 0.04334422]]\n","\n","Average MAE Loss:\n","[0.07023033 0.04746279 0.04794367 0.04696544]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.05111437 0.04762604]\n"," [0.05116684 0.04375874]\n"," [0.05065932 0.04439906]\n"," [0.05052911 0.04356141]]\n","\n","Average MAE Loss:\n","[0.0493702  0.04746279 0.04752919 0.04704526]\n","\n","Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05057781 0.04401997]\n"," [0.05116684 0.04375874]\n"," [0.05042247 0.04413299]\n"," [0.05069778 0.04344472]]\n","\n","Average MAE Loss:\n","[0.04729889 0.04746279 0.04727773 0.04707125]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05023814 0.04404495]\n"," [0.05116684 0.04375874]\n"," [0.05046156 0.04409523]\n"," [0.0506606  0.04347582]]\n","\n","Average MAE Loss:\n","[0.04714154 0.04746279 0.04727839 0.04706821]\n","\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0498752  0.04395381]\n"," [0.05116684 0.04375874]\n"," [0.05046613 0.04406479]\n"," [0.05065072 0.04351071]]\n","\n","Average MAE Loss:\n","[0.0469145  0.04746279 0.04726546 0.04708071]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04983453 0.04384435]\n"," [0.05116684 0.04375874]\n"," [0.05050258 0.04406128]\n"," [0.0506122  0.04353858]]\n","\n","Average MAE Loss:\n","[0.04683944 0.04746279 0.04728193 0.04707539]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04978805 0.04387026]\n"," [0.05116684 0.04375874]\n"," [0.05049033 0.04404823]\n"," [0.05061539 0.0435323 ]]\n","\n","Average MAE Loss:\n","[0.04682916 0.04746279 0.04726928 0.04707385]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04903363 0.04314493]\n"," [0.05260509 0.04403979]\n"," [0.04996707 0.04282609]\n"," [0.05015484 0.04292528]]\n","\n","Average MAE Loss:\n","[0.04608928 0.04832244 0.04639658 0.04654006]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04893719 0.04312217]\n"," [0.05260509 0.04403979]\n"," [0.04961753 0.04369073]\n"," [0.04984707 0.04333029]]\n","\n","Average MAE Loss:\n","[0.04602968 0.04832244 0.04665413 0.04658868]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04870589 0.04318118]\n"," [0.05260509 0.04403979]\n"," [0.04976222 0.04354643]\n"," [0.05003119 0.04312408]]\n","\n","Average MAE Loss:\n","[0.04594353 0.04832244 0.04665432 0.04657764]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04882349 0.0431134 ]\n"," [0.05260509 0.04403979]\n"," [0.0497685  0.04352375]\n"," [0.05003716 0.04322222]]\n","\n","Average MAE Loss:\n","[0.04596845 0.04832244 0.04664612 0.04662969]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04879076 0.04315191]\n"," [0.05260509 0.04403979]\n"," [0.04977605 0.04359265]\n"," [0.05007481 0.04318233]]\n","\n","Average MAE Loss:\n","[0.04597133 0.04832244 0.04668435 0.04662857]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04882395 0.04313874]\n"," [0.05260509 0.04403979]\n"," [0.04978231 0.04359166]\n"," [0.05005304 0.04320436]]\n","\n","Average MAE Loss:\n","[0.04598135 0.04832244 0.04668698 0.0466287 ]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04884506 0.04313964]\n"," [0.05260509 0.04403979]\n"," [0.04979567 0.04361105]\n"," [0.05005532 0.04321445]]\n","\n","Average MAE Loss:\n","[0.04599235 0.04832244 0.04670336 0.04663488]\n","\n","Epoch 00035: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04998904 0.04279023]\n"," [0.05005532 0.04321445]\n"," [0.04869488 0.04313789]\n"," [0.04965136 0.04313961]]\n","\n","Average MAE Loss:\n","[0.04638964 0.04663488 0.04591638 0.04639549]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04887769 0.04295215]\n"," [0.05005532 0.04321445]\n"," [0.04870105 0.04323841]\n"," [0.04964937 0.04290191]]\n","\n","Average MAE Loss:\n","[0.04591492 0.04663488 0.04596973 0.04627564]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04884262 0.04293551]\n"," [0.05005532 0.04321445]\n"," [0.04888399 0.04333536]\n"," [0.04967179 0.04291517]]\n","\n","Average MAE Loss:\n","[0.04588906 0.04663488 0.04610967 0.04629348]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04879138 0.04297851]\n"," [0.05005532 0.04321445]\n"," [0.04909579 0.04337207]\n"," [0.04969436 0.042947  ]]\n","\n","Average MAE Loss:\n","[0.04588494 0.04663488 0.04623393 0.04632068]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.0487582  0.04300968]\n"," [0.05005532 0.04321445]\n"," [0.04926383 0.04340311]\n"," [0.04973841 0.04299713]]\n","\n","Average MAE Loss:\n","[0.04588394 0.04663488 0.04633347 0.04636777]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04877185 0.04302976]\n"," [0.05005532 0.04321445]\n"," [0.049319   0.04342999]\n"," [0.04977011 0.04304617]]\n","\n","Average MAE Loss:\n","[0.0459008  0.04663488 0.04637449 0.04640814]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04875221 0.04302793]\n"," [0.05005532 0.04321445]\n"," [0.04936839 0.04346438]\n"," [0.04977646 0.04306607]]\n","\n","Average MAE Loss:\n","[0.04589007 0.04663488 0.04641639 0.04642127]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04906078 0.04259508]\n"," [0.04915795 0.04277433]\n"," [0.04915018 0.04285549]\n"," [0.04919944 0.04277259]]\n","\n","Average MAE Loss:\n","[0.04582793 0.04596614 0.04600283 0.04598601]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04888871 0.04274557]\n"," [0.04915795 0.04277433]\n"," [0.0491561  0.04298127]\n"," [0.04927044 0.0427979 ]]\n","\n","Average MAE Loss:\n","[0.04581714 0.04596614 0.04606868 0.04603417]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04878335 0.0428653 ]\n"," [0.04915795 0.04277433]\n"," [0.04917663 0.04303446]\n"," [0.04934061 0.04285485]]\n","\n","Average MAE Loss:\n","[0.04582433 0.04596614 0.04610555 0.04609773]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0487469  0.0429062 ]\n"," [0.04915795 0.04277433]\n"," [0.04920648 0.04307863]\n"," [0.04941002 0.04291277]]\n","\n","Average MAE Loss:\n","[0.04582655 0.04596614 0.04614256 0.04616139]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04873279 0.0429319 ]\n"," [0.04915795 0.04277433]\n"," [0.0492387  0.04311951]\n"," [0.04947424 0.04295713]]\n","\n","Average MAE Loss:\n","[0.04583234 0.04596614 0.0461791  0.04621569]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04871027 0.04295133]\n"," [0.04915795 0.04277433]\n"," [0.04927172 0.04315922]\n"," [0.04950099 0.04297638]]\n","\n","Average MAE Loss:\n","[0.0458308  0.04596614 0.04621547 0.04623869]\n","\n","Epoch 00048: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04871204 0.04295269]\n"," [0.04915795 0.04277433]\n"," [0.04928721 0.04317644]\n"," [0.04952564 0.0429923 ]]\n","\n","Average MAE Loss:\n","[0.04583236 0.04596614 0.04623182 0.04625897]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04906914 0.04263761]\n"," [0.04952564 0.0429923 ]\n"," [0.04869197 0.04294469]\n"," [0.04930121 0.04310837]]\n","\n","Average MAE Loss:\n","[0.04585337 0.04625897 0.04581833 0.04620479]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04899059 0.04262694]\n"," [0.04952564 0.0429923 ]\n"," [0.04866356 0.04293893]\n"," [0.04933278 0.04302549]]\n","\n","Average MAE Loss:\n","[0.04580876 0.04625897 0.04580125 0.04617914]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04890126 0.04269054]\n"," [0.04952564 0.0429923 ]\n"," [0.04863828 0.04293857]\n"," [0.04935189 0.04299415]]\n","\n","Average MAE Loss:\n","[0.0457959  0.04625897 0.04578842 0.04617302]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04882291 0.04275136]\n"," [0.04952564 0.0429923 ]\n"," [0.04861677 0.04294281]\n"," [0.04936963 0.04297115]]\n","\n","Average MAE Loss:\n","[0.04578713 0.04625897 0.04577979 0.04617039]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04878553 0.04278404]\n"," [0.04952564 0.0429923 ]\n"," [0.04860043 0.04294845]\n"," [0.04938578 0.0429542 ]]\n","\n","Average MAE Loss:\n","[0.04578478 0.04625897 0.04577444 0.04616999]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04876035 0.04282531]\n"," [0.04952564 0.0429923 ]\n"," [0.04858922 0.04295626]\n"," [0.04939964 0.04294291]]\n","\n","Average MAE Loss:\n","[0.04579283 0.04625897 0.04577274 0.04617128]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04874399 0.04285245]\n"," [0.04952564 0.0429923 ]\n"," [0.04858404 0.04296642]\n"," [0.04940624 0.04293855]]\n","\n","Average MAE Loss:\n","[0.04579822 0.04625897 0.04577523 0.04617239]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04886369 0.04263606]\n"," [0.04888121 0.04269137]\n"," [0.04888368 0.04272164]\n"," [0.04888827 0.04268875]]\n","\n","Average MAE Loss:\n","[0.04574987 0.04578629 0.04580266 0.04578851]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04883911 0.04267347]\n"," [0.04888121 0.04269137]\n"," [0.04888298 0.04275239]\n"," [0.04889657 0.04268744]]\n","\n","Average MAE Loss:\n","[0.04575629 0.04578629 0.04581768 0.045792  ]\n","\n","Epoch 00058: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04876925 0.04273687]\n"," [0.04888121 0.04269137]\n"," [0.04888412 0.04276783]\n"," [0.04890567 0.04268609]]\n","\n","Average MAE Loss:\n","[0.04575306 0.04578629 0.04582597 0.04579588]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04873424 0.04277967]\n"," [0.04888121 0.04269137]\n"," [0.04888579 0.04278348]\n"," [0.04891489 0.04268537]]\n","\n","Average MAE Loss:\n","[0.04575695 0.04578629 0.04583463 0.04580013]\n","\n","\n","epochs finished with time:47.227216720581055\n","\n","[[0.04873424 0.04277967]\n"," [0.04888121 0.04269137]\n"," [0.04888579 0.04278348]\n"," [0.04891489 0.04268537]]\n","00:01:30.42991487099971\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05016652 0.06903835]\n"," [0.08241    0.10488542]\n"," [0.07308529 0.095424  ]\n"," [0.05346987 0.07220697]]\n","\n","Average MAE Loss:\n","[0.05960244 0.09364771 0.08425464 0.06283842]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04999947 0.06812658]\n"," [0.08241    0.10488542]\n"," [0.07186357 0.09282435]\n"," [0.05078544 0.07037578]]\n","\n","Average MAE Loss:\n","[0.05906302 0.09364771 0.08234396 0.06058061]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04712897 0.06706344]\n"," [0.08241    0.10488542]\n"," [0.07040393 0.09232322]\n"," [0.04964338 0.06937747]]\n","\n","Average MAE Loss:\n","[0.05709621 0.09364771 0.08136358 0.05951043]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04757797 0.06677742]\n"," [0.08241    0.10488542]\n"," [0.07032891 0.09209429]\n"," [0.04880471 0.06877826]]\n","\n","Average MAE Loss:\n","[0.05717769 0.09364771 0.0812116  0.05879148]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04685494 0.0665428 ]\n"," [0.08241    0.10488542]\n"," [0.07033643 0.09213196]\n"," [0.04847421 0.06875516]]\n","\n","Average MAE Loss:\n","[0.05669887 0.09364771 0.08123419 0.05861468]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04732148 0.06711682]\n"," [0.08241    0.10488542]\n"," [0.07024804 0.09208944]\n"," [0.04817375 0.06852804]]\n","\n","Average MAE Loss:\n","[0.05721915 0.09364771 0.08116874 0.05835089]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04735656 0.06726827]\n"," [0.08241    0.10488542]\n"," [0.07099518 0.092656  ]\n"," [0.04846908 0.06886373]]\n","\n","Average MAE Loss:\n","[0.05731242 0.09364771 0.08182559 0.05866641]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05126815 0.06957783]\n"," [0.04846908 0.06886373]\n"," [0.05249732 0.07104795]\n"," [0.07121135 0.09228374]]\n","\n","Average MAE Loss:\n","[0.06042299 0.05866641 0.06177263 0.08174754]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05104311 0.06899466]\n"," [0.04846908 0.06886373]\n"," [0.04624157 0.06700783]\n"," [0.06992974 0.09145072]]\n","\n","Average MAE Loss:\n","[0.06001888 0.05866641 0.0566247  0.08069023]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04622769 0.06570611]\n"," [0.04846908 0.06886373]\n"," [0.045865   0.06655002]\n"," [0.06953986 0.09101267]]\n","\n","Average MAE Loss:\n","[0.0559669  0.05866641 0.05620751 0.08027627]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04619217 0.06616079]\n"," [0.04846908 0.06886373]\n"," [0.04604311 0.06688935]\n"," [0.06925469 0.09094054]]\n","\n","Average MAE Loss:\n","[0.05617648 0.05866641 0.05646623 0.08009761]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0463231  0.06624929]\n"," [0.04846908 0.06886373]\n"," [0.04571167 0.06638234]\n"," [0.0692289  0.09096948]]\n","\n","Average MAE Loss:\n","[0.05628619 0.05866641 0.05604701 0.08009919]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04612907 0.06593383]\n"," [0.04846908 0.06886373]\n"," [0.04574191 0.06621587]\n"," [0.06931568 0.09086352]]\n","\n","Average MAE Loss:\n","[0.05603145 0.05866641 0.05597889 0.0800896 ]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04618287 0.06573319]\n"," [0.04846908 0.06886373]\n"," [0.04585381 0.06637552]\n"," [0.06966627 0.09106317]]\n","\n","Average MAE Loss:\n","[0.05595803 0.05866641 0.05611466 0.08036472]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04153135 0.0600119 ]\n"," [0.07774587 0.1002173 ]\n"," [0.04374585 0.0635158 ]\n"," [0.04697045 0.06777091]]\n","\n","Average MAE Loss:\n","[0.05077163 0.08898158 0.05363082 0.05737068]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04073568 0.06011737]\n"," [0.07774587 0.1002173 ]\n"," [0.0413323  0.06191432]\n"," [0.04184812 0.06211142]]\n","\n","Average MAE Loss:\n","[0.05042652 0.08898158 0.05162331 0.05197977]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04074888 0.06021461]\n"," [0.07774587 0.1002173 ]\n"," [0.04092699 0.06150634]\n"," [0.04184651 0.06111494]]\n","\n","Average MAE Loss:\n","[0.05048174 0.08898158 0.05121666 0.05148073]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04050561 0.05972339]\n"," [0.07774587 0.1002173 ]\n"," [0.04106653 0.0613541 ]\n"," [0.04108418 0.06062557]]\n","\n","Average MAE Loss:\n","[0.0501145  0.08898158 0.05121032 0.05085487]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04053661 0.05963157]\n"," [0.07774587 0.1002173 ]\n"," [0.04131521 0.06147247]\n"," [0.04113551 0.06062755]]\n","\n","Average MAE Loss:\n","[0.05008409 0.08898158 0.05139384 0.05088153]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04059154 0.05948495]\n"," [0.07774587 0.1002173 ]\n"," [0.04121973 0.06137811]\n"," [0.04120159 0.06058124]]\n","\n","Average MAE Loss:\n","[0.05003824 0.08898158 0.05129892 0.05089142]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04068851 0.0596353 ]\n"," [0.07774587 0.1002173 ]\n"," [0.04102218 0.06133389]\n"," [0.04103098 0.06054974]]\n","\n","Average MAE Loss:\n","[0.05016191 0.08898158 0.05117803 0.05079036]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.05905815 0.08113918]\n"," [0.04103098 0.06054974]\n"," [0.0411003  0.06183463]\n"," [0.04065173 0.06071751]]\n","\n","Average MAE Loss:\n","[0.07009867 0.05079036 0.05146747 0.05068462]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0427751  0.06061092]\n"," [0.04103098 0.06054974]\n"," [0.04095448 0.06156818]\n"," [0.0408475  0.06064123]]\n","\n","Average MAE Loss:\n","[0.05169301 0.05079036 0.05126133 0.05074436]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04053625 0.05938634]\n"," [0.04103098 0.06054974]\n"," [0.04082916 0.06118278]\n"," [0.04096679 0.06070349]]\n","\n","Average MAE Loss:\n","[0.0499613  0.05079036 0.05100597 0.05083514]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04035584 0.05947972]\n"," [0.04103098 0.06054974]\n"," [0.04114322 0.06118738]\n"," [0.04105618 0.0607392 ]]\n","\n","Average MAE Loss:\n","[0.04991778 0.05079036 0.0511653  0.05089769]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04047056 0.05915507]\n"," [0.04103098 0.06054974]\n"," [0.04110044 0.06126796]\n"," [0.04110791 0.06076387]]\n","\n","Average MAE Loss:\n","[0.04981282 0.05079036 0.0511842  0.05093589]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04048291 0.0593028 ]\n"," [0.04103098 0.06054974]\n"," [0.04092164 0.06112601]\n"," [0.04114702 0.06076312]]\n","\n","Average MAE Loss:\n","[0.04989285 0.05079036 0.05102383 0.05095507]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04033313 0.05917721]\n"," [0.04103098 0.06054974]\n"," [0.04101811 0.06121121]\n"," [0.04113023 0.06075016]]\n","\n","Average MAE Loss:\n","[0.04975517 0.05079036 0.05111466 0.05094019]\n","\n","Epoch 00028: reducing learning rate of group 0 to 5.0000e-04.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04027214 0.05899068]\n"," [0.0416077  0.06210271]\n"," [0.04047496 0.06051787]\n"," [0.04023872 0.06008562]]\n","\n","Average MAE Loss:\n","[0.04963141 0.0518552  0.05049641 0.05016217]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04020463 0.05906571]\n"," [0.0416077  0.06210271]\n"," [0.04033825 0.06064113]\n"," [0.0406817  0.06009959]]\n","\n","Average MAE Loss:\n","[0.04963517 0.0518552  0.05048969 0.05039065]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03991659 0.05906426]\n"," [0.0416077  0.06210271]\n"," [0.04028146 0.06049629]\n"," [0.04041184 0.05999158]]\n","\n","Average MAE Loss:\n","[0.04949042 0.0518552  0.05038887 0.05020171]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04004971 0.05891122]\n"," [0.0416077  0.06210271]\n"," [0.04037802 0.06059973]\n"," [0.0405072  0.06004353]]\n","\n","Average MAE Loss:\n","[0.04948047 0.0518552  0.05048888 0.05027536]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03997992 0.0588678 ]\n"," [0.0416077  0.06210271]\n"," [0.04028259 0.06052433]\n"," [0.04055194 0.06006818]]\n","\n","Average MAE Loss:\n","[0.04942386 0.0518552  0.05040346 0.05031006]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04001455 0.05892332]\n"," [0.0416077  0.06210271]\n"," [0.04035899 0.06062508]\n"," [0.04055451 0.06006276]]\n","\n","Average MAE Loss:\n","[0.04946893 0.0518552  0.05049204 0.05030864]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03995021 0.05888957]\n"," [0.0416077  0.06210271]\n"," [0.04043352 0.06065451]\n"," [0.04056202 0.06006851]]\n","\n","Average MAE Loss:\n","[0.04941989 0.0518552  0.05054401 0.05031526]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04046752 0.05911814]\n"," [0.04056202 0.06006851]\n"," [0.03986075 0.05932702]\n"," [0.04012595 0.06036254]]\n","\n","Average MAE Loss:\n","[0.04979283 0.05031526 0.04959389 0.05024425]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04015948 0.05908085]\n"," [0.04056202 0.06006851]\n"," [0.03987581 0.05995653]\n"," [0.03994669 0.06010736]]\n","\n","Average MAE Loss:\n","[0.04962016 0.05031526 0.04991617 0.05002703]\n","\n","Epoch 00037: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04020367 0.05910775]\n"," [0.04056202 0.06006851]\n"," [0.04004253 0.0602656 ]\n"," [0.03995408 0.05997877]]\n","\n","Average MAE Loss:\n","[0.04965571 0.05031526 0.05015407 0.04996642]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04021315 0.05907082]\n"," [0.04056202 0.06006851]\n"," [0.04013822 0.06038544]\n"," [0.04001358 0.05992   ]]\n","\n","Average MAE Loss:\n","[0.04964199 0.05031526 0.05026183 0.04996679]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04015921 0.05906969]\n"," [0.04056202 0.06006851]\n"," [0.04018919 0.06046421]\n"," [0.04007415 0.05989518]]\n","\n","Average MAE Loss:\n","[0.04961445 0.05031526 0.0503267  0.04998467]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04016492 0.05907219]\n"," [0.04056202 0.06006851]\n"," [0.0402218  0.06048138]\n"," [0.04013607 0.05990572]]\n","\n","Average MAE Loss:\n","[0.04961856 0.05031526 0.05035159 0.05002089]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04019292 0.05908503]\n"," [0.04056202 0.06006851]\n"," [0.04025555 0.0604994 ]\n"," [0.04018854 0.05991855]]\n","\n","Average MAE Loss:\n","[0.04963897 0.05031526 0.05037747 0.05005355]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03985231 0.05929029]\n"," [0.03986869 0.05953729]\n"," [0.03985488 0.05967895]\n"," [0.03988292 0.05953132]]\n","\n","Average MAE Loss:\n","[0.0495713  0.04970299 0.04976691 0.04970712]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03997045 0.0590996 ]\n"," [0.03986869 0.05953729]\n"," [0.03992688 0.05990226]\n"," [0.03991578 0.05954502]]\n","\n","Average MAE Loss:\n","[0.04953502 0.04970299 0.04991457 0.0497304 ]\n","\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04008061 0.05904442]\n"," [0.03986869 0.05953729]\n"," [0.03997279 0.05999003]\n"," [0.03996203 0.05957298]]\n","\n","Average MAE Loss:\n","[0.04956251 0.04970299 0.04998141 0.04976751]\n","\n","Epoch 00045: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04010054 0.05904657]\n"," [0.03986869 0.05953729]\n"," [0.04001935 0.06007433]\n"," [0.04000584 0.05960045]]\n","\n","Average MAE Loss:\n","[0.04957356 0.04970299 0.05004684 0.04980315]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04011507 0.05904767]\n"," [0.03986869 0.05953729]\n"," [0.04004971 0.06014261]\n"," [0.04005306 0.05962981]]\n","\n","Average MAE Loss:\n","[0.04958137 0.04970299 0.05009616 0.04984144]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04012866 0.05905013]\n"," [0.03986869 0.05953729]\n"," [0.04008803 0.06020547]\n"," [0.04007281 0.05964486]]\n","\n","Average MAE Loss:\n","[0.0495894  0.04970299 0.05014675 0.04985883]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04013338 0.05904508]\n"," [0.03986869 0.05953729]\n"," [0.04010592 0.06023099]\n"," [0.0400935  0.05965975]]\n","\n","Average MAE Loss:\n","[0.04958923 0.04970299 0.05016845 0.04987663]\n","\n","Epoch 00049: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03984447 0.05945664]\n"," [0.0400935  0.05965975]\n"," [0.04008885 0.0590791 ]\n"," [0.04005747 0.06018268]]\n","\n","Average MAE Loss:\n","[0.04965056 0.04987663 0.04958398 0.05012007]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03983964 0.05935798]\n"," [0.0400935  0.05965975]\n"," [0.04002129 0.05913135]\n"," [0.03999138 0.06011026]]\n","\n","Average MAE Loss:\n","[0.04959881 0.04987663 0.04957632 0.05005082]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03986019 0.05928537]\n"," [0.0400935  0.05965975]\n"," [0.03996597 0.05918817]\n"," [0.03996553 0.06007779]]\n","\n","Average MAE Loss:\n","[0.04957278 0.04987663 0.04957707 0.05002166]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03988695 0.05922636]\n"," [0.0400935  0.05965975]\n"," [0.03992382 0.05924793]\n"," [0.03994461 0.06004951]]\n","\n","Average MAE Loss:\n","[0.04955665 0.04987663 0.04958587 0.04999706]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03989854 0.0592033 ]\n"," [0.0400935  0.05965975]\n"," [0.03989838 0.05932118]\n"," [0.03992801 0.06002533]]\n","\n","Average MAE Loss:\n","[0.04955092 0.04987663 0.04960978 0.04997667]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03990941 0.0591839 ]\n"," [0.0400935  0.05965975]\n"," [0.03988949 0.05940202]\n"," [0.03991452 0.06000388]]\n","\n","Average MAE Loss:\n","[0.04954666 0.04987663 0.04964576 0.0499592 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03991938 0.05916589]\n"," [0.0400935  0.05965975]\n"," [0.03988888 0.05943872]\n"," [0.03990949 0.0599946 ]]\n","\n","Average MAE Loss:\n","[0.04954264 0.04987663 0.0496638  0.04995205]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03978914 0.05938825]\n"," [0.03979551 0.05943394]\n"," [0.03979514 0.05947243]\n"," [0.03979531 0.05943284]]\n","\n","Average MAE Loss:\n","[0.0495887  0.04961473 0.04963378 0.04961407]\n","\n","Epoch 00057: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03979046 0.05936483]\n"," [0.03979551 0.05943394]\n"," [0.03979749 0.05950909]\n"," [0.03979567 0.05943403]]\n","\n","Average MAE Loss:\n","[0.04957764 0.04961473 0.04965329 0.04961485]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03979421 0.05934307]\n"," [0.03979551 0.05943394]\n"," [0.03980256 0.05954704]\n"," [0.0397965  0.05943565]]\n","\n","Average MAE Loss:\n","[0.04956864 0.04961473 0.0496748  0.04961608]\n","\n","Epoch 00059: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03980017 0.05932352]\n"," [0.03979551 0.05943394]\n"," [0.03980572 0.05956547]\n"," [0.03979768 0.05943751]]\n","\n","Average MAE Loss:\n","[0.04956185 0.04961473 0.04968559 0.0496176 ]\n","\n","\n","epochs finished with time:48.4274115562439\n","\n","[[0.03980017 0.05932352]\n"," [0.03979551 0.05943394]\n"," [0.03980572 0.05956547]\n"," [0.03979768 0.05943751]]\n","00:01:31.104647212999225\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08620406 0.08987431]\n"," [0.10227573 0.10665325]\n"," [0.06738078 0.06995083]\n"," [0.06864773 0.0676787 ]]\n","\n","Average MAE Loss:\n","[0.08803919 0.10446449 0.06866581 0.06816322]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08504242 0.08940801]\n"," [0.10227573 0.10665325]\n"," [0.06182925 0.06414675]\n"," [0.06301571 0.06551763]]\n","\n","Average MAE Loss:\n","[0.08722521 0.10446449 0.062988   0.06426667]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08530316 0.0893191 ]\n"," [0.10227573 0.10665325]\n"," [0.0621103  0.06453025]\n"," [0.061936   0.0627574 ]]\n","\n","Average MAE Loss:\n","[0.08731113 0.10446449 0.06332027 0.0623467 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08548911 0.08957022]\n"," [0.10227573 0.10665325]\n"," [0.0618384  0.06448017]\n"," [0.06298547 0.06538318]]\n","\n","Average MAE Loss:\n","[0.08752966 0.10446449 0.06315929 0.06418433]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0854921  0.08952421]\n"," [0.10227573 0.10665325]\n"," [0.06196151 0.06462971]\n"," [0.06315376 0.06523491]]\n","\n","Average MAE Loss:\n","[0.08750815 0.10446449 0.06329561 0.06419433]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08558246 0.08962713]\n"," [0.10227573 0.10665325]\n"," [0.0613235  0.06408396]\n"," [0.06335857 0.06609835]]\n","\n","Average MAE Loss:\n","[0.0876048  0.10446449 0.06270373 0.06472846]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08642969 0.09048409]\n"," [0.10227573 0.10665325]\n"," [0.06188529 0.06447795]\n"," [0.06304597 0.0653521 ]]\n","\n","Average MAE Loss:\n","[0.08845689 0.10446449 0.06318162 0.06419903]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06117788 0.06523182]\n"," [0.06304597 0.0653521 ]\n"," [0.08659829 0.0907281 ]\n"," [0.06266543 0.06537642]]\n","\n","Average MAE Loss:\n","[0.06320485 0.06419903 0.0886632  0.06402092]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.0584054  0.06217065]\n"," [0.06304597 0.0653521 ]\n"," [0.08580003 0.08985195]\n"," [0.06243277 0.06479782]]\n","\n","Average MAE Loss:\n","[0.06028803 0.06419903 0.08782599 0.06361529]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05766376 0.06071392]\n"," [0.06304597 0.0653521 ]\n"," [0.08562694 0.08972052]\n"," [0.06290369 0.06559219]]\n","\n","Average MAE Loss:\n","[0.05918884 0.06419903 0.08767373 0.06424794]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05855955 0.0622753 ]\n"," [0.06304597 0.0653521 ]\n"," [0.08536212 0.08936226]\n"," [0.06261929 0.06525283]]\n","\n","Average MAE Loss:\n","[0.06041742 0.06419903 0.08736219 0.06393606]\n","\n","Epoch 00011: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05858794 0.06178004]\n"," [0.06304597 0.0653521 ]\n"," [0.08523867 0.08923627]\n"," [0.06122238 0.0634051 ]]\n","\n","Average MAE Loss:\n","[0.06018399 0.06419903 0.08723747 0.06231374]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05859238 0.06175237]\n"," [0.06304597 0.0653521 ]\n"," [0.08497923 0.08887619]\n"," [0.06049662 0.06195259]]\n","\n","Average MAE Loss:\n","[0.06017237 0.06419903 0.08692771 0.06122461]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05875634 0.06200127]\n"," [0.06304597 0.0653521 ]\n"," [0.08470494 0.08841863]\n"," [0.06080442 0.06235898]]\n","\n","Average MAE Loss:\n","[0.0603788  0.06419903 0.08656178 0.0615817 ]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06556734 0.06990175]\n"," [0.09971352 0.10411811]\n"," [0.06324133 0.06680815]\n"," [0.07611379 0.08040297]]\n","\n","Average MAE Loss:\n","[0.06773455 0.10191581 0.06502474 0.07825838]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.05198498 0.0545355 ]\n"," [0.09971352 0.10411811]\n"," [0.05086677 0.05288232]\n"," [0.05147874 0.05186873]]\n","\n","Average MAE Loss:\n","[0.05326024 0.10191581 0.05187455 0.05167373]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04765079 0.04926844]\n"," [0.09971352 0.10411811]\n"," [0.0484152  0.04970516]\n"," [0.05150125 0.05300971]]\n","\n","Average MAE Loss:\n","[0.04845961 0.10191581 0.04906018 0.05225548]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.04863086 0.05047774]\n"," [0.09971352 0.10411811]\n"," [0.04843536 0.04987087]\n"," [0.05040343 0.05141431]]\n","\n","Average MAE Loss:\n","[0.0495543  0.10191581 0.04915312 0.05090887]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04830003 0.04989201]\n"," [0.09971352 0.10411811]\n"," [0.04860017 0.04998713]\n"," [0.05055761 0.05165702]]\n","\n","Average MAE Loss:\n","[0.04909602 0.10191581 0.04929365 0.05110731]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04811684 0.0497987 ]\n"," [0.09971352 0.10411811]\n"," [0.04826059 0.0494363 ]\n"," [0.05058287 0.05178741]]\n","\n","Average MAE Loss:\n","[0.04895777 0.10191581 0.04884845 0.05118514]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04831102 0.05001573]\n"," [0.09971352 0.10411811]\n"," [0.04832545 0.04957867]\n"," [0.05052286 0.05161241]]\n","\n","Average MAE Loss:\n","[0.04916338 0.10191581 0.04895206 0.05106763]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09505154 0.09944936]\n"," [0.05052286 0.05161241]\n"," [0.0482585  0.0494721 ]\n"," [0.04900409 0.05027125]]\n","\n","Average MAE Loss:\n","[0.09725045 0.05106763 0.0488653  0.04963767]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.08479741 0.08935413]\n"," [0.05052286 0.05161241]\n"," [0.04863973 0.05008491]\n"," [0.04944945 0.05023733]]\n","\n","Average MAE Loss:\n","[0.08707577 0.05106763 0.04936232 0.04984339]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.05319773 0.05549071]\n"," [0.05052286 0.05161241]\n"," [0.0484545  0.04985771]\n"," [0.0496369  0.05049233]]\n","\n","Average MAE Loss:\n","[0.05434422 0.05106763 0.0491561  0.05006462]\n","\n","Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.05080201 0.05302597]\n"," [0.05052286 0.05161241]\n"," [0.04841752 0.04981529]\n"," [0.04964837 0.05055615]]\n","\n","Average MAE Loss:\n","[0.05191399 0.05106763 0.04911641 0.05010226]\n","\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04859004 0.04995324]\n"," [0.05052286 0.05161241]\n"," [0.04836425 0.04979636]\n"," [0.04958459 0.05046248]]\n","\n","Average MAE Loss:\n","[0.04927164 0.05106763 0.0490803  0.05002354]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04870709 0.0504127 ]\n"," [0.05052286 0.05161241]\n"," [0.04838459 0.04979089]\n"," [0.04962876 0.05054177]]\n","\n","Average MAE Loss:\n","[0.0495599  0.05106763 0.04908774 0.05008527]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04847255 0.0501185 ]\n"," [0.05052286 0.05161241]\n"," [0.04838696 0.04977537]\n"," [0.04959861 0.05049973]]\n","\n","Average MAE Loss:\n","[0.04929552 0.05106763 0.04908117 0.05004917]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05119463 0.05351971]\n"," [0.05587482 0.05907378]\n"," [0.04920828 0.05069826]\n"," [0.0481116  0.0483008 ]]\n","\n","Average MAE Loss:\n","[0.05235717 0.0574743  0.04995327 0.0482062 ]\n","\n","Epoch 00029: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04921975 0.0509383 ]\n"," [0.05587482 0.05907378]\n"," [0.0475601  0.04812692]\n"," [0.04932163 0.05036547]]\n","\n","Average MAE Loss:\n","[0.05007903 0.0574743  0.04784351 0.04984355]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04833631 0.0497442 ]\n"," [0.05587482 0.05907378]\n"," [0.04799479 0.04913529]\n"," [0.04913144 0.0498883 ]]\n","\n","Average MAE Loss:\n","[0.04904026 0.0574743  0.04856504 0.04950987]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04801293 0.04929509]\n"," [0.05587482 0.05907378]\n"," [0.04785303 0.04891881]\n"," [0.0492521  0.0500708 ]]\n","\n","Average MAE Loss:\n","[0.04865401 0.0574743  0.04838592 0.04966145]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04788995 0.04913175]\n"," [0.05587482 0.05907378]\n"," [0.04787848 0.04897831]\n"," [0.04931531 0.05023416]]\n","\n","Average MAE Loss:\n","[0.04851085 0.0574743  0.0484284  0.04977474]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04785063 0.04908076]\n"," [0.05587482 0.05907378]\n"," [0.04783664 0.04898342]\n"," [0.04926891 0.05015514]]\n","\n","Average MAE Loss:\n","[0.04846569 0.0574743  0.04841003 0.04971202]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04781435 0.04902941]\n"," [0.05587482 0.05907378]\n"," [0.04785002 0.04902992]\n"," [0.04927683 0.05017374]]\n","\n","Average MAE Loss:\n","[0.04842188 0.0574743  0.04843997 0.04972528]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05461377 0.05764059]\n"," [0.04927683 0.05017374]\n"," [0.04764593 0.04875556]\n"," [0.0481424  0.04941215]]\n","\n","Average MAE Loss:\n","[0.05612718 0.04972528 0.04820075 0.04877727]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05288384 0.05560909]\n"," [0.04927683 0.05017374]\n"," [0.04749843 0.04848733]\n"," [0.04825915 0.04940809]]\n","\n","Average MAE Loss:\n","[0.05424646 0.04972528 0.04799288 0.04883362]\n","\n","Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05147141 0.05389306]\n"," [0.04927683 0.05017374]\n"," [0.04744513 0.04841623]\n"," [0.04832952 0.04942337]]\n","\n","Average MAE Loss:\n","[0.05268223 0.04972528 0.04793068 0.04887644]\n","\n","Epoch 00038: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05040952 0.05252979]\n"," [0.04927683 0.05017374]\n"," [0.04743759 0.04841182]\n"," [0.0484283  0.04949   ]]\n","\n","Average MAE Loss:\n","[0.05146966 0.04972528 0.04792471 0.04895915]\n","\n","Epoch 00039: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05000561 0.05199679]\n"," [0.04927683 0.05017374]\n"," [0.04743772 0.04841691]\n"," [0.04852071 0.04953764]]\n","\n","Average MAE Loss:\n","[0.0510012  0.04972528 0.04792731 0.04902918]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04967198 0.05155338]\n"," [0.04927683 0.05017374]\n"," [0.04745006 0.04844045]\n"," [0.04862085 0.04960994]]\n","\n","Average MAE Loss:\n","[0.05061268 0.04972528 0.04794526 0.04911539]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04938673 0.05117631]\n"," [0.04927683 0.05017374]\n"," [0.04746597 0.04846786]\n"," [0.04866697 0.04964492]]\n","\n","Average MAE Loss:\n","[0.05028152 0.04972528 0.04796691 0.04915594]\n","\n","Epoch 00042: reducing learning rate of group 0 to 7.8125e-06.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04841907 0.04958868]\n"," [0.04855894 0.04978687]\n"," [0.04848274 0.04967794]\n"," [0.04853619 0.04970533]]\n","\n","Average MAE Loss:\n","[0.04900387 0.0491729  0.04908034 0.04912076]\n","\n","Epoch 00043: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04837942 0.04953908]\n"," [0.04855894 0.04978687]\n"," [0.04836062 0.04950981]\n"," [0.04850944 0.04960905]]\n","\n","Average MAE Loss:\n","[0.04895925 0.0491729  0.04893522 0.04905924]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04834576 0.04949975]\n"," [0.04855894 0.04978687]\n"," [0.04824923 0.04936122]\n"," [0.04851232 0.04956592]]\n","\n","Average MAE Loss:\n","[0.04892275 0.0491729  0.04880522 0.04903912]\n","\n","Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04831377 0.0494636 ]\n"," [0.04855894 0.04978687]\n"," [0.04815516 0.04924064]\n"," [0.04852508 0.04956249]]\n","\n","Average MAE Loss:\n","[0.04888869 0.0491729  0.0486979  0.04904378]\n","\n","Epoch 00046: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04828277 0.04942868]\n"," [0.04855894 0.04978687]\n"," [0.04811715 0.04919295]\n"," [0.04853761 0.04956063]]\n","\n","Average MAE Loss:\n","[0.04885572 0.0491729  0.04865505 0.04904912]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04826761 0.04941166]\n"," [0.04855894 0.04978687]\n"," [0.0480844  0.04915278]\n"," [0.04855089 0.04956202]]\n","\n","Average MAE Loss:\n","[0.04883964 0.0491729  0.04861859 0.04905645]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04825274 0.04939527]\n"," [0.04855894 0.04978687]\n"," [0.04805466 0.04911692]\n"," [0.04856534 0.0495641 ]]\n","\n","Average MAE Loss:\n","[0.048824   0.0491729  0.04858579 0.04906472]\n","\n","Epoch 00049: reducing learning rate of group 0 to 3.9063e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0485395  0.04976298]\n"," [0.04856534 0.0495641 ]\n"," [0.0482095  0.04933552]\n"," [0.04807031 0.04913024]]\n","\n","Average MAE Loss:\n","[0.04915124 0.04906472 0.04877251 0.04860028]\n","\n","Epoch 00050: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04851807 0.04973575]\n"," [0.04856534 0.0495641 ]\n"," [0.04818343 0.0492993 ]\n"," [0.0480966  0.0491564 ]]\n","\n","Average MAE Loss:\n","[0.04912691 0.04906472 0.04874136 0.0486265 ]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04850749 0.04972236]\n"," [0.04856534 0.0495641 ]\n"," [0.04815814 0.0492637 ]\n"," [0.04812355 0.04918285]]\n","\n","Average MAE Loss:\n","[0.04911493 0.04906472 0.04871092 0.0486532 ]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04849708 0.04970919]\n"," [0.04856534 0.0495641 ]\n"," [0.04813381 0.04922963]\n"," [0.04814899 0.04920741]]\n","\n","Average MAE Loss:\n","[0.04910313 0.04906472 0.04868172 0.0486782 ]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04848678 0.04969626]\n"," [0.04856534 0.0495641 ]\n"," [0.04811057 0.04919793]\n"," [0.0481611  0.04921884]]\n","\n","Average MAE Loss:\n","[0.04909152 0.04906472 0.04865425 0.04868997]\n","\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04847663 0.04968359]\n"," [0.04856534 0.0495641 ]\n"," [0.04809948 0.04918297]\n"," [0.04817295 0.04923042]]\n","\n","Average MAE Loss:\n","[0.04908011 0.04906472 0.04864123 0.04870169]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04847159 0.0496773 ]\n"," [0.04856534 0.0495641 ]\n"," [0.04808867 0.04916844]\n"," [0.04818447 0.04924158]]\n","\n","Average MAE Loss:\n","[0.04907445 0.04906472 0.04862855 0.04871302]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.0483086  0.04938686]\n"," [0.04831251 0.04939148]\n"," [0.04830104 0.04937602]\n"," [0.04832027 0.04939711]]\n","\n","Average MAE Loss:\n","[0.04884773 0.04885199 0.04883853 0.04885869]\n","\n","Epoch 00057: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04830544 0.04938348]\n"," [0.04831251 0.04939148]\n"," [0.04828867 0.04935943]\n"," [0.04832313 0.04939809]]\n","\n","Average MAE Loss:\n","[0.04884446 0.04885199 0.04882405 0.04886061]\n","\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04830238 0.04938032]\n"," [0.04831251 0.04939148]\n"," [0.04828247 0.04935119]\n"," [0.04832575 0.04939866]]\n","\n","Average MAE Loss:\n","[0.04884135 0.04885199 0.04881683 0.04886221]\n","\n","Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04830084 0.04937875]\n"," [0.04831251 0.04939148]\n"," [0.04827629 0.04934304]\n"," [0.04832825 0.04939914]]\n","\n","Average MAE Loss:\n","[0.0488398  0.04885199 0.04880967 0.0488637 ]\n","\n","\n","epochs finished with time:48.33151602745056\n","\n","[[0.04830084 0.04937875]\n"," [0.04831251 0.04939148]\n"," [0.04827629 0.04934304]\n"," [0.04832825 0.04939914]]\n","00:01:32.22867194900027\n"]}],"source":["# seed = 10 table = 4/8 fixed folds CNN_2\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Simple2_CNN_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awLyDtOtrTvy"},"outputs":[],"source":["# seed = 10 table = 4/8 fixed folds CNN_2 dropout\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Try_Simple2_CNN_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357813,"status":"ok","timestamp":1689945650335,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"HtPulKVJlSLF","outputId":"9c210539-3d2e-4aa3-e058-c1b08e6ad4b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:500\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 1.]\n"," [1. 1. 0.]\n"," [1. 1. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07161249 0.08753487]\n"," [0.06609653 0.08078363]\n"," [0.05705494 0.06765399]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07957368 0.07344008 0.06235446]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07070872 0.08634324]\n"," [0.06591281 0.08030224]\n"," [0.05279149 0.06364293]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07852598 0.07310752 0.05821721]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07447436 0.08846008]\n"," [0.06601703 0.08011893]\n"," [0.05378799 0.0648124 ]]\n","\n","Average MAE Loss:\n","[0.08553374 0.08146722 0.07306798 0.0593002 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07183462 0.08665596]\n"," [0.06706276 0.08087224]\n"," [0.05540858 0.0660313 ]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07924529 0.0739675  0.06071994]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07307006 0.08742145]\n"," [0.06600247 0.08003813]\n"," [0.05101451 0.06192622]]\n","\n","Average MAE Loss:\n","[0.08553374 0.08024576 0.0730203  0.05647036]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07209928 0.08681184]\n"," [0.06482335 0.07938031]\n"," [0.05389551 0.0644877 ]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07945556 0.07210183 0.0591916 ]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06992184 0.08496827]\n"," [0.06624976 0.08027623]\n"," [0.05202268 0.06296932]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07744505 0.07326299 0.057496  ]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.06992184 0.08496827]\n"," [0.06771166 0.08092781]\n"," [0.05525804 0.06758156]\n"," [0.04967266 0.0600095 ]]\n","\n","Average MAE Loss:\n","[0.07744505 0.07431973 0.0614198  0.05484108]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06992184 0.08496827]\n"," [0.06699523 0.08034128]\n"," [0.04876314 0.06124656]\n"," [0.05325876 0.06239355]]\n","\n","Average MAE Loss:\n","[0.07744505 0.07366825 0.05500485 0.05782616]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.06992184 0.08496827]\n"," [0.06471049 0.07839444]\n"," [0.05078295 0.06286309]\n"," [0.05474289 0.06396703]]\n","\n","Average MAE Loss:\n","[0.07744505 0.07155246 0.05682302 0.05935496]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06992184 0.08496827]\n"," [0.06435677 0.07828764]\n"," [0.0495856  0.06195074]\n"," [0.04864528 0.05843472]]\n","\n","Average MAE Loss:\n","[0.07744505 0.0713222  0.05576817 0.05354   ]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.06992184 0.08496827]\n"," [0.06478878 0.07838864]\n"," [0.05048388 0.06257415]\n"," [0.04936918 0.05894806]]\n","\n","Average MAE Loss:\n","[0.07744505 0.07158871 0.05652902 0.05415862]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06992184 0.08496827]\n"," [0.06458234 0.0783114 ]\n"," [0.04991165 0.06224089]\n"," [0.04922908 0.05888702]]\n","\n","Average MAE Loss:\n","[0.07744505 0.07144687 0.05607627 0.05405805]\n","\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06992184 0.08496827]\n"," [0.06440918 0.07821377]\n"," [0.04609246 0.05955784]\n"," [0.04533131 0.05579281]]\n","\n","Average MAE Loss:\n","[0.07744505 0.07131148 0.05282515 0.05056206]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07353277 0.08977309]\n"," [0.05537064 0.06493603]\n"," [0.0420065  0.05416875]\n"," [0.0431536  0.05412664]]\n","\n","Average MAE Loss:\n","[0.08165293 0.06015334 0.04808763 0.04864012]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07353277 0.08977309]\n"," [0.05083882 0.06056449]\n"," [0.0414296  0.05293878]\n"," [0.04469775 0.05459865]]\n","\n","Average MAE Loss:\n","[0.08165293 0.05570166 0.04718419 0.0496482 ]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07353277 0.08977309]\n"," [0.04524108 0.05516217]\n"," [0.03986738 0.05188808]\n"," [0.04234387 0.05246214]]\n","\n","Average MAE Loss:\n","[0.08165293 0.05020162 0.04587773 0.047403  ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07353277 0.08977309]\n"," [0.03896065 0.05079347]\n"," [0.03956371 0.05180519]\n"," [0.03993466 0.05075413]]\n","\n","Average MAE Loss:\n","[0.08165293 0.04487706 0.04568445 0.04534439]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07353277 0.08977309]\n"," [0.03965463 0.05091037]\n"," [0.04013668 0.05201591]\n"," [0.0393759  0.05037167]]\n","\n","Average MAE Loss:\n","[0.08165293 0.0452825  0.0460763  0.04487379]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07353277 0.08977309]\n"," [0.04002951 0.05114811]\n"," [0.04013163 0.05202315]\n"," [0.03979742 0.05053115]]\n","\n","Average MAE Loss:\n","[0.08165293 0.04558881 0.04607739 0.04516428]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07353277 0.08977309]\n"," [0.0392935  0.05058348]\n"," [0.03971574 0.05175852]\n"," [0.03969879 0.05056795]]\n","\n","Average MAE Loss:\n","[0.08165293 0.04493849 0.04573713 0.04513337]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.0392935  0.05058348]\n"," [0.03995292 0.05097921]\n"," [0.03915228 0.05097654]\n"," [0.0525103  0.06175597]]\n","\n","Average MAE Loss:\n","[0.04493849 0.04546607 0.04506441 0.05713313]\n","\n","Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0392935  0.05058348]\n"," [0.03908093 0.05033141]\n"," [0.03930476 0.05148591]\n"," [0.03984134 0.05184954]]\n","\n","Average MAE Loss:\n","[0.04493849 0.04470617 0.04539534 0.04584544]\n","\n","Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.0392935  0.05058348]\n"," [0.03933206 0.05043743]\n"," [0.03986485 0.05176923]\n"," [0.04011274 0.05080193]]\n","\n","Average MAE Loss:\n","[0.04493849 0.04488475 0.04581704 0.04545733]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.0392935  0.05058348]\n"," [0.03929941 0.05045831]\n"," [0.03999566 0.05180055]\n"," [0.03923265 0.05020392]]\n","\n","Average MAE Loss:\n","[0.04493849 0.04487886 0.04589811 0.04471828]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.0392935  0.05058348]\n"," [0.03942689 0.05052967]\n"," [0.03941262 0.05145969]\n"," [0.03929775 0.05021253]]\n","\n","Average MAE Loss:\n","[0.04493849 0.04497828 0.04543616 0.04475514]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.0392935  0.05058348]\n"," [0.0393363  0.05047294]\n"," [0.04025461 0.05198728]\n"," [0.03947946 0.05033277]]\n","\n","Average MAE Loss:\n","[0.04493849 0.04490462 0.04612094 0.04490611]\n","\n","Epoch 00027: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.0392935  0.05058348]\n"," [0.03933159 0.05045916]\n"," [0.03997393 0.05174088]\n"," [0.03916289 0.05012032]]\n","\n","Average MAE Loss:\n","[0.04493849 0.04489538 0.04585741 0.04464161]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03795611 0.04994611]\n"," [0.0381458  0.04958313]\n"," [0.0391312  0.05058866]\n"," [0.03967706 0.04998477]]\n","\n","Average MAE Loss:\n","[0.04395111 0.04386446 0.04485993 0.04483091]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03795611 0.04994611]\n"," [0.0387347  0.04984999]\n"," [0.03915189 0.0508491 ]\n"," [0.03868078 0.04955904]]\n","\n","Average MAE Loss:\n","[0.04395111 0.04429235 0.0450005  0.04411991]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03795611 0.04994611]\n"," [0.03872005 0.04979182]\n"," [0.039469   0.0511601 ]\n"," [0.03893603 0.04967025]]\n","\n","Average MAE Loss:\n","[0.04395111 0.04425594 0.04531455 0.04430314]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03795611 0.04994611]\n"," [0.03879399 0.04983073]\n"," [0.03938738 0.05109054]\n"," [0.03881413 0.04959065]]\n","\n","Average MAE Loss:\n","[0.04395111 0.04431236 0.04523896 0.04420239]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03795611 0.04994611]\n"," [0.03882639 0.04987813]\n"," [0.03933705 0.05106591]\n"," [0.03876944 0.04952193]]\n","\n","Average MAE Loss:\n","[0.04395111 0.04435226 0.04520148 0.04414569]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03795611 0.04994611]\n"," [0.03885132 0.04989309]\n"," [0.03949475 0.05117864]\n"," [0.03887951 0.04961197]]\n","\n","Average MAE Loss:\n","[0.04395111 0.0443722  0.0453367  0.04424574]\n","\n","Epoch 00034: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03795611 0.04994611]\n"," [0.03886706 0.04990185]\n"," [0.03944684 0.05113918]\n"," [0.03875165 0.04949486]]\n","\n","Average MAE Loss:\n","[0.04395111 0.04438445 0.04529301 0.04412325]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03886706 0.04990185]\n"," [0.03906726 0.05074408]\n"," [0.03861074 0.04965997]\n"," [0.0385679  0.04940359]]\n","\n","Average MAE Loss:\n","[0.04438445 0.04490567 0.04413536 0.04398574]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03886706 0.04990185]\n"," [0.03874364 0.05036438]\n"," [0.03883671 0.05022716]\n"," [0.03886677 0.04950707]]\n","\n","Average MAE Loss:\n","[0.04438445 0.04455401 0.04453193 0.04418692]\n","\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00037: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03886706 0.04990185]\n"," [0.03865575 0.05024726]\n"," [0.03917631 0.0506613 ]\n"," [0.03878513 0.04952257]]\n","\n","Average MAE Loss:\n","[0.04438445 0.0444515  0.04491881 0.04415385]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03886706 0.04990185]\n"," [0.03860276 0.05016153]\n"," [0.03934184 0.05088689]\n"," [0.03880319 0.04953499]]\n","\n","Average MAE Loss:\n","[0.04438445 0.04438215 0.04511437 0.04416909]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03886706 0.04990185]\n"," [0.03857984 0.05009827]\n"," [0.03943784 0.05103886]\n"," [0.03882326 0.04954901]]\n","\n","Average MAE Loss:\n","[0.04438445 0.04433905 0.04523835 0.04418614]\n","\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03886706 0.04990185]\n"," [0.03857715 0.05005496]\n"," [0.03948643 0.05110219]\n"," [0.03884593 0.04956021]]\n","\n","Average MAE Loss:\n","[0.04438445 0.04431606 0.04529431 0.04420307]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03886706 0.04990185]\n"," [0.03858067 0.05003819]\n"," [0.03950214 0.05113702]\n"," [0.03883687 0.04955494]]\n","\n","Average MAE Loss:\n","[0.04438445 0.04430943 0.04531958 0.04419591]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03830334 0.04958598]\n"," [0.03831006 0.04958352]\n"," [0.03846546 0.04981242]\n"," [0.03839535 0.04945987]]\n","\n","Average MAE Loss:\n","[0.04394466 0.04394679 0.04413894 0.04392761]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03830334 0.04958598]\n"," [0.03832899 0.04959356]\n"," [0.03873405 0.05016157]\n"," [0.03859299 0.04945591]]\n","\n","Average MAE Loss:\n","[0.04394466 0.04396128 0.04444781 0.04402445]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03830334 0.04958598]\n"," [0.03835281 0.049606  ]\n"," [0.03885066 0.05030884]\n"," [0.0386842  0.04947081]]\n","\n","Average MAE Loss:\n","[0.04394466 0.0439794  0.04457975 0.04407751]\n","\n","Epoch 00045: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03830334 0.04958598]\n"," [0.03836601 0.04961295]\n"," [0.03895004 0.05042616]\n"," [0.03873064 0.04947799]]\n","\n","Average MAE Loss:\n","[0.04394466 0.04398948 0.0446881  0.04410432]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03830334 0.04958598]\n"," [0.03838013 0.0496194 ]\n"," [0.03903176 0.05052362]\n"," [0.03875243 0.04947381]]\n","\n","Average MAE Loss:\n","[0.04394466 0.04399976 0.04477769 0.04411312]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03830334 0.04958598]\n"," [0.03839421 0.04962619]\n"," [0.03909802 0.05060464]\n"," [0.03876584 0.04948695]]\n","\n","Average MAE Loss:\n","[0.04394466 0.0440102  0.04485133 0.04412639]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03830334 0.04958598]\n"," [0.03840729 0.04963237]\n"," [0.03912868 0.0506403 ]\n"," [0.03877273 0.04950047]]\n","\n","Average MAE Loss:\n","[0.04394466 0.04401983 0.04488449 0.0441366 ]\n","\n","Epoch 00049: reducing learning rate of group 0 to 3.9063e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03840729 0.04963237]\n"," [0.03911075 0.05062144]\n"," [0.03874857 0.04951798]\n"," [0.038336   0.04950575]]\n","\n","Average MAE Loss:\n","[0.04401983 0.04486609 0.04413328 0.04392088]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03840729 0.04963237]\n"," [0.03908537 0.05059524]\n"," [0.03872465 0.04955099]\n"," [0.03842558 0.04945514]]\n","\n","Average MAE Loss:\n","[0.04401983 0.0448403  0.04413782 0.04394036]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03840729 0.04963237]\n"," [0.03906033 0.05056893]\n"," [0.03870943 0.04959407]\n"," [0.03852326 0.04945108]]\n","\n","Average MAE Loss:\n","[0.04401983 0.04481463 0.04415175 0.04398717]\n","\n","Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03840729 0.04963237]\n"," [0.0390367  0.05054368]\n"," [0.03870404 0.04961877]\n"," [0.03859096 0.04945846]]\n","\n","Average MAE Loss:\n","[0.04401983 0.04479019 0.0441614  0.04402471]\n","\n","Epoch 00053: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03840729 0.04963237]\n"," [0.03902562 0.05053145]\n"," [0.03870176 0.04964602]\n"," [0.03863981 0.04947139]]\n","\n","Average MAE Loss:\n","[0.04401983 0.04477853 0.04417389 0.0440556 ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03840729 0.04963237]\n"," [0.03901474 0.05051951]\n"," [0.03870227 0.04967501]\n"," [0.03866056 0.04947628]]\n","\n","Average MAE Loss:\n","[0.04401983 0.04476712 0.04418864 0.04406842]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03840729 0.04963237]\n"," [0.03900415 0.05050796]\n"," [0.03870509 0.04970436]\n"," [0.03867453 0.04947892]]\n","\n","Average MAE Loss:\n","[0.04401983 0.04475606 0.04420472 0.04407673]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03850216 0.04964222]\n"," [0.03849903 0.04963828]\n"," [0.03851017 0.04966246]\n"," [0.03850497 0.04960291]]\n","\n","Average MAE Loss:\n","[0.04407219 0.04406866 0.04408631 0.04405394]\n","\n","Epoch 00057: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03850216 0.04964222]\n"," [0.03849798 0.04963722]\n"," [0.03851912 0.04968278]\n"," [0.0385194  0.04956222]]\n","\n","Average MAE Loss:\n","[0.04407219 0.0440676  0.04410095 0.04404081]\n","\n","Epoch 00058: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03850216 0.04964222]\n"," [0.03849704 0.0496363 ]\n"," [0.03852905 0.04970331]\n"," [0.03852945 0.04954585]]\n","\n","Average MAE Loss:\n","[0.04407219 0.04406667 0.04411618 0.04403765]\n","\n","Epoch 00059: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03850216 0.04964222]\n"," [0.03849613 0.04963548]\n"," [0.03853951 0.04972396]\n"," [0.03854104 0.04953471]]\n","\n","Average MAE Loss:\n","[0.04407219 0.0440658  0.04413174 0.04403788]\n","\n","\n","epochs finished with time:49.17244482040405\n","\n","[[0.03850216 0.04964222]\n"," [0.03849613 0.04963548]\n"," [0.03853951 0.04972396]\n"," [0.03854104 0.04953471]]\n","00:01:14.957412120999834\n","------------------------------------Fold [2/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08820359 0.1070202 ]\n"," [0.06499147 0.0828348 ]\n"," [0.07579447 0.09383956]\n"," [0.0555804  0.07151648]]\n","\n","Average MAE Loss:\n","[0.09761189 0.07391314 0.08481702 0.06354844]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08820359 0.1070202 ]\n"," [0.05152011 0.06700295]\n"," [0.0745787  0.09302923]\n"," [0.05727749 0.07014431]]\n","\n","Average MAE Loss:\n","[0.09761189 0.05926153 0.08380396 0.0637109 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08820359 0.1070202 ]\n"," [0.05647535 0.0675881 ]\n"," [0.07417338 0.09278455]\n"," [0.05654665 0.06869617]]\n","\n","Average MAE Loss:\n","[0.09761189 0.06203173 0.08347897 0.06262141]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08820359 0.1070202 ]\n"," [0.05680853 0.06653015]\n"," [0.07426742 0.09270256]\n"," [0.05805771 0.06960949]]\n","\n","Average MAE Loss:\n","[0.09761189 0.06166934 0.08348499 0.0638336 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08820359 0.1070202 ]\n"," [0.05292992 0.06527107]\n"," [0.07487633 0.09274228]\n"," [0.05617306 0.06845107]]\n","\n","Average MAE Loss:\n","[0.09761189 0.05910049 0.08380931 0.06231206]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08820359 0.1070202 ]\n"," [0.05508462 0.066239  ]\n"," [0.07493509 0.09295728]\n"," [0.05376019 0.06672385]]\n","\n","Average MAE Loss:\n","[0.09761189 0.06066181 0.08394619 0.06024202]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08820359 0.1070202 ]\n"," [0.05129339 0.06394336]\n"," [0.0746384  0.0925411 ]\n"," [0.05320028 0.06666835]]\n","\n","Average MAE Loss:\n","[0.09761189 0.05761837 0.08358975 0.05993432]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05129339 0.06394336]\n"," [0.07493836 0.09181427]\n"," [0.05122827 0.06643956]\n"," [0.04958865 0.06681139]]\n","\n","Average MAE Loss:\n","[0.05761837 0.08337632 0.05883391 0.05820002]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05129339 0.06394336]\n"," [0.07617237 0.09117415]\n"," [0.05064228 0.06512607]\n"," [0.05434612 0.06781534]]\n","\n","Average MAE Loss:\n","[0.05761837 0.08367326 0.05788418 0.06108073]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05129339 0.06394336]\n"," [0.0699685  0.08719066]\n"," [0.04994477 0.06521014]\n"," [0.05442805 0.06681573]]\n","\n","Average MAE Loss:\n","[0.05761837 0.07857958 0.05757746 0.06062189]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05129339 0.06394336]\n"," [0.07271391 0.08835374]\n"," [0.05000858 0.06527108]\n"," [0.05211965 0.06535084]]\n","\n","Average MAE Loss:\n","[0.05761837 0.08053383 0.05763983 0.05873525]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05129339 0.06394336]\n"," [0.07016252 0.08679842]\n"," [0.05004399 0.0650437 ]\n"," [0.05389245 0.06655049]]\n","\n","Average MAE Loss:\n","[0.05761837 0.07848047 0.05754384 0.06022147]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05129339 0.06394336]\n"," [0.07218408 0.08778501]\n"," [0.05015137 0.06511131]\n"," [0.04706655 0.06289032]]\n","\n","Average MAE Loss:\n","[0.05761837 0.07998455 0.05763134 0.05497843]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05129339 0.06394336]\n"," [0.06888152 0.08641654]\n"," [0.05014645 0.06501305]\n"," [0.05699434 0.06775101]]\n","\n","Average MAE Loss:\n","[0.05761837 0.07764903 0.05757975 0.06237268]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.08231508 0.10119866]\n"," [0.05805429 0.06623826]\n"," [0.04982902 0.05828925]\n"," [0.05745416 0.06523312]]\n","\n","Average MAE Loss:\n","[0.09175687 0.06214627 0.05405914 0.06134364]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.08231508 0.10119866]\n"," [0.04119221 0.05876132]\n"," [0.04040318 0.054765  ]\n"," [0.04138995 0.05553907]]\n","\n","Average MAE Loss:\n","[0.09175687 0.04997677 0.04758409 0.04846451]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.08231508 0.10119866]\n"," [0.04085431 0.05608855]\n"," [0.04035563 0.055011  ]\n"," [0.04025984 0.05529915]]\n","\n","Average MAE Loss:\n","[0.09175687 0.04847143 0.04768332 0.04777949]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.08231508 0.10119866]\n"," [0.03955435 0.05600832]\n"," [0.04096213 0.05447187]\n"," [0.03942492 0.05552677]]\n","\n","Average MAE Loss:\n","[0.09175687 0.04778133 0.047717   0.04747584]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.08231508 0.10119866]\n"," [0.03978958 0.0556496 ]\n"," [0.04064124 0.05452243]\n"," [0.03985014 0.05489758]]\n","\n","Average MAE Loss:\n","[0.09175687 0.04771959 0.04758183 0.04737386]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.08231508 0.10119866]\n"," [0.0396458  0.0555841 ]\n"," [0.04038739 0.05458131]\n"," [0.03951573 0.05510138]]\n","\n","Average MAE Loss:\n","[0.09175687 0.04761495 0.04748435 0.04730855]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.08231508 0.10119866]\n"," [0.03966184 0.05551557]\n"," [0.04067681 0.0544233 ]\n"," [0.03956212 0.05494748]]\n","\n","Average MAE Loss:\n","[0.09175687 0.0475887  0.04755006 0.0472548 ]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.03966184 0.05551557]\n"," [0.03904591 0.05457236]\n"," [0.04109432 0.05506878]\n"," [0.05550586 0.07420232]]\n","\n","Average MAE Loss:\n","[0.0475887  0.04680913 0.04808155 0.06485409]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.03966184 0.05551557]\n"," [0.03957471 0.0547068 ]\n"," [0.04017819 0.05444099]\n"," [0.04576766 0.05800308]]\n","\n","Average MAE Loss:\n","[0.0475887  0.04714075 0.04730959 0.05188537]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.03966184 0.05551557]\n"," [0.03948853 0.05502255]\n"," [0.04053075 0.05426874]\n"," [0.03968934 0.05600775]]\n","\n","Average MAE Loss:\n","[0.0475887  0.04725554 0.04739974 0.04784855]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.03966184 0.05551557]\n"," [0.03957556 0.05499253]\n"," [0.04028799 0.05423098]\n"," [0.04002187 0.05493779]]\n","\n","Average MAE Loss:\n","[0.0475887  0.04728405 0.04725948 0.04747983]\n","\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.03966184 0.05551557]\n"," [0.03964047 0.05500408]\n"," [0.04015673 0.05433018]\n"," [0.03932548 0.05486127]]\n","\n","Average MAE Loss:\n","[0.0475887  0.04732228 0.04724345 0.04709337]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03966184 0.05551557]\n"," [0.03963624 0.05496521]\n"," [0.04022578 0.05418315]\n"," [0.03925845 0.05482322]]\n","\n","Average MAE Loss:\n","[0.0475887  0.04730073 0.04720446 0.04704083]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03966184 0.05551557]\n"," [0.03959189 0.05494368]\n"," [0.04019789 0.05430909]\n"," [0.03920092 0.05477319]]\n","\n","Average MAE Loss:\n","[0.0475887  0.04726779 0.04725349 0.04698706]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03876895 0.05603474]\n"," [0.03826061 0.05462295]\n"," [0.04027959 0.05426278]\n"," [0.03870132 0.05402009]]\n","\n","Average MAE Loss:\n","[0.04740185 0.04644178 0.04727118 0.04636071]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03876895 0.05603474]\n"," [0.03885423 0.05438422]\n"," [0.04014551 0.05398626]\n"," [0.03895904 0.05435492]]\n","\n","Average MAE Loss:\n","[0.04740185 0.04661923 0.04706589 0.04665698]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03876895 0.05603474]\n"," [0.03898937 0.05455653]\n"," [0.03965343 0.05403575]\n"," [0.03908931 0.05444275]]\n","\n","Average MAE Loss:\n","[0.04740185 0.04677295 0.04684459 0.04676603]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03876895 0.05603474]\n"," [0.03909684 0.05457986]\n"," [0.04006549 0.05390397]\n"," [0.03905486 0.05449201]]\n","\n","Average MAE Loss:\n","[0.04740185 0.04683835 0.04698473 0.04677343]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03876895 0.05603474]\n"," [0.03916026 0.05454895]\n"," [0.0398924  0.05395125]\n"," [0.03906977 0.05447607]]\n","\n","Average MAE Loss:\n","[0.04740185 0.04685461 0.04692182 0.04677292]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03876895 0.05603474]\n"," [0.03915984 0.05452576]\n"," [0.03972471 0.05389808]\n"," [0.03904024 0.05447184]]\n","\n","Average MAE Loss:\n","[0.04740185 0.0468428  0.0468114  0.04675604]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03876895 0.05603474]\n"," [0.03914979 0.05451717]\n"," [0.03995482 0.05391269]\n"," [0.03904414 0.05449131]]\n","\n","Average MAE Loss:\n","[0.04740185 0.04683348 0.04693375 0.04676773]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03914979 0.05451717]\n"," [0.0393277  0.05372198]\n"," [0.04049398 0.05451304]\n"," [0.03816075 0.05439183]]\n","\n","Average MAE Loss:\n","[0.04683348 0.04652484 0.04750351 0.04627629]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03914979 0.05451717]\n"," [0.03884724 0.05367573]\n"," [0.03990646 0.05412672]\n"," [0.0385552  0.05403035]]\n","\n","Average MAE Loss:\n","[0.04683348 0.04626149 0.04701659 0.04629277]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03914979 0.05451717]\n"," [0.03865374 0.05372808]\n"," [0.04002333 0.05391976]\n"," [0.03870895 0.05420036]]\n","\n","Average MAE Loss:\n","[0.04683348 0.04619091 0.04697154 0.04645466]\n","\n","Epoch 00038: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03914979 0.05451717]\n"," [0.03860041 0.05381043]\n"," [0.03983014 0.05386204]\n"," [0.03881838 0.05428898]]\n","\n","Average MAE Loss:\n","[0.04683348 0.04620542 0.04684609 0.04655368]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03914979 0.05451717]\n"," [0.03862878 0.05391049]\n"," [0.03985991 0.05383388]\n"," [0.03889331 0.05435461]]\n","\n","Average MAE Loss:\n","[0.04683348 0.04626964 0.04684689 0.04662396]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03914979 0.05451717]\n"," [0.03868746 0.05400335]\n"," [0.0398434  0.0538516 ]\n"," [0.03891372 0.05436591]]\n","\n","Average MAE Loss:\n","[0.04683348 0.0463454  0.0468475  0.04663982]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03914979 0.05451717]\n"," [0.0387539  0.05407493]\n"," [0.03978451 0.05385593]\n"," [0.03892165 0.05437452]]\n","\n","Average MAE Loss:\n","[0.04683348 0.04641441 0.04682022 0.04664808]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00042: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03854665 0.05366724]\n"," [0.03854096 0.05373019]\n"," [0.03887628 0.05352927]\n"," [0.03839555 0.05371224]]\n","\n","Average MAE Loss:\n","[0.04610695 0.04613557 0.04620278 0.0460539 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03854665 0.05366724]\n"," [0.03855741 0.05381027]\n"," [0.03926748 0.05360178]\n"," [0.03833287 0.0538095 ]]\n","\n","Average MAE Loss:\n","[0.04610695 0.04618384 0.04643463 0.04607119]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03854665 0.05366724]\n"," [0.03858833 0.05388371]\n"," [0.03948052 0.05370345]\n"," [0.03840052 0.05389564]]\n","\n","Average MAE Loss:\n","[0.04610695 0.04623602 0.04659198 0.04614808]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03854665 0.05366724]\n"," [0.03861565 0.05394919]\n"," [0.03958185 0.05374233]\n"," [0.03850251 0.05398097]]\n","\n","Average MAE Loss:\n","[0.04610695 0.04628242 0.04666209 0.04624174]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03854665 0.05366724]\n"," [0.03865132 0.05400748]\n"," [0.03963977 0.05377194]\n"," [0.03860182 0.0540572 ]]\n","\n","Average MAE Loss:\n","[0.04610695 0.0463294  0.04670585 0.04632951]\n","\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03854665 0.05366724]\n"," [0.03867129 0.0540314 ]\n"," [0.03965004 0.05377157]\n"," [0.03864222 0.0540804 ]]\n","\n","Average MAE Loss:\n","[0.04610695 0.04635134 0.04671081 0.04636131]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03854665 0.05366724]\n"," [0.03869191 0.05405191]\n"," [0.03965513 0.05376448]\n"," [0.03866787 0.05410289]]\n","\n","Average MAE Loss:\n","[0.04610695 0.04637191 0.0467098  0.04638538]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03869191 0.05405191]\n"," [0.03949478 0.05369873]\n"," [0.0387389  0.05393733]\n"," [0.03845738 0.05369128]]\n","\n","Average MAE Loss:\n","[0.04637191 0.04659676 0.04633812 0.04607433]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03869191 0.05405191]\n"," [0.03930504 0.053625  ]\n"," [0.03898169 0.05382051]\n"," [0.03837205 0.05373778]]\n","\n","Average MAE Loss:\n","[0.04637191 0.04646502 0.0464011  0.04605491]\n","\n","Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00051: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00051: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03869191 0.05405191]\n"," [0.03922023 0.05359566]\n"," [0.03908295 0.05380256]\n"," [0.03835219 0.05376202]]\n","\n","Average MAE Loss:\n","[0.04637191 0.04640795 0.04644275 0.0460571 ]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03869191 0.05405191]\n"," [0.03914386 0.05357383]\n"," [0.0391624  0.05379432]\n"," [0.03834495 0.05378321]]\n","\n","Average MAE Loss:\n","[0.04637191 0.04635885 0.04647836 0.04606408]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03869191 0.05405191]\n"," [0.0390734  0.0535582 ]\n"," [0.03923539 0.05378217]\n"," [0.03834831 0.05380327]]\n","\n","Average MAE Loss:\n","[0.04637191 0.0463158  0.04650878 0.04607579]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03869191 0.05405191]\n"," [0.03900947 0.05354803]\n"," [0.03930495 0.05377696]\n"," [0.03836043 0.05382257]]\n","\n","Average MAE Loss:\n","[0.04637191 0.04627875 0.04654096 0.0460915 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00055: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03869191 0.05405191]\n"," [0.03898014 0.05354472]\n"," [0.0393303  0.05377292]\n"," [0.03836882 0.0538308 ]]\n","\n","Average MAE Loss:\n","[0.04637191 0.04626243 0.04655161 0.04609981]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03858075 0.05360588]\n"," [0.03856669 0.05361462]\n"," [0.03864343 0.0535761 ]\n"," [0.0385532  0.05361502]]\n","\n","Average MAE Loss:\n","[0.04609331 0.04609066 0.04610976 0.04608411]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03858075 0.05360588]\n"," [0.0385568  0.05362386]\n"," [0.03872848 0.05354333]\n"," [0.03851441 0.05362796]]\n","\n","Average MAE Loss:\n","[0.04609331 0.04609033 0.04613591 0.04607118]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03858075 0.05360588]\n"," [0.03854921 0.0536327 ]\n"," [0.03879899 0.05352571]\n"," [0.03848023 0.0536417 ]]\n","\n","Average MAE Loss:\n","[0.04609331 0.04609096 0.04616235 0.04606097]\n","\n","Epoch 00059: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03858075 0.05360588]\n"," [0.038543   0.05364119]\n"," [0.03885975 0.05352185]\n"," [0.03846532 0.05364844]]\n","\n","Average MAE Loss:\n","[0.04609331 0.0460921  0.0461908  0.04605688]\n","\n","\n","epochs finished with time:49.60728573799133\n","\n","[[0.03858075 0.05360588]\n"," [0.038543   0.05364119]\n"," [0.03885975 0.05352185]\n"," [0.03846532 0.05364844]]\n","00:01:13.550318051999966\n","------------------------------------Fold [3/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08764951 0.09420715]\n"," [0.08806124 0.09229367]\n"," [0.07562431 0.08114851]\n"," [0.0534669  0.05855686]]\n","\n","Average MAE Loss:\n","[0.09092833 0.09017746 0.07838641 0.05601188]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08764951 0.09420715]\n"," [0.08105531 0.08577695]\n"," [0.07456529 0.0805647 ]\n"," [0.05023362 0.05602484]]\n","\n","Average MAE Loss:\n","[0.09092833 0.08341613 0.077565   0.05312923]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08764951 0.09420715]\n"," [0.07933713 0.08453522]\n"," [0.07471435 0.07984799]\n"," [0.05023158 0.05551099]]\n","\n","Average MAE Loss:\n","[0.09092833 0.08193618 0.07728117 0.05287129]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08764951 0.09420715]\n"," [0.07979083 0.08478408]\n"," [0.07466492 0.07997608]\n"," [0.04971924 0.05493357]]\n","\n","Average MAE Loss:\n","[0.09092833 0.08228746 0.0773205  0.05232641]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08764951 0.09420715]\n"," [0.08044238 0.08482166]\n"," [0.0746811  0.07991631]\n"," [0.04984786 0.05499035]]\n","\n","Average MAE Loss:\n","[0.09092833 0.08263202 0.07729871 0.05241911]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08764951 0.09420715]\n"," [0.07990469 0.08464977]\n"," [0.07416341 0.07960283]\n"," [0.04967003 0.05486296]]\n","\n","Average MAE Loss:\n","[0.09092833 0.08227723 0.07688312 0.0522665 ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08764951 0.09420715]\n"," [0.07975477 0.08461242]\n"," [0.07472381 0.07997254]\n"," [0.04976111 0.0548619 ]]\n","\n","Average MAE Loss:\n","[0.09092833 0.0821836  0.07734817 0.0523115 ]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07975477 0.08461242]\n"," [0.07490779 0.08004609]\n"," [0.05245091 0.05640642]\n"," [0.05511396 0.0591381 ]]\n","\n","Average MAE Loss:\n","[0.0821836  0.07747694 0.05442866 0.05712603]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07975477 0.08461242]\n"," [0.07508482 0.08003988]\n"," [0.05264669 0.056729  ]\n"," [0.04967551 0.05441319]]\n","\n","Average MAE Loss:\n","[0.0821836  0.07756235 0.05468784 0.05204435]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07975477 0.08461242]\n"," [0.07435547 0.0793426 ]\n"," [0.05401148 0.05747456]\n"," [0.04630808 0.05164767]]\n","\n","Average MAE Loss:\n","[0.0821836  0.07684903 0.05574302 0.04897788]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07975477 0.08461242]\n"," [0.07428419 0.07950868]\n"," [0.05348004 0.05708089]\n"," [0.0462333  0.05164582]]\n","\n","Average MAE Loss:\n","[0.0821836  0.07689644 0.05528047 0.04893956]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07975477 0.08461242]\n"," [0.07411886 0.07956693]\n"," [0.05466313 0.0578539 ]\n"," [0.04635418 0.05174644]]\n","\n","Average MAE Loss:\n","[0.0821836  0.07684289 0.05625852 0.04905031]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07975477 0.08461242]\n"," [0.07418884 0.07944377]\n"," [0.05076789 0.05525839]\n"," [0.04614279 0.05153703]]\n","\n","Average MAE Loss:\n","[0.0821836  0.07681631 0.05301314 0.04883991]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07975477 0.08461242]\n"," [0.07406706 0.07939002]\n"," [0.05063592 0.05535495]\n"," [0.04673609 0.0520421 ]]\n","\n","Average MAE Loss:\n","[0.0821836  0.07672854 0.05299543 0.0493891 ]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.08355246 0.09006087]\n"," [0.04695897 0.05036385]\n"," [0.04519158 0.05009205]\n"," [0.04552158 0.05023031]]\n","\n","Average MAE Loss:\n","[0.08680667 0.04866141 0.04764181 0.04787594]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.08355246 0.09006087]\n"," [0.04591664 0.04941727]\n"," [0.04573561 0.04969148]\n"," [0.04280524 0.04814939]]\n","\n","Average MAE Loss:\n","[0.08680667 0.04766696 0.04771355 0.04547731]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.08355246 0.09006087]\n"," [0.04484901 0.04931991]\n"," [0.0442385  0.04801855]\n"," [0.04228506 0.04692385]]\n","\n","Average MAE Loss:\n","[0.08680667 0.04708446 0.04612852 0.04460445]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.08355246 0.09006087]\n"," [0.04496851 0.04947818]\n"," [0.04292128 0.04727304]\n"," [0.04221755 0.04705312]]\n","\n","Average MAE Loss:\n","[0.08680667 0.04722335 0.04509716 0.04463534]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.08355246 0.09006087]\n"," [0.04502943 0.04919034]\n"," [0.04314136 0.04726754]\n"," [0.04209866 0.04677337]]\n","\n","Average MAE Loss:\n","[0.08680667 0.04710989 0.04520445 0.04443602]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.08355246 0.09006087]\n"," [0.0446652  0.04908498]\n"," [0.04318988 0.04733121]\n"," [0.04198389 0.04674783]]\n","\n","Average MAE Loss:\n","[0.08680667 0.04687509 0.04526054 0.04436586]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.08355246 0.09006087]\n"," [0.04483467 0.04924243]\n"," [0.04279078 0.04714118]\n"," [0.0418763  0.04654377]]\n","\n","Average MAE Loss:\n","[0.08680667 0.04703855 0.04496598 0.04421003]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04483467 0.04924243]\n"," [0.04350974 0.04813381]\n"," [0.04254226 0.04665226]\n"," [0.05310779 0.05614056]]\n","\n","Average MAE Loss:\n","[0.04703855 0.04582177 0.04459726 0.05462418]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04483467 0.04924243]\n"," [0.04408373 0.04864701]\n"," [0.0429693  0.04702089]\n"," [0.04225966 0.04774445]]\n","\n","Average MAE Loss:\n","[0.04703855 0.04636537 0.0449951  0.04500206]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04483467 0.04924243]\n"," [0.04396763 0.04851625]\n"," [0.04290742 0.04690151]\n"," [0.04172268 0.04669253]]\n","\n","Average MAE Loss:\n","[0.04703855 0.04624194 0.04490446 0.0442076 ]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04483467 0.04924243]\n"," [0.04395329 0.04843221]\n"," [0.04280435 0.04680881]\n"," [0.04150115 0.0466124 ]]\n","\n","Average MAE Loss:\n","[0.04703855 0.04619275 0.04480658 0.04405677]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04483467 0.04924243]\n"," [0.04402935 0.04853556]\n"," [0.0426379  0.04683872]\n"," [0.04140594 0.04639327]]\n","\n","Average MAE Loss:\n","[0.04703855 0.04628245 0.04473831 0.0438996 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04483467 0.04924243]\n"," [0.04397332 0.04841426]\n"," [0.04278338 0.04681435]\n"," [0.04149163 0.04650321]]\n","\n","Average MAE Loss:\n","[0.04703855 0.04619379 0.04479886 0.04399742]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04483467 0.04924243]\n"," [0.04400222 0.04844976]\n"," [0.04283988 0.04684601]\n"," [0.04145058 0.04639845]]\n","\n","Average MAE Loss:\n","[0.04703855 0.04622599 0.04484295 0.04392452]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04263735 0.04775125]\n"," [0.04238207 0.04689   ]\n"," [0.04244674 0.04647169]\n"," [0.04122189 0.0462531 ]]\n","\n","Average MAE Loss:\n","[0.0451943  0.04463603 0.04445921 0.04373749]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04263735 0.04775125]\n"," [0.04304968 0.04743995]\n"," [0.04233865 0.04644827]\n"," [0.04091226 0.0458115 ]]\n","\n","Average MAE Loss:\n","[0.0451943  0.04524481 0.04439346 0.04336188]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04263735 0.04775125]\n"," [0.04321875 0.04757015]\n"," [0.04249164 0.04648201]\n"," [0.0409697  0.0458015 ]]\n","\n","Average MAE Loss:\n","[0.0451943  0.04539445 0.04448682 0.0433856 ]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04263735 0.04775125]\n"," [0.04330751 0.04764278]\n"," [0.04235693 0.04642982]\n"," [0.04080258 0.04571984]]\n","\n","Average MAE Loss:\n","[0.0451943  0.04547514 0.04439338 0.04326121]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04263735 0.04775125]\n"," [0.04336989 0.04767641]\n"," [0.04238452 0.04639959]\n"," [0.04084921 0.04575072]]\n","\n","Average MAE Loss:\n","[0.0451943  0.04552315 0.04439205 0.04329997]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04263735 0.04775125]\n"," [0.04336149 0.04764766]\n"," [0.04237897 0.04638381]\n"," [0.04066487 0.04569612]]\n","\n","Average MAE Loss:\n","[0.0451943  0.04550457 0.04438139 0.04318049]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04263735 0.04775125]\n"," [0.04337514 0.0476619 ]\n"," [0.04235876 0.04640151]\n"," [0.0407776  0.04578957]]\n","\n","Average MAE Loss:\n","[0.0451943  0.04551852 0.04438013 0.04328359]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04337514 0.0476619 ]\n"," [0.04184166 0.046512  ]\n"," [0.04094372 0.04544539]\n"," [0.04096536 0.04619398]]\n","\n","Average MAE Loss:\n","[0.04551852 0.04417683 0.04319456 0.04357967]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04337514 0.0476619 ]\n"," [0.042083   0.04679944]\n"," [0.04185198 0.04599655]\n"," [0.04084391 0.0458515 ]]\n","\n","Average MAE Loss:\n","[0.04551852 0.04444122 0.04392426 0.04334771]\n","\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04337514 0.0476619 ]\n"," [0.04238077 0.04688222]\n"," [0.0421933  0.04629462]\n"," [0.04081328 0.04578694]]\n","\n","Average MAE Loss:\n","[0.04551852 0.04463149 0.04424396 0.04330011]\n","\n","Epoch 00038: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04337514 0.0476619 ]\n"," [0.04266166 0.04706578]\n"," [0.04216718 0.04629109]\n"," [0.04072196 0.04604182]]\n","\n","Average MAE Loss:\n","[0.04551852 0.04486372 0.04422913 0.04338189]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04337514 0.0476619 ]\n"," [0.04285412 0.04721132]\n"," [0.04217937 0.04630747]\n"," [0.04060186 0.04573774]]\n","\n","Average MAE Loss:\n","[0.04551852 0.04503272 0.04424342 0.0431698 ]\n","\n","Epoch 00040: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00040: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04337514 0.0476619 ]\n"," [0.04290606 0.04725038]\n"," [0.04220364 0.04630389]\n"," [0.04068381 0.04582966]]\n","\n","Average MAE Loss:\n","[0.04551852 0.04507822 0.04425376 0.04325674]\n","\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04337514 0.0476619 ]\n"," [0.04295582 0.04729119]\n"," [0.04221673 0.04630065]\n"," [0.04064008 0.04587457]]\n","\n","Average MAE Loss:\n","[0.04551852 0.04512351 0.04425869 0.04325733]\n","\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04142713 0.04606432]\n"," [0.04151526 0.04621127]\n"," [0.04168076 0.04585304]\n"," [0.04084399 0.04580935]]\n","\n","Average MAE Loss:\n","[0.04374572 0.04386326 0.0437669  0.04332667]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04142713 0.04606432]\n"," [0.04172196 0.04639079]\n"," [0.04192131 0.04595164]\n"," [0.04066372 0.0458525 ]]\n","\n","Average MAE Loss:\n","[0.04374572 0.04405638 0.04393648 0.04325811]\n","\n","Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00044: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04142713 0.04606432]\n"," [0.04194174 0.04655582]\n"," [0.04197828 0.04601676]\n"," [0.04069461 0.04581762]]\n","\n","Average MAE Loss:\n","[0.04374572 0.04424878 0.04399752 0.04325612]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04142713 0.04606432]\n"," [0.04212487 0.04669426]\n"," [0.04202634 0.04607393]\n"," [0.04070812 0.04583471]]\n","\n","Average MAE Loss:\n","[0.04374572 0.04440956 0.04405014 0.04327141]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04142713 0.04606432]\n"," [0.04227671 0.04680834]\n"," [0.04205803 0.04610961]\n"," [0.04072552 0.04583953]]\n","\n","Average MAE Loss:\n","[0.04374572 0.04454252 0.04408382 0.04328253]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04142713 0.04606432]\n"," [0.04233809 0.04685454]\n"," [0.04209275 0.04613918]\n"," [0.04070468 0.04582645]]\n","\n","Average MAE Loss:\n","[0.04374572 0.04459631 0.04411596 0.04326556]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00048: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04142713 0.04606432]\n"," [0.04239289 0.04689472]\n"," [0.04209995 0.04615074]\n"," [0.0407171  0.04583321]]\n","\n","Average MAE Loss:\n","[0.04374572 0.0446438  0.04412535 0.04327516]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04239289 0.04689472]\n"," [0.0418758  0.04608509]\n"," [0.04065204 0.04568222]\n"," [0.04114045 0.04592339]]\n","\n","Average MAE Loss:\n","[0.0446438  0.04398045 0.04316713 0.04353192]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04239289 0.04689472]\n"," [0.04170347 0.04613042]\n"," [0.04063831 0.04552457]\n"," [0.0409237  0.04581821]]\n","\n","Average MAE Loss:\n","[0.0446438  0.04391695 0.04308144 0.04337096]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04239289 0.04689472]\n"," [0.04167417 0.04617481]\n"," [0.04068416 0.04544071]\n"," [0.0408079  0.04578563]]\n","\n","Average MAE Loss:\n","[0.0446438  0.04392449 0.04306243 0.04329676]\n","\n","Epoch 00052: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04239289 0.04689472]\n"," [0.04166402 0.04621863]\n"," [0.04075371 0.04540029]\n"," [0.0407849  0.04579595]]\n","\n","Average MAE Loss:\n","[0.0446438  0.04394133 0.043077   0.04329042]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04239289 0.04689472]\n"," [0.04166939 0.04625727]\n"," [0.04083509 0.04538388]\n"," [0.04076527 0.04580071]]\n","\n","Average MAE Loss:\n","[0.0446438  0.04396333 0.04310949 0.04328299]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04239289 0.04689472]\n"," [0.04168399 0.04629245]\n"," [0.04091997 0.04539605]\n"," [0.04074953 0.04580084]]\n","\n","Average MAE Loss:\n","[0.0446438  0.04398822 0.04315801 0.04327519]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04239289 0.04689472]\n"," [0.04169419 0.04630964]\n"," [0.04100841 0.04542334]\n"," [0.04074226 0.04580735]]\n","\n","Average MAE Loss:\n","[0.0446438  0.04400191 0.04321587 0.0432748 ]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00056: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04108562 0.04580971]\n"," [0.0411018  0.04583807]\n"," [0.04111851 0.04576252]\n"," [0.04103018 0.04580262]]\n","\n","Average MAE Loss:\n","[0.04344767 0.04346993 0.04344052 0.0434164 ]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04108562 0.04580971]\n"," [0.04112206 0.04587094]\n"," [0.04116822 0.04570952]\n"," [0.04095849 0.04579214]]\n","\n","Average MAE Loss:\n","[0.04344767 0.0434965  0.04343887 0.04337532]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04108562 0.04580971]\n"," [0.04114588 0.04590235]\n"," [0.04122533 0.04567588]\n"," [0.04089801 0.04577719]]\n","\n","Average MAE Loss:\n","[0.04344767 0.04352411 0.0434506  0.0433376 ]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04108562 0.04580971]\n"," [0.04117247 0.04593171]\n"," [0.04128399 0.04566203]\n"," [0.04085908 0.0457749 ]]\n","\n","Average MAE Loss:\n","[0.04344767 0.04355209 0.04347301 0.04331699]\n","\n","Epoch 00060: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00060: reducing learning rate of group 0 to 1.5625e-05.\n","\n","epochs finished with time:49.16852927207947\n","\n","[[0.04108562 0.04580971]\n"," [0.04117247 0.04593171]\n"," [0.04128399 0.04566203]\n"," [0.04085908 0.0457749 ]]\n","00:01:8.019731719000447\n","------------------------------------Fold [4/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07806952 0.08025616]\n"," [0.05508842 0.05665909]\n"," [0.06715205 0.06923798]\n"," [0.07134764 0.07189003]]\n","\n","Average MAE Loss:\n","[0.07916284 0.05587375 0.06819502 0.07161883]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07806952 0.08025616]\n"," [0.04945493 0.04969317]\n"," [0.06577501 0.06697003]\n"," [0.07006991 0.07086568]]\n","\n","Average MAE Loss:\n","[0.07916284 0.04957405 0.06637252 0.07046779]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07806952 0.08025616]\n"," [0.05268629 0.05174512]\n"," [0.06509545 0.06647768]\n"," [0.07224893 0.07251683]]\n","\n","Average MAE Loss:\n","[0.07916284 0.05221571 0.06578656 0.07238288]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07806952 0.08025616]\n"," [0.05030681 0.04975514]\n"," [0.0651259  0.06625438]\n"," [0.07222342 0.07250519]]\n","\n","Average MAE Loss:\n","[0.07916284 0.05003097 0.06569014 0.0723643 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07806952 0.08025616]\n"," [0.04657447 0.0465859 ]\n"," [0.06563595 0.06666687]\n"," [0.07038467 0.07102731]]\n","\n","Average MAE Loss:\n","[0.07916284 0.04658019 0.06615141 0.07070599]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07806952 0.08025616]\n"," [0.04411289 0.04400155]\n"," [0.06473702 0.06591661]\n"," [0.0703389  0.07094872]]\n","\n","Average MAE Loss:\n","[0.07916284 0.04405722 0.06532682 0.07064381]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07806952 0.08025616]\n"," [0.04344863 0.04357713]\n"," [0.06507229 0.066203  ]\n"," [0.07166473 0.07211304]]\n","\n","Average MAE Loss:\n","[0.07916284 0.04351288 0.06563764 0.07188888]\n","\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04344863 0.04357713]\n"," [0.06738182 0.06757472]\n"," [0.06845979 0.06973868]\n"," [0.05607884 0.05587109]]\n","\n","Average MAE Loss:\n","[0.04351288 0.06747827 0.06909923 0.05597496]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04344863 0.04357713]\n"," [0.07007741 0.06983105]\n"," [0.06703553 0.06886271]\n"," [0.05363835 0.05371445]]\n","\n","Average MAE Loss:\n","[0.04351288 0.06995423 0.06794912 0.0536764 ]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04344863 0.04357713]\n"," [0.06586187 0.0649306 ]\n"," [0.06843688 0.06925021]\n"," [0.0463533  0.04706889]]\n","\n","Average MAE Loss:\n","[0.04351288 0.06539624 0.06884355 0.0467111 ]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04344863 0.04357713]\n"," [0.06066205 0.0613227 ]\n"," [0.06754105 0.06840318]\n"," [0.05011392 0.05030625]]\n","\n","Average MAE Loss:\n","[0.04351288 0.06099238 0.06797212 0.05021008]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04344863 0.04357713]\n"," [0.05950994 0.06033172]\n"," [0.06716742 0.06827869]\n"," [0.05042936 0.05049551]]\n","\n","Average MAE Loss:\n","[0.04351288 0.05992083 0.06772305 0.05046244]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04344863 0.04357713]\n"," [0.0594058  0.0602283 ]\n"," [0.06700272 0.06822861]\n"," [0.04948661 0.0497658 ]]\n","\n","Average MAE Loss:\n","[0.04351288 0.05981705 0.06761567 0.04962621]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04344863 0.04357713]\n"," [0.05911151 0.06016147]\n"," [0.06727295 0.06828704]\n"," [0.04518423 0.04625463]]\n","\n","Average MAE Loss:\n","[0.04351288 0.05963649 0.06778    0.04571943]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07396587 0.07614821]\n"," [0.06675847 0.06575449]\n"," [0.04200333 0.04418527]\n"," [0.04526103 0.04444371]]\n","\n","Average MAE Loss:\n","[0.07505704 0.06625648 0.0430943  0.04485237]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07396587 0.07614821]\n"," [0.03701779 0.03905752]\n"," [0.0395661  0.03982525]\n"," [0.04111873 0.0415234 ]]\n","\n","Average MAE Loss:\n","[0.07505704 0.03803765 0.03969567 0.04132107]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07396587 0.07614821]\n"," [0.03725248 0.03745667]\n"," [0.03877952 0.03852519]\n"," [0.03699752 0.03800967]]\n","\n","Average MAE Loss:\n","[0.07505704 0.03735457 0.03865235 0.03750359]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07396587 0.07614821]\n"," [0.03620481 0.03627912]\n"," [0.03798894 0.03820174]\n"," [0.03614148 0.03746437]]\n","\n","Average MAE Loss:\n","[0.07505704 0.03624196 0.03809534 0.03680293]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07396587 0.07614821]\n"," [0.03626679 0.03652082]\n"," [0.03850578 0.03845042]\n"," [0.0366715  0.03811372]]\n","\n","Average MAE Loss:\n","[0.07505704 0.03639381 0.0384781  0.03739261]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07396587 0.07614821]\n"," [0.03645163 0.03647854]\n"," [0.03821656 0.03829823]\n"," [0.0361145  0.03761367]]\n","\n","Average MAE Loss:\n","[0.07505704 0.03646509 0.0382574  0.03686409]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07396587 0.07614821]\n"," [0.03635856 0.03640905]\n"," [0.03830174 0.03830566]\n"," [0.03614967 0.0377155 ]]\n","\n","Average MAE Loss:\n","[0.07505704 0.03638381 0.0383037  0.03693259]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.03635856 0.03640905]\n"," [0.03601615 0.03636586]\n"," [0.03658789 0.03728496]\n"," [0.07339355 0.07194687]]\n","\n","Average MAE Loss:\n","[0.03638381 0.036191   0.03693643 0.07267021]\n","\n","Epoch 00022: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.03635856 0.03640905]\n"," [0.03664235 0.03639027]\n"," [0.03690771 0.0371641 ]\n"," [0.03616354 0.0390349 ]]\n","\n","Average MAE Loss:\n","[0.03638381 0.03651631 0.03703591 0.03759922]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.03635856 0.03640905]\n"," [0.03619944 0.03601705]\n"," [0.03713692 0.03718183]\n"," [0.03738356 0.03903643]]\n","\n","Average MAE Loss:\n","[0.03638381 0.03610825 0.03715938 0.03821   ]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.03635856 0.03640905]\n"," [0.03623211 0.03609096]\n"," [0.03710564 0.03711833]\n"," [0.03685522 0.03849907]]\n","\n","Average MAE Loss:\n","[0.03638381 0.03616153 0.03711198 0.03767714]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.03635856 0.03640905]\n"," [0.03614042 0.03596983]\n"," [0.03714027 0.03716916]\n"," [0.03619616 0.03797495]]\n","\n","Average MAE Loss:\n","[0.03638381 0.03605513 0.03715472 0.03708555]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03635856 0.03640905]\n"," [0.03618861 0.03604266]\n"," [0.03717504 0.03716589]\n"," [0.036509   0.03827066]]\n","\n","Average MAE Loss:\n","[0.03638381 0.03611564 0.03717046 0.03738983]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03635856 0.03640905]\n"," [0.03619142 0.03600352]\n"," [0.03712976 0.03712033]\n"," [0.03624146 0.03811653]]\n","\n","Average MAE Loss:\n","[0.03638381 0.03609747 0.03712505 0.037179  ]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03448116 0.03627197]\n"," [0.03619072 0.03587794]\n"," [0.03769227 0.03733688]\n"," [0.03502527 0.035645  ]]\n","\n","Average MAE Loss:\n","[0.03537657 0.03603433 0.03751457 0.03533514]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03448116 0.03627197]\n"," [0.03486349 0.03487376]\n"," [0.03597761 0.03610195]\n"," [0.03586998 0.03674426]]\n","\n","Average MAE Loss:\n","[0.03537657 0.03486862 0.03603978 0.03630712]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03448116 0.03627197]\n"," [0.03534507 0.03515683]\n"," [0.03692107 0.03686162]\n"," [0.03510713 0.03643446]]\n","\n","Average MAE Loss:\n","[0.03537657 0.03525095 0.03689135 0.0357708 ]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03448116 0.03627197]\n"," [0.03496743 0.0348702 ]\n"," [0.03661228 0.03672965]\n"," [0.03524999 0.03670089]]\n","\n","Average MAE Loss:\n","[0.03537657 0.03491882 0.03667097 0.03597544]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03448116 0.03627197]\n"," [0.03509402 0.03499611]\n"," [0.03682555 0.03684457]\n"," [0.03522046 0.03682274]]\n","\n","Average MAE Loss:\n","[0.03537657 0.03504506 0.03683506 0.0360216 ]\n","\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03448116 0.03627197]\n"," [0.03514179 0.03495668]\n"," [0.03691274 0.03691384]\n"," [0.0352328  0.03687669]]\n","\n","Average MAE Loss:\n","[0.03537657 0.03504923 0.03691329 0.03605475]\n","\n","Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00034: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03448116 0.03627197]\n"," [0.03515884 0.03499455]\n"," [0.03682584 0.03684557]\n"," [0.0352248  0.03691176]]\n","\n","Average MAE Loss:\n","[0.03537657 0.03507669 0.0368357  0.03606828]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03515884 0.03499455]\n"," [0.03495599 0.03533528]\n"," [0.03517848 0.03659637]\n"," [0.03423439 0.03539271]]\n","\n","Average MAE Loss:\n","[0.03507669 0.03514563 0.03588743 0.03481355]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03515884 0.03499455]\n"," [0.03518781 0.03516645]\n"," [0.03542752 0.03648569]\n"," [0.03494785 0.03571791]]\n","\n","Average MAE Loss:\n","[0.03507669 0.03517713 0.03595661 0.03533288]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03515884 0.03499455]\n"," [0.03526837 0.03507111]\n"," [0.03577398 0.03651585]\n"," [0.03529033 0.03618028]]\n","\n","Average MAE Loss:\n","[0.03507669 0.03516974 0.03614492 0.03573531]\n","\n","Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03515884 0.03499455]\n"," [0.03524303 0.03504254]\n"," [0.03600861 0.03654401]\n"," [0.03521719 0.03632525]]\n","\n","Average MAE Loss:\n","[0.03507669 0.03514279 0.03627631 0.03577122]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03515884 0.03499455]\n"," [0.03521625 0.03502186]\n"," [0.03620394 0.036594  ]\n"," [0.03514493 0.03643259]]\n","\n","Average MAE Loss:\n","[0.03507669 0.03511906 0.03639897 0.03578876]\n","\n","Epoch 00040: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03515884 0.03499455]\n"," [0.03518652 0.03499841]\n"," [0.03625251 0.03659453]\n"," [0.03515722 0.03650156]]\n","\n","Average MAE Loss:\n","[0.03507669 0.03509247 0.03642352 0.03582939]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03515884 0.03499455]\n"," [0.03515058 0.03497693]\n"," [0.03630756 0.03660158]\n"," [0.03515705 0.03655664]]\n","\n","Average MAE Loss:\n","[0.03507669 0.03506376 0.03645457 0.03585684]\n","\n","Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03491089 0.03523793]\n"," [0.03487639 0.03514887]\n"," [0.03512537 0.03534695]\n"," [0.03492558 0.03535284]]\n","\n","Average MAE Loss:\n","[0.03507441 0.03501263 0.03523616 0.03513921]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03491089 0.03523793]\n"," [0.03487264 0.03506609]\n"," [0.03539398 0.03550371]\n"," [0.03497844 0.03553241]]\n","\n","Average MAE Loss:\n","[0.03507441 0.03496936 0.03544885 0.03525542]\n","\n","Epoch 00044: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00044: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03491089 0.03523793]\n"," [0.03489586 0.03501768]\n"," [0.03560201 0.03566505]\n"," [0.03500257 0.03561972]]\n","\n","Average MAE Loss:\n","[0.03507441 0.03495677 0.03563353 0.03531115]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03491089 0.03523793]\n"," [0.03492528 0.03498395]\n"," [0.03572968 0.03577948]\n"," [0.03501821 0.03569363]]\n","\n","Average MAE Loss:\n","[0.03507441 0.03495461 0.03575458 0.03535592]\n","\n","Epoch 00046: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03491089 0.03523793]\n"," [0.03494526 0.03497221]\n"," [0.03583614 0.03587176]\n"," [0.03502857 0.03575663]]\n","\n","Average MAE Loss:\n","[0.03507441 0.03495874 0.03585395 0.0353926 ]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03491089 0.03523793]\n"," [0.03496124 0.03496452]\n"," [0.03589248 0.03591795]\n"," [0.03504048 0.03581464]]\n","\n","Average MAE Loss:\n","[0.03507441 0.03496288 0.03590521 0.03542756]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03491089 0.03523793]\n"," [0.0349753  0.03495627]\n"," [0.03593966 0.03596181]\n"," [0.03504596 0.03584196]]\n","\n","Average MAE Loss:\n","[0.03507441 0.03496578 0.03595074 0.03544396]\n","\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0349753  0.03495627]\n"," [0.03574279 0.0357846 ]\n"," [0.03507566 0.03582286]\n"," [0.03491142 0.03527003]]\n","\n","Average MAE Loss:\n","[0.03496578 0.0357637  0.03544926 0.03509073]\n","\n","Epoch 00050: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0349753  0.03495627]\n"," [0.0356237  0.03567936]\n"," [0.03512854 0.03581282]\n"," [0.03492236 0.03531431]]\n","\n","Average MAE Loss:\n","[0.03496578 0.03565153 0.03547068 0.03511834]\n","\n","Epoch 00051: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0349753  0.03495627]\n"," [0.03551883 0.03558308]\n"," [0.03516796 0.03582079]\n"," [0.03493347 0.03535876]]\n","\n","Average MAE Loss:\n","[0.03496578 0.03555096 0.03549437 0.03514612]\n","\n","Epoch 00052: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0349753  0.03495627]\n"," [0.03543215 0.03550084]\n"," [0.03520672 0.03583123]\n"," [0.0349389  0.0353806 ]]\n","\n","Average MAE Loss:\n","[0.03496578 0.03546649 0.03551898 0.03515975]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0349753  0.03495627]\n"," [0.03536012 0.03543178]\n"," [0.03524534 0.03584517]\n"," [0.0349438  0.03540233]]\n","\n","Average MAE Loss:\n","[0.03496578 0.03539595 0.03554526 0.03517307]\n","\n","Epoch 00054: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0349753  0.03495627]\n"," [0.03532979 0.03540266]\n"," [0.03528584 0.0358611 ]\n"," [0.03494839 0.03542394]]\n","\n","Average MAE Loss:\n","[0.03496578 0.03536622 0.03557347 0.03518616]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0349753  0.03495627]\n"," [0.03530144 0.03537676]\n"," [0.03530547 0.0358696 ]\n"," [0.0349528  0.0354453 ]]\n","\n","Average MAE Loss:\n","[0.03496578 0.0353391  0.03558753 0.03519905]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.9531e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03500728 0.03528265]\n"," [0.03498977 0.03526197]\n"," [0.03504084 0.03529807]\n"," [0.03500926 0.03529245]]\n","\n","Average MAE Loss:\n","[0.03514496 0.03512587 0.03516946 0.03515085]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03500728 0.03528265]\n"," [0.034978   0.03524268]\n"," [0.03508216 0.03532144]\n"," [0.03501129 0.03530297]]\n","\n","Average MAE Loss:\n","[0.03514496 0.03511034 0.0352018  0.03515713]\n","\n","Epoch 00058: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00058: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03500728 0.03528265]\n"," [0.03497303 0.0352337 ]\n"," [0.03512256 0.03534572]\n"," [0.03501329 0.03531364]]\n","\n","Average MAE Loss:\n","[0.03514496 0.03510336 0.03523414 0.03516347]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03500728 0.03528265]\n"," [0.03496849 0.03522519]\n"," [0.03516049 0.03536888]\n"," [0.03501532 0.03532441]]\n","\n","Average MAE Loss:\n","[0.03514496 0.03509684 0.03526469 0.03516986]\n","\n","Epoch 00060: reducing learning rate of group 0 to 9.7656e-07.\n","\n","epochs finished with time:50.551989793777466\n","\n","[[0.03500728 0.03528265]\n"," [0.03496849 0.03522519]\n"," [0.03516049 0.03536888]\n"," [0.03501532 0.03532441]]\n","00:01:10.217449281999507\n","------------------------------------Fold [5/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08695787 0.09360404]\n"," [0.05971701 0.06268563]\n"," [0.07623713 0.08112507]\n"," [0.07635405 0.08167867]]\n","\n","Average MAE Loss:\n","[0.09028096 0.06120132 0.0786811  0.07901636]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08695787 0.09360404]\n"," [0.05631114 0.05850102]\n"," [0.07438135 0.08001392]\n"," [0.07628827 0.08172424]]\n","\n","Average MAE Loss:\n","[0.09028096 0.05740608 0.07719764 0.07900625]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08695787 0.09360404]\n"," [0.05214087 0.05442691]\n"," [0.07419057 0.0796051 ]\n"," [0.07611518 0.08166315]]\n","\n","Average MAE Loss:\n","[0.09028096 0.05328389 0.07689784 0.07888917]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08695787 0.09360404]\n"," [0.05244182 0.05376295]\n"," [0.07453908 0.07988757]\n"," [0.07457081 0.08058852]]\n","\n","Average MAE Loss:\n","[0.09028096 0.05310238 0.07721332 0.07757967]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08695787 0.09360404]\n"," [0.05333173 0.05477459]\n"," [0.07468863 0.07997992]\n"," [0.07444502 0.08046074]]\n","\n","Average MAE Loss:\n","[0.09028096 0.05405316 0.07733427 0.07745288]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08695787 0.09360404]\n"," [0.05291336 0.05442894]\n"," [0.07428848 0.07958741]\n"," [0.07459873 0.08054638]]\n","\n","Average MAE Loss:\n","[0.09028096 0.05367115 0.07693795 0.07757255]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08695787 0.09360404]\n"," [0.05187294 0.05355102]\n"," [0.07442775 0.07961556]\n"," [0.07476516 0.08048832]]\n","\n","Average MAE Loss:\n","[0.09028096 0.05271198 0.07702165 0.07762674]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05187294 0.05355102]\n"," [0.07477547 0.07995352]\n"," [0.07581748 0.08121365]\n"," [0.05479853 0.05803452]]\n","\n","Average MAE Loss:\n","[0.05271198 0.0773645  0.07851557 0.05641653]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05187294 0.05355102]\n"," [0.07203993 0.07678623]\n"," [0.0752181  0.08074118]\n"," [0.05427688 0.05761708]]\n","\n","Average MAE Loss:\n","[0.05271198 0.07441308 0.07797964 0.05594698]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05187294 0.05355102]\n"," [0.07298733 0.07729188]\n"," [0.0751106  0.08050159]\n"," [0.0526271  0.0561169 ]]\n","\n","Average MAE Loss:\n","[0.05271198 0.07513961 0.07780609 0.054372  ]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.05187294 0.05355102]\n"," [0.07399391 0.0782484 ]\n"," [0.0750338  0.08049807]\n"," [0.04883278 0.05370887]]\n","\n","Average MAE Loss:\n","[0.05271198 0.07612116 0.07776593 0.05127082]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00011: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.05187294 0.05355102]\n"," [0.06984007 0.07525696]\n"," [0.07501288 0.080617  ]\n"," [0.04894537 0.05368938]]\n","\n","Average MAE Loss:\n","[0.05271198 0.07254852 0.07781494 0.05131737]\n","\n","Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.05187294 0.05355102]\n"," [0.06978499 0.07494126]\n"," [0.07500676 0.08048105]\n"," [0.04929912 0.05375252]]\n","\n","Average MAE Loss:\n","[0.05271198 0.07236313 0.07774391 0.05152582]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.05187294 0.05355102]\n"," [0.06985498 0.0750389 ]\n"," [0.07503059 0.08048368]\n"," [0.04964862 0.05391298]]\n","\n","Average MAE Loss:\n","[0.05271198 0.07244694 0.07775714 0.0517808 ]\n","\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.08206706 0.08860065]\n"," [0.05471585 0.056708  ]\n"," [0.04849061 0.05145429]\n"," [0.04365185 0.04871946]]\n","\n","Average MAE Loss:\n","[0.08533385 0.05571193 0.04997245 0.04618565]\n","\n","Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.08206706 0.08860065]\n"," [0.0429818  0.04697528]\n"," [0.04471774 0.04774343]\n"," [0.0431277  0.04704842]]\n","\n","Average MAE Loss:\n","[0.08533385 0.04497854 0.04623058 0.04508806]\n","\n","Epoch 00016: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.08206706 0.08860065]\n"," [0.04234339 0.04595092]\n"," [0.04426711 0.04712033]\n"," [0.04231879 0.04617475]]\n","\n","Average MAE Loss:\n","[0.08533385 0.04414715 0.04569372 0.04424677]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.08206706 0.08860065]\n"," [0.04244382 0.04589261]\n"," [0.04406628 0.04700095]\n"," [0.04231444 0.0461781 ]]\n","\n","Average MAE Loss:\n","[0.08533385 0.04416822 0.04553361 0.04424627]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.08206706 0.08860065]\n"," [0.0424529  0.04584961]\n"," [0.04401243 0.04681413]\n"," [0.04182714 0.04580526]]\n","\n","Average MAE Loss:\n","[0.08533385 0.04415126 0.04541328 0.0438162 ]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.08206706 0.08860065]\n"," [0.04250253 0.04584178]\n"," [0.04407765 0.04675649]\n"," [0.04143208 0.04575681]]\n","\n","Average MAE Loss:\n","[0.08533385 0.04417215 0.04541707 0.04359445]\n","\n","Epoch 00020: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.08206706 0.08860065]\n"," [0.04252925 0.04584032]\n"," [0.0440231  0.04671486]\n"," [0.04131749 0.04562991]]\n","\n","Average MAE Loss:\n","[0.08533385 0.04418478 0.04536898 0.0434737 ]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04252925 0.04584032]\n"," [0.04314143 0.04610982]\n"," [0.0420201  0.04590378]\n"," [0.05266126 0.05379662]]\n","\n","Average MAE Loss:\n","[0.04418478 0.04462563 0.04396194 0.05322894]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04252925 0.04584032]\n"," [0.04281565 0.04579424]\n"," [0.04306137 0.04614303]\n"," [0.04311824 0.0478379 ]]\n","\n","Average MAE Loss:\n","[0.04418478 0.04430494 0.0446022  0.04547807]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04252925 0.04584032]\n"," [0.0428808  0.04586776]\n"," [0.04320637 0.04621737]\n"," [0.04162443 0.0460544 ]]\n","\n","Average MAE Loss:\n","[0.04418478 0.04437428 0.04471187 0.04383942]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04252925 0.04584032]\n"," [0.04294427 0.04597916]\n"," [0.04329889 0.04629325]\n"," [0.04175707 0.04621293]]\n","\n","Average MAE Loss:\n","[0.04418478 0.04446172 0.04479607 0.043985  ]\n","\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04252925 0.04584032]\n"," [0.0429232  0.04600048]\n"," [0.04342031 0.04634234]\n"," [0.04120915 0.04600926]]\n","\n","Average MAE Loss:\n","[0.04418478 0.04446184 0.04488132 0.0436092 ]\n","\n","Epoch 00026: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04252925 0.04584032]\n"," [0.04290602 0.04600257]\n"," [0.04340997 0.0463437 ]\n"," [0.04119978 0.04581049]]\n","\n","Average MAE Loss:\n","[0.04418478 0.04445429 0.04487683 0.04350513]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04252925 0.04584032]\n"," [0.04289181 0.04600942]\n"," [0.04341396 0.04636377]\n"," [0.0411468  0.04581522]]\n","\n","Average MAE Loss:\n","[0.04418478 0.04445061 0.04488886 0.04348101]\n","\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04247347 0.0466383 ]\n"," [0.04191606 0.04583257]\n"," [0.04184367 0.04540443]\n"," [0.04086702 0.04502309]]\n","\n","Average MAE Loss:\n","[0.04455589 0.04387431 0.04362405 0.04294506]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04247347 0.0466383 ]\n"," [0.04172792 0.04538646]\n"," [0.04253729 0.04559032]\n"," [0.04065223 0.04536987]]\n","\n","Average MAE Loss:\n","[0.04455589 0.04355719 0.04406381 0.04301105]\n","\n","Epoch 00030: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04247347 0.0466383 ]\n"," [0.04180554 0.04534362]\n"," [0.04290679 0.04584477]\n"," [0.04066589 0.04524745]]\n","\n","Average MAE Loss:\n","[0.04455589 0.04357458 0.04437578 0.04295667]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04247347 0.0466383 ]\n"," [0.04188277 0.04536787]\n"," [0.04307663 0.04592878]\n"," [0.04062099 0.04507266]]\n","\n","Average MAE Loss:\n","[0.04455589 0.04362532 0.0445027  0.04284682]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04247347 0.0466383 ]\n"," [0.04194388 0.04538498]\n"," [0.04312652 0.04598133]\n"," [0.0406129  0.04528952]]\n","\n","Average MAE Loss:\n","[0.04455589 0.04366443 0.04455392 0.04295121]\n","\n","Epoch 00033: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04247347 0.0466383 ]\n"," [0.04199634 0.04540079]\n"," [0.04314146 0.04600349]\n"," [0.04067953 0.04517686]]\n","\n","Average MAE Loss:\n","[0.04455589 0.04369857 0.04457248 0.04292819]\n","\n","Epoch 00034: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04247347 0.0466383 ]\n"," [0.04201528 0.04540715]\n"," [0.04316618 0.04602096]\n"," [0.04063421 0.04515497]]\n","\n","Average MAE Loss:\n","[0.04455589 0.04371122 0.04459357 0.04289459]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04201528 0.04540715]\n"," [0.04291161 0.04582197]\n"," [0.04076602 0.04507162]\n"," [0.04083695 0.04495998]]\n","\n","Average MAE Loss:\n","[0.04371122 0.04436679 0.04291882 0.04289846]\n","\n","Epoch 00036: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04201528 0.04540715]\n"," [0.04263443 0.04562066]\n"," [0.0410741  0.04504598]\n"," [0.0406225  0.04531419]]\n","\n","Average MAE Loss:\n","[0.04371122 0.04412755 0.04306004 0.04296835]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04201528 0.04540715]\n"," [0.04243262 0.04547986]\n"," [0.04142153 0.0451228 ]\n"," [0.04053964 0.04512091]]\n","\n","Average MAE Loss:\n","[0.04371122 0.04395624 0.04327217 0.04283028]\n","\n","Epoch 00038: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04201528 0.04540715]\n"," [0.04236027 0.04542518]\n"," [0.04175504 0.04524828]\n"," [0.04057209 0.04520493]]\n","\n","Average MAE Loss:\n","[0.04371122 0.04389272 0.04350166 0.04288851]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04201528 0.04540715]\n"," [0.04230097 0.04537755]\n"," [0.04201973 0.04536052]\n"," [0.04053132 0.0451597 ]]\n","\n","Average MAE Loss:\n","[0.04371122 0.04383926 0.04369013 0.04284551]\n","\n","Epoch 00040: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00040: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04201528 0.04540715]\n"," [0.04224982 0.04533752]\n"," [0.04212064 0.04540819]\n"," [0.04056795 0.04518337]]\n","\n","Average MAE Loss:\n","[0.04371122 0.04379367 0.04376441 0.04287566]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04201528 0.04540715]\n"," [0.0422069  0.04530305]\n"," [0.04220334 0.04545086]\n"," [0.04055567 0.04517669]]\n","\n","Average MAE Loss:\n","[0.04371122 0.04375497 0.0438271  0.04286618]\n","\n","Epoch 00042: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00042: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04136923 0.04502057]\n"," [0.04136578 0.04501059]\n"," [0.04147454 0.04504633]\n"," [0.04101421 0.04497144]]\n","\n","Average MAE Loss:\n","[0.0431949  0.04318818 0.04326043 0.04299282]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04136923 0.04502057]\n"," [0.04137103 0.04500274]\n"," [0.04163373 0.04509678]\n"," [0.0407261  0.04499015]]\n","\n","Average MAE Loss:\n","[0.0431949  0.04318688 0.04336525 0.04285813]\n","\n","Epoch 00044: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04136923 0.04502057]\n"," [0.04137795 0.04499589]\n"," [0.04171585 0.04512932]\n"," [0.04062454 0.04502079]]\n","\n","Average MAE Loss:\n","[0.0431949  0.04318692 0.04342259 0.04282266]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04136923 0.04502057]\n"," [0.04138619 0.04499099]\n"," [0.04179109 0.04516258]\n"," [0.04059841 0.04506243]]\n","\n","Average MAE Loss:\n","[0.0431949  0.04318859 0.04347684 0.04283042]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04136923 0.04502057]\n"," [0.04139596 0.0449882 ]\n"," [0.04186159 0.04519882]\n"," [0.04058477 0.04509268]]\n","\n","Average MAE Loss:\n","[0.0431949  0.04319208 0.04353021 0.04283872]\n","\n","Epoch 00047: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00047: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04136923 0.04502057]\n"," [0.04140124 0.04498728]\n"," [0.0419262  0.0452325 ]\n"," [0.04057125 0.04512005]]\n","\n","Average MAE Loss:\n","[0.0431949  0.04319426 0.04357935 0.04284565]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04136923 0.04502057]\n"," [0.04140679 0.04498676]\n"," [0.04195583 0.04524893]\n"," [0.04056875 0.04512774]]\n","\n","Average MAE Loss:\n","[0.0431949  0.04319678 0.04360238 0.04284825]\n","\n","Epoch 00049: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04140679 0.04498676]\n"," [0.04194447 0.04524006]\n"," [0.04057593 0.04510818]\n"," [0.04115557 0.04497593]]\n","\n","Average MAE Loss:\n","[0.04319678 0.04359226 0.04284206 0.04306575]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04140679 0.04498676]\n"," [0.04192931 0.04523076]\n"," [0.04058881 0.04507995]\n"," [0.04094833 0.0449735 ]]\n","\n","Average MAE Loss:\n","[0.04319678 0.04358004 0.04283438 0.04296092]\n","\n","Epoch 00051: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00051: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04140679 0.04498676]\n"," [0.04192164 0.04522604]\n"," [0.04061011 0.04505364]\n"," [0.04081433 0.04498529]]\n","\n","Average MAE Loss:\n","[0.04319678 0.04357384 0.04283187 0.04289981]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04140679 0.04498676]\n"," [0.04191408 0.04522117]\n"," [0.0406366  0.04502971]\n"," [0.04073691 0.04499276]]\n","\n","Average MAE Loss:\n","[0.04319678 0.04356762 0.04283316 0.04286483]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04140679 0.04498676]\n"," [0.04190667 0.04521619]\n"," [0.04066738 0.04500974]\n"," [0.04070987 0.04500125]]\n","\n","Average MAE Loss:\n","[0.04319678 0.04356143 0.04283856 0.04285556]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04140679 0.04498676]\n"," [0.04189957 0.04521123]\n"," [0.04070181 0.04499276]\n"," [0.04068549 0.04501432]]\n","\n","Average MAE Loss:\n","[0.04319678 0.0435554  0.04284729 0.04284991]\n","\n","Epoch 00055: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00055: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04140679 0.04498676]\n"," [0.041896   0.04520874]\n"," [0.04072007 0.04498544]\n"," [0.04066919 0.04502639]]\n","\n","Average MAE Loss:\n","[0.04319678 0.04355237 0.04285275 0.04284779]\n","\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04105097 0.04495437]\n"," [0.04105022 0.04495285]\n"," [0.04107077 0.04495456]\n"," [0.0409753  0.04495747]]\n","\n","Average MAE Loss:\n","[0.04300267 0.04300153 0.04301267 0.04296638]\n","\n","Epoch 00057: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04105097 0.04495437]\n"," [0.04105019 0.04495116]\n"," [0.04109066 0.04495589]\n"," [0.04092653 0.04495887]]\n","\n","Average MAE Loss:\n","[0.04300267 0.04300067 0.04302327 0.0429427 ]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04105097 0.04495437]\n"," [0.04105033 0.04494948]\n"," [0.04111129 0.04495756]\n"," [0.04088537 0.04496282]]\n","\n","Average MAE Loss:\n","[0.04300267 0.0429999  0.04303443 0.04292409]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04105097 0.04495437]\n"," [0.04105061 0.04494786]\n"," [0.04112181 0.04495851]\n"," [0.04085169 0.04496729]]\n","\n","Average MAE Loss:\n","[0.04300267 0.04299923 0.04304016 0.04290949]\n","\n","\n","epochs finished with time:50.89196848869324\n","\n","[[0.04105097 0.04495437]\n"," [0.04105061 0.04494786]\n"," [0.04112181 0.04495851]\n"," [0.04085169 0.04496729]]\n","00:01:10.72397786900001\n"]}],"source":["# seed = 500 table = 4/8 fixed folds CNN_2\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_500/4_8/global_test/GNNs/'\n","args.save_name='Simple2_CNN_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=500,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiVQVTfYsMjH"},"outputs":[],"source":["args.C=50\n","args.D=50"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":52159,"status":"error","timestamp":1689947071945,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"59HYl6igsHWH","outputId":"a3075047-0f63-44f8-cd34-cde414fccd71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:500\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 1.]\n"," [1. 1. 0.]\n"," [1. 1. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07161249 0.08753487]\n"," [0.06609653 0.08078363]\n"," [0.05705494 0.06765399]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07957368 0.07344008 0.06235446]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07070872 0.08634324]\n"," [0.06591281 0.08030224]\n"," [0.05279149 0.06364293]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07852598 0.07310752 0.05821721]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07447436 0.08846008]\n"," [0.06601703 0.08011893]\n"," [0.05378799 0.0648124 ]]\n","\n","Average MAE Loss:\n","[0.08553374 0.08146722 0.07306798 0.0593002 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07183462 0.08665596]\n"," [0.06706276 0.08087224]\n"," [0.05540858 0.0660313 ]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07924529 0.0739675  0.06071994]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07307006 0.08742145]\n"," [0.06600247 0.08003813]\n"," [0.05101451 0.06192622]]\n","\n","Average MAE Loss:\n","[0.08553374 0.08024576 0.0730203  0.05647036]\n","\n","Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07209928 0.08681184]\n"," [0.06482335 0.07938031]\n"," [0.05389551 0.0644877 ]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07945556 0.07210183 0.0591916 ]\n","\n","Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06992184 0.08496827]\n"," [0.06624976 0.08027623]\n"," [0.05202268 0.06296932]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07744505 0.07326299 0.057496  ]\n","\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06994776 0.08490108]\n"," [0.06568888 0.07983573]\n"," [0.05139612 0.06229815]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07742442 0.0727623  0.05684713]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07037697 0.0850812 ]\n"," [0.06596487 0.07997217]\n"," [0.0490207  0.06038871]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07772909 0.07296852 0.0547047 ]\n","\n","Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06993096 0.08487888]\n"," [0.06558931 0.07969337]\n"," [0.0474141  0.05925292]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07740492 0.07264134 0.05333351]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06998251 0.08487903]\n"," [0.0645909  0.0790781 ]\n"," [0.0509224  0.06203538]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07743077 0.0718345  0.05647889]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07008101 0.08496337]\n"," [0.06453567 0.07911506]\n"," [0.04807449 0.05958319]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07752219 0.07182536 0.05382884]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06996434 0.08487568]\n"," [0.0645952  0.07900383]\n"," [0.04748975 0.05907286]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07742001 0.07179952 0.05328131]\n","\n","Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06994821 0.08486671]\n"," [0.06478507 0.07906973]\n"," [0.0469202  0.05868444]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07740746 0.0719274  0.05280232]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06996037 0.08486643]\n"," [0.06483607 0.07907344]\n"," [0.0465257  0.05842149]]\n","\n","Average MAE Loss:\n","[0.08553374 0.0774134  0.07195475 0.0524736 ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06999247 0.0848784 ]\n"," [0.06474382 0.07905001]\n"," [0.04659953 0.05849491]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07743543 0.07189691 0.05254722]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991284 0.08484325]\n"," [0.06462992 0.07898502]\n"," [0.04676096 0.05860502]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07737805 0.07180747 0.05268299]\n","\n","Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.07000984 0.08489107]\n"," [0.06484647 0.07907893]\n"," [0.04661813 0.05846615]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07745045 0.0719627  0.05254214]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06990471 0.08483905]\n"," [0.06489314 0.07910827]\n"," [0.04661698 0.05844401]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07737188 0.07200071 0.05253049]\n","\n","Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.0699779  0.08486903]\n"," [0.06475099 0.07904796]\n"," [0.0461439  0.05810623]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07742347 0.07189948 0.05212506]\n","\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06993685 0.08485588]\n"," [0.06483168 0.07906725]\n"," [0.04651738 0.0582921 ]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07739636 0.07194947 0.05240474]\n","\n","Epoch 00021: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06994771 0.08485659]\n"," [0.06486291 0.07908719]\n"," [0.04620871 0.05811058]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07740215 0.07197505 0.05215964]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06993538 0.08485221]\n"," [0.06484352 0.07908155]\n"," [0.04629163 0.05814583]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07739379 0.07196254 0.05221873]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06992559 0.08484886]\n"," [0.06484457 0.07908908]\n"," [0.04640697 0.05820777]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738722 0.07196682 0.05230737]\n","\n","Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06993221 0.08485352]\n"," [0.06483646 0.07908139]\n"," [0.04629031 0.05812639]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07739287 0.07195892 0.05220835]\n","\n","Epoch 00025: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06993032 0.0848542 ]\n"," [0.06486233 0.07909076]\n"," [0.04640932 0.05819677]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07739226 0.07197655 0.05230305]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06992433 0.08485132]\n"," [0.06484535 0.07908151]\n"," [0.04630964 0.05814151]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738783 0.07196343 0.05222557]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06992381 0.08485088]\n"," [0.0648513  0.07908263]\n"," [0.04635176 0.05815712]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738735 0.07196696 0.05225444]\n","\n","Epoch 00028: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06992233 0.08485062]\n"," [0.06485373 0.07908654]\n"," [0.04636058 0.05815467]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738647 0.07197013 0.05225762]\n","\n","Epoch 00029: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00029: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00029: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06992289 0.08485043]\n"," [0.06486208 0.079093  ]\n"," [0.04636423 0.05815677]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738666 0.07197754 0.0522605 ]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991994 0.08484935]\n"," [0.06486163 0.0790924 ]\n"," [0.04635422 0.05815571]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738464 0.07197701 0.05225496]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.0699199  0.08484905]\n"," [0.06486003 0.07909157]\n"," [0.04635635 0.05815357]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738447 0.0719758  0.05225496]\n","\n","Epoch 00032: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991803 0.08484834]\n"," [0.06486104 0.07909225]\n"," [0.04637107 0.05816011]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738319 0.07197664 0.05226559]\n","\n","Epoch 00033: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991776 0.08484842]\n"," [0.06486372 0.0790939 ]\n"," [0.04636481 0.05815805]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738309 0.07197881 0.05226143]\n","\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991941 0.08484911]\n"," [0.06486389 0.07909404]\n"," [0.04636721 0.05815796]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738426 0.07197897 0.05226259]\n","\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991931 0.08484896]\n"," [0.06486344 0.07909377]\n"," [0.04636315 0.05815663]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738413 0.0719786  0.05225989]\n","\n","Epoch 00036: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991812 0.08484816]\n"," [0.06486316 0.07909365]\n"," [0.04636773 0.05816084]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738314 0.07197841 0.05226428]\n","\n","Epoch 00037: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00037: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991763 0.08484801]\n"," [0.06486402 0.07909421]\n"," [0.04636568 0.05816047]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738282 0.07197911 0.05226307]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991798 0.0848481 ]\n"," [0.06486408 0.07909415]\n"," [0.04636611 0.05815983]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738304 0.07197911 0.05226297]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991791 0.08484805]\n"," [0.06486437 0.07909434]\n"," [0.04636546 0.05815967]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738298 0.07197935 0.05226257]\n","\n","Epoch 00040: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991737 0.08484787]\n"," [0.06486395 0.07909412]\n"," [0.04636628 0.05815994]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738262 0.07197904 0.05226311]\n","\n","Epoch 00041: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00041: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991777 0.08484805]\n"," [0.06486454 0.07909455]\n"," [0.04636664 0.05815982]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738291 0.07197955 0.05226323]\n","\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991785 0.08484806]\n"," [0.06486486 0.07909479]\n"," [0.04636688 0.05816011]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738295 0.07197982 0.0522635 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991781 0.08484804]\n"," [0.06486484 0.07909479]\n"," [0.04636647 0.05815983]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738293 0.07197981 0.05226315]\n","\n","Epoch 00044: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991781 0.08484805]\n"," [0.06486477 0.07909474]\n"," [0.04636721 0.05816023]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738293 0.07197976 0.05226372]\n","\n","Epoch 00045: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00045: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch 00045: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991784 0.08484806]\n"," [0.06486496 0.07909483]\n"," [0.04636681 0.05815986]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738295 0.0719799  0.05226334]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991783 0.08484807]\n"," [0.06486496 0.0790948 ]\n"," [0.04636658 0.05815971]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738295 0.07197988 0.05226315]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991783 0.08484808]\n"," [0.06486496 0.0790948 ]\n"," [0.04636635 0.05815966]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738295 0.07197988 0.052263  ]\n","\n","Epoch 00048: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991783 0.08484807]\n"," [0.06486495 0.07909479]\n"," [0.04636658 0.05815973]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738295 0.07197987 0.05226315]\n","\n","Epoch 00049: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00049: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch 00049: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.07742107 0.09364641]\n"," [0.06991788 0.08484809]\n"," [0.06486512 0.07909489]\n"," [0.04636648 0.05815958]]\n","\n","Average MAE Loss:\n","[0.08553374 0.07738298 0.07198    0.05226303]\n","\n","Central Aggregation\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07526274 0.09149369]\n"," [0.07524298 0.09147515]\n"," [0.07498072 0.09121565]]\n","\n","Average MAE Loss:\n","[0.08348204 0.08337822 0.08335907 0.08309819]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07513977 0.09137164]\n"," [0.0751051  0.09134068]\n"," [0.07448204 0.09072498]]\n","\n","Average MAE Loss:\n","[0.08348204 0.08325571 0.08322289 0.08260351]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.9531e-06.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07502012 0.09125328]\n"," [0.07497253 0.09121219]\n"," [0.07422547 0.09047349]]\n","\n","Average MAE Loss:\n","[0.08348204 0.0831367  0.08309236 0.08234948]\n","\n","Epoch 00053: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch 00053: reducing learning rate of group 0 to 4.8828e-07.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07496294 0.09119658]\n"," [0.07490824 0.09115075]\n"," [0.0739854  0.09023426]]\n","\n","Average MAE Loss:\n","[0.08348204 0.08307976 0.08302949 0.08210983]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07490976 0.09114392]\n"," [0.07484578 0.09109105]\n"," [0.07376037 0.09000914]]\n","\n","Average MAE Loss:\n","[0.08348204 0.08302684 0.08296841 0.08188475]\n","\n","Epoch 00055: reducing learning rate of group 0 to 1.2207e-07.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07485929 0.09109375]\n"," [0.07478589 0.09103338]\n"," [0.07354023 0.08978918]]\n","\n","Average MAE Loss:\n","[0.08348204 0.08297652 0.08290963 0.08166471]\n","\n","Epoch 00056: reducing learning rate of group 0 to 9.7656e-07.\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07481095 0.09104589]\n"," [0.0747278  0.09097645]\n"," [0.07343305 0.08968172]]\n","\n","Average MAE Loss:\n","[0.08348204 0.08292842 0.08285213 0.08155738]\n","\n","Epoch 00057: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch 00057: reducing learning rate of group 0 to 2.4414e-07.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.0753669  0.09159719]\n"," [0.07478749 0.09102276]\n"," [0.07469966 0.09094867]\n"," [0.07333058 0.08957868]]\n","\n","Average MAE Loss:\n","[0.08348204 0.08290513 0.08282416 0.08145463]\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-132-26880d6a667d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_500/4_8/global_test/GNNs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Try'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_cnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbosity_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-109-ef2fa5082490>\u001b[0m in \u001b[0;36mtrain_cnns\u001b[0;34m(args, dataset, seed, ratio, verbose, train_validate_verbose, train_validate_verbosity_epochs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m# Train the current hospital at the current timepoint for 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mmae_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch_gnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_hospital\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;31m# Updating the hospital\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-115-0e12e7c0896f>\u001b[0m in \u001b[0;36mtrain_one_epoch_gnns\u001b[0;34m(args, hospital, train_data, table_hospital)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Update the weights of the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmae_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mhospital\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_div_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# seed = 500 table = 4/8 fixed folds CNN_2\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_500/4_8/global_test/GNNs/'\n","args.save_name='Try'\n","train_cnns(args, dataset,seed=500,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"tY7gZyeQaHdO"},"source":["#### CNN3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1258,"status":"error","timestamp":1690029793925,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"t1yhWVwIaJiZ","outputId":"d0ff7827-0995-4a1c-a844-49dd6bd15aed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-1210837de387>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Simple3_CNN_4D-FED-GNN++'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_cnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbosity_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-2b4583b4e3e1>\u001b[0m in \u001b[0;36mtrain_cnns\u001b[0;34m(args, dataset, seed, ratio, verbose, train_validate_verbose, train_validate_verbosity_epochs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m# Train the current hospital at the current timepoint for 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mmae_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch_gnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_hospital\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;31m# Updating the hospital\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-7904a58cbf49>\u001b[0m in \u001b[0;36mtrain_one_epoch_gnns\u001b[0;34m(args, hospital, train_data, table_hospital)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Update the weights of the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mmae_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mhospital\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# seed = 10 table = 4/8 fixed folds CNN_2\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Simple3_CNN_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"BDKGEEKpqE9-"},"source":["### Resnets"]},{"cell_type":"markdown","metadata":{"id":"q-PakAeOC1mx"},"source":["**Tried resnet 18,50,101 no good result**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Mc6PXBU8Z_F"},"outputs":[],"source":["import torch.nn.functional as F\n","from torchvision import models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBPx85H_8ffi"},"outputs":[],"source":["class Resnet50(nn.Module):\n","    def __init__(self):\n","        super(Resnet50, self).__init__()\n","\n","        # Load a pre-trained ResNet model and remove the last layer (fc layer)\n","        self.resnet = models.resnet18(weights=None)\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Modify the first layer to get 1 input channel\n","        in_features = self.resnet.fc.in_features\n","\n","        # Replace the last layer (fc layer) with a new one\n","        self.resnet.fc = nn.Linear(in_features, 35*35)\n","\n","    def forward(self, x):\n","        # Pass the input tensor through each of our operations (or 'layers')\n","        x = x.view(1,1,x.shape[0],x.shape[1])\n","        pred = F.relu(self.resnet(x))\n","        pred = pred.view(35,35)\n","        return pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1690036584797,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"6cQ3-LxK-g_B","outputId":"8f6ec631-d117-4167-b51e-bdd4557ebf0a"},"outputs":[{"data":{"text/plain":["tensor([[0.0000, 0.0000, 0.0951,  ..., 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.3166, 0.0000,  ..., 0.2617, 0.5820, 0.8956],\n","        [0.0000, 0.0000, 0.0742,  ..., 0.0000, 0.0000, 0.0000],\n","        ...,\n","        [0.0000, 0.0000, 0.0309,  ..., 0.7270, 0.0693, 0.0000],\n","        [0.0000, 0.0000, 0.0337,  ..., 0.0000, 0.5709, 0.0000],\n","        [0.3863, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n","       grad_fn=<ViewBackward0>)"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["model = Resnet50()\n","model(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1690036588019,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"HrIuMYyY5Gaq","outputId":"bd7d71e8-6f20-4dfe-e178-028e90a064b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 11798665\n","Trainable parameters: 11798665\n"]}],"source":["model = Resnet50()\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWx961YD_TNI"},"outputs":[],"source":["class Hospital_resnet50():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = Resnet50().to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=5,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":29091,"status":"error","timestamp":1690036624290,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"LI40jogxAfYi","outputId":"a871a395-8b37-4b4c-8f87-4b7af9e82c65"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08592845 0.08281599]\n"," [0.07618625 0.0732338 ]\n"," [0.08539023 0.08210879]\n"," [0.11087855 0.10611269]]\n","\n","Average MAE Loss:\n","[0.08437222 0.07471003 0.08374951 0.10849562]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08661643 0.08300977]\n"," [0.08161425 0.07880727]\n"," [0.08653458 0.08286455]\n"," [0.08588463 0.08277113]]\n","\n","Average MAE Loss:\n","[0.0848131  0.08021076 0.08469957 0.08432788]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08675836 0.08305413]\n"," [0.09307665 0.09023138]\n"," [0.08681499 0.08309603]\n"," [0.0858959  0.08247726]]\n","\n","Average MAE Loss:\n","[0.08490625 0.09165402 0.08495551 0.08418658]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08716867 0.0833856 ]\n"," [0.10636272 0.1031898 ]\n"," [0.08679196 0.08325998]\n"," [0.08625749 0.08246068]]\n","\n","Average MAE Loss:\n","[0.08527713 0.10477626 0.08502597 0.08435909]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08725308 0.08356883]\n"," [0.12035779 0.11716322]\n"," [0.08688134 0.08310843]\n"," [0.08640953 0.08234793]]\n","\n","Average MAE Loss:\n","[0.08541096 0.1187605  0.08499488 0.08437873]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08727308 0.08360193]\n"," [0.1348533  0.13144805]\n"," [0.08687932 0.08306754]\n"," [0.08661931 0.08269882]]\n","\n","Average MAE Loss:\n","[0.0854375  0.13315068 0.08497343 0.08465906]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08722134 0.08359671]\n"," [0.14985679 0.14552932]\n"," [0.08682582 0.08302381]\n"," [0.08643277 0.08231418]]\n","\n","Average MAE Loss:\n","[0.08540902 0.14769306 0.08492481 0.08437347]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08438274 0.08161021]\n"," [0.0866629  0.08237893]\n"," [0.08716987 0.08358852]\n"," [0.08754553 0.08328693]]\n","\n","Average MAE Loss:\n","[0.08299648 0.08452092 0.0853792  0.08541623]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08553023 0.08269231]\n"," [0.08688157 0.08245705]\n"," [0.0871667  0.08354877]\n"," [0.08738551 0.08328594]]\n","\n","Average MAE Loss:\n","[0.08411127 0.08466931 0.08535773 0.08533573]\n","\n","Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.0857935  0.08304479]\n"," [0.0870797  0.08251598]\n"," [0.08725265 0.08364077]\n"," [0.08742245 0.08319088]]\n","\n","Average MAE Loss:\n","[0.08441914 0.08479784 0.08544671 0.08530667]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08596178 0.08310243]\n"," [0.08725254 0.08255506]\n"," [0.08724996 0.08365224]\n"," [0.08769857 0.08344111]]\n","\n","Average MAE Loss:\n","[0.08453211 0.0849038  0.0854511  0.08556984]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08633032 0.08325765]\n"," [0.08740937 0.08257717]\n"," [0.08724945 0.08365501]\n"," [0.08777461 0.0836444 ]]\n","\n","Average MAE Loss:\n","[0.08479398 0.08499327 0.08545223 0.08570951]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0863202  0.08325658]\n"," [0.08756882 0.08266052]\n"," [0.08724936 0.08365544]\n"," [0.08758167 0.08388075]]\n","\n","Average MAE Loss:\n","[0.08478839 0.08511467 0.0854524  0.08573121]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-4698394ac60a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Resnet50_4D-FED-GNN++'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_cnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_validate_verbosity_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-59-6cfcbe3d540a>\u001b[0m in \u001b[0;36mtrain_cnns\u001b[0;34m(args, dataset, seed, ratio, verbose, train_validate_verbose, train_validate_verbosity_epochs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m# Train the current hospital at the current timepoint for 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mmae_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch_gnns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhospital\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_hospital\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;31m# Updating the hospital\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-7904a58cbf49>\u001b[0m in \u001b[0;36mtrain_one_epoch_gnns\u001b[0;34m(args, hospital, train_data, table_hospital)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Update the weights of the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mmae_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mhospital\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# seed = 10 table = 4/8 fixed folds CNN_2 dropout\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='Resnet50_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"8KHFUzhADA6_"},"source":["### VGG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0SEUyZ8DDE2"},"outputs":[],"source":["import torch.nn.functional as F\n","from torchvision import models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2747,"status":"ok","timestamp":1690370082056,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"yEUk7HZoKT8p","outputId":"7d644498-0e40-4011-9908-11cd1b1c817f"},"outputs":[{"data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): ReLU(inplace=True)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): ReLU(inplace=True)\n","    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): ReLU(inplace=True)\n","    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (33): ReLU(inplace=True)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): ReLU(inplace=True)\n","    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["models.vgg19(weights=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wuHCWZhQDVFF"},"outputs":[],"source":["class VGG(nn.Module):\n","    def __init__(self):\n","        super(VGG, self).__init__()\n","\n","        # Load a VGG model and remove the last layer (fc layer)\n","        self.vgg = models.vgg19(weights=None)  # Turn off pre-training\n","        self.vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  # Modify the first layer to get 1 input channel\n","        in_features = self.vgg.classifier[6].in_features\n","\n","        # Replace the last layer (fc layer) with a new one\n","        self.vgg.classifier[6] = nn.Linear(in_features, 35*35)\n","\n","    def forward(self, x):\n","        # Pass the input tensor through each of our operations (or 'layers')\n","        x = x.view(1,1,x.shape[0],x.shape[1])\n","        pred = F.relu(self.vgg(x))\n","        pred = pred.view(35,35)\n","        return pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2501,"status":"ok","timestamp":1690370094146,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"FekcCnLlDR2W","outputId":"a150a2a7-ab93-4cdb-aeb5-a2beed521fa9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 144587913\n","Trainable parameters: 144587913\n"]}],"source":["model = VGG()\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Total parameters:\", total_params)\n","print(\"Trainable parameters:\", trainable_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUBabIrkEm4g"},"outputs":[],"source":["class Hospital_vgg():\n","    def __init__(self, args,device):\n","        \"\"\"\n","        Hospital object contains a GNN and an optimizer for each timepoint\n","\n","        Hospital object can update GNN-layer wise weights of its GNNs\n","        \"\"\"\n","\n","        self.model = VGG().to(device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=args.lr)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min',factor=0.5,patience=5,verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587959,"status":"ok","timestamp":1690370704745,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"IudYbyWdKc6j","outputId":"bda51ce2-fa14-432b-b064-5ee1651a44e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 1. 1.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.5\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05137331 0.04872084]\n"," [0.08400421 0.08107341]\n"," [0.05294302 0.0504333 ]\n"," [0.05192866 0.04903244]]\n","\n","Average MAE Loss:\n","[0.05004708 0.08253881 0.05168816 0.05048055]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.05631251 0.05447202]\n"," [0.08400421 0.08107341]\n"," [0.04722659 0.04550089]\n"," [0.04832608 0.0464755 ]]\n","\n","Average MAE Loss:\n","[0.05539227 0.08253881 0.04636374 0.04740079]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04171986 0.03899236]\n"," [0.08400421 0.08107341]\n"," [0.04533347 0.04332423]\n"," [0.04588033 0.04396379]]\n","\n","Average MAE Loss:\n","[0.04035611 0.08253881 0.04432885 0.04492206]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04145054 0.03862505]\n"," [0.08400421 0.08107341]\n"," [0.04550554 0.04338966]\n"," [0.04948786 0.04770737]]\n","\n","Average MAE Loss:\n","[0.04003779 0.08253881 0.0444476  0.04859762]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04102066 0.03815486]\n"," [0.08400421 0.08107341]\n"," [0.04559781 0.04315906]\n"," [0.04453561 0.0427208 ]]\n","\n","Average MAE Loss:\n","[0.03958776 0.08253881 0.04437843 0.04362821]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04112215 0.03819382]\n"," [0.08400421 0.08107341]\n"," [0.04502717 0.0426464 ]\n"," [0.04426686 0.04240426]]\n","\n","Average MAE Loss:\n","[0.03965798 0.08253881 0.04383678 0.04333556]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04116442 0.03822201]\n"," [0.08400421 0.08107341]\n"," [0.04409751 0.04199483]\n"," [0.04454303 0.04280924]]\n","\n","Average MAE Loss:\n","[0.03969322 0.08253881 0.04304617 0.04367613]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.0490224  0.04603152]\n"," [0.04454303 0.04280924]\n"," [0.04351604 0.04122752]\n"," [0.04542189 0.04314018]]\n","\n","Average MAE Loss:\n","[0.04752696 0.04367613 0.04237178 0.04428104]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04265267 0.04004643]\n"," [0.04454303 0.04280924]\n"," [0.04203298 0.03961812]\n"," [0.04444267 0.04208744]]\n","\n","Average MAE Loss:\n","[0.04134955 0.04367613 0.04082555 0.04326505]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.04112002 0.03875225]\n"," [0.04454303 0.04280924]\n"," [0.04199963 0.03961313]\n"," [0.04329499 0.04125392]]\n","\n","Average MAE Loss:\n","[0.03993614 0.04367613 0.04080638 0.04227446]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04048474 0.03794096]\n"," [0.04454303 0.04280924]\n"," [0.04150297 0.03935027]\n"," [0.04304169 0.04107162]]\n","\n","Average MAE Loss:\n","[0.03921285 0.04367613 0.04042662 0.04205666]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04094326 0.03849351]\n"," [0.04454303 0.04280924]\n"," [0.04250394 0.04045132]\n"," [0.04291335 0.04062777]]\n","\n","Average MAE Loss:\n","[0.03971839 0.04367613 0.04147763 0.04177056]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04038299 0.03808183]\n"," [0.04454303 0.04280924]\n"," [0.04098408 0.03853311]\n"," [0.04253413 0.04051489]]\n","\n","Average MAE Loss:\n","[0.03923241 0.04367613 0.03975859 0.04152451]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04024964 0.03809938]\n"," [0.04454303 0.04280924]\n"," [0.04079752 0.03874115]\n"," [0.04227897 0.0404332 ]]\n","\n","Average MAE Loss:\n","[0.03917451 0.04367613 0.03976933 0.04135609]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04090338 0.03845378]\n"," [0.07692676 0.07375861]\n"," [0.04082042 0.03867039]\n"," [0.04200129 0.03966454]]\n","\n","Average MAE Loss:\n","[0.03967858 0.07534268 0.0397454  0.04083291]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04007932 0.03786402]\n"," [0.07692676 0.07375861]\n"," [0.04158595 0.03910794]\n"," [0.03894654 0.03736365]]\n","\n","Average MAE Loss:\n","[0.03897167 0.07534268 0.04034695 0.0381551 ]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.03988433 0.03690491]\n"," [0.07692676 0.07375861]\n"," [0.0397494  0.03822515]\n"," [0.03923501 0.03745071]]\n","\n","Average MAE Loss:\n","[0.03839462 0.07534268 0.03898727 0.03834286]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.03928193 0.03654024]\n"," [0.07692676 0.07375861]\n"," [0.03994567 0.03807421]\n"," [0.03878365 0.03714829]]\n","\n","Average MAE Loss:\n","[0.03791108 0.07534268 0.03900994 0.03796597]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.0390484  0.03627182]\n"," [0.07692676 0.07375861]\n"," [0.03976101 0.03768377]\n"," [0.03953175 0.03754565]]\n","\n","Average MAE Loss:\n","[0.03766011 0.07534268 0.03872239 0.0385387 ]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03918803 0.03639095]\n"," [0.07692676 0.07375861]\n"," [0.03956254 0.03760475]\n"," [0.0387998  0.03724601]]\n","\n","Average MAE Loss:\n","[0.03778949 0.07534268 0.03858364 0.0380229 ]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03927833 0.03647465]\n"," [0.07692676 0.07375861]\n"," [0.03953713 0.03758867]\n"," [0.03847743 0.03702818]]\n","\n","Average MAE Loss:\n","[0.03787649 0.07534268 0.0385629  0.03775281]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.04117047 0.03859954]\n"," [0.03847743 0.03702818]\n"," [0.04082139 0.03877514]\n"," [0.03995059 0.03771248]]\n","\n","Average MAE Loss:\n","[0.039885   0.03775281 0.03979827 0.03883154]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04163928 0.03853289]\n"," [0.03847743 0.03702818]\n"," [0.04018874 0.03795624]\n"," [0.03900946 0.0373967 ]]\n","\n","Average MAE Loss:\n","[0.04008609 0.03775281 0.03907249 0.03820308]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.03924891 0.03656428]\n"," [0.03847743 0.03702818]\n"," [0.03975933 0.03733352]\n"," [0.03861783 0.03704114]]\n","\n","Average MAE Loss:\n","[0.03790659 0.03775281 0.03854643 0.03782948]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.03940733 0.03641548]\n"," [0.03847743 0.03702818]\n"," [0.03959582 0.03753611]\n"," [0.03900874 0.03756148]]\n","\n","Average MAE Loss:\n","[0.03791141 0.03775281 0.03856596 0.03828511]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.03896707 0.03642964]\n"," [0.03847743 0.03702818]\n"," [0.03965567 0.03768876]\n"," [0.03876325 0.03687745]]\n","\n","Average MAE Loss:\n","[0.03769836 0.03775281 0.03867222 0.03782035]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03869461 0.03594202]\n"," [0.03847743 0.03702818]\n"," [0.03929176 0.03742031]\n"," [0.03897831 0.03745284]]\n","\n","Average MAE Loss:\n","[0.03731831 0.03775281 0.03835603 0.03821557]\n","\n","Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03866512 0.03589978]\n"," [0.03847743 0.03702818]\n"," [0.03964422 0.03740455]\n"," [0.03818128 0.03645341]]\n","\n","Average MAE Loss:\n","[0.03728245 0.03775281 0.03852438 0.03731735]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03860632 0.03607279]\n"," [0.04296567 0.04041703]\n"," [0.03941377 0.0374622 ]\n"," [0.03795238 0.03596593]]\n","\n","Average MAE Loss:\n","[0.03733955 0.04169135 0.03843798 0.03695915]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.03828702 0.03551932]\n"," [0.04296567 0.04041703]\n"," [0.03926272 0.03739295]\n"," [0.0380438  0.03638991]]\n","\n","Average MAE Loss:\n","[0.03690317 0.04169135 0.03832783 0.03721685]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03829023 0.03569038]\n"," [0.04296567 0.04041703]\n"," [0.03894643 0.03700709]\n"," [0.03804625 0.03642223]]\n","\n","Average MAE Loss:\n","[0.0369903  0.04169135 0.03797676 0.03723424]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03832441 0.03577938]\n"," [0.04296567 0.04041703]\n"," [0.03943521 0.03763584]\n"," [0.03813218 0.0365252 ]]\n","\n","Average MAE Loss:\n","[0.03705189 0.04169135 0.03853552 0.03732869]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03817997 0.03549375]\n"," [0.04296567 0.04041703]\n"," [0.03878356 0.03655538]\n"," [0.03812033 0.03646858]]\n","\n","Average MAE Loss:\n","[0.03683686 0.04169135 0.03766947 0.03729445]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.03835112 0.03576062]\n"," [0.04296567 0.04041703]\n"," [0.03904775 0.03708839]\n"," [0.03785766 0.03630257]]\n","\n","Average MAE Loss:\n","[0.03705587 0.04169135 0.03806807 0.03708012]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03828491 0.03558825]\n"," [0.04296567 0.04041703]\n"," [0.03919452 0.03707941]\n"," [0.03786497 0.03632827]]\n","\n","Average MAE Loss:\n","[0.03693658 0.04169135 0.03813697 0.03709662]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03861573 0.03613396]\n"," [0.03786497 0.03632827]\n"," [0.03913955 0.03693943]\n"," [0.03830984 0.0362972 ]]\n","\n","Average MAE Loss:\n","[0.03737484 0.03709662 0.03803949 0.03730352]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.03824568 0.03557191]\n"," [0.03786497 0.03632827]\n"," [0.03965724 0.03772796]\n"," [0.03780857 0.03597851]]\n","\n","Average MAE Loss:\n","[0.03690879 0.03709662 0.0386926  0.03689354]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03832651 0.03565018]\n"," [0.03786497 0.03632827]\n"," [0.03900999 0.03692464]\n"," [0.03774076 0.03603491]]\n","\n","Average MAE Loss:\n","[0.03698835 0.03709662 0.03796732 0.03688784]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03829929 0.0356752 ]\n"," [0.03786497 0.03632827]\n"," [0.0393353  0.03721937]\n"," [0.03797418 0.03630671]]\n","\n","Average MAE Loss:\n","[0.03698724 0.03709662 0.03827734 0.03714045]\n","\n","Epoch 00039: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00039: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03819    0.03555138]\n"," [0.03786497 0.03632827]\n"," [0.03968979 0.03774247]\n"," [0.03780373 0.03616457]]\n","\n","Average MAE Loss:\n","[0.03687069 0.03709662 0.03871613 0.03698415]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03841234 0.03581609]\n"," [0.03786497 0.03632827]\n"," [0.03907991 0.03725052]\n"," [0.03780329 0.03617828]]\n","\n","Average MAE Loss:\n","[0.03711421 0.03709662 0.03816521 0.03699078]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03829688 0.03566218]\n"," [0.03786497 0.03632827]\n"," [0.03899861 0.03697322]\n"," [0.03791816 0.03632199]]\n","\n","Average MAE Loss:\n","[0.03697953 0.03709662 0.03798592 0.03712008]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03766902 0.03546171]\n"," [0.03762622 0.03559656]\n"," [0.03853382 0.03656359]\n"," [0.03756486 0.03567225]]\n","\n","Average MAE Loss:\n","[0.03656537 0.03661139 0.03754871 0.03661855]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03802157 0.03552287]\n"," [0.03762622 0.03559656]\n"," [0.03863432 0.03653952]\n"," [0.03765033 0.0358352 ]]\n","\n","Average MAE Loss:\n","[0.03677222 0.03661139 0.03758692 0.03674276]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03811631 0.03551524]\n"," [0.03762622 0.03559656]\n"," [0.03902478 0.03702142]\n"," [0.03782514 0.03606739]]\n","\n","Average MAE Loss:\n","[0.03681577 0.03661139 0.0380231  0.03694627]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03829447 0.03566524]\n"," [0.03762622 0.03559656]\n"," [0.03908561 0.0370857 ]\n"," [0.03788576 0.03614745]]\n","\n","Average MAE Loss:\n","[0.03697986 0.03661139 0.03808566 0.0370166 ]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03826732 0.03559689]\n"," [0.03762622 0.03559656]\n"," [0.03876846 0.03673054]\n"," [0.03792999 0.03623611]]\n","\n","Average MAE Loss:\n","[0.0369321  0.03661139 0.0377495  0.03708305]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03821512 0.03557689]\n"," [0.03762622 0.03559656]\n"," [0.03877581 0.03674145]\n"," [0.03785834 0.03617939]]\n","\n","Average MAE Loss:\n","[0.036896   0.03661139 0.03775863 0.03701887]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03816687 0.0355627 ]\n"," [0.03762622 0.03559656]\n"," [0.03877292 0.03682357]\n"," [0.037852   0.03619296]]\n","\n","Average MAE Loss:\n","[0.03686479 0.03661139 0.03779825 0.03702248]\n","\n","Epoch 00049: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00049: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00049: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03759035 0.03550625]\n"," [0.037852   0.03619296]\n"," [0.03803092 0.03557802]\n"," [0.03837116 0.03647608]]\n","\n","Average MAE Loss:\n","[0.0365483  0.03702248 0.03680447 0.03742362]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03772095 0.03550038]\n"," [0.037852   0.03619296]\n"," [0.03814841 0.03587072]\n"," [0.03801146 0.03621122]]\n","\n","Average MAE Loss:\n","[0.03661066 0.03702248 0.03700956 0.03711134]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03781233 0.0354429 ]\n"," [0.037852   0.03619296]\n"," [0.03865727 0.03654629]\n"," [0.03776188 0.03604775]]\n","\n","Average MAE Loss:\n","[0.03662762 0.03702248 0.03760178 0.03690481]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03794806 0.03549602]\n"," [0.037852   0.03619296]\n"," [0.03891467 0.03686008]\n"," [0.03762637 0.03592349]]\n","\n","Average MAE Loss:\n","[0.03672204 0.03702248 0.03788738 0.03677493]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03804232 0.03553427]\n"," [0.037852   0.03619296]\n"," [0.03895121 0.03690412]\n"," [0.03762283 0.03593146]]\n","\n","Average MAE Loss:\n","[0.03678829 0.03702248 0.03792767 0.03677715]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03816054 0.03561166]\n"," [0.037852   0.03619296]\n"," [0.03893847 0.03688225]\n"," [0.03766808 0.03600888]]\n","\n","Average MAE Loss:\n","[0.0368861  0.03702248 0.03791036 0.03683848]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00055: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03818189 0.03559683]\n"," [0.037852   0.03619296]\n"," [0.03896278 0.03693971]\n"," [0.03769196 0.0360427 ]]\n","\n","Average MAE Loss:\n","[0.03688936 0.03702248 0.03795125 0.03686733]\n","\n","Epoch 00056: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00056: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03762565 0.03561258]\n"," [0.03761985 0.03567658]\n"," [0.03776005 0.03579327]\n"," [0.03761296 0.03570296]]\n","\n","Average MAE Loss:\n","[0.03661912 0.03664822 0.03677666 0.03665796]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03766367 0.03554595]\n"," [0.03761985 0.03567658]\n"," [0.03809699 0.03608632]\n"," [0.0376096  0.03573506]]\n","\n","Average MAE Loss:\n","[0.03660481 0.03664822 0.03709165 0.03667233]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03770254 0.03549306]\n"," [0.03761985 0.03567658]\n"," [0.03819257 0.03615139]\n"," [0.03762064 0.03577312]]\n","\n","Average MAE Loss:\n","[0.0365978  0.03664822 0.03717198 0.03669688]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03775308 0.03548232]\n"," [0.03761985 0.03567658]\n"," [0.03839724 0.03637462]\n"," [0.03765066 0.03582828]]\n","\n","Average MAE Loss:\n","[0.0366177  0.03664822 0.03738593 0.03673947]\n","\n","\n","epochs finished with time:92.90197968482971\n","\n","[[0.03775308 0.03548232]\n"," [0.03761985 0.03567658]\n"," [0.03839724 0.03637462]\n"," [0.03765066 0.03582828]]\n","00:01:58.42404056500072\n","------------------------------------Fold [2/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.07231218 0.07421154]\n"," [0.11453133 0.11279516]\n"," [0.08313011 0.08175583]\n"," [0.0971334  0.09565951]]\n","\n","Average MAE Loss:\n","[0.07326186 0.11366324 0.08244297 0.09639646]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.07130106 0.07242892]\n"," [0.11453133 0.11279516]\n"," [0.07075741 0.07100349]\n"," [0.0713979  0.0718814 ]]\n","\n","Average MAE Loss:\n","[0.07186499 0.11366324 0.07088045 0.07163965]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.06558282 0.06646162]\n"," [0.11453133 0.11279516]\n"," [0.06942149 0.06954927]\n"," [0.07247173 0.07084761]]\n","\n","Average MAE Loss:\n","[0.06602222 0.11366324 0.06948538 0.07165967]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.06505572 0.06589481]\n"," [0.11453133 0.11279516]\n"," [0.06850526 0.06951197]\n"," [0.07330714 0.07184565]]\n","\n","Average MAE Loss:\n","[0.06547527 0.11366324 0.06900862 0.0725764 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.06455626 0.06547873]\n"," [0.11453133 0.11279516]\n"," [0.06826484 0.06876121]\n"," [0.07150832 0.07019124]]\n","\n","Average MAE Loss:\n","[0.06501749 0.11366324 0.06851302 0.07084978]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.06588027 0.06634803]\n"," [0.11453133 0.11279516]\n"," [0.06835485 0.06878284]\n"," [0.07055957 0.06914477]]\n","\n","Average MAE Loss:\n","[0.06611415 0.11366324 0.06856884 0.06985217]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.06611121 0.0663708 ]\n"," [0.11453133 0.11279516]\n"," [0.0695003  0.06960167]\n"," [0.06768508 0.06706288]]\n","\n","Average MAE Loss:\n","[0.06624101 0.11366324 0.06955099 0.06737398]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.07032075 0.07335343]\n"," [0.06768508 0.06706288]\n"," [0.06687297 0.0681272 ]\n"," [0.0764524  0.07437477]]\n","\n","Average MAE Loss:\n","[0.07183709 0.06737398 0.06750008 0.07541358]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.06548666 0.06701857]\n"," [0.06768508 0.06706288]\n"," [0.06655187 0.06734467]\n"," [0.06820151 0.06747814]]\n","\n","Average MAE Loss:\n","[0.06625261 0.06737398 0.06694827 0.06783982]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.064797   0.0649765 ]\n"," [0.06768508 0.06706288]\n"," [0.06581708 0.06702144]\n"," [0.06748064 0.06722387]]\n","\n","Average MAE Loss:\n","[0.06488675 0.06737398 0.06641926 0.06735225]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.06373935 0.06460428]\n"," [0.06768508 0.06706288]\n"," [0.06500177 0.06638267]\n"," [0.06777131 0.06726625]]\n","\n","Average MAE Loss:\n","[0.06417182 0.06737398 0.06569222 0.06751878]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.06471803 0.06503417]\n"," [0.06768508 0.06706288]\n"," [0.06457561 0.06601925]\n"," [0.06693295 0.06632389]]\n","\n","Average MAE Loss:\n","[0.0648761  0.06737398 0.06529743 0.06662842]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.06452335 0.06489167]\n"," [0.06768508 0.06706288]\n"," [0.06495233 0.06580738]\n"," [0.06842784 0.0670775 ]]\n","\n","Average MAE Loss:\n","[0.06470751 0.06737398 0.06537986 0.06775267]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.06451921 0.06483229]\n"," [0.06768508 0.06706288]\n"," [0.06636624 0.06716958]\n"," [0.06796185 0.06701601]]\n","\n","Average MAE Loss:\n","[0.06467575 0.06737398 0.06676791 0.06748893]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.06219599 0.06417446]\n"," [0.10845999 0.10662094]\n"," [0.06569963 0.06705909]\n"," [0.06503584 0.06648176]]\n","\n","Average MAE Loss:\n","[0.06318522 0.10754046 0.06637936 0.0657588 ]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.0625793  0.06340288]\n"," [0.10845999 0.10662094]\n"," [0.0641017  0.06524811]\n"," [0.06557192 0.06482714]]\n","\n","Average MAE Loss:\n","[0.06299109 0.10754046 0.06467491 0.06519953]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.06231565 0.06275018]\n"," [0.10845999 0.10662094]\n"," [0.06400163 0.06483913]\n"," [0.06442643 0.06365315]]\n","\n","Average MAE Loss:\n","[0.06253292 0.10754046 0.06442038 0.06403979]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.06308239 0.06312967]\n"," [0.10845999 0.10662094]\n"," [0.06350102 0.06430992]\n"," [0.0631947  0.06302711]]\n","\n","Average MAE Loss:\n","[0.06310603 0.10754046 0.06390547 0.0631109 ]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.06197991 0.06275408]\n"," [0.10845999 0.10662094]\n"," [0.06540045 0.06569714]\n"," [0.06382835 0.06350581]]\n","\n","Average MAE Loss:\n","[0.062367   0.10754046 0.06554879 0.06366708]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.06263397 0.06276019]\n"," [0.10845999 0.10662094]\n"," [0.06373898 0.06412649]\n"," [0.06461632 0.06384361]]\n","\n","Average MAE Loss:\n","[0.06269708 0.10754046 0.06393273 0.06422997]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.06212475 0.06235915]\n"," [0.10845999 0.10662094]\n"," [0.06351893 0.06405439]\n"," [0.06244468 0.06294866]]\n","\n","Average MAE Loss:\n","[0.06224195 0.10754046 0.06378666 0.06269667]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07729942 0.07584816]\n"," [0.06244468 0.06294866]\n"," [0.0640408  0.06447847]\n"," [0.06504982 0.06470073]]\n","\n","Average MAE Loss:\n","[0.07657379 0.06269667 0.06425964 0.06487527]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.06636891 0.06619048]\n"," [0.06244468 0.06294866]\n"," [0.06307312 0.06391458]\n"," [0.06223242 0.06269557]]\n","\n","Average MAE Loss:\n","[0.0662797  0.06269667 0.06349385 0.06246399]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.06239442 0.06305996]\n"," [0.06244468 0.06294866]\n"," [0.06325919 0.06408631]\n"," [0.06363153 0.06320773]]\n","\n","Average MAE Loss:\n","[0.06272719 0.06269667 0.06367275 0.06341963]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.06281026 0.0630819 ]\n"," [0.06244468 0.06294866]\n"," [0.06283478 0.06360352]\n"," [0.06218356 0.0625419 ]]\n","\n","Average MAE Loss:\n","[0.06294608 0.06269667 0.06321915 0.06236273]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.06248587 0.06271151]\n"," [0.06244468 0.06294866]\n"," [0.06268834 0.06344154]\n"," [0.06357661 0.0630973 ]]\n","\n","Average MAE Loss:\n","[0.06259869 0.06269667 0.06306494 0.06333695]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.06254642 0.06264426]\n"," [0.06244468 0.06294866]\n"," [0.06249153 0.06345702]\n"," [0.06362412 0.06309198]]\n","\n","Average MAE Loss:\n","[0.06259534 0.06269667 0.06297428 0.06335805]\n","\n","Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.06189181 0.06221127]\n"," [0.06244468 0.06294866]\n"," [0.06255891 0.06335539]\n"," [0.0628513  0.06241816]]\n","\n","Average MAE Loss:\n","[0.06205154 0.06269667 0.06295715 0.06263473]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06104655 0.06148426]\n"," [0.07053656 0.06906762]\n"," [0.06204525 0.06314569]\n"," [0.06155357 0.06213889]]\n","\n","Average MAE Loss:\n","[0.0612654  0.06980209 0.06259547 0.06184623]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06161646 0.06178573]\n"," [0.07053656 0.06906762]\n"," [0.06264203 0.06354904]\n"," [0.06293221 0.06245282]]\n","\n","Average MAE Loss:\n","[0.0617011  0.06980209 0.06309553 0.06269251]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06116518 0.0615975 ]\n"," [0.07053656 0.06906762]\n"," [0.06256559 0.06358973]\n"," [0.06222631 0.06210787]]\n","\n","Average MAE Loss:\n","[0.06138134 0.06980209 0.06307766 0.06216709]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06140862 0.06179404]\n"," [0.07053656 0.06906762]\n"," [0.06219909 0.06316558]\n"," [0.06209974 0.0620029 ]]\n","\n","Average MAE Loss:\n","[0.06160133 0.06980209 0.06268233 0.06205132]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06115144 0.06175336]\n"," [0.07053656 0.06906762]\n"," [0.06268771 0.06377906]\n"," [0.06234066 0.06194156]]\n","\n","Average MAE Loss:\n","[0.0614524  0.06980209 0.06323338 0.06214111]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06169603 0.06207814]\n"," [0.07053656 0.06906762]\n"," [0.06230783 0.06329811]\n"," [0.06227844 0.0619601 ]]\n","\n","Average MAE Loss:\n","[0.06188709 0.06980209 0.06280297 0.06211927]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06150258 0.06199899]\n"," [0.07053656 0.06906762]\n"," [0.0626695  0.06344819]\n"," [0.06283018 0.06213117]]\n","\n","Average MAE Loss:\n","[0.06175078 0.06980209 0.06305884 0.06248067]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00035: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00035: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06347979 0.06307397]\n"," [0.06283018 0.06213117]\n"," [0.06116905 0.06214052]\n"," [0.06212173 0.06253601]]\n","\n","Average MAE Loss:\n","[0.06327688 0.06248067 0.06165479 0.06232887]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06098482 0.06140831]\n"," [0.06283018 0.06213117]\n"," [0.06203769 0.06306973]\n"," [0.06200645 0.06231247]]\n","\n","Average MAE Loss:\n","[0.06119656 0.06248067 0.06255371 0.06215946]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06114906 0.06147531]\n"," [0.06283018 0.06213117]\n"," [0.06193715 0.062954  ]\n"," [0.06232597 0.06231372]]\n","\n","Average MAE Loss:\n","[0.06131218 0.06248067 0.06244557 0.06231984]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06119006 0.06157067]\n"," [0.06283018 0.06213117]\n"," [0.06193401 0.0628621 ]\n"," [0.0616675  0.06186741]]\n","\n","Average MAE Loss:\n","[0.06138037 0.06248067 0.06239805 0.06176746]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06139904 0.06175798]\n"," [0.06283018 0.06213117]\n"," [0.06198093 0.06299151]\n"," [0.06192915 0.0618829 ]]\n","\n","Average MAE Loss:\n","[0.06157851 0.06248067 0.06248622 0.06190602]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06120327 0.06169404]\n"," [0.06283018 0.06213117]\n"," [0.06233716 0.06319745]\n"," [0.06242284 0.06209033]]\n","\n","Average MAE Loss:\n","[0.06144865 0.06248067 0.06276731 0.06225658]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06117417 0.06163859]\n"," [0.06283018 0.06213117]\n"," [0.0621803  0.06290239]\n"," [0.06200817 0.0617533 ]]\n","\n","Average MAE Loss:\n","[0.06140638 0.06248067 0.06254135 0.06188073]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00042: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.06155127 0.06159937]\n"," [0.06246292 0.06219843]\n"," [0.06155717 0.06182661]\n"," [0.06151193 0.06164608]]\n","\n","Average MAE Loss:\n","[0.06157532 0.06233068 0.06169189 0.06157901]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.06126661 0.06140963]\n"," [0.06246292 0.06219843]\n"," [0.06155647 0.06207373]\n"," [0.06147417 0.06162729]]\n","\n","Average MAE Loss:\n","[0.06133812 0.06233068 0.0618151  0.06155073]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.06107488 0.06132132]\n"," [0.06246292 0.06219843]\n"," [0.0617985  0.06234919]\n"," [0.06174139 0.06162907]]\n","\n","Average MAE Loss:\n","[0.0611981  0.06233068 0.06207384 0.06168523]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.06096089 0.06131773]\n"," [0.06246292 0.06219843]\n"," [0.06179109 0.06248981]\n"," [0.06188392 0.06166219]]\n","\n","Average MAE Loss:\n","[0.06113931 0.06233068 0.06214045 0.06177306]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.06106869 0.06139863]\n"," [0.06246292 0.06219843]\n"," [0.06160753 0.0625205 ]\n"," [0.06208823 0.06169954]]\n","\n","Average MAE Loss:\n","[0.06123366 0.06233068 0.06206402 0.06189388]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.06107851 0.06140004]\n"," [0.06246292 0.06219843]\n"," [0.06172452 0.0626182 ]\n"," [0.06200963 0.06168306]]\n","\n","Average MAE Loss:\n","[0.06123928 0.06233068 0.06217136 0.06184634]\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.06107198 0.06143361]\n"," [0.06246292 0.06219843]\n"," [0.06176241 0.06265743]\n"," [0.06185878 0.06168468]]\n","\n","Average MAE Loss:\n","[0.0612528  0.06233068 0.06220992 0.06177173]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.06185363 0.06177368]\n"," [0.06185878 0.06168468]\n"," [0.06104913 0.06144533]\n"," [0.06204309 0.06250185]]\n","\n","Average MAE Loss:\n","[0.06181366 0.06177173 0.06124723 0.06227247]\n","\n","Epoch 00050: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.06119663 0.0613935 ]\n"," [0.06185878 0.06168468]\n"," [0.06104861 0.06154266]\n"," [0.06194008 0.06223002]]\n","\n","Average MAE Loss:\n","[0.06129506 0.06177173 0.06129563 0.06208505]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.06120561 0.06139459]\n"," [0.06185878 0.06168468]\n"," [0.0609966  0.06165429]\n"," [0.06159251 0.06190903]]\n","\n","Average MAE Loss:\n","[0.0613001  0.06177173 0.06132545 0.06175077]\n","\n","Epoch 00052: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.06123562 0.0614224 ]\n"," [0.06185878 0.06168468]\n"," [0.06111084 0.0618319 ]\n"," [0.06162932 0.06184307]]\n","\n","Average MAE Loss:\n","[0.06132901 0.06177173 0.06147137 0.06173619]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.06115199 0.06139396]\n"," [0.06185878 0.06168468]\n"," [0.06119367 0.06195455]\n"," [0.06174872 0.06187211]]\n","\n","Average MAE Loss:\n","[0.06127297 0.06177173 0.06157411 0.06181042]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.06106278 0.0613687 ]\n"," [0.06185878 0.06168468]\n"," [0.06132423 0.06210037]\n"," [0.06157416 0.06171287]]\n","\n","Average MAE Loss:\n","[0.06121574 0.06177173 0.0617123  0.06164351]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.06110246 0.06140949]\n"," [0.06185878 0.06168468]\n"," [0.06147206 0.0622539 ]\n"," [0.06155826 0.06166267]]\n","\n","Average MAE Loss:\n","[0.06125597 0.06177173 0.06186298 0.06161046]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00056: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00056: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06114923 0.06139917]\n"," [0.06123813 0.06145253]\n"," [0.06122777 0.061509  ]\n"," [0.06147374 0.06151654]]\n","\n","Average MAE Loss:\n","[0.0612742  0.06134533 0.06136838 0.06149514]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06110467 0.06137449]\n"," [0.06123813 0.06145253]\n"," [0.06124172 0.06160343]\n"," [0.06163722 0.06156617]]\n","\n","Average MAE Loss:\n","[0.06123958 0.06134533 0.06142257 0.06160169]\n","\n","Epoch 00058: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06105343 0.06134562]\n"," [0.06123813 0.06145253]\n"," [0.06126657 0.06170118]\n"," [0.06170337 0.06161967]]\n","\n","Average MAE Loss:\n","[0.06119952 0.06134533 0.06148388 0.06166152]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06104161 0.06133838]\n"," [0.06123813 0.06145253]\n"," [0.06130339 0.06178858]\n"," [0.06176502 0.06164578]]\n","\n","Average MAE Loss:\n","[0.06118999 0.06134533 0.06154598 0.0617054 ]\n","\n","\n","epochs finished with time:92.95838618278503\n","\n","[[0.06104161 0.06133838]\n"," [0.06123813 0.06145253]\n"," [0.06130339 0.06178858]\n"," [0.06176502 0.06164578]]\n","00:01:59.192204072000095\n","------------------------------------Fold [3/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05872196 0.05466568]\n"," [0.09797762 0.0868255 ]\n"," [0.06382369 0.05557458]\n"," [0.07621734 0.06571111]]\n","\n","Average MAE Loss:\n","[0.05669382 0.09240156 0.05969914 0.07096422]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.05103264 0.04442518]\n"," [0.09797762 0.0868255 ]\n"," [0.0547142  0.04854381]\n"," [0.05900925 0.05473598]]\n","\n","Average MAE Loss:\n","[0.04772891 0.09240156 0.051629   0.05687261]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0481408  0.0429396 ]\n"," [0.09797762 0.0868255 ]\n"," [0.05343208 0.04656859]\n"," [0.05346766 0.04808293]]\n","\n","Average MAE Loss:\n","[0.0455402  0.09240156 0.05000034 0.0507753 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04817798 0.0424927 ]\n"," [0.09797762 0.0868255 ]\n"," [0.05274459 0.0459068 ]\n"," [0.05467814 0.04658677]]\n","\n","Average MAE Loss:\n","[0.04533534 0.09240156 0.0493257  0.05063246]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04754891 0.04221515]\n"," [0.09797762 0.0868255 ]\n"," [0.05157207 0.04577228]\n"," [0.05215998 0.04577409]]\n","\n","Average MAE Loss:\n","[0.04488203 0.09240156 0.04867218 0.04896704]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04802203 0.04229225]\n"," [0.09797762 0.0868255 ]\n"," [0.05157385 0.04522375]\n"," [0.05397156 0.04578627]]\n","\n","Average MAE Loss:\n","[0.04515714 0.09240156 0.0483988  0.04987891]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.04988938 0.04268371]\n"," [0.09797762 0.0868255 ]\n"," [0.05121653 0.04523373]\n"," [0.05248058 0.04519115]]\n","\n","Average MAE Loss:\n","[0.04628655 0.09240156 0.04822513 0.04883587]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05944387 0.05187336]\n"," [0.05248058 0.04519115]\n"," [0.05011553 0.04376041]\n"," [0.05423573 0.04603042]]\n","\n","Average MAE Loss:\n","[0.05565862 0.04883587 0.04693797 0.05013307]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05191378 0.04559365]\n"," [0.05248058 0.04519115]\n"," [0.04970682 0.04239298]\n"," [0.05360765 0.04570828]]\n","\n","Average MAE Loss:\n","[0.04875371 0.04883587 0.0460499  0.04965797]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05141133 0.04463192]\n"," [0.05248058 0.04519115]\n"," [0.04911026 0.04309426]\n"," [0.0515799  0.04431515]]\n","\n","Average MAE Loss:\n","[0.04802163 0.04883587 0.04610226 0.04794753]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04956239 0.04339302]\n"," [0.05248058 0.04519115]\n"," [0.04851602 0.04256596]\n"," [0.05041443 0.0444224 ]]\n","\n","Average MAE Loss:\n","[0.0464777  0.04883587 0.04554099 0.04741842]\n","\n","Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.04986427 0.04317774]\n"," [0.05248058 0.04519115]\n"," [0.04866829 0.04163294]\n"," [0.05046634 0.04402022]]\n","\n","Average MAE Loss:\n","[0.04652101 0.04883587 0.04515061 0.04724328]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.04815818 0.04277708]\n"," [0.05248058 0.04519115]\n"," [0.04825643 0.04236756]\n"," [0.04998742 0.0438014 ]]\n","\n","Average MAE Loss:\n","[0.04546763 0.04883587 0.04531199 0.04689441]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0484905  0.04263931]\n"," [0.05248058 0.04519115]\n"," [0.04826492 0.04165759]\n"," [0.0503615  0.04385202]]\n","\n","Average MAE Loss:\n","[0.04556491 0.04883587 0.04496125 0.04710676]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.05096682 0.04497327]\n"," [0.0916696  0.08048098]\n"," [0.0480465  0.04429337]\n"," [0.04930592 0.04333015]]\n","\n","Average MAE Loss:\n","[0.04797005 0.08607529 0.04616994 0.04631803]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04797968 0.04151008]\n"," [0.0916696  0.08048098]\n"," [0.04805305 0.0420348 ]\n"," [0.04783027 0.04175974]]\n","\n","Average MAE Loss:\n","[0.04474488 0.08607529 0.04504392 0.044795  ]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04570849 0.04095627]\n"," [0.0916696  0.08048098]\n"," [0.04790086 0.04178748]\n"," [0.04824875 0.04116657]]\n","\n","Average MAE Loss:\n","[0.04333238 0.08607529 0.04484417 0.04470766]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0447698  0.04072768]\n"," [0.0916696  0.08048098]\n"," [0.04770339 0.04224514]\n"," [0.04751784 0.04088824]]\n","\n","Average MAE Loss:\n","[0.04274874 0.08607529 0.04497426 0.04420304]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04639548 0.04053755]\n"," [0.0916696  0.08048098]\n"," [0.04738865 0.04348997]\n"," [0.04736355 0.0425736 ]]\n","\n","Average MAE Loss:\n","[0.04346652 0.08607529 0.04543931 0.04496857]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04519203 0.04051342]\n"," [0.0916696  0.08048098]\n"," [0.04768296 0.04098556]\n"," [0.04724862 0.04034061]]\n","\n","Average MAE Loss:\n","[0.04285273 0.08607529 0.04433426 0.04379462]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.0457584  0.04022858]\n"," [0.0916696  0.08048098]\n"," [0.04657697 0.04166861]\n"," [0.04932422 0.04128845]]\n","\n","Average MAE Loss:\n","[0.04299349 0.08607529 0.04412279 0.04530633]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.08017503 0.06917592]\n"," [0.04932422 0.04128845]\n"," [0.04846235 0.04174783]\n"," [0.04707117 0.04111277]]\n","\n","Average MAE Loss:\n","[0.07467548 0.04530633 0.04510509 0.04409197]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04820662 0.04307837]\n"," [0.04932422 0.04128845]\n"," [0.04677498 0.04140912]\n"," [0.04806383 0.04082068]]\n","\n","Average MAE Loss:\n","[0.0456425  0.04530633 0.04409205 0.04444225]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04681168 0.0404766 ]\n"," [0.04932422 0.04128845]\n"," [0.04771597 0.04150058]\n"," [0.04823507 0.04101677]]\n","\n","Average MAE Loss:\n","[0.04364414 0.04530633 0.04460828 0.04462592]\n","\n","Epoch 00024: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04539971 0.04020739]\n"," [0.04932422 0.04128845]\n"," [0.04690257 0.04175917]\n"," [0.04720362 0.04163809]]\n","\n","Average MAE Loss:\n","[0.04280355 0.04530633 0.04433087 0.04442085]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04530533 0.04018398]\n"," [0.04932422 0.04128845]\n"," [0.04657488 0.04086642]\n"," [0.04839312 0.04060765]]\n","\n","Average MAE Loss:\n","[0.04274466 0.04530633 0.04372065 0.04450038]\n","\n","Epoch 00026: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04518325 0.0401054 ]\n"," [0.04932422 0.04128845]\n"," [0.04672755 0.04124955]\n"," [0.04736085 0.04061588]]\n","\n","Average MAE Loss:\n","[0.04264432 0.04530633 0.04398855 0.04398837]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04476462 0.04033449]\n"," [0.04932422 0.04128845]\n"," [0.04770281 0.04079075]\n"," [0.04735992 0.04031167]]\n","\n","Average MAE Loss:\n","[0.04254956 0.04530633 0.04424678 0.0438358 ]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04678346 0.03984655]\n"," [0.05737709 0.04747824]\n"," [0.04673749 0.04275966]\n"," [0.04620235 0.04058141]]\n","\n","Average MAE Loss:\n","[0.04331501 0.05242767 0.04474858 0.04339188]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04466295 0.03993821]\n"," [0.05737709 0.04747824]\n"," [0.04629635 0.04077462]\n"," [0.04722969 0.03995297]]\n","\n","Average MAE Loss:\n","[0.04230058 0.05242767 0.04353549 0.04359133]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04519588 0.03967833]\n"," [0.05737709 0.04747824]\n"," [0.04628921 0.04063307]\n"," [0.04635593 0.04031509]]\n","\n","Average MAE Loss:\n","[0.0424371  0.05242767 0.04346114 0.04333551]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04450845 0.03978928]\n"," [0.05737709 0.04747824]\n"," [0.04588795 0.04095922]\n"," [0.04644594 0.03989039]]\n","\n","Average MAE Loss:\n","[0.04214887 0.05242767 0.04342359 0.04316817]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04477895 0.03970042]\n"," [0.05737709 0.04747824]\n"," [0.04569389 0.04016738]\n"," [0.046409   0.03991381]]\n","\n","Average MAE Loss:\n","[0.04223968 0.05242767 0.04293063 0.04316141]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.04455483 0.03973228]\n"," [0.05737709 0.04747824]\n"," [0.04657242 0.04014086]\n"," [0.0460529  0.04027927]]\n","\n","Average MAE Loss:\n","[0.04214355 0.05242767 0.04335664 0.04316608]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04442802 0.03970769]\n"," [0.05737709 0.04747824]\n"," [0.04642805 0.04257318]\n"," [0.04662415 0.03985075]]\n","\n","Average MAE Loss:\n","[0.04206786 0.05242767 0.04450062 0.04323745]\n","\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04835564 0.04068461]\n"," [0.04662415 0.03985075]\n"," [0.04620869 0.0407484 ]\n"," [0.04681485 0.03991402]]\n","\n","Average MAE Loss:\n","[0.04452012 0.04323745 0.04347855 0.04336443]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04487968 0.03992473]\n"," [0.04662415 0.03985075]\n"," [0.04614338 0.04083953]\n"," [0.04641632 0.03967942]]\n","\n","Average MAE Loss:\n","[0.04240221 0.04323745 0.04349145 0.04304787]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04527933 0.0396295 ]\n"," [0.04662415 0.03985075]\n"," [0.04556683 0.04023139]\n"," [0.04603289 0.03988106]]\n","\n","Average MAE Loss:\n","[0.04245442 0.04323745 0.04289911 0.04295698]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04467851 0.0398479 ]\n"," [0.04662415 0.03985075]\n"," [0.04638089 0.040126  ]\n"," [0.04615179 0.03975228]]\n","\n","Average MAE Loss:\n","[0.0422632  0.04323745 0.04325345 0.04295204]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.0446692  0.03987637]\n"," [0.04662415 0.03985075]\n"," [0.04581494 0.04019106]\n"," [0.04631072 0.03976844]]\n","\n","Average MAE Loss:\n","[0.04227278 0.04323745 0.043003   0.04303958]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04494921 0.03978363]\n"," [0.04662415 0.03985075]\n"," [0.04579369 0.04066988]\n"," [0.04591738 0.04001928]]\n","\n","Average MAE Loss:\n","[0.04236642 0.04323745 0.04323179 0.04296833]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04464187 0.0398144 ]\n"," [0.04662415 0.03985075]\n"," [0.04633871 0.04034534]\n"," [0.04618175 0.03959555]]\n","\n","Average MAE Loss:\n","[0.04222813 0.04323745 0.04334203 0.04288865]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04575902 0.03923094]\n"," [0.04697842 0.03970469]\n"," [0.04584709 0.04170011]\n"," [0.04574813 0.03959338]]\n","\n","Average MAE Loss:\n","[0.04249498 0.04334156 0.0437736  0.04267075]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04503238 0.03923873]\n"," [0.04697842 0.03970469]\n"," [0.04617001 0.04060921]\n"," [0.04581353 0.03971655]]\n","\n","Average MAE Loss:\n","[0.04213556 0.04334156 0.04338961 0.04276504]\n","\n","Epoch 00044: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04486561 0.03928647]\n"," [0.04697842 0.03970469]\n"," [0.04584691 0.04094243]\n"," [0.04618378 0.03955922]]\n","\n","Average MAE Loss:\n","[0.04207604 0.04334156 0.04339467 0.0428715 ]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04489869 0.03930565]\n"," [0.04697842 0.03970469]\n"," [0.04593331 0.0400889 ]\n"," [0.04604094 0.03954729]]\n","\n","Average MAE Loss:\n","[0.04210217 0.04334156 0.0430111  0.04279412]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04454296 0.03940931]\n"," [0.04697842 0.03970469]\n"," [0.04574379 0.04029024]\n"," [0.04634997 0.03955404]]\n","\n","Average MAE Loss:\n","[0.04197614 0.04334156 0.04301701 0.042952  ]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04435158 0.03953841]\n"," [0.04697842 0.03970469]\n"," [0.04552612 0.04060957]\n"," [0.04614462 0.03974509]]\n","\n","Average MAE Loss:\n","[0.04194499 0.04334156 0.04306785 0.04294485]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04433116 0.03956085]\n"," [0.04697842 0.03970469]\n"," [0.04553899 0.04011629]\n"," [0.04629759 0.03969456]]\n","\n","Average MAE Loss:\n","[0.04194601 0.04334156 0.04282764 0.04299607]\n","\n","Epoch 00049: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04591358 0.03929413]\n"," [0.04629759 0.03969456]\n"," [0.04473009 0.03970825]\n"," [0.04559749 0.03937066]]\n","\n","Average MAE Loss:\n","[0.04260386 0.04299607 0.04221917 0.04248407]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.04513274 0.03925238]\n"," [0.04629759 0.03969456]\n"," [0.0456455  0.0401558 ]\n"," [0.04557638 0.0394193 ]]\n","\n","Average MAE Loss:\n","[0.04219256 0.04299607 0.04290065 0.04249784]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04488353 0.03935757]\n"," [0.04629759 0.03969456]\n"," [0.0459388  0.04038785]\n"," [0.04553484 0.03959203]]\n","\n","Average MAE Loss:\n","[0.04212055 0.04299607 0.04316332 0.04256343]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04464227 0.03943013]\n"," [0.04629759 0.03969456]\n"," [0.04595164 0.04080012]\n"," [0.04567542 0.03952113]]\n","\n","Average MAE Loss:\n","[0.0420362  0.04299607 0.04337588 0.04259828]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04459914 0.03946777]\n"," [0.04629759 0.03969456]\n"," [0.04574993 0.04046028]\n"," [0.04569463 0.03964312]]\n","\n","Average MAE Loss:\n","[0.04203345 0.04299607 0.0431051  0.04266887]\n","\n","Epoch 00054: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04452439 0.03949291]\n"," [0.04629759 0.03969456]\n"," [0.04566732 0.04023956]\n"," [0.04571255 0.03969108]]\n","\n","Average MAE Loss:\n","[0.04200865 0.04299607 0.04295344 0.04270182]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04456044 0.03948719]\n"," [0.04629759 0.03969456]\n"," [0.04573092 0.04082616]\n"," [0.04592357 0.03961389]]\n","\n","Average MAE Loss:\n","[0.04202382 0.04299607 0.04327854 0.04276873]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00056: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00056: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04531302 0.03911362]\n"," [0.04541573 0.03916102]\n"," [0.04512369 0.0395387 ]\n"," [0.04526219 0.03923194]]\n","\n","Average MAE Loss:\n","[0.04221332 0.04228837 0.04233119 0.04224706]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.0451181  0.0391072 ]\n"," [0.04541573 0.03916102]\n"," [0.04528221 0.04002562]\n"," [0.04547232 0.03924013]]\n","\n","Average MAE Loss:\n","[0.04211265 0.04228837 0.04265392 0.04235622]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04499211 0.03911687]\n"," [0.04541573 0.03916102]\n"," [0.04552511 0.04017512]\n"," [0.04544842 0.03936154]]\n","\n","Average MAE Loss:\n","[0.04205449 0.04228837 0.04285012 0.04240498]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04496222 0.0391407 ]\n"," [0.04541573 0.03916102]\n"," [0.04552002 0.04061611]\n"," [0.04557713 0.03944462]]\n","\n","Average MAE Loss:\n","[0.04205146 0.04228837 0.04306806 0.04251087]\n","\n","Epoch 00060: reducing learning rate of group 0 to 3.1250e-05.\n","\n","epochs finished with time:92.37273502349854\n","\n","[[0.04496222 0.0391407 ]\n"," [0.04541573 0.03916102]\n"," [0.04552002 0.04061611]\n"," [0.04557713 0.03944462]]\n","00:01:54.53829608700016\n","------------------------------------Fold [4/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05123502 0.06872562]\n"," [0.08058786 0.10310742]\n"," [0.05443951 0.07246623]\n"," [0.05175413 0.07090619]]\n","\n","Average MAE Loss:\n","[0.05998032 0.09184764 0.06345287 0.06133016]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04431421 0.06149982]\n"," [0.08058786 0.10310742]\n"," [0.04470686 0.06523218]\n"," [0.05217002 0.06900886]]\n","\n","Average MAE Loss:\n","[0.05290702 0.09184764 0.05496952 0.06058944]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04033619 0.05912092]\n"," [0.08058786 0.10310742]\n"," [0.04308569 0.06298186]\n"," [0.04671589 0.06605461]]\n","\n","Average MAE Loss:\n","[0.04972855 0.09184764 0.05303377 0.05638525]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04005076 0.05962726]\n"," [0.08058786 0.10310742]\n"," [0.04263687 0.06358192]\n"," [0.04594105 0.06519458]]\n","\n","Average MAE Loss:\n","[0.04983901 0.09184764 0.05310939 0.05556782]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.04036479 0.05970101]\n"," [0.08058786 0.10310742]\n"," [0.04180242 0.06213134]\n"," [0.04518714 0.06495923]]\n","\n","Average MAE Loss:\n","[0.0500329  0.09184764 0.05196688 0.05507318]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.0403124  0.05991101]\n"," [0.08058786 0.10310742]\n"," [0.04179128 0.06274559]\n"," [0.04517383 0.06465605]]\n","\n","Average MAE Loss:\n","[0.05011171 0.09184764 0.05226843 0.05491494]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.03958552 0.05880802]\n"," [0.08058786 0.10310742]\n"," [0.04142929 0.06209129]\n"," [0.04479927 0.06469487]]\n","\n","Average MAE Loss:\n","[0.04919677 0.09184764 0.05176029 0.05474707]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.04473399 0.06337692]\n"," [0.04479927 0.06469487]\n"," [0.04091373 0.06158524]\n"," [0.04455398 0.06445035]]\n","\n","Average MAE Loss:\n","[0.05405545 0.05474707 0.05124949 0.05450216]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.04311686 0.06081102]\n"," [0.04479927 0.06469487]\n"," [0.04057216 0.06125925]\n"," [0.04364201 0.06424028]]\n","\n","Average MAE Loss:\n","[0.05196394 0.05474707 0.0509157  0.05394114]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.03939435 0.05846365]\n"," [0.04479927 0.06469487]\n"," [0.04016716 0.0607396 ]\n"," [0.04187945 0.06130963]]\n","\n","Average MAE Loss:\n","[0.048929   0.05474707 0.05045338 0.05159454]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.03943908 0.05788995]\n"," [0.04479927 0.06469487]\n"," [0.03976623 0.05998261]\n"," [0.04387261 0.06199794]]\n","\n","Average MAE Loss:\n","[0.04866452 0.05474707 0.04987442 0.05293527]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.03926093 0.05813219]\n"," [0.04479927 0.06469487]\n"," [0.03929536 0.05966868]\n"," [0.04144505 0.06111637]]\n","\n","Average MAE Loss:\n","[0.04869656 0.05474707 0.04948202 0.05128071]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.03960657 0.05925085]\n"," [0.04479927 0.06469487]\n"," [0.03952319 0.05971249]\n"," [0.04128582 0.06089691]]\n","\n","Average MAE Loss:\n","[0.04942871 0.05474707 0.04961784 0.05109137]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.0407328  0.06057501]\n"," [0.04479927 0.06469487]\n"," [0.03953983 0.05948451]\n"," [0.04121266 0.06058921]]\n","\n","Average MAE Loss:\n","[0.0506539  0.05474707 0.04951217 0.05090093]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.03873781 0.05773476]\n"," [0.07384275 0.09626211]\n"," [0.04002998 0.06001206]\n"," [0.04136455 0.05993307]]\n","\n","Average MAE Loss:\n","[0.04823628 0.08505243 0.05002102 0.05064881]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.03795495 0.05677596]\n"," [0.07384275 0.09626211]\n"," [0.04030138 0.05945156]\n"," [0.03843243 0.05828024]]\n","\n","Average MAE Loss:\n","[0.04736546 0.08505243 0.04987647 0.04835634]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.0376781  0.05640564]\n"," [0.07384275 0.09626211]\n"," [0.03859832 0.05798749]\n"," [0.03846672 0.05786781]]\n","\n","Average MAE Loss:\n","[0.04704187 0.08505243 0.0482929  0.04816726]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.03771728 0.05617661]\n"," [0.07384275 0.09626211]\n"," [0.03835661 0.05825534]\n"," [0.03928681 0.05793324]]\n","\n","Average MAE Loss:\n","[0.04694694 0.08505243 0.04830597 0.04861002]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.03740704 0.05644872]\n"," [0.07384275 0.09626211]\n"," [0.04000465 0.05880318]\n"," [0.03839759 0.05799335]]\n","\n","Average MAE Loss:\n","[0.04692788 0.08505243 0.04940392 0.04819547]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.03744991 0.05602561]\n"," [0.07384275 0.09626211]\n"," [0.03787374 0.05817042]\n"," [0.03846103 0.05728776]]\n","\n","Average MAE Loss:\n","[0.04673776 0.08505243 0.04802208 0.0478744 ]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.03729412 0.05599463]\n"," [0.07384275 0.09626211]\n"," [0.03763487 0.05827288]\n"," [0.03800506 0.0575146 ]]\n","\n","Average MAE Loss:\n","[0.04664438 0.08505243 0.04795387 0.04775983]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.03934201 0.0588485 ]\n"," [0.03800506 0.0575146 ]\n"," [0.03771459 0.05861341]\n"," [0.03880049 0.05844991]]\n","\n","Average MAE Loss:\n","[0.04909526 0.04775983 0.048164   0.0486252 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.0385256  0.05774916]\n"," [0.03800506 0.0575146 ]\n"," [0.03788307 0.05823279]\n"," [0.04000568 0.05852075]]\n","\n","Average MAE Loss:\n","[0.04813738 0.04775983 0.04805793 0.04926321]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.03756387 0.0563291 ]\n"," [0.03800506 0.0575146 ]\n"," [0.03835586 0.05791231]\n"," [0.03810639 0.05750523]]\n","\n","Average MAE Loss:\n","[0.04694648 0.04775983 0.04813408 0.04780581]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.03734148 0.05584179]\n"," [0.03800506 0.0575146 ]\n"," [0.03772881 0.05766116]\n"," [0.038266   0.05807253]]\n","\n","Average MAE Loss:\n","[0.04659164 0.04775983 0.04769498 0.04816927]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.03779729 0.05620415]\n"," [0.03800506 0.0575146 ]\n"," [0.03748644 0.05736892]\n"," [0.03872967 0.05760792]]\n","\n","Average MAE Loss:\n","[0.04700072 0.04775983 0.04742768 0.04816879]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.03811936 0.05607226]\n"," [0.03800506 0.0575146 ]\n"," [0.03761525 0.05744941]\n"," [0.03796821 0.05775721]]\n","\n","Average MAE Loss:\n","[0.04709581 0.04775983 0.04753233 0.04786271]\n","\n","Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.03754717 0.05619021]\n"," [0.03800506 0.0575146 ]\n"," [0.03785164 0.05784216]\n"," [0.03813435 0.05741291]]\n","\n","Average MAE Loss:\n","[0.04686869 0.04775983 0.0478469  0.04777363]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.03745012 0.05555244]\n"," [0.041384   0.0623478 ]\n"," [0.04034475 0.05881274]\n"," [0.03725786 0.05645341]]\n","\n","Average MAE Loss:\n","[0.04650128 0.0518659  0.04957875 0.04685564]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.037327   0.05539132]\n"," [0.041384   0.0623478 ]\n"," [0.0367601  0.05698454]\n"," [0.03759483 0.05664504]]\n","\n","Average MAE Loss:\n","[0.04635916 0.0518659  0.04687232 0.04711993]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.03694649 0.05548727]\n"," [0.041384   0.0623478 ]\n"," [0.03757227 0.0572478 ]\n"," [0.03752311 0.05659547]]\n","\n","Average MAE Loss:\n","[0.04621688 0.0518659  0.04741004 0.04705929]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.03695039 0.05516978]\n"," [0.041384   0.0623478 ]\n"," [0.03710351 0.05715399]\n"," [0.03739588 0.05662449]]\n","\n","Average MAE Loss:\n","[0.04606009 0.0518659  0.04712875 0.04701019]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.03712255 0.05529547]\n"," [0.041384   0.0623478 ]\n"," [0.0367017  0.05693171]\n"," [0.03744391 0.05654243]]\n","\n","Average MAE Loss:\n","[0.04620901 0.0518659  0.0468167  0.04699317]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.0371114  0.05542403]\n"," [0.041384   0.0623478 ]\n"," [0.03674811 0.05680771]\n"," [0.03791385 0.05664716]]\n","\n","Average MAE Loss:\n","[0.04626772 0.0518659  0.04677791 0.04728051]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.03714154 0.05546787]\n"," [0.041384   0.0623478 ]\n"," [0.03727326 0.05695909]\n"," [0.03755681 0.05662394]]\n","\n","Average MAE Loss:\n","[0.0463047  0.0518659  0.04711617 0.04709038]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.03896196 0.0563576 ]\n"," [0.03755681 0.05662394]\n"," [0.03686293 0.05695239]\n"," [0.03702439 0.05653483]]\n","\n","Average MAE Loss:\n","[0.04765978 0.04709038 0.04690766 0.04677961]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.0376043  0.0555378 ]\n"," [0.03755681 0.05662394]\n"," [0.03701372 0.05701356]\n"," [0.03704606 0.05631825]]\n","\n","Average MAE Loss:\n","[0.04657105 0.04709038 0.04701364 0.04668215]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.03713785 0.05572991]\n"," [0.03755681 0.05662394]\n"," [0.03688652 0.0568051 ]\n"," [0.03711234 0.05635217]]\n","\n","Average MAE Loss:\n","[0.04643388 0.04709038 0.04684581 0.04673226]\n","\n","Epoch 00038: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.03740479 0.05553827]\n"," [0.03755681 0.05662394]\n"," [0.03707468 0.05681344]\n"," [0.0373243  0.05643909]]\n","\n","Average MAE Loss:\n","[0.04647153 0.04709038 0.04694406 0.0468817 ]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.03701199 0.05546764]\n"," [0.03755681 0.05662394]\n"," [0.03687476 0.05690837]\n"," [0.03724892 0.05652717]]\n","\n","Average MAE Loss:\n","[0.04623981 0.04709038 0.04689157 0.04688804]\n","\n","Epoch 00040: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.03724542 0.05550893]\n"," [0.03755681 0.05662394]\n"," [0.03691275 0.05682871]\n"," [0.03719542 0.05652506]]\n","\n","Average MAE Loss:\n","[0.04637717 0.04709038 0.04687073 0.04686024]\n","\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.03704762 0.05554767]\n"," [0.03755681 0.05662394]\n"," [0.03698106 0.0568555 ]\n"," [0.03740018 0.05650746]]\n","\n","Average MAE Loss:\n","[0.04629765 0.04709038 0.04691828 0.04695382]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03694404 0.05549115]\n"," [0.03652908 0.05602362]\n"," [0.03660506 0.05613656]\n"," [0.03656062 0.05584782]]\n","\n","Average MAE Loss:\n","[0.04621759 0.04627635 0.04637081 0.04620422]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03717343 0.05538681]\n"," [0.03652908 0.05602362]\n"," [0.0366926  0.05652266]\n"," [0.03681938 0.05591416]]\n","\n","Average MAE Loss:\n","[0.04628012 0.04627635 0.04660763 0.04636677]\n","\n","Epoch 00044: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03692    0.05533091]\n"," [0.03652908 0.05602362]\n"," [0.03704861 0.05676491]\n"," [0.03718344 0.05612592]]\n","\n","Average MAE Loss:\n","[0.04612545 0.04627635 0.04690676 0.04665468]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03704808 0.05530477]\n"," [0.03652908 0.05602362]\n"," [0.03677552 0.05663836]\n"," [0.03733843 0.05624722]]\n","\n","Average MAE Loss:\n","[0.04617642 0.04627635 0.04670694 0.04679282]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03695165 0.05531375]\n"," [0.03652908 0.05602362]\n"," [0.03663025 0.05670881]\n"," [0.03713384 0.05622832]]\n","\n","Average MAE Loss:\n","[0.0461327  0.04627635 0.04666953 0.04668108]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03681478 0.05527862]\n"," [0.03652908 0.05602362]\n"," [0.03712898 0.05683264]\n"," [0.03710651 0.05623535]]\n","\n","Average MAE Loss:\n","[0.0460467  0.04627635 0.04698081 0.04667093]\n","\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03685836 0.05527445]\n"," [0.03652908 0.05602362]\n"," [0.03687422 0.05671198]\n"," [0.03729615 0.0562941 ]]\n","\n","Average MAE Loss:\n","[0.0460664  0.04627635 0.0467931  0.04679513]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00049: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00049: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03655401 0.05552507]\n"," [0.03729615 0.0562941 ]\n"," [0.03649161 0.05537928]\n"," [0.03674043 0.05648102]]\n","\n","Average MAE Loss:\n","[0.04603954 0.04679513 0.04593545 0.04661073]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03685939 0.05540077]\n"," [0.03729615 0.0562941 ]\n"," [0.03646263 0.05577979]\n"," [0.03664616 0.0562357 ]]\n","\n","Average MAE Loss:\n","[0.04613008 0.04679513 0.04612121 0.04644093]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03690453 0.0553444 ]\n"," [0.03729615 0.0562941 ]\n"," [0.03661538 0.05613715]\n"," [0.03654068 0.05610097]]\n","\n","Average MAE Loss:\n","[0.04612446 0.04679513 0.04637626 0.04632082]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03687827 0.05529067]\n"," [0.03729615 0.0562941 ]\n"," [0.03661466 0.05634441]\n"," [0.03656629 0.05604035]]\n","\n","Average MAE Loss:\n","[0.04608447 0.04679513 0.04647953 0.04630332]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03708773 0.05530119]\n"," [0.03729615 0.0562941 ]\n"," [0.03656477 0.05644513]\n"," [0.03668975 0.05604142]]\n","\n","Average MAE Loss:\n","[0.04619446 0.04679513 0.04650495 0.04636558]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03699666 0.05527931]\n"," [0.03729615 0.0562941 ]\n"," [0.03661538 0.05649439]\n"," [0.03691178 0.05608723]]\n","\n","Average MAE Loss:\n","[0.04613798 0.04679513 0.04655488 0.0464995 ]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00055: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03705902 0.05526682]\n"," [0.03729615 0.0562941 ]\n"," [0.03669846 0.05654451]\n"," [0.03692037 0.05608965]]\n","\n","Average MAE Loss:\n","[0.04616292 0.04679513 0.04662149 0.04650501]\n","\n","Epoch 00056: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00056: reducing learning rate of group 0 to 1.2500e-04.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03646767 0.05550651]\n"," [0.03644287 0.05562664]\n"," [0.03642605 0.05574227]\n"," [0.03651519 0.05565451]]\n","\n","Average MAE Loss:\n","[0.04598709 0.04603475 0.04608416 0.04608485]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03651328 0.0553914 ]\n"," [0.03644287 0.05562664]\n"," [0.03646046 0.05593501]\n"," [0.03658743 0.05569865]]\n","\n","Average MAE Loss:\n","[0.04595234 0.04603475 0.04619774 0.04614304]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03666322 0.05534311]\n"," [0.03644287 0.05562664]\n"," [0.03649313 0.05609659]\n"," [0.0366389  0.05574213]]\n","\n","Average MAE Loss:\n","[0.04600316 0.04603475 0.04629486 0.04619052]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03681692 0.05532708]\n"," [0.03644287 0.05562664]\n"," [0.0365435  0.05623582]\n"," [0.03670434 0.05577012]]\n","\n","Average MAE Loss:\n","[0.046072   0.04603475 0.04638966 0.04623723]\n","\n","\n","epochs finished with time:95.1925916671753\n","\n","[[0.03681692 0.05532708]\n"," [0.03644287 0.05562664]\n"," [0.0365435  0.05623582]\n"," [0.03670434 0.05577012]]\n","00:01:57.38095755900031\n","------------------------------------Fold [5/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.05432982 0.05689225]\n"," [0.10038102 0.10476877]\n"," [0.05808461 0.05755613]\n"," [0.06274761 0.06449892]]\n","\n","Average MAE Loss:\n","[0.05561104 0.10257489 0.05782037 0.06362326]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.04746763 0.04869822]\n"," [0.10038102 0.10476877]\n"," [0.05337321 0.05550274]\n"," [0.05725548 0.05753472]]\n","\n","Average MAE Loss:\n","[0.04808293 0.10257489 0.05443798 0.0573951 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.04751992 0.04920696]\n"," [0.10038102 0.10476877]\n"," [0.05362729 0.05595005]\n"," [0.05583687 0.05656478]]\n","\n","Average MAE Loss:\n","[0.04836344 0.10257489 0.05478867 0.05620082]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.04937971 0.05174205]\n"," [0.10038102 0.10476877]\n"," [0.05221762 0.05435659]\n"," [0.05800341 0.06054895]]\n","\n","Average MAE Loss:\n","[0.05056088 0.10257489 0.0532871  0.05927618]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.05039642 0.05328524]\n"," [0.10038102 0.10476877]\n"," [0.05248039 0.05463753]\n"," [0.05476229 0.05570782]]\n","\n","Average MAE Loss:\n","[0.05184083 0.10257489 0.05355896 0.05523505]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.04899593 0.05126741]\n"," [0.10038102 0.10476877]\n"," [0.05110627 0.0527989 ]\n"," [0.0571052  0.05982849]]\n","\n","Average MAE Loss:\n","[0.05013167 0.10257489 0.05195258 0.05846685]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.05104754 0.05370918]\n"," [0.10038102 0.10476877]\n"," [0.05053705 0.05211353]\n"," [0.05573224 0.05837836]]\n","\n","Average MAE Loss:\n","[0.05237836 0.10257489 0.05132529 0.0570553 ]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.05557986 0.05585181]\n"," [0.05573224 0.05837836]\n"," [0.04955497 0.05051553]\n"," [0.05234134 0.05313004]]\n","\n","Average MAE Loss:\n","[0.05571584 0.0570553  0.05003525 0.05273569]\n","\n","Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.05081989 0.05323227]\n"," [0.05573224 0.05837836]\n"," [0.04875873 0.05067322]\n"," [0.05277344 0.0541251 ]]\n","\n","Average MAE Loss:\n","[0.05202608 0.0570553  0.04971598 0.05344927]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.05039322 0.05300459]\n"," [0.05573224 0.05837836]\n"," [0.05032018 0.05219535]\n"," [0.05387801 0.05580406]]\n","\n","Average MAE Loss:\n","[0.0516989  0.0570553  0.05125776 0.05484103]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.04949835 0.0524415 ]\n"," [0.05573224 0.05837836]\n"," [0.04758455 0.04888135]\n"," [0.04997835 0.04938544]]\n","\n","Average MAE Loss:\n","[0.05096993 0.0570553  0.04823295 0.04968189]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.0472642  0.04943063]\n"," [0.05573224 0.05837836]\n"," [0.04775585 0.04848041]\n"," [0.04992714 0.05088047]]\n","\n","Average MAE Loss:\n","[0.04834741 0.0570553  0.04811813 0.0504038 ]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.0478166  0.05003104]\n"," [0.05573224 0.05837836]\n"," [0.04932311 0.05117019]\n"," [0.05296908 0.05524764]]\n","\n","Average MAE Loss:\n","[0.04892382 0.0570553  0.05024665 0.05410836]\n","\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.04675061 0.04902859]\n"," [0.05573224 0.05837836]\n"," [0.04806435 0.05020393]\n"," [0.0493905  0.05061442]]\n","\n","Average MAE Loss:\n","[0.0478896  0.0570553  0.04913414 0.05000246]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.04792645 0.05045082]\n"," [0.09403765 0.098416  ]\n"," [0.04916978 0.05043985]\n"," [0.04836059 0.04960436]]\n","\n","Average MAE Loss:\n","[0.04918863 0.09622683 0.04980482 0.04898248]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.04945005 0.05221324]\n"," [0.09403765 0.098416  ]\n"," [0.04875715 0.05048731]\n"," [0.05035183 0.0523347 ]]\n","\n","Average MAE Loss:\n","[0.05083165 0.09622683 0.04962223 0.05134326]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.04434593 0.04577538]\n"," [0.09403765 0.098416  ]\n"," [0.04685709 0.04870247]\n"," [0.04678956 0.04698906]]\n","\n","Average MAE Loss:\n","[0.04506065 0.09622683 0.04777978 0.04688931]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.0447275  0.04646398]\n"," [0.09403765 0.098416  ]\n"," [0.04563437 0.04691214]\n"," [0.04695342 0.04808448]]\n","\n","Average MAE Loss:\n","[0.04559574 0.09622683 0.04627326 0.04751895]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.04467912 0.04642255]\n"," [0.09403765 0.098416  ]\n"," [0.04522952 0.04648111]\n"," [0.0487228  0.0503058 ]]\n","\n","Average MAE Loss:\n","[0.04555083 0.09622683 0.04585532 0.0495143 ]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.04488504 0.04636412]\n"," [0.09403765 0.098416  ]\n"," [0.04559564 0.04701825]\n"," [0.04596523 0.04507794]]\n","\n","Average MAE Loss:\n","[0.04562458 0.09622683 0.04630695 0.04552158]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.04427502 0.04563656]\n"," [0.09403765 0.098416  ]\n"," [0.04664451 0.04838456]\n"," [0.04936834 0.05119598]]\n","\n","Average MAE Loss:\n","[0.04495579 0.09622683 0.04751454 0.05028216]\n","\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07972375 0.08418401]\n"," [0.04936834 0.05119598]\n"," [0.04804917 0.05046696]\n"," [0.0491387  0.0512791 ]]\n","\n","Average MAE Loss:\n","[0.08195388 0.05028216 0.04925806 0.0502089 ]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.04466745 0.04565278]\n"," [0.04936834 0.05119598]\n"," [0.04718897 0.04953972]\n"," [0.04639169 0.04705795]]\n","\n","Average MAE Loss:\n","[0.04516012 0.05028216 0.04836435 0.04672482]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.04792058 0.05006784]\n"," [0.04936834 0.05119598]\n"," [0.04492258 0.04537993]\n"," [0.05118735 0.05331232]]\n","\n","Average MAE Loss:\n","[0.04899421 0.05028216 0.04515125 0.05224984]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.04446883 0.04598864]\n"," [0.04936834 0.05119598]\n"," [0.04577304 0.04675726]\n"," [0.045516   0.04493692]]\n","\n","Average MAE Loss:\n","[0.04522873 0.05028216 0.04626515 0.04522646]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.04485215 0.0463944 ]\n"," [0.04936834 0.05119598]\n"," [0.0452221  0.04623591]\n"," [0.04700661 0.04802915]]\n","\n","Average MAE Loss:\n","[0.04562327 0.05028216 0.04572901 0.04751788]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.04422597 0.04542108]\n"," [0.04936834 0.05119598]\n"," [0.04443677 0.04492061]\n"," [0.04674545 0.04768132]]\n","\n","Average MAE Loss:\n","[0.04482353 0.05028216 0.04467869 0.04721338]\n","\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.04473137 0.04611725]\n"," [0.04936834 0.05119598]\n"," [0.04519195 0.04648901]\n"," [0.04809526 0.04949931]]\n","\n","Average MAE Loss:\n","[0.04542431 0.05028216 0.04584048 0.04879729]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.04335964 0.0437025 ]\n"," [0.058383   0.06177722]\n"," [0.04451987 0.04452794]\n"," [0.04531828 0.0452254 ]]\n","\n","Average MAE Loss:\n","[0.04353107 0.06008011 0.04452391 0.04527184]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.04446825 0.04590581]\n"," [0.058383   0.06177722]\n"," [0.04493231 0.0464251 ]\n"," [0.04619661 0.04636193]]\n","\n","Average MAE Loss:\n","[0.04518703 0.06008011 0.04567871 0.04627927]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.04423492 0.0457297 ]\n"," [0.058383   0.06177722]\n"," [0.04447614 0.04533121]\n"," [0.04755978 0.0491985 ]]\n","\n","Average MAE Loss:\n","[0.04498231 0.06008011 0.04490368 0.04837914]\n","\n","Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.04407783 0.04549691]\n"," [0.058383   0.06177722]\n"," [0.04470423 0.04552108]\n"," [0.04605755 0.04713547]]\n","\n","Average MAE Loss:\n","[0.04478737 0.06008011 0.04511266 0.04659651]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.04382373 0.04482049]\n"," [0.058383   0.06177722]\n"," [0.04635    0.04816946]\n"," [0.04649331 0.04763886]]\n","\n","Average MAE Loss:\n","[0.04432211 0.06008011 0.04725973 0.04706608]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.0438713  0.04496408]\n"," [0.058383   0.06177722]\n"," [0.04511426 0.04679884]\n"," [0.04603633 0.04684116]]\n","\n","Average MAE Loss:\n","[0.04441769 0.06008011 0.04595655 0.04643875]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.04486642 0.04636324]\n"," [0.058383   0.06177722]\n"," [0.04457133 0.04584829]\n"," [0.04564986 0.04635088]]\n","\n","Average MAE Loss:\n","[0.04561483 0.06008011 0.04520981 0.04600037]\n","\n","Epoch 00035: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00035: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.04692318 0.04865988]\n"," [0.04564986 0.04635088]\n"," [0.04374457 0.04474641]\n"," [0.04482627 0.04599701]]\n","\n","Average MAE Loss:\n","[0.04779153 0.04600037 0.04424549 0.04541164]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.04343506 0.04381505]\n"," [0.04564986 0.04635088]\n"," [0.04484779 0.04632292]\n"," [0.04521908 0.04619937]]\n","\n","Average MAE Loss:\n","[0.04362506 0.04600037 0.04558535 0.04570922]\n","\n","Epoch 00037: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.04397751 0.04503793]\n"," [0.04564986 0.04635088]\n"," [0.0448857  0.04641124]\n"," [0.04548885 0.04655122]]\n","\n","Average MAE Loss:\n","[0.04450772 0.04600037 0.04564847 0.04602003]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.04388699 0.04491029]\n"," [0.04564986 0.04635088]\n"," [0.0442778  0.04574276]\n"," [0.04548173 0.04663184]]\n","\n","Average MAE Loss:\n","[0.04439864 0.04600037 0.04501028 0.04605678]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.04420499 0.04539622]\n"," [0.04564986 0.04635088]\n"," [0.04430045 0.04565477]\n"," [0.04542899 0.04635151]]\n","\n","Average MAE Loss:\n","[0.0448006  0.04600037 0.04497761 0.04589025]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.04391059 0.04494882]\n"," [0.04564986 0.04635088]\n"," [0.04438051 0.04586229]\n"," [0.04552696 0.04628139]]\n","\n","Average MAE Loss:\n","[0.04442971 0.04600037 0.0451214  0.04590417]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.04417278 0.04535849]\n"," [0.04564986 0.04635088]\n"," [0.04491212 0.04644693]\n"," [0.04541744 0.04597879]]\n","\n","Average MAE Loss:\n","[0.04476564 0.04600037 0.04567953 0.04569812]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00042: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.04491575 0.04621729]\n"," [0.0461552  0.04789272]\n"," [0.04512456 0.04638632]\n"," [0.04498448 0.04609815]]\n","\n","Average MAE Loss:\n","[0.04556652 0.04702396 0.04575544 0.04554131]\n","\n","Epoch 00043: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.04419022 0.04515203]\n"," [0.0461552  0.04789272]\n"," [0.04396472 0.04497486]\n"," [0.04502069 0.04608601]]\n","\n","Average MAE Loss:\n","[0.04467113 0.04702396 0.04446979 0.04555335]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.04403425 0.04493447]\n"," [0.0461552  0.04789272]\n"," [0.04413228 0.04539822]\n"," [0.04523517 0.0462984 ]]\n","\n","Average MAE Loss:\n","[0.04448436 0.04702396 0.04476525 0.04576679]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.04390935 0.04479381]\n"," [0.0461552  0.04789272]\n"," [0.04415594 0.04550642]\n"," [0.04515495 0.04607435]]\n","\n","Average MAE Loss:\n","[0.04435158 0.04702396 0.04483118 0.04561465]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.04405655 0.04514237]\n"," [0.0461552  0.04789272]\n"," [0.04431365 0.04569731]\n"," [0.04563587 0.04674857]]\n","\n","Average MAE Loss:\n","[0.04459946 0.04702396 0.04500548 0.04619222]\n","\n","Epoch 00047: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.04389692 0.04494905]\n"," [0.0461552  0.04789272]\n"," [0.0442305  0.04559688]\n"," [0.04589502 0.04709994]]\n","\n","Average MAE Loss:\n","[0.04442299 0.04702396 0.04491369 0.04649748]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00048: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.04377268 0.04475595]\n"," [0.0461552  0.04789272]\n"," [0.04429131 0.04571687]\n"," [0.04572122 0.04677816]]\n","\n","Average MAE Loss:\n","[0.04426432 0.04702396 0.04500409 0.04624969]\n","\n","Epoch 00049: reducing learning rate of group 0 to 6.2500e-05.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.04539854 0.04691821]\n"," [0.04572122 0.04677816]\n"," [0.0434668  0.04421183]\n"," [0.0443527  0.04580115]]\n","\n","Average MAE Loss:\n","[0.04615838 0.04624969 0.04383932 0.04507692]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0446609  0.04588368]\n"," [0.04572122 0.04677816]\n"," [0.04334367 0.04405888]\n"," [0.04460513 0.04612779]]\n","\n","Average MAE Loss:\n","[0.04527229 0.04624969 0.04370127 0.04536646]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.04426428 0.04532752]\n"," [0.04572122 0.04677816]\n"," [0.04362581 0.04453146]\n"," [0.04469648 0.04618692]]\n","\n","Average MAE Loss:\n","[0.0447959  0.04624969 0.04407864 0.0454417 ]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.04412951 0.04517786]\n"," [0.04572122 0.04677816]\n"," [0.04370948 0.04468401]\n"," [0.04472029 0.04613439]]\n","\n","Average MAE Loss:\n","[0.04465369 0.04624969 0.04419675 0.04542734]\n","\n","Epoch 00053: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.04404619 0.04505571]\n"," [0.04572122 0.04677816]\n"," [0.04378311 0.04479479]\n"," [0.04475625 0.04611516]]\n","\n","Average MAE Loss:\n","[0.04455095 0.04624969 0.04428895 0.0454357 ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.04399432 0.04498895]\n"," [0.04572122 0.04677816]\n"," [0.04382278 0.04484084]\n"," [0.04484497 0.04619471]]\n","\n","Average MAE Loss:\n","[0.04449163 0.04624969 0.04433181 0.04551984]\n","\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.04391486 0.04487705]\n"," [0.04572122 0.04677816]\n"," [0.04377345 0.04481145]\n"," [0.04488739 0.04617005]]\n","\n","Average MAE Loss:\n","[0.04439595 0.04624969 0.04429245 0.04552872]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.1250e-05.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04433212 0.04544199]\n"," [0.04438141 0.04549481]\n"," [0.04394587 0.04495133]\n"," [0.04437781 0.04545974]]\n","\n","Average MAE Loss:\n","[0.04488705 0.04493811 0.0444486  0.04491878]\n","\n","Epoch 00057: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04422839 0.0452963 ]\n"," [0.04438141 0.04549481]\n"," [0.043931   0.04497876]\n"," [0.04440173 0.04543968]]\n","\n","Average MAE Loss:\n","[0.04476234 0.04493811 0.04445488 0.0449207 ]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04417232 0.04521493]\n"," [0.04438141 0.04549481]\n"," [0.04401629 0.04515248]\n"," [0.04448133 0.04551714]]\n","\n","Average MAE Loss:\n","[0.04469362 0.04493811 0.04458439 0.04499924]\n","\n","Epoch 00059: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04414342 0.04517748]\n"," [0.04438141 0.04549481]\n"," [0.04413583 0.0453823 ]\n"," [0.04453846 0.04557142]]\n","\n","Average MAE Loss:\n","[0.04466045 0.04493811 0.04475906 0.04505494]\n","\n","\n","epochs finished with time:95.62211084365845\n","\n","[[0.04414342 0.04517748]\n"," [0.04438141 0.04549481]\n"," [0.04413583 0.0453823 ]\n"," [0.04453846 0.04557142]]\n","00:01:57.96374776800076\n"]}],"source":["# seed = 10 table = 4/8 fixed folds VGG19\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/4_8/global_test/GNNs/'\n","args.save_name='VGG19_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=4/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":439645,"status":"ok","timestamp":1690371144326,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"_sUzx2ArEq02","outputId":"18e18917-ef48-415b-a80f-d88acdbf30e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train NEW\n","Fixed seed:10\n","\n","Table:\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 1. 0.]\n"," [1. 0. 1.]]\n","Ratio:0.25\n","\n","running on GPU\n","TRAIN deterministic algorithms\n","\n","Creating the fold dictionary...\n","\n","Data for hospital 0\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 1\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 2\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","Data for hospital 3\n","fold:0\n","fold:1\n","fold:2\n","fold:3\n","fold:4\n","------------------------------------Fold [1/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08390902 0.08091476]\n"," [0.08400421 0.08107341]\n"," [0.06247937 0.05982874]\n"," [0.05456273 0.05181147]]\n","\n","Average MAE Loss:\n","[0.08241189 0.08253881 0.06115406 0.0531871 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08390902 0.08091476]\n"," [0.08400421 0.08107341]\n"," [0.04634947 0.04490671]\n"," [0.05022066 0.04880004]]\n","\n","Average MAE Loss:\n","[0.08241189 0.08253881 0.04562809 0.04951035]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08390902 0.08091476]\n"," [0.08400421 0.08107341]\n"," [0.04537708 0.04344658]\n"," [0.05335641 0.0519672 ]]\n","\n","Average MAE Loss:\n","[0.08241189 0.08253881 0.04441183 0.0526618 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08390902 0.08091476]\n"," [0.08400421 0.08107341]\n"," [0.04467778 0.04274013]\n"," [0.04653421 0.04518097]]\n","\n","Average MAE Loss:\n","[0.08241189 0.08253881 0.04370895 0.04585759]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08390902 0.08091476]\n"," [0.08400421 0.08107341]\n"," [0.04491677 0.042939  ]\n"," [0.04410151 0.04243732]]\n","\n","Average MAE Loss:\n","[0.08241189 0.08253881 0.04392789 0.04326941]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08390902 0.08091476]\n"," [0.08400421 0.08107341]\n"," [0.04378471 0.0419172 ]\n"," [0.04549367 0.04343667]]\n","\n","Average MAE Loss:\n","[0.08241189 0.08253881 0.04285096 0.04446517]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08390902 0.08091476]\n"," [0.08400421 0.08107341]\n"," [0.04462735 0.04234188]\n"," [0.04384474 0.0422413 ]]\n","\n","Average MAE Loss:\n","[0.08241189 0.08253881 0.04348462 0.04304302]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08400421 0.08107341]\n"," [0.04384474 0.0422413 ]\n"," [0.05415208 0.05144876]\n"," [0.04419237 0.0427318 ]]\n","\n","Average MAE Loss:\n","[0.08253881 0.04304302 0.05280042 0.04346209]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08400421 0.08107341]\n"," [0.04384474 0.0422413 ]\n"," [0.04908258 0.04693817]\n"," [0.04304974 0.04116636]]\n","\n","Average MAE Loss:\n","[0.08253881 0.04304302 0.04801038 0.04210805]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.08400421 0.08107341]\n"," [0.04384474 0.0422413 ]\n"," [0.04615256 0.04409151]\n"," [0.04299789 0.04103264]]\n","\n","Average MAE Loss:\n","[0.08253881 0.04304302 0.04512204 0.04201526]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08400421 0.08107341]\n"," [0.04384474 0.0422413 ]\n"," [0.04948966 0.04730159]\n"," [0.04265496 0.04073959]]\n","\n","Average MAE Loss:\n","[0.08253881 0.04304302 0.04839563 0.04169727]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08400421 0.08107341]\n"," [0.04384474 0.0422413 ]\n"," [0.04533578 0.04283563]\n"," [0.04190865 0.0398896 ]]\n","\n","Average MAE Loss:\n","[0.08253881 0.04304302 0.0440857  0.04089913]\n","\n","Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.08400421 0.08107341]\n"," [0.04384474 0.0422413 ]\n"," [0.04360141 0.0415575 ]\n"," [0.04200591 0.04034036]]\n","\n","Average MAE Loss:\n","[0.08253881 0.04304302 0.04257945 0.04117314]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.08400421 0.08107341]\n"," [0.04384474 0.0422413 ]\n"," [0.04285976 0.04104321]\n"," [0.04136589 0.03980326]]\n","\n","Average MAE Loss:\n","[0.08253881 0.04304302 0.04195149 0.04058457]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.08026589 0.07714019]\n"," [0.04182277 0.039873  ]\n"," [0.04152521 0.03896495]]\n","\n","Average MAE Loss:\n","[0.07870304 0.07870304 0.04084789 0.04024508]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.08026589 0.07714019]\n"," [0.04105228 0.03910668]\n"," [0.03940756 0.03778287]]\n","\n","Average MAE Loss:\n","[0.07870304 0.07870304 0.04007948 0.03859521]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.08026589 0.07714019]\n"," [0.04049999 0.03895001]\n"," [0.0390599  0.03772151]]\n","\n","Average MAE Loss:\n","[0.07870304 0.07870304 0.039725   0.0383907 ]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.08026589 0.07714019]\n"," [0.03983237 0.038108  ]\n"," [0.039303   0.03777282]]\n","\n","Average MAE Loss:\n","[0.07870304 0.07870304 0.03897018 0.03853791]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.08026589 0.07714019]\n"," [0.04010374 0.03804303]\n"," [0.03898299 0.03725146]]\n","\n","Average MAE Loss:\n","[0.07870304 0.07870304 0.03907339 0.03811722]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.08026589 0.07714019]\n"," [0.03941288 0.03739166]\n"," [0.03896331 0.03741623]]\n","\n","Average MAE Loss:\n","[0.07870304 0.07870304 0.03840227 0.03818977]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.08026589 0.07714019]\n"," [0.0393351  0.03769248]\n"," [0.03850836 0.0371052 ]]\n","\n","Average MAE Loss:\n","[0.07870304 0.07870304 0.03851379 0.03780678]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.03850836 0.0371052 ]\n"," [0.07159559 0.06845477]\n"," [0.0396715  0.03862036]]\n","\n","Average MAE Loss:\n","[0.07870304 0.03780678 0.07002518 0.03914593]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.03850836 0.0371052 ]\n"," [0.04187732 0.03986475]\n"," [0.03903878 0.03756942]]\n","\n","Average MAE Loss:\n","[0.07870304 0.03780678 0.04087103 0.0383041 ]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.03850836 0.0371052 ]\n"," [0.04031683 0.03886352]\n"," [0.0388884  0.03739034]]\n","\n","Average MAE Loss:\n","[0.07870304 0.03780678 0.03959017 0.03813937]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.03850836 0.0371052 ]\n"," [0.04068829 0.03920578]\n"," [0.03848398 0.03705507]]\n","\n","Average MAE Loss:\n","[0.07870304 0.03780678 0.03994703 0.03776952]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.03850836 0.0371052 ]\n"," [0.03970526 0.03786917]\n"," [0.03913169 0.03753124]]\n","\n","Average MAE Loss:\n","[0.07870304 0.03780678 0.03878722 0.03833147]\n","\n","Epoch 00026: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.03850836 0.0371052 ]\n"," [0.03953107 0.03786849]\n"," [0.03951122 0.03786786]]\n","\n","Average MAE Loss:\n","[0.07870304 0.03780678 0.03869978 0.03868954]\n","\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.08026589 0.07714019]\n"," [0.03850836 0.0371052 ]\n"," [0.04018731 0.03859542]\n"," [0.03865403 0.03738176]]\n","\n","Average MAE Loss:\n","[0.07870304 0.03780678 0.03939136 0.03801789]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.05505472 0.05258055]\n"," [0.04204783 0.0398389 ]\n"," [0.04359749 0.04176323]]\n","\n","Average MAE Loss:\n","[0.05381764 0.05381764 0.04094337 0.04268036]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.05505472 0.05258055]\n"," [0.0396642  0.03815475]\n"," [0.03894601 0.03755387]]\n","\n","Average MAE Loss:\n","[0.05381764 0.05381764 0.03890947 0.03824994]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.05505472 0.05258055]\n"," [0.03889875 0.03728586]\n"," [0.03887723 0.03711661]]\n","\n","Average MAE Loss:\n","[0.05381764 0.05381764 0.03809231 0.03799692]\n","\n","Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.05505472 0.05258055]\n"," [0.03888662 0.03716453]\n"," [0.03837892 0.03683333]]\n","\n","Average MAE Loss:\n","[0.05381764 0.05381764 0.03802557 0.03760613]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.05505472 0.05258055]\n"," [0.03915454 0.03744516]\n"," [0.03809693 0.0366336 ]]\n","\n","Average MAE Loss:\n","[0.05381764 0.05381764 0.03829985 0.03736526]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.05505472 0.05258055]\n"," [0.03908793 0.03737429]\n"," [0.03807215 0.03663734]]\n","\n","Average MAE Loss:\n","[0.05381764 0.05381764 0.03823111 0.03735474]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.05505472 0.05258055]\n"," [0.03892964 0.03707448]\n"," [0.03813361 0.03679065]]\n","\n","Average MAE Loss:\n","[0.05381764 0.05381764 0.03800206 0.03746213]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.03813361 0.03679065]\n"," [0.04385329 0.04164707]\n"," [0.03798252 0.03625917]]\n","\n","Average MAE Loss:\n","[0.05381764 0.03746213 0.04275018 0.03712085]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.03813361 0.03679065]\n"," [0.03914135 0.03748154]\n"," [0.0382471  0.03692583]]\n","\n","Average MAE Loss:\n","[0.05381764 0.03746213 0.03831144 0.03758646]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.03813361 0.03679065]\n"," [0.03880446 0.03716921]\n"," [0.03822572 0.03675652]]\n","\n","Average MAE Loss:\n","[0.05381764 0.03746213 0.03798684 0.03749112]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.03813361 0.03679065]\n"," [0.03910185 0.03734739]\n"," [0.03823433 0.0366982 ]]\n","\n","Average MAE Loss:\n","[0.05381764 0.03746213 0.03822462 0.03746627]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.03813361 0.03679065]\n"," [0.03929463 0.03753178]\n"," [0.03821553 0.03671777]]\n","\n","Average MAE Loss:\n","[0.05381764 0.03746213 0.03841321 0.03746665]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.03813361 0.03679065]\n"," [0.0390078  0.03719106]\n"," [0.03810395 0.03668359]]\n","\n","Average MAE Loss:\n","[0.05381764 0.03746213 0.03809943 0.03739377]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05505472 0.05258055]\n"," [0.03813361 0.03679065]\n"," [0.03889626 0.03704626]\n"," [0.03815968 0.03664337]]\n","\n","Average MAE Loss:\n","[0.05381764 0.03746213 0.03797126 0.03740152]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00042: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.0401526  0.03805766]\n"," [0.03789342 0.03607513]\n"," [0.03765681 0.03613038]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03910513 0.03698428 0.0368936 ]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.0401526  0.03805766]\n"," [0.03887265 0.03713702]\n"," [0.03833982 0.03702248]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03910513 0.03800484 0.03768115]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.0401526  0.03805766]\n"," [0.03894534 0.0372223 ]\n"," [0.03784092 0.03658327]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03910513 0.03808382 0.0372121 ]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.0401526  0.03805766]\n"," [0.03883329 0.03687692]\n"," [0.03790312 0.03658047]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03910513 0.03785511 0.03724179]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.0401526  0.03805766]\n"," [0.03910245 0.03722954]\n"," [0.03827646 0.03694617]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03910513 0.03816599 0.03761131]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.0401526  0.03805766]\n"," [0.0389594  0.0370832 ]\n"," [0.03800294 0.0364882 ]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03910513 0.0380213  0.03724557]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.0401526  0.03805766]\n"," [0.03883042 0.03708672]\n"," [0.03803522 0.03658464]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03910513 0.03795857 0.03730993]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00049: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00049: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.03803522 0.03658464]\n"," [0.03828493 0.03630405]\n"," [0.03838381 0.03683656]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03730993 0.03729449 0.03761018]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.03803522 0.03658464]\n"," [0.03802156 0.03642916]\n"," [0.03784335 0.03630207]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03730993 0.03722536 0.03707271]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.03803522 0.03658464]\n"," [0.03882275 0.03719934]\n"," [0.0377009  0.03622173]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03730993 0.03801105 0.03696132]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.03803522 0.03658464]\n"," [0.03865514 0.03693895]\n"," [0.03769051 0.03622674]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03730993 0.03779705 0.03695863]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.03803522 0.03658464]\n"," [0.03870373 0.03690891]\n"," [0.03768529 0.03621452]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03730993 0.03780632 0.0369499 ]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.03803522 0.03658464]\n"," [0.03873518 0.03698927]\n"," [0.03782068 0.03644789]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03730993 0.03786223 0.03713428]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00055: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00055: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0401526  0.03805766]\n"," [0.03803522 0.03658464]\n"," [0.03871976 0.03698642]\n"," [0.03784673 0.03649472]]\n","\n","Average MAE Loss:\n","[0.03910513 0.03730993 0.03785309 0.03717072]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03773421 0.03605166]\n"," [0.03773421 0.03605166]\n"," [0.03766474 0.03608603]\n"," [0.03769314 0.03604139]]\n","\n","Average MAE Loss:\n","[0.03689294 0.03689294 0.03687538 0.03686726]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03773421 0.03605166]\n"," [0.03773421 0.03605166]\n"," [0.03788866 0.03633346]\n"," [0.03763393 0.03606874]]\n","\n","Average MAE Loss:\n","[0.03689294 0.03689294 0.03711106 0.03685133]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03773421 0.03605166]\n"," [0.03773421 0.03605166]\n"," [0.03810472 0.03651662]\n"," [0.0376627  0.03613598]]\n","\n","Average MAE Loss:\n","[0.03689294 0.03689294 0.03731067 0.03689934]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03773421 0.03605166]\n"," [0.03773421 0.03605166]\n"," [0.03821866 0.03658028]\n"," [0.03768511 0.03616217]]\n","\n","Average MAE Loss:\n","[0.03689294 0.03689294 0.03739947 0.03692364]\n","\n","\n","epochs finished with time:64.03865003585815\n","\n","[[0.03773421 0.03605166]\n"," [0.03773421 0.03605166]\n"," [0.03821866 0.03658028]\n"," [0.03768511 0.03616217]]\n","00:01:30.79895573399972\n","------------------------------------Fold [2/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.11435201 0.11263422]\n"," [0.11453133 0.11279516]\n"," [0.07839805 0.08029577]\n"," [0.07900541 0.07747073]]\n","\n","Average MAE Loss:\n","[0.11349311 0.11366324 0.07934691 0.07823807]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.11435201 0.11263422]\n"," [0.11453133 0.11279516]\n"," [0.06977221 0.07084145]\n"," [0.07262943 0.07235411]]\n","\n","Average MAE Loss:\n","[0.11349311 0.11366324 0.07030683 0.07249177]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.11435201 0.11263422]\n"," [0.11453133 0.11279516]\n"," [0.06960627 0.07026399]\n"," [0.06963196 0.07090439]]\n","\n","Average MAE Loss:\n","[0.11349311 0.11366324 0.06993513 0.07026818]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.11435201 0.11263422]\n"," [0.11453133 0.11279516]\n"," [0.06782721 0.06843354]\n"," [0.07220234 0.07094123]]\n","\n","Average MAE Loss:\n","[0.11349311 0.11366324 0.06813037 0.07157179]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.11435201 0.11263422]\n"," [0.11453133 0.11279516]\n"," [0.06856067 0.06866622]\n"," [0.06879527 0.06865979]]\n","\n","Average MAE Loss:\n","[0.11349311 0.11366324 0.06861344 0.06872753]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.11435201 0.11263422]\n"," [0.11453133 0.11279516]\n"," [0.06823918 0.0687282 ]\n"," [0.06904158 0.06873241]]\n","\n","Average MAE Loss:\n","[0.11349311 0.11366324 0.06848369 0.068887  ]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.11435201 0.11263422]\n"," [0.11453133 0.11279516]\n"," [0.06932966 0.06946918]\n"," [0.07339347 0.07181887]]\n","\n","Average MAE Loss:\n","[0.11349311 0.11366324 0.06939942 0.07260617]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.11453133 0.11279516]\n"," [0.07339347 0.07181887]\n"," [0.0779543  0.07867823]\n"," [0.06940742 0.06845409]]\n","\n","Average MAE Loss:\n","[0.11366324 0.07260617 0.07831627 0.06893075]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.11453133 0.11279516]\n"," [0.07339347 0.07181887]\n"," [0.07447653 0.07475914]\n"," [0.06964133 0.06837393]]\n","\n","Average MAE Loss:\n","[0.11366324 0.07260617 0.07461783 0.06900763]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.11453133 0.11279516]\n"," [0.07339347 0.07181887]\n"," [0.07023458 0.07181183]\n"," [0.06682565 0.06648406]]\n","\n","Average MAE Loss:\n","[0.11366324 0.07260617 0.0710232  0.06665485]\n","\n","Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.11453133 0.11279516]\n"," [0.07339347 0.07181887]\n"," [0.06964488 0.07091019]\n"," [0.06700436 0.06627638]]\n","\n","Average MAE Loss:\n","[0.11366324 0.07260617 0.07027753 0.06664037]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.11453133 0.11279516]\n"," [0.07339347 0.07181887]\n"," [0.06869326 0.06963191]\n"," [0.06819901 0.0669248 ]]\n","\n","Average MAE Loss:\n","[0.11366324 0.07260617 0.06916258 0.06756191]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.11453133 0.11279516]\n"," [0.07339347 0.07181887]\n"," [0.06831869 0.06935573]\n"," [0.06645031 0.06591594]]\n","\n","Average MAE Loss:\n","[0.11366324 0.07260617 0.06883721 0.06618313]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.11453133 0.11279516]\n"," [0.07339347 0.07181887]\n"," [0.0682689  0.06923698]\n"," [0.06632275 0.06579921]]\n","\n","Average MAE Loss:\n","[0.11366324 0.07260617 0.06875294 0.06606098]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.11160215 0.10973842]\n"," [0.06955493 0.06888139]\n"," [0.06488148 0.06524963]]\n","\n","Average MAE Loss:\n","[0.11067029 0.11067029 0.06921816 0.06506556]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.11160215 0.10973842]\n"," [0.06615125 0.0670708 ]\n"," [0.06475537 0.06427816]]\n","\n","Average MAE Loss:\n","[0.11067029 0.11067029 0.06661102 0.06451677]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.11160215 0.10973842]\n"," [0.06319418 0.0647872 ]\n"," [0.06512656 0.06444153]]\n","\n","Average MAE Loss:\n","[0.11067029 0.11067029 0.06399069 0.06478405]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.11160215 0.10973842]\n"," [0.06383451 0.06492259]\n"," [0.06307822 0.0634147 ]]\n","\n","Average MAE Loss:\n","[0.11067029 0.11067029 0.06437855 0.06324646]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.11160215 0.10973842]\n"," [0.06318487 0.06404684]\n"," [0.06244006 0.06302753]]\n","\n","Average MAE Loss:\n","[0.11067029 0.11067029 0.06361585 0.06273379]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.11160215 0.10973842]\n"," [0.06361231 0.06422055]\n"," [0.06483301 0.06448599]]\n","\n","Average MAE Loss:\n","[0.11067029 0.11067029 0.06391643 0.0646595 ]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.11160215 0.10973842]\n"," [0.06326278 0.06428654]\n"," [0.06420289 0.06400818]]\n","\n","Average MAE Loss:\n","[0.11067029 0.11067029 0.06377466 0.06410553]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.06420289 0.06400818]\n"," [0.10062739 0.09895061]\n"," [0.06380064 0.06356312]]\n","\n","Average MAE Loss:\n","[0.11067029 0.06410553 0.099789   0.06368188]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.06420289 0.06400818]\n"," [0.06494219 0.06556599]\n"," [0.06409634 0.06346228]]\n","\n","Average MAE Loss:\n","[0.11067029 0.06410553 0.06525409 0.06377931]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.06420289 0.06400818]\n"," [0.06552251 0.06614832]\n"," [0.06569254 0.0646297 ]]\n","\n","Average MAE Loss:\n","[0.11067029 0.06410553 0.06583542 0.06516112]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.06420289 0.06400818]\n"," [0.06386774 0.06468829]\n"," [0.06318667 0.06269966]]\n","\n","Average MAE Loss:\n","[0.11067029 0.06410553 0.06427801 0.06294316]\n","\n","Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.06420289 0.06400818]\n"," [0.06330188 0.06422156]\n"," [0.06380106 0.06295532]]\n","\n","Average MAE Loss:\n","[0.11067029 0.06410553 0.06376172 0.06337819]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.06420289 0.06400818]\n"," [0.06306782 0.06405974]\n"," [0.06333292 0.06306407]]\n","\n","Average MAE Loss:\n","[0.11067029 0.06410553 0.06356378 0.06319849]\n","\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.11160215 0.10973842]\n"," [0.06420289 0.06400818]\n"," [0.06324922 0.0641462 ]\n"," [0.06425166 0.06357641]]\n","\n","Average MAE Loss:\n","[0.11067029 0.06410553 0.06369771 0.06391404]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.08768333 0.08595081]\n"," [0.07099196 0.06952   ]\n"," [0.06432049 0.0639067 ]]\n","\n","Average MAE Loss:\n","[0.08681707 0.08681707 0.07025598 0.06411359]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.08768333 0.08595081]\n"," [0.06193777 0.06323023]\n"," [0.06193793 0.06238672]]\n","\n","Average MAE Loss:\n","[0.08681707 0.08681707 0.062584   0.06216232]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.08768333 0.08595081]\n"," [0.062527   0.06336199]\n"," [0.06238431 0.06245707]]\n","\n","Average MAE Loss:\n","[0.08681707 0.08681707 0.0629445  0.06242069]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.08768333 0.08595081]\n"," [0.06226797 0.06344286]\n"," [0.06292359 0.06243036]]\n","\n","Average MAE Loss:\n","[0.08681707 0.08681707 0.06285541 0.06267698]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.08768333 0.08595081]\n"," [0.062328   0.06341619]\n"," [0.06276286 0.06247735]]\n","\n","Average MAE Loss:\n","[0.08681707 0.08681707 0.0628721  0.06262011]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.08768333 0.08595081]\n"," [0.06234287 0.06337872]\n"," [0.06295146 0.06273313]]\n","\n","Average MAE Loss:\n","[0.08681707 0.08681707 0.06286079 0.0628423 ]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.08768333 0.08595081]\n"," [0.06232455 0.06332542]\n"," [0.06236719 0.06211161]]\n","\n","Average MAE Loss:\n","[0.08681707 0.08681707 0.06282498 0.0622394 ]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.06236719 0.06211161]\n"," [0.07159795 0.07001379]\n"," [0.06307059 0.06285807]]\n","\n","Average MAE Loss:\n","[0.08681707 0.0622394  0.07080587 0.06296433]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch 00036: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.06236719 0.06211161]\n"," [0.06307353 0.06305202]\n"," [0.06285859 0.06242983]]\n","\n","Average MAE Loss:\n","[0.08681707 0.0622394  0.06306278 0.06264421]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.06236719 0.06211161]\n"," [0.0618009  0.0627502 ]\n"," [0.06241803 0.0622428 ]]\n","\n","Average MAE Loss:\n","[0.08681707 0.0622394  0.06227555 0.06233042]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.06236719 0.06211161]\n"," [0.06193399 0.06290619]\n"," [0.06207507 0.06212425]]\n","\n","Average MAE Loss:\n","[0.08681707 0.0622394  0.06242009 0.06209966]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.06236719 0.06211161]\n"," [0.06192684 0.06298977]\n"," [0.06219264 0.06214018]]\n","\n","Average MAE Loss:\n","[0.08681707 0.0622394  0.0624583  0.06216641]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.06236719 0.06211161]\n"," [0.06217737 0.06309929]\n"," [0.06223377 0.06220976]]\n","\n","Average MAE Loss:\n","[0.08681707 0.0622394  0.06263833 0.06222177]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.08768333 0.08595081]\n"," [0.06236719 0.06211161]\n"," [0.06222344 0.0631484 ]\n"," [0.06250158 0.06223552]]\n","\n","Average MAE Loss:\n","[0.08681707 0.0622394  0.06268592 0.06236855]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.0675515  0.06634372]\n"," [0.06417318 0.06362203]\n"," [0.06230891 0.06222517]]\n","\n","Average MAE Loss:\n","[0.06694761 0.06694761 0.0638976  0.06226704]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.0675515  0.06634372]\n"," [0.06187922 0.06236386]\n"," [0.06197866 0.06199057]]\n","\n","Average MAE Loss:\n","[0.06694761 0.06694761 0.06212154 0.06198461]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.0675515  0.06634372]\n"," [0.06172389 0.06248817]\n"," [0.06218919 0.06207799]]\n","\n","Average MAE Loss:\n","[0.06694761 0.06694761 0.06210603 0.06213359]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.0675515  0.06634372]\n"," [0.06235405 0.06298969]\n"," [0.0619276  0.06202598]]\n","\n","Average MAE Loss:\n","[0.06694761 0.06694761 0.06267187 0.06197679]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.0675515  0.06634372]\n"," [0.06206313 0.0628637 ]\n"," [0.06211636 0.06202464]]\n","\n","Average MAE Loss:\n","[0.06694761 0.06694761 0.06246342 0.0620705 ]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.0675515  0.06634372]\n"," [0.06209716 0.06297172]\n"," [0.0630126  0.06256721]]\n","\n","Average MAE Loss:\n","[0.06694761 0.06694761 0.06253444 0.06278991]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.0675515  0.06634372]\n"," [0.06237257 0.06318837]\n"," [0.06235063 0.06205516]]\n","\n","Average MAE Loss:\n","[0.06694761 0.06694761 0.06278047 0.0622029 ]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.06235063 0.06205516]\n"," [0.0643528  0.06375622]\n"," [0.06223929 0.06270193]]\n","\n","Average MAE Loss:\n","[0.06694761 0.0622029  0.06405451 0.06247061]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.06235063 0.06205516]\n"," [0.06197086 0.06237482]\n"," [0.0617508  0.06224489]]\n","\n","Average MAE Loss:\n","[0.06694761 0.0622029  0.06217284 0.06199785]\n","\n","Epoch 00051: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.06235063 0.06205516]\n"," [0.06162913 0.06236077]\n"," [0.06237087 0.06241295]]\n","\n","Average MAE Loss:\n","[0.06694761 0.0622029  0.06199495 0.06239191]\n","\n","Epoch 00052: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.06235063 0.06205516]\n"," [0.06156582 0.06242447]\n"," [0.06220298 0.06223769]]\n","\n","Average MAE Loss:\n","[0.06694761 0.0622029  0.06199515 0.06222034]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.06235063 0.06205516]\n"," [0.06167321 0.06251083]\n"," [0.06208718 0.06209872]]\n","\n","Average MAE Loss:\n","[0.06694761 0.0622029  0.06209202 0.06209295]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.06235063 0.06205516]\n"," [0.0617802  0.06263543]\n"," [0.06212083 0.06207287]]\n","\n","Average MAE Loss:\n","[0.06694761 0.0622029  0.06220781 0.06209685]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.0675515  0.06634372]\n"," [0.06235063 0.06205516]\n"," [0.06187989 0.06274745]\n"," [0.0623161  0.06220096]]\n","\n","Average MAE Loss:\n","[0.06694761 0.0622029  0.06231367 0.06225853]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.06309561 0.06280343]\n"," [0.06309561 0.06280343]\n"," [0.06229464 0.06231806]\n"," [0.06241311 0.06231057]]\n","\n","Average MAE Loss:\n","[0.06294952 0.06294952 0.06230635 0.06236184]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.06309561 0.06280343]\n"," [0.06309561 0.06280343]\n"," [0.06169665 0.06213596]\n"," [0.06172459 0.06192433]]\n","\n","Average MAE Loss:\n","[0.06294952 0.06294952 0.0619163  0.06182446]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.06309561 0.06280343]\n"," [0.06309561 0.06280343]\n"," [0.06170983 0.06224526]\n"," [0.06175996 0.06192148]]\n","\n","Average MAE Loss:\n","[0.06294952 0.06294952 0.06197754 0.06184072]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.06309561 0.06280343]\n"," [0.06309561 0.06280343]\n"," [0.06175156 0.06233818]\n"," [0.06239686 0.06222574]]\n","\n","Average MAE Loss:\n","[0.06294952 0.06294952 0.06204487 0.0623113 ]\n","\n","\n","epochs finished with time:63.7852885723114\n","\n","[[0.06309561 0.06280343]\n"," [0.06309561 0.06280343]\n"," [0.06175156 0.06233818]\n"," [0.06239686 0.06222574]]\n","00:01:29.39431856400006\n","------------------------------------Fold [3/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.0977859  0.08670831]\n"," [0.09797762 0.0868255 ]\n"," [0.05895551 0.05289064]\n"," [0.06156543 0.0547852 ]]\n","\n","Average MAE Loss:\n","[0.0922471  0.09240156 0.05592307 0.05817531]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.0977859  0.08670831]\n"," [0.09797762 0.0868255 ]\n"," [0.05906903 0.05033164]\n"," [0.05955079 0.05604129]]\n","\n","Average MAE Loss:\n","[0.0922471  0.09240156 0.05470034 0.05779604]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.0977859  0.08670831]\n"," [0.09797762 0.0868255 ]\n"," [0.05459937 0.04693235]\n"," [0.05916509 0.05052523]]\n","\n","Average MAE Loss:\n","[0.0922471  0.09240156 0.05076586 0.05484516]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.0977859  0.08670831]\n"," [0.09797762 0.0868255 ]\n"," [0.05335129 0.0464912 ]\n"," [0.05498094 0.04879626]]\n","\n","Average MAE Loss:\n","[0.0922471  0.09240156 0.04992125 0.0518886 ]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.0977859  0.08670831]\n"," [0.09797762 0.0868255 ]\n"," [0.05170945 0.0462081 ]\n"," [0.05412336 0.04945996]]\n","\n","Average MAE Loss:\n","[0.0922471  0.09240156 0.04895878 0.05179166]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.0977859  0.08670831]\n"," [0.09797762 0.0868255 ]\n"," [0.05105902 0.04623748]\n"," [0.05477107 0.0471819 ]]\n","\n","Average MAE Loss:\n","[0.0922471  0.09240156 0.04864825 0.05097649]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.0977859  0.08670831]\n"," [0.09797762 0.0868255 ]\n"," [0.05151977 0.04521912]\n"," [0.05414307 0.04670416]]\n","\n","Average MAE Loss:\n","[0.0922471  0.09240156 0.04836944 0.05042362]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.09797762 0.0868255 ]\n"," [0.05414307 0.04670416]\n"," [0.06463666 0.0553016 ]\n"," [0.05491149 0.04666651]]\n","\n","Average MAE Loss:\n","[0.09240156 0.05042362 0.05996913 0.050789  ]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.09797762 0.0868255 ]\n"," [0.05414307 0.04670416]\n"," [0.05609448 0.0486345 ]\n"," [0.05496781 0.04665237]]\n","\n","Average MAE Loss:\n","[0.09240156 0.05042362 0.05236449 0.05081009]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.09797762 0.0868255 ]\n"," [0.05414307 0.04670416]\n"," [0.05384926 0.0461404 ]\n"," [0.05303466 0.04531865]]\n","\n","Average MAE Loss:\n","[0.09240156 0.05042362 0.04999483 0.04917666]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.09797762 0.0868255 ]\n"," [0.05414307 0.04670416]\n"," [0.05202999 0.04557194]\n"," [0.0511695  0.0448915 ]]\n","\n","Average MAE Loss:\n","[0.09240156 0.05042362 0.04880097 0.0480305 ]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.09797762 0.0868255 ]\n"," [0.05414307 0.04670416]\n"," [0.05158586 0.04447623]\n"," [0.05155721 0.04451276]]\n","\n","Average MAE Loss:\n","[0.09240156 0.05042362 0.04803105 0.04803498]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.09797762 0.0868255 ]\n"," [0.05414307 0.04670416]\n"," [0.0536271  0.04527325]\n"," [0.05158574 0.04442065]]\n","\n","Average MAE Loss:\n","[0.09240156 0.05042362 0.04945018 0.0480032 ]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.09797762 0.0868255 ]\n"," [0.05414307 0.04670416]\n"," [0.05150135 0.04775325]\n"," [0.05134087 0.04427448]]\n","\n","Average MAE Loss:\n","[0.09240156 0.05042362 0.0496273  0.04780767]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.09357777 0.08241559]\n"," [0.04879179 0.04540668]\n"," [0.05039138 0.04508298]]\n","\n","Average MAE Loss:\n","[0.08799668 0.08799668 0.04709924 0.04773718]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.09357777 0.08241559]\n"," [0.04945419 0.04233652]\n"," [0.05003944 0.0426292 ]]\n","\n","Average MAE Loss:\n","[0.08799668 0.08799668 0.04589535 0.04633432]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.09357777 0.08241559]\n"," [0.04901264 0.04593208]\n"," [0.05326022 0.0442202 ]]\n","\n","Average MAE Loss:\n","[0.08799668 0.08799668 0.04747236 0.04874021]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.09357777 0.08241559]\n"," [0.04714902 0.0424948 ]\n"," [0.048687   0.04133183]]\n","\n","Average MAE Loss:\n","[0.08799668 0.08799668 0.04482191 0.04500941]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.09357777 0.08241559]\n"," [0.0481757  0.04176587]\n"," [0.04907045 0.04148669]]\n","\n","Average MAE Loss:\n","[0.08799668 0.08799668 0.04497079 0.04527857]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.09357777 0.08241559]\n"," [0.04820106 0.04431539]\n"," [0.04863172 0.04121106]]\n","\n","Average MAE Loss:\n","[0.08799668 0.08799668 0.04625823 0.04492139]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.09357777 0.08241559]\n"," [0.04734978 0.04115142]\n"," [0.04851314 0.04119333]]\n","\n","Average MAE Loss:\n","[0.08799668 0.08799668 0.0442506  0.04485323]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.04851314 0.04119333]\n"," [0.05138328 0.04460643]\n"," [0.04873355 0.04111643]]\n","\n","Average MAE Loss:\n","[0.08799668 0.04485323 0.04799486 0.04492499]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.04851314 0.04119333]\n"," [0.05107732 0.04305713]\n"," [0.04841791 0.04140382]]\n","\n","Average MAE Loss:\n","[0.08799668 0.04485323 0.04706722 0.04491087]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.04851314 0.04119333]\n"," [0.04790466 0.04270004]\n"," [0.04800711 0.04170167]]\n","\n","Average MAE Loss:\n","[0.08799668 0.04485323 0.04530235 0.04485439]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.04851314 0.04119333]\n"," [0.04776818 0.04434709]\n"," [0.04787801 0.04128437]]\n","\n","Average MAE Loss:\n","[0.08799668 0.04485323 0.04605763 0.04458119]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.04851314 0.04119333]\n"," [0.04874719 0.04172126]\n"," [0.04805442 0.04128394]]\n","\n","Average MAE Loss:\n","[0.08799668 0.04485323 0.04523422 0.04466918]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.04851314 0.04119333]\n"," [0.04880886 0.04207928]\n"," [0.04873605 0.04112227]]\n","\n","Average MAE Loss:\n","[0.08799668 0.04485323 0.04544407 0.04492916]\n","\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00027: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.09357777 0.08241559]\n"," [0.04851314 0.04119333]\n"," [0.0473381  0.04232078]\n"," [0.04845576 0.04126087]]\n","\n","Average MAE Loss:\n","[0.08799668 0.04485323 0.04482944 0.04485831]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.06940253 0.0585203 ]\n"," [0.04780787 0.04075855]\n"," [0.05206785 0.04983064]]\n","\n","Average MAE Loss:\n","[0.06396141 0.06396141 0.04428321 0.05094925]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.06940253 0.0585203 ]\n"," [0.04692993 0.04149356]\n"," [0.0488608  0.04108815]]\n","\n","Average MAE Loss:\n","[0.06396141 0.06396141 0.04421175 0.04497447]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.06940253 0.0585203 ]\n"," [0.04683817 0.04069154]\n"," [0.04842031 0.04113303]]\n","\n","Average MAE Loss:\n","[0.06396141 0.06396141 0.04376485 0.04477667]\n","\n","Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.06940253 0.0585203 ]\n"," [0.04692151 0.04138848]\n"," [0.04721324 0.04128137]]\n","\n","Average MAE Loss:\n","[0.06396141 0.06396141 0.04415499 0.04424731]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.06940253 0.0585203 ]\n"," [0.04680923 0.04089615]\n"," [0.04753449 0.04072338]]\n","\n","Average MAE Loss:\n","[0.06396141 0.06396141 0.04385269 0.04412893]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.06940253 0.0585203 ]\n"," [0.04653431 0.04089419]\n"," [0.04754237 0.0406644 ]]\n","\n","Average MAE Loss:\n","[0.06396141 0.06396141 0.04371425 0.04410339]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.06940253 0.0585203 ]\n"," [0.04649042 0.04090658]\n"," [0.04691636 0.0408434 ]]\n","\n","Average MAE Loss:\n","[0.06396141 0.06396141 0.0436985  0.04387988]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.04691636 0.0408434 ]\n"," [0.05093604 0.04283819]\n"," [0.04644688 0.04011955]]\n","\n","Average MAE Loss:\n","[0.06396141 0.04387988 0.04688712 0.04328322]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.04691636 0.0408434 ]\n"," [0.04781777 0.04420843]\n"," [0.0465559  0.04049332]]\n","\n","Average MAE Loss:\n","[0.06396141 0.04387988 0.0460131  0.04352461]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.04691636 0.0408434 ]\n"," [0.04677131 0.04051749]\n"," [0.04719558 0.04035709]]\n","\n","Average MAE Loss:\n","[0.06396141 0.04387988 0.0436444  0.04377634]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.04691636 0.0408434 ]\n"," [0.04700281 0.04191572]\n"," [0.04695178 0.04040514]]\n","\n","Average MAE Loss:\n","[0.06396141 0.04387988 0.04445927 0.04367846]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.04691636 0.0408434 ]\n"," [0.04681465 0.04108014]\n"," [0.04675992 0.04051897]]\n","\n","Average MAE Loss:\n","[0.06396141 0.04387988 0.04394739 0.04363945]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.04691636 0.0408434 ]\n"," [0.04684135 0.04091009]\n"," [0.04727357 0.04037892]]\n","\n","Average MAE Loss:\n","[0.06396141 0.04387988 0.04387572 0.04382624]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.06940253 0.0585203 ]\n"," [0.04691636 0.0408434 ]\n"," [0.04662596 0.04125828]\n"," [0.04679432 0.04087347]]\n","\n","Average MAE Loss:\n","[0.06396141 0.04387988 0.04394212 0.04383389]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00042: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.05060062 0.04207977]\n"," [0.04647852 0.04163297]\n"," [0.04712088 0.04023862]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04634019 0.04405574 0.04367975]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.05060062 0.04207977]\n"," [0.04662522 0.04083323]\n"," [0.04662294 0.04068378]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04634019 0.04372923 0.04365336]\n","\n","Epoch 00044: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.05060062 0.04207977]\n"," [0.04657473 0.04073035]\n"," [0.04681107 0.04046275]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04634019 0.04365254 0.04363691]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.05060062 0.04207977]\n"," [0.04641188 0.04094914]\n"," [0.04702886 0.04028106]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04634019 0.04368051 0.04365496]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.05060062 0.04207977]\n"," [0.0464959  0.04075032]\n"," [0.04681481 0.04038589]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04634019 0.04362311 0.04360035]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.05060062 0.04207977]\n"," [0.04641643 0.04120023]\n"," [0.04674084 0.04040285]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04634019 0.04380833 0.04357184]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00048: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.05060062 0.04207977]\n"," [0.04634406 0.04083321]\n"," [0.04674277 0.04033088]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04634019 0.04358863 0.04353683]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.04674277 0.04033088]\n"," [0.0470264  0.04013083]\n"," [0.0462328  0.04021866]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04353683 0.04357861 0.04322573]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.04674277 0.04033088]\n"," [0.04632623 0.04115829]\n"," [0.04629092 0.03993252]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04353683 0.04374226 0.04311172]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.04674277 0.04033088]\n"," [0.04618249 0.0405402 ]\n"," [0.04627404 0.03989053]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04353683 0.04336135 0.04308229]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.04674277 0.04033088]\n"," [0.04626834 0.04090712]\n"," [0.04619737 0.03995394]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04353683 0.04358773 0.04307565]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.04674277 0.04033088]\n"," [0.04644642 0.04093135]\n"," [0.04635859 0.03985557]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04353683 0.04368889 0.04310708]\n","\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.04674277 0.04033088]\n"," [0.04646447 0.04076432]\n"," [0.04632342 0.03999806]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04353683 0.0436144  0.04316074]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05060062 0.04207977]\n"," [0.04674277 0.04033088]\n"," [0.04649688 0.0407018 ]\n"," [0.0463731  0.04011494]]\n","\n","Average MAE Loss:\n","[0.04634019 0.04353683 0.04359934 0.04324402]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.04704359 0.03993403]\n"," [0.04704359 0.03993403]\n"," [0.04632883 0.04018145]\n"," [0.04661038 0.0399738 ]]\n","\n","Average MAE Loss:\n","[0.04348881 0.04348881 0.04325514 0.04329209]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.04704359 0.03993403]\n"," [0.04704359 0.03993403]\n"," [0.04627457 0.04102721]\n"," [0.04651853 0.03997784]]\n","\n","Average MAE Loss:\n","[0.04348881 0.04348881 0.04365089 0.04324819]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.04704359 0.03993403]\n"," [0.04704359 0.03993403]\n"," [0.04636193 0.04083749]\n"," [0.04654052 0.04002531]]\n","\n","Average MAE Loss:\n","[0.04348881 0.04348881 0.04359971 0.04328291]\n","\n","Epoch 00059: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.04704359 0.03993403]\n"," [0.04704359 0.03993403]\n"," [0.04637993 0.0411583 ]\n"," [0.0465725  0.04002365]]\n","\n","Average MAE Loss:\n","[0.04348881 0.04348881 0.04376912 0.04329808]\n","\n","\n","epochs finished with time:63.2876193523407\n","\n","[[0.04704359 0.03993403]\n"," [0.04704359 0.03993403]\n"," [0.04637993 0.0411583 ]\n"," [0.0465725  0.04002365]]\n","00:01:25.435583679000047\n","------------------------------------Fold [4/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.08049297 0.1028825 ]\n"," [0.08058786 0.10310742]\n"," [0.05039275 0.07067963]\n"," [0.05276004 0.07062007]]\n","\n","Average MAE Loss:\n","[0.09168774 0.09184764 0.06053619 0.06169006]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.08049297 0.1028825 ]\n"," [0.08058786 0.10310742]\n"," [0.04452194 0.06487244]\n"," [0.04678952 0.06561996]]\n","\n","Average MAE Loss:\n","[0.09168774 0.09184764 0.05469719 0.05620474]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.08049297 0.1028825 ]\n"," [0.08058786 0.10310742]\n"," [0.04293267 0.06343513]\n"," [0.04472093 0.06461987]]\n","\n","Average MAE Loss:\n","[0.09168774 0.09184764 0.0531839  0.0546704 ]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.08049297 0.1028825 ]\n"," [0.08058786 0.10310742]\n"," [0.04257991 0.06291846]\n"," [0.04469382 0.06421766]]\n","\n","Average MAE Loss:\n","[0.09168774 0.09184764 0.05274919 0.05445574]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.08049297 0.1028825 ]\n"," [0.08058786 0.10310742]\n"," [0.04307741 0.06383938]\n"," [0.0442557  0.06378417]]\n","\n","Average MAE Loss:\n","[0.09168774 0.09184764 0.05345839 0.05401993]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.08049297 0.1028825 ]\n"," [0.08058786 0.10310742]\n"," [0.04231798 0.06255925]\n"," [0.04411277 0.06306627]]\n","\n","Average MAE Loss:\n","[0.09168774 0.09184764 0.05243861 0.05358952]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.08049297 0.1028825 ]\n"," [0.08058786 0.10310742]\n"," [0.04171    0.06173761]\n"," [0.04501843 0.06309384]]\n","\n","Average MAE Loss:\n","[0.09168774 0.09184764 0.0517238  0.05405614]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.08058786 0.10310742]\n"," [0.04501843 0.06309384]\n"," [0.04976847 0.07047309]\n"," [0.0440982  0.06353375]]\n","\n","Average MAE Loss:\n","[0.09184764 0.05405614 0.06012078 0.05381598]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.08058786 0.10310742]\n"," [0.04501843 0.06309384]\n"," [0.04457954 0.06450982]\n"," [0.04209746 0.06171306]]\n","\n","Average MAE Loss:\n","[0.09184764 0.05405614 0.05454468 0.05190526]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.08058786 0.10310742]\n"," [0.04501843 0.06309384]\n"," [0.04325789 0.06300905]\n"," [0.04247712 0.06259972]]\n","\n","Average MAE Loss:\n","[0.09184764 0.05405614 0.05313347 0.05253842]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.08058786 0.10310742]\n"," [0.04501843 0.06309384]\n"," [0.04179768 0.06206146]\n"," [0.04154159 0.06079828]]\n","\n","Average MAE Loss:\n","[0.09184764 0.05405614 0.05192957 0.05116993]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.08058786 0.10310742]\n"," [0.04501843 0.06309384]\n"," [0.0412019  0.06145974]\n"," [0.04116982 0.06069104]]\n","\n","Average MAE Loss:\n","[0.09184764 0.05405614 0.05133082 0.05093043]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.08058786 0.10310742]\n"," [0.04501843 0.06309384]\n"," [0.04176565 0.06281861]\n"," [0.04137638 0.06077931]]\n","\n","Average MAE Loss:\n","[0.09184764 0.05405614 0.05229213 0.05107785]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.08058786 0.10310742]\n"," [0.04501843 0.06309384]\n"," [0.04082237 0.06151988]\n"," [0.04132553 0.06106552]]\n","\n","Average MAE Loss:\n","[0.09184764 0.05405614 0.05117112 0.05119552]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.07620043 0.09862815]\n"," [0.041342   0.061278  ]\n"," [0.04246769 0.06064617]]\n","\n","Average MAE Loss:\n","[0.08741429 0.08741429 0.05131    0.05155693]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.07620043 0.09862815]\n"," [0.03904505 0.05960073]\n"," [0.03960539 0.05918301]]\n","\n","Average MAE Loss:\n","[0.08741429 0.08741429 0.04932289 0.0493942 ]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.07620043 0.09862815]\n"," [0.0393208  0.05863323]\n"," [0.03936479 0.05863049]]\n","\n","Average MAE Loss:\n","[0.08741429 0.08741429 0.04897702 0.04899764]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.07620043 0.09862815]\n"," [0.03861356 0.05827615]\n"," [0.0391665  0.05896336]]\n","\n","Average MAE Loss:\n","[0.08741429 0.08741429 0.04844485 0.04906493]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.07620043 0.09862815]\n"," [0.03839542 0.05834581]\n"," [0.03888494 0.05802353]]\n","\n","Average MAE Loss:\n","[0.08741429 0.08741429 0.04837062 0.04845423]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.07620043 0.09862815]\n"," [0.03836486 0.05880423]\n"," [0.03939094 0.05779372]]\n","\n","Average MAE Loss:\n","[0.08741429 0.08741429 0.04858454 0.04859233]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.07620043 0.09862815]\n"," [0.03926334 0.05844316]\n"," [0.03893217 0.05869189]]\n","\n","Average MAE Loss:\n","[0.08741429 0.08741429 0.04885325 0.04881203]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.03893217 0.05869189]\n"," [0.042082   0.06052508]\n"," [0.0392189  0.05817534]]\n","\n","Average MAE Loss:\n","[0.08741429 0.04881203 0.05130354 0.04869712]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.03893217 0.05869189]\n"," [0.04009243 0.06035953]\n"," [0.03912061 0.05812258]]\n","\n","Average MAE Loss:\n","[0.08741429 0.04881203 0.05022598 0.04862159]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.03893217 0.05869189]\n"," [0.03940629 0.0591994 ]\n"," [0.03858029 0.0581263 ]]\n","\n","Average MAE Loss:\n","[0.08741429 0.04881203 0.04930285 0.0483533 ]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.03893217 0.05869189]\n"," [0.03880337 0.05880768]\n"," [0.03872854 0.05780369]]\n","\n","Average MAE Loss:\n","[0.08741429 0.04881203 0.04880552 0.04826611]\n","\n","Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.03893217 0.05869189]\n"," [0.03919615 0.05898511]\n"," [0.03846648 0.05795152]]\n","\n","Average MAE Loss:\n","[0.08741429 0.04881203 0.04909063 0.048209  ]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.03893217 0.05869189]\n"," [0.03881179 0.0584334 ]\n"," [0.03876812 0.05792852]]\n","\n","Average MAE Loss:\n","[0.08741429 0.04881203 0.04862259 0.04834832]\n","\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.07620043 0.09862815]\n"," [0.03893217 0.05869189]\n"," [0.03906887 0.05862032]\n"," [0.0386045  0.05763672]]\n","\n","Average MAE Loss:\n","[0.08741429 0.04881203 0.04884459 0.04812061]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.05420366 0.07610869]\n"," [0.03732393 0.05714247]\n"," [0.04312379 0.06029586]]\n","\n","Average MAE Loss:\n","[0.06515617 0.06515617 0.0472332  0.05170983]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.05420366 0.07610869]\n"," [0.0376521  0.05716431]\n"," [0.03851538 0.05768913]]\n","\n","Average MAE Loss:\n","[0.06515617 0.06515617 0.0474082  0.04810226]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.05420366 0.07610869]\n"," [0.03786657 0.05741747]\n"," [0.0380741  0.05720798]]\n","\n","Average MAE Loss:\n","[0.06515617 0.06515617 0.04764202 0.04764104]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.05420366 0.07610869]\n"," [0.03757197 0.05722741]\n"," [0.03910072 0.05755118]]\n","\n","Average MAE Loss:\n","[0.06515617 0.06515617 0.04739969 0.04832595]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.05420366 0.07610869]\n"," [0.03724407 0.05715151]\n"," [0.03838637 0.05753887]]\n","\n","Average MAE Loss:\n","[0.06515617 0.06515617 0.04719779 0.04796262]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.05420366 0.07610869]\n"," [0.03762993 0.05721493]\n"," [0.03804815 0.05731763]]\n","\n","Average MAE Loss:\n","[0.06515617 0.06515617 0.04742243 0.04768289]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.05420366 0.07610869]\n"," [0.0376752  0.05723191]\n"," [0.03817359 0.05727089]]\n","\n","Average MAE Loss:\n","[0.06515617 0.06515617 0.04745356 0.04772224]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.03817359 0.05727089]\n"," [0.03783472 0.05793234]\n"," [0.03887044 0.05735285]]\n","\n","Average MAE Loss:\n","[0.06515617 0.04772224 0.04788353 0.04811164]\n","\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.03817359 0.05727089]\n"," [0.03770873 0.05716914]\n"," [0.03861672 0.05733541]]\n","\n","Average MAE Loss:\n","[0.06515617 0.04772224 0.04743893 0.04797606]\n","\n","Epoch 00037: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.03817359 0.05727089]\n"," [0.03780931 0.05747667]\n"," [0.03807977 0.05703884]]\n","\n","Average MAE Loss:\n","[0.06515617 0.04772224 0.04764299 0.0475593 ]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.03817359 0.05727089]\n"," [0.03731548 0.05717664]\n"," [0.03787797 0.0570347 ]]\n","\n","Average MAE Loss:\n","[0.06515617 0.04772224 0.04724606 0.04745634]\n","\n","Epoch 00039: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.03817359 0.05727089]\n"," [0.03746395 0.05713016]\n"," [0.03814042 0.0569834 ]]\n","\n","Average MAE Loss:\n","[0.06515617 0.04772224 0.04729705 0.04756191]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.03817359 0.05727089]\n"," [0.0375023  0.05715191]\n"," [0.0379068  0.05695336]]\n","\n","Average MAE Loss:\n","[0.06515617 0.04772224 0.04732711 0.04743008]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.05420366 0.07610869]\n"," [0.03817359 0.05727089]\n"," [0.03741261 0.05716083]\n"," [0.03785568 0.05687024]]\n","\n","Average MAE Loss:\n","[0.06515617 0.04772224 0.04728672 0.04736296]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03935882 0.05991511]\n"," [0.03695762 0.05682195]\n"," [0.03803097 0.05694118]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04963696 0.04688979 0.04748607]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03935882 0.05991511]\n"," [0.03756879 0.05684024]\n"," [0.03782681 0.05693373]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04963696 0.04720451 0.04738027]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03935882 0.05991511]\n"," [0.03707842 0.05683876]\n"," [0.03792316 0.05686275]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04963696 0.04695859 0.04739296]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03935882 0.05991511]\n"," [0.03715882 0.05697292]\n"," [0.03781222 0.05676612]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04963696 0.04706587 0.04728917]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03935882 0.05991511]\n"," [0.03714086 0.05702139]\n"," [0.03770741 0.05689627]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04963696 0.04708112 0.04730184]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03935882 0.05991511]\n"," [0.03761991 0.05727557]\n"," [0.03832481 0.05698938]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04963696 0.04744774 0.04765709]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03935882 0.05991511]\n"," [0.03738581 0.05713978]\n"," [0.03861427 0.05713966]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04963696 0.0472628  0.04787697]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00049: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03861427 0.05713966]\n"," [0.0378052  0.05798289]\n"," [0.03742738 0.05658192]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04787697 0.04789404 0.04700465]\n","\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03861427 0.05713966]\n"," [0.03693547 0.05664779]\n"," [0.03787841 0.05677772]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04787697 0.04679163 0.04732807]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03861427 0.05713966]\n"," [0.03703697 0.05664433]\n"," [0.03779498 0.05674873]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04787697 0.04684065 0.04727186]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03861427 0.05713966]\n"," [0.03710429 0.05671877]\n"," [0.03781984 0.05681821]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04787697 0.04691153 0.04731902]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03861427 0.05713966]\n"," [0.03717903 0.05684058]\n"," [0.03794313 0.05687786]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04787697 0.0470098  0.0474105 ]\n","\n","Epoch 00054: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03861427 0.05713966]\n"," [0.0371892  0.0569161 ]\n"," [0.03801423 0.05698938]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04787697 0.04705265 0.04750181]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.03935882 0.05991511]\n"," [0.03861427 0.05713966]\n"," [0.03731747 0.05700933]\n"," [0.03849127 0.05714864]]\n","\n","Average MAE Loss:\n","[0.04963696 0.04787697 0.0471634  0.04781995]\n","\n","Epoch 00056: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.03705019 0.05666851]\n"," [0.03705019 0.05666851]\n"," [0.03695954 0.0565488 ]\n"," [0.03728042 0.05654716]]\n","\n","Average MAE Loss:\n","[0.04685935 0.04685935 0.04675417 0.04691379]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.03705019 0.05666851]\n"," [0.03705019 0.05666851]\n"," [0.03695289 0.05655645]\n"," [0.03787471 0.05677236]]\n","\n","Average MAE Loss:\n","[0.04685935 0.04685935 0.04675467 0.04732353]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.03705019 0.05666851]\n"," [0.03705019 0.05666851]\n"," [0.0370656  0.0566346 ]\n"," [0.03778452 0.05676004]]\n","\n","Average MAE Loss:\n","[0.04685935 0.04685935 0.0468501  0.04727228]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.03705019 0.05666851]\n"," [0.03705019 0.05666851]\n"," [0.03699164 0.05671591]\n"," [0.03780718 0.05678059]]\n","\n","Average MAE Loss:\n","[0.04685935 0.04685935 0.04685378 0.04729388]\n","\n","\n","epochs finished with time:64.79451942443848\n","\n","[[0.03705019 0.05666851]\n"," [0.03705019 0.05666851]\n"," [0.03699164 0.05671591]\n"," [0.03780718 0.05678059]]\n","00:01:27.1313648510004\n","------------------------------------Fold [5/5]-----------------------------------------\n","VGG\n","VGG\n","VGG\n","VGG\n","Epoch:1,val_loss:\n","Total MAE Loss\n","[[0.10021771 0.10465545]\n"," [0.10038102 0.10476877]\n"," [0.06063706 0.06074467]\n"," [0.06626964 0.06899577]]\n","\n","Average MAE Loss:\n","[0.10243658 0.10257489 0.06069086 0.0676327 ]\n","\n","Epoch:2,val_loss:\n","Total MAE Loss\n","[[0.10021771 0.10465545]\n"," [0.10038102 0.10476877]\n"," [0.05406043 0.05585957]\n"," [0.05924679 0.06134862]]\n","\n","Average MAE Loss:\n","[0.10243658 0.10257489 0.05496    0.0602977 ]\n","\n","Epoch:3,val_loss:\n","Total MAE Loss\n","[[0.10021771 0.10465545]\n"," [0.10038102 0.10476877]\n"," [0.05201062 0.05242775]\n"," [0.0547975  0.05594509]]\n","\n","Average MAE Loss:\n","[0.10243658 0.10257489 0.05221918 0.05537129]\n","\n","Epoch:4,val_loss:\n","Total MAE Loss\n","[[0.10021771 0.10465545]\n"," [0.10038102 0.10476877]\n"," [0.05265925 0.05478211]\n"," [0.05912967 0.06235026]]\n","\n","Average MAE Loss:\n","[0.10243658 0.10257489 0.05372068 0.06073996]\n","\n","Epoch:5,val_loss:\n","Total MAE Loss\n","[[0.10021771 0.10465545]\n"," [0.10038102 0.10476877]\n"," [0.05195867 0.05372752]\n"," [0.05528707 0.0574242 ]]\n","\n","Average MAE Loss:\n","[0.10243658 0.10257489 0.05284309 0.05635564]\n","\n","Epoch:6,val_loss:\n","Total MAE Loss\n","[[0.10021771 0.10465545]\n"," [0.10038102 0.10476877]\n"," [0.05075952 0.05197679]\n"," [0.05437249 0.05609795]]\n","\n","Average MAE Loss:\n","[0.10243658 0.10257489 0.05136815 0.05523522]\n","\n","Epoch:7,val_loss:\n","Total MAE Loss\n","[[0.10021771 0.10465545]\n"," [0.10038102 0.10476877]\n"," [0.05039897 0.05075265]\n"," [0.05512577 0.05758346]]\n","\n","Average MAE Loss:\n","[0.10243658 0.10257489 0.05057581 0.05635462]\n","\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:8,val_loss:\n","Total MAE Loss\n","[[0.10038102 0.10476877]\n"," [0.05512577 0.05758346]\n"," [0.06477047 0.06820179]\n"," [0.05576272 0.05748201]]\n","\n","Average MAE Loss:\n","[0.10257489 0.05635462 0.06648613 0.05662237]\n","\n","Epoch:9,val_loss:\n","Total MAE Loss\n","[[0.10038102 0.10476877]\n"," [0.05512577 0.05758346]\n"," [0.0550148  0.05661641]\n"," [0.05293592 0.05402282]]\n","\n","Average MAE Loss:\n","[0.10257489 0.05635462 0.05581561 0.05347937]\n","\n","Epoch:10,val_loss:\n","Total MAE Loss\n","[[0.10038102 0.10476877]\n"," [0.05512577 0.05758346]\n"," [0.05440061 0.05732305]\n"," [0.05471302 0.05706227]]\n","\n","Average MAE Loss:\n","[0.10257489 0.05635462 0.05586183 0.05588765]\n","\n","Epoch:11,val_loss:\n","Total MAE Loss\n","[[0.10038102 0.10476877]\n"," [0.05512577 0.05758346]\n"," [0.0517067  0.05362688]\n"," [0.05155195 0.05295583]]\n","\n","Average MAE Loss:\n","[0.10257489 0.05635462 0.05266679 0.05225389]\n","\n","Epoch:12,val_loss:\n","Total MAE Loss\n","[[0.10038102 0.10476877]\n"," [0.05512577 0.05758346]\n"," [0.05113715 0.05314548]\n"," [0.05075615 0.05241363]]\n","\n","Average MAE Loss:\n","[0.10257489 0.05635462 0.05214132 0.05158489]\n","\n","Epoch:13,val_loss:\n","Total MAE Loss\n","[[0.10038102 0.10476877]\n"," [0.05512577 0.05758346]\n"," [0.05176434 0.05446067]\n"," [0.04980705 0.05036935]]\n","\n","Average MAE Loss:\n","[0.10257489 0.05635462 0.05311251 0.0500882 ]\n","\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch 00013: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch:14,val_loss:\n","Total MAE Loss\n","[[0.10038102 0.10476877]\n"," [0.05512577 0.05758346]\n"," [0.04924875 0.05067863]\n"," [0.05179748 0.0536063 ]]\n","\n","Average MAE Loss:\n","[0.10257489 0.05635462 0.04996369 0.05270189]\n","\n","Epoch 00014: reducing learning rate of group 0 to 2.5000e-04.\n","Central Aggregation\n","Epoch:15,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.09663969 0.10099319]\n"," [0.04944932 0.05024386]\n"," [0.04913546 0.04928631]]\n","\n","Average MAE Loss:\n","[0.09881644 0.09881644 0.04984659 0.04921089]\n","\n","Epoch:16,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.09663969 0.10099319]\n"," [0.049239   0.05068358]\n"," [0.04855605 0.0497418 ]]\n","\n","Average MAE Loss:\n","[0.09881644 0.09881644 0.04996129 0.04914893]\n","\n","Epoch:17,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.09663969 0.10099319]\n"," [0.04553118 0.04633887]\n"," [0.04769758 0.04813565]]\n","\n","Average MAE Loss:\n","[0.09881644 0.09881644 0.04593503 0.04791662]\n","\n","Epoch:18,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.09663969 0.10099319]\n"," [0.04600241 0.04781773]\n"," [0.04783696 0.04872474]]\n","\n","Average MAE Loss:\n","[0.09881644 0.09881644 0.04691007 0.04828085]\n","\n","Epoch:19,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.09663969 0.10099319]\n"," [0.04594624 0.04751903]\n"," [0.04747575 0.04802687]]\n","\n","Average MAE Loss:\n","[0.09881644 0.09881644 0.04673263 0.04775131]\n","\n","Epoch:20,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.09663969 0.10099319]\n"," [0.04780091 0.05013755]\n"," [0.049316   0.05087494]]\n","\n","Average MAE Loss:\n","[0.09881644 0.09881644 0.04896923 0.05009547]\n","\n","Epoch 00020: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:21,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.09663969 0.10099319]\n"," [0.04507653 0.04658036]\n"," [0.04815297 0.04969136]]\n","\n","Average MAE Loss:\n","[0.09881644 0.09881644 0.04582845 0.04892216]\n","\n","Epoch 00021: reducing learning rate of group 0 to 1.2500e-04.\n","4D-FED-GNN++\n","Epoch:22,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.04815297 0.04969136]\n"," [0.08581021 0.09010597]\n"," [0.04831142 0.04889871]]\n","\n","Average MAE Loss:\n","[0.09881644 0.04892216 0.08795809 0.04860506]\n","\n","Epoch:23,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.04815297 0.04969136]\n"," [0.04931342 0.05042472]\n"," [0.04702998 0.04855749]]\n","\n","Average MAE Loss:\n","[0.09881644 0.04892216 0.04986907 0.04779374]\n","\n","Epoch:24,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.04815297 0.04969136]\n"," [0.04828337 0.05023772]\n"," [0.04907526 0.05157143]]\n","\n","Average MAE Loss:\n","[0.09881644 0.04892216 0.04926055 0.05032334]\n","\n","Epoch:25,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.04815297 0.04969136]\n"," [0.04654275 0.04751753]\n"," [0.04590849 0.04644727]]\n","\n","Average MAE Loss:\n","[0.09881644 0.04892216 0.04703014 0.04617788]\n","\n","Epoch:26,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.04815297 0.04969136]\n"," [0.04624151 0.047332  ]\n"," [0.04661474 0.04744324]]\n","\n","Average MAE Loss:\n","[0.09881644 0.04892216 0.04678676 0.04702899]\n","\n","Epoch:27,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.04815297 0.04969136]\n"," [0.04660428 0.04795503]\n"," [0.04638325 0.04753527]]\n","\n","Average MAE Loss:\n","[0.09881644 0.04892216 0.04727966 0.04695926]\n","\n","Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch 00027: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:28,val_loss:\n","Total MAE Loss\n","[[0.09663969 0.10099319]\n"," [0.04815297 0.04969136]\n"," [0.04651725 0.0478564 ]\n"," [0.0472526  0.04850737]]\n","\n","Average MAE Loss:\n","[0.09881644 0.04892216 0.04718682 0.04787998]\n","\n","Epoch 00028: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:29,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.07120613 0.07530126]\n"," [0.05396613 0.05684713]\n"," [0.0461396  0.0446768 ]]\n","\n","Average MAE Loss:\n","[0.07325369 0.07325369 0.05540663 0.0454082 ]\n","\n","Epoch:30,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.07120613 0.07530126]\n"," [0.04435416 0.04437927]\n"," [0.04795204 0.04898971]]\n","\n","Average MAE Loss:\n","[0.07325369 0.07325369 0.04436671 0.04847088]\n","\n","Epoch:31,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.07120613 0.07530126]\n"," [0.04510422 0.04662367]\n"," [0.04757003 0.04910535]]\n","\n","Average MAE Loss:\n","[0.07325369 0.07325369 0.04586394 0.04833769]\n","\n","Epoch:32,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.07120613 0.07530126]\n"," [0.04446185 0.04559648]\n"," [0.0462687  0.04683201]]\n","\n","Average MAE Loss:\n","[0.07325369 0.07325369 0.04502917 0.04655036]\n","\n","Epoch:33,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.07120613 0.07530126]\n"," [0.04517341 0.04685086]\n"," [0.0472267  0.04874655]]\n","\n","Average MAE Loss:\n","[0.07325369 0.07325369 0.04601214 0.04798662]\n","\n","Epoch:34,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.07120613 0.07530126]\n"," [0.04485821 0.04651548]\n"," [0.04563634 0.04636869]]\n","\n","Average MAE Loss:\n","[0.07325369 0.07325369 0.04568684 0.04600252]\n","\n","Epoch 00034: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:35,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.07120613 0.07530126]\n"," [0.04442695 0.04572558]\n"," [0.04716236 0.0481075 ]]\n","\n","Average MAE Loss:\n","[0.07325369 0.07325369 0.04507627 0.04763493]\n","\n","Epoch 00035: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch 00035: reducing learning rate of group 0 to 5.0000e-04.\n","4D-FED-GNN++\n","Epoch:36,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.04716236 0.0481075 ]\n"," [0.05664989 0.05992764]\n"," [0.0464575  0.04813051]]\n","\n","Average MAE Loss:\n","[0.07325369 0.04763493 0.05828877 0.04729401]\n","\n","Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:37,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.04716236 0.0481075 ]\n"," [0.04758473 0.04929623]\n"," [0.04609605 0.0472965 ]]\n","\n","Average MAE Loss:\n","[0.07325369 0.04763493 0.04844048 0.04669628]\n","\n","Epoch:38,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.04716236 0.0481075 ]\n"," [0.04490021 0.04580879]\n"," [0.04600627 0.04716948]]\n","\n","Average MAE Loss:\n","[0.07325369 0.04763493 0.0453545  0.04658788]\n","\n","Epoch:39,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.04716236 0.0481075 ]\n"," [0.04453314 0.0455118 ]\n"," [0.04625731 0.04743907]]\n","\n","Average MAE Loss:\n","[0.07325369 0.04763493 0.04502247 0.04684819]\n","\n","Epoch:40,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.04716236 0.0481075 ]\n"," [0.04465255 0.04602485]\n"," [0.04590602 0.04706734]]\n","\n","Average MAE Loss:\n","[0.07325369 0.04763493 0.0453387  0.04648668]\n","\n","Epoch:41,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.04716236 0.0481075 ]\n"," [0.04520023 0.04703457]\n"," [0.04709488 0.04893902]]\n","\n","Average MAE Loss:\n","[0.07325369 0.04763493 0.0461174  0.04801695]\n","\n","Epoch 00041: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00041: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch:42,val_loss:\n","Total MAE Loss\n","[[0.07120613 0.07530126]\n"," [0.04716236 0.0481075 ]\n"," [0.04479674 0.04646101]\n"," [0.04596333 0.04736394]]\n","\n","Average MAE Loss:\n","[0.07325369 0.04763493 0.04562887 0.04666364]\n","\n","Epoch 00042: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch 00042: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:43,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.05223991 0.05498064]\n"," [0.04988427 0.05229111]\n"," [0.04593627 0.04728009]]\n","\n","Average MAE Loss:\n","[0.05361027 0.05361027 0.05108769 0.04660818]\n","\n","Epoch:44,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.05223991 0.05498064]\n"," [0.04698662 0.04882961]\n"," [0.04500649 0.04551303]]\n","\n","Average MAE Loss:\n","[0.05361027 0.05361027 0.04790811 0.04525976]\n","\n","Epoch:45,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.05223991 0.05498064]\n"," [0.04537717 0.04677386]\n"," [0.0463729  0.04781293]]\n","\n","Average MAE Loss:\n","[0.05361027 0.05361027 0.04607552 0.04709291]\n","\n","Epoch:46,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.05223991 0.05498064]\n"," [0.04487821 0.04614795]\n"," [0.04684429 0.04843408]]\n","\n","Average MAE Loss:\n","[0.05361027 0.05361027 0.04551308 0.04763919]\n","\n","Epoch:47,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.05223991 0.05498064]\n"," [0.04480032 0.04606946]\n"," [0.04571482 0.04646143]]\n","\n","Average MAE Loss:\n","[0.05361027 0.05361027 0.04543489 0.04608813]\n","\n","Epoch:48,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.05223991 0.05498064]\n"," [0.04479592 0.04612423]\n"," [0.04587509 0.04655732]]\n","\n","Average MAE Loss:\n","[0.05361027 0.05361027 0.04546007 0.04621621]\n","\n","Epoch 00048: reducing learning rate of group 0 to 7.8125e-06.\n","Epoch 00048: reducing learning rate of group 0 to 3.1250e-05.\n","Epoch:49,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.05223991 0.05498064]\n"," [0.04483649 0.04619585]\n"," [0.04591968 0.04679669]]\n","\n","Average MAE Loss:\n","[0.05361027 0.05361027 0.04551617 0.04635818]\n","\n","Epoch 00049: reducing learning rate of group 0 to 7.8125e-06.\n","4D-FED-GNN++\n","Epoch:50,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.04591968 0.04679669]\n"," [0.05102132 0.05356538]\n"," [0.04498101 0.04634444]]\n","\n","Average MAE Loss:\n","[0.05361027 0.04635818 0.05229335 0.04566273]\n","\n","Epoch 00050: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch:51,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.04591968 0.04679669]\n"," [0.04939634 0.05170141]\n"," [0.04524182 0.04651877]]\n","\n","Average MAE Loss:\n","[0.05361027 0.04635818 0.05054888 0.04588029]\n","\n","Epoch:52,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.04591968 0.04679669]\n"," [0.04793742 0.04998788]\n"," [0.04533901 0.04649229]]\n","\n","Average MAE Loss:\n","[0.05361027 0.04635818 0.04896265 0.04591565]\n","\n","Epoch:53,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.04591968 0.04679669]\n"," [0.04687784 0.04869655]\n"," [0.04550571 0.04654068]]\n","\n","Average MAE Loss:\n","[0.05361027 0.04635818 0.0477872  0.0460232 ]\n","\n","Epoch:54,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.04591968 0.04679669]\n"," [0.04606431 0.04764828]\n"," [0.04568352 0.04673403]]\n","\n","Average MAE Loss:\n","[0.05361027 0.04635818 0.0468563  0.04620877]\n","\n","Epoch 00054: reducing learning rate of group 0 to 1.5625e-05.\n","Epoch:55,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.04591968 0.04679669]\n"," [0.04578285 0.04729041]\n"," [0.04599375 0.04720561]]\n","\n","Average MAE Loss:\n","[0.05361027 0.04635818 0.04653663 0.04659968]\n","\n","Epoch 00055: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch:56,val_loss:\n","Total MAE Loss\n","[[0.05223991 0.05498064]\n"," [0.04591968 0.04679669]\n"," [0.04563799 0.04710595]\n"," [0.04559084 0.04650604]]\n","\n","Average MAE Loss:\n","[0.05361027 0.04635818 0.04637197 0.04604844]\n","\n","Epoch 00056: reducing learning rate of group 0 to 3.9063e-06.\n","Epoch 00056: reducing learning rate of group 0 to 6.2500e-05.\n","Central Aggregation\n","Epoch:57,val_loss:\n","Total MAE Loss\n","[[0.0468884  0.04859779]\n"," [0.0468884  0.04859779]\n"," [0.04665927 0.04831878]\n"," [0.04620486 0.04762273]]\n","\n","Average MAE Loss:\n","[0.04774309 0.04774309 0.04748902 0.04691379]\n","\n","Epoch:58,val_loss:\n","Total MAE Loss\n","[[0.0468884  0.04859779]\n"," [0.0468884  0.04859779]\n"," [0.04638217 0.04797875]\n"," [0.04571016 0.046855  ]]\n","\n","Average MAE Loss:\n","[0.04774309 0.04774309 0.04718046 0.04628258]\n","\n","Epoch:59,val_loss:\n","Total MAE Loss\n","[[0.0468884  0.04859779]\n"," [0.0468884  0.04859779]\n"," [0.04613403 0.047681  ]\n"," [0.04562461 0.04663143]]\n","\n","Average MAE Loss:\n","[0.04774309 0.04774309 0.04690751 0.04612802]\n","\n","Epoch:60,val_loss:\n","Total MAE Loss\n","[[0.0468884  0.04859779]\n"," [0.0468884  0.04859779]\n"," [0.0458926  0.04739502]\n"," [0.04570163 0.04674552]]\n","\n","Average MAE Loss:\n","[0.04774309 0.04774309 0.04664381 0.04622357]\n","\n","Epoch 00060: reducing learning rate of group 0 to 7.8125e-06.\n","\n","epochs finished with time:64.5043716430664\n","\n","[[0.0468884  0.04859779]\n"," [0.0468884  0.04859779]\n"," [0.0458926  0.04739502]\n"," [0.04570163 0.04674552]]\n","00:01:26.81963008799994\n"]}],"source":["# seed = 10 table = 2/8 fixed folds VGG19\n","args.mode='4D-FED-GNN++'\n","args.save_path = '/content/drive/MyDrive/Dissertation/4D-FedGNN-Plus_mine/Real/manual_seed_10/2_8/global_test/GNNs/'\n","args.save_name='VGG19_4D-FED-GNN++'\n","train_cnns(args, dataset,seed=10,ratio=2/8,verbose=False,train_validate_verbose=True,train_validate_verbosity_epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"UBgANcioID-d"},"source":[]},{"cell_type":"markdown","metadata":{"id":"2kUqdyWRj5qF"},"source":["# Check\n"]},{"cell_type":"markdown","metadata":{"id":"tHprtfbUku3t"},"source":["## seed 10 4_8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1690367227203,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"jkFW1y3WfvvW","outputId":"3cd09fba-d396-4b1e-e225-a5b67d38404f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04698905 0.0494738 ]\n","Hospital 1:\n","[0.04699336 0.04948168]\n","Hospital 2:\n","[0.04695561 0.04947038]\n","Hospital 3:\n","[0.04703576 0.04953545]\n","\n","Total sum:[0.04699344 0.04949033]\n"]}],"source":["# dropout 0.2->0.3->0.4\n","check(name='CNN2_dropout_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690368755524,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"2bQnmREf1BTp","outputId":"a18e7d8d-7fbe-49c2-8944-aff329394239"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04868297 0.05111571]\n","Hospital 1:\n","[0.0486493  0.05113705]\n","Hospital 2:\n","[0.04863589 0.0511804 ]\n","Hospital 3:\n","[0.04865658 0.05114117]\n","\n","Total sum:[0.04865619 0.05114359]\n"]}],"source":["# CNN2\n","check(name='CNN2_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1690038105485,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"yZy0ZyYHHqpv","outputId":"58ab68b4-f3b3-4313-cacc-37ca694a963b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.0450016  0.04764346]\n","Hospital 1:\n","[0.04515179 0.04779888]\n","Hospital 2:\n","[0.04527794 0.04818096]\n","Hospital 3:\n","[0.04554866 0.04799936]\n","\n","Total sum:[0.045245   0.04790566]\n"]}],"source":["#VGG19\n","check(name='VGG19_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":282,"status":"ok","timestamp":1690024865704,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"JdIBvQ0yj7Wu","outputId":"2b81f648-596c-40ef-d5ec-6ba97ec92db9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04830841 0.05091142]\n","Hospital 1:\n","[0.04834165 0.05092986]\n","Hospital 2:\n","[0.04836772 0.05100498]\n","Hospital 3:\n","[0.04834954 0.05092387]\n","\n","Total sum:[0.04834183 0.05094253]\n"]}],"source":["check(name='Simple2_CNN_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdnHuLVCxRaA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5DL6MoT5xRkX"},"source":["## seed 10 2_8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381,"status":"ok","timestamp":1690367784053,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"bETBuXK_xRkY","outputId":"aa4e6ff4-e157-4670-b911-a9f20141a9fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04888912 0.05139125]\n","Hospital 1:\n","[0.04888912 0.05139125]\n","Hospital 2:\n","[0.04854155 0.05121583]\n","Hospital 3:\n","[0.04854986 0.05125947]\n","\n","Total sum:[0.04871741 0.05131445]\n"]}],"source":["# dropout 0.2->0.3->0.4\n","check(name='CNN2_dropout_4D-FED-GNN++',seed=10,ratio='2_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690368770721,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"aquz4ziE1Gsb","outputId":"59f0fdd1-f651-4ad3-9bd3-0c722ae00906"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04960811 0.05220952]\n","Hospital 1:\n","[0.04960811 0.05220952]\n","Hospital 2:\n","[0.04949756 0.05219058]\n","Hospital 3:\n","[0.0494956  0.05217005]\n","\n","Total sum:[0.04955234 0.05219492]\n"]}],"source":["# CNN2\n","check(name='CNN2_4D-FED-GNN++',seed=10,ratio='2_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1690038105485,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"z-HEX360xRkY","outputId":"58ab68b4-f3b3-4313-cacc-37ca694a963b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.0450016  0.04764346]\n","Hospital 1:\n","[0.04515179 0.04779888]\n","Hospital 2:\n","[0.04527794 0.04818096]\n","Hospital 3:\n","[0.04554866 0.04799936]\n","\n","Total sum:[0.045245   0.04790566]\n"]}],"source":["#VGG16\n","check(name='VGG19_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1690024230687,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"r2ehT-rBxRkY","outputId":"23431a53-d4fb-463c-a5a0-9be7cd697307"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04864759 0.05125555]\n","Hospital 1:\n","[0.04873149 0.0512792 ]\n","Hospital 2:\n","[0.04871789 0.05137924]\n","Hospital 3:\n","[0.04876227 0.05130414]\n","\n","Total sum:[0.04871481 0.05130454]\n"]}],"source":["\n","check(name='New_Custom_Simple2_CNN_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1690022767434,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"AUWTgyT4xRkZ","outputId":"d383a4cf-7483-4a1e-d78d-ce56ed2d76f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04969393 0.05235872]\n","Hospital 1:\n","[0.0496548  0.05241038]\n","Hospital 2:\n","[0.04975457 0.05259932]\n","Hospital 3:\n","[0.04968568 0.05241673]\n","\n","Total sum:[0.04969725 0.05244629]\n"]}],"source":["check(name='New_Custom_Simple2_CNN_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":282,"status":"ok","timestamp":1690024865704,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"Xo6tRmlpxRkZ","outputId":"2b81f648-596c-40ef-d5ec-6ba97ec92db9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.04830841 0.05091142]\n","Hospital 1:\n","[0.04834165 0.05092986]\n","Hospital 2:\n","[0.04836772 0.05100498]\n","Hospital 3:\n","[0.04834954 0.05092387]\n","\n","Total sum:[0.04834183 0.05094253]\n"]}],"source":["check(name='Simple2_CNN_4D-FED-GNN++',seed=10,ratio='4_8')"]},{"cell_type":"markdown","metadata":{"id":"aFtt199MlMgF"},"source":["## seed 500 4_8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1689945665368,"user":{"displayName":"Павел Бозмаров","userId":"13372247782600851572"},"user_tz":-60},"id":"DtHLggFplPrl","outputId":"920e5464-03e9-42eb-cb5b-21084f6fb96b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hospital 0:\n","[0.03884535 0.04585896]\n","Hospital 1:\n","[0.03884614 0.04587628]\n","Hospital 2:\n","[0.03899311 0.04584705]\n","Hospital 3:\n","[0.03874649 0.04584995]\n","\n","Total sum:[0.03885777 0.04585806]\n"]}],"source":["check(name='Simple2_CNN_4D-FED-GNN++',seed=500,ratio='4_8')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["SuyWYNaidZie","90ooXFhVd5Ci","wq-tRUzrK_Zw","VyNXSEppLzXg","v7zuUNtIu26b","aHtyfjWEQe11","boAUzmyY1b45","gCeqTVgMstrm","VoaPdwAI_3OO","cBsklrSsPUtm","ke3Z16hUbTSE","E77zvOJPlTul","tY7gZyeQaHdO","BDKGEEKpqE9-","8KHFUzhADA6_","2kUqdyWRj5qF","tHprtfbUku3t","5DL6MoT5xRkX"],"provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"vscode":{"interpreter":{"hash":"0f5c6f72bc62c81ae52e96e9a3a4236b77333ef45d4cdc0c3574ebd317f415f5"}}},"nbformat":4,"nbformat_minor":0}